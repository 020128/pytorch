.. automodule:: torch.distributed.optim
.. currentmodule:: torch.distributed.fsdp

FullyShardedDataParallel
========================

.. autoclass:: torch.distributed.fsdp.FullyShardedDataParallel
  :members:
