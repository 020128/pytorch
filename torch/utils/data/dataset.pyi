# This base template ("dataset.pyi.in") is generated from mypy stubgen with minimal editing for code injection
# The output file will be "dataset.pyi".
# Note that, for mypy, .pyi file takes precedent over .py file, such that we must define the interface for other
# classes/objects here, even though we are not injecting extra code into them at the moment.

from ... import Generator as Generator, Tensor as Tensor
from torch import default_generator as default_generator, randperm as randperm
from torch.utils.data._typing import _DataPipeMeta
from typing import Any, Callable, Dict, Generic, Iterable, Iterator, List, Optional, Sequence, Tuple, TypeVar, Union
try:
    from torchdata.datapipes.iter.util.extractor import CompressionType
except ImportError:
    pass


T_co = TypeVar('T_co', covariant=True)
T = TypeVar('T')
UNTRACABLE_DATAFRAME_PIPES: Any


class DataChunk(list, Generic[T]):
    items: Any = ...
    def __init__(self, items: Any) -> None: ...
    def as_str(self, indent: str = ...): ...
    def __iter__(self) -> Iterator[T]: ...
    def raw_iterator(self) -> T: ...

class Dataset(Generic[T_co]):
    functions: Dict[str, Callable] = ...
    def __getitem__(self, index: Any) -> T_co: ...
    def __add__(self, other: Dataset[T_co]) -> ConcatDataset[T_co]: ...
    def __getattr__(self, attribute_name: Any): ...
    @classmethod
    def register_function(cls, function_name: Any, function: Any) -> None: ...
    @classmethod
    def register_datapipe_as_function(cls, function_name: Any, cls_to_register: Any, enable_df_api_tracing: bool = ...): ...

class MapDataPipe(Generic[T_co]):
    functions: Dict[str, Callable] = ...
    def __getitem__(self, index: Any) -> T_co: ...
    def __add__(self, other: Dataset[T_co]) -> ConcatDataset[T_co]: ...
    def __getattr__(self, attribute_name: Any): ...
    @classmethod
    def register_function(cls, function_name: Any, function: Any) -> None: ...
    @classmethod
    def register_datapipe_as_function(cls, function_name: Any, cls_to_register: Any, enable_df_api_tracing: bool = ...): ...
    # Functional form of 'BatcherMapDataPipe'
    def batch(self, batch_size: int, drop_last: bool = False, wrapper_class=DataChunk) -> MapDataPipe: ...
    # Functional form of 'ConcaterMapDataPipe'
    def concat(self, *datapipes: MapDataPipe) -> MapDataPipe: ...
    # Functional form of 'MapperMapDataPipe'
    def map(self, fn: Callable= ..., fn_args: Optional[Tuple] = None, fn_kwargs: Optional[Dict] = None) -> MapDataPipe: ...
    # Functional form of 'ShufflerMapDataPipe'
    def shuffle(self, *, indices: Optional[List] = None) -> MapDataPipe: ...
    # Functional form of 'ZipperMapDataPipe'
    def zip(self, *datapipes: MapDataPipe[T_co]) -> MapDataPipe: ...

class IterableDataset(Dataset[T_co], metaclass=_DataPipeMeta):
    functions: Dict[str, Callable] = ...
    reduce_ex_hook: Optional[Callable] = ...
    getstate_hook: Optional[Callable] = ...
    def __iter__(self) -> Iterator[T_co]: ...
    def __add__(self, other: Dataset[T_co]) -> Any: ...
    def __getattr__(self, attribute_name: Any): ...
    def __reduce_ex__(self, *args: Any, **kwargs: Any): ...
    @classmethod
    def set_reduce_ex_hook(cls, hook_fn: Any) -> None: ...
    @classmethod
    def set_getstate_hook(cls, hook_fn: Any) -> None: ...
    # Functional form of 'BatcherIterDataPipe'
    def batch(self, batch_size: int, drop_last: bool = False, wrapper_class=DataChunk) -> IterDataPipe: ...
    # Functional form of 'CollatorIterDataPipe'
    def collate(self, collate_fn: Callable= ..., fn_args: Optional[Tuple] = None, fn_kwargs: Optional[Dict] = None) -> IterDataPipe: ...
    # Functional form of 'ConcaterIterDataPipe'
    def concat(self, *datapipes: IterDataPipe) -> IterDataPipe: ...
    # Functional form of 'RoutedDecoderIterDataPipe'
    def decode(self, *handlers: Callable, key_fn: Callable= ...) -> IterDataPipe: ...
    # Functional form of 'DemultiplexerIterDataPipe'
    def demux(self, num_instances: int, classifier_fn: Callable[[T_co], Optional[int]], drop_none: bool = False, buffer_size: int = 1000) -> List[IterDataPipe]: ...
    # Functional form of 'FilterIterDataPipe'
    def filter(self, filter_fn: Callable, fn_args: Optional[Tuple] = None, fn_kwargs: Optional[Dict] = None, drop_empty_batches: bool = True) -> IterDataPipe: ...
    # Functional form of 'ForkerIterDataPipe'
    def fork(self, num_instances: int, buffer_size: int = 1000) -> List[IterDataPipe]: ...
    # Functional form of 'GrouperIterDataPipe'
    def groupby(self, group_key_fn: Callable, *, buffer_size: int = 10000, group_size: Optional[int] = None, unbatch_level: int = 0, guaranteed_group_size: Optional[int] = None, drop_remaining: bool = False) -> IterDataPipe: ...
    # Functional form of 'MapperIterDataPipe'
    def map(self, fn: Callable, input_col=None, output_col=None, *, fn_args: Optional[Tuple] = None, fn_kwargs: Optional[Dict] = None) -> IterDataPipe: ...
    # Functional form of 'MultiplexerIterDataPipe'
    def mux(self, *datapipes) -> IterDataPipe: ...
    # Functional form of 'ShardingFilterIterDataPipe'
    def sharding_filter(self) -> IterDataPipe: ...
    # Functional form of 'ShufflerIterDataPipe'
    def shuffle(self, *, default: bool = True, buffer_size: int = 10000, unbatch_level: int = 0) -> IterDataPipe: ...
    # Functional form of 'UnBatcherIterDataPipe'
    def unbatch(self, unbatch_level: int = 1) -> IterDataPipe: ...
    # Functional form of 'ZipperIterDataPipe'
    def zip(self, *datapipes: IterDataPipe) -> IterDataPipe: ...
    # Functional form of 'IndexAdderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def add_index(self, index_name: str = "index") -> IterDataPipe: ...
    # Functional form of 'BucketBatcherIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def bucketbatch(self, batch_size: int, drop_last: bool = False, batch_num: int = 100, bucket_num: int = 1, sort_key: Optional[Callable] = None, in_batch_shuffle: bool = True) -> IterDataPipe: ...
    # Functional form of 'HashCheckerIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def check_hash(self, hash_dict: Dict[str, str], hash_type: str = "sha256", rewind: bool = True) -> IterDataPipe: ...
    # Functional form of 'CyclerIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def cycle(self, count: Optional[int] = None) -> IterDataPipe: ...
    # Functional form of 'EndOnDiskCacheHolderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def end_caching(self, mode="w", filepath_fn=None, *, same_filepath_fn=False, skip_read=False) -> IterDataPipe: ...
    # Functional form of 'EnumeratorIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def enumerate(self, starting_index: int = 0) -> IterDataPipe: ...
    # Functional form of 'ExtractorIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def extract(self, file_type: Optional[Union[str, CompressionType]] = None) -> IterDataPipe: ...
    # Functional form of 'HeaderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def header(self, limit: int = 10) -> IterDataPipe: ...
    # Functional form of 'InMemoryCacheHolderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def in_memory_cache(self, size: Optional[int] = None) -> IterDataPipe: ...
    # Functional form of 'ParagraphAggregatorIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def lines_to_paragraphs(self, joiner: Callable= ...) -> IterDataPipe: ...
    # Functional form of 'IoPathFileLoaderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def load_file_by_iopath(self, mode: str = "r", pathmgr=None) -> IterDataPipe: ...
    # Functional form of 'RarArchiveLoaderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def load_from_rar(self, *, length: int = -1) -> IterDataPipe: ...
    # Functional form of 'OnDiskCacheHolderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def on_disk_cache(self, filepath_fn: Optional[Callable] = None, hash_dict: Dict[str, str] = None, hash_type: str = "sha256", extra_check_fn: Optional[Callable[[str], bool]] = None) -> IterDataPipe: ...
    # Functional form of 'FSSpecFileOpenerIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def open_file_by_fsspec(self, mode: str = "r") -> IterDataPipe: ...
    # Functional form of 'CSVParserIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def parse_csv(self, *, skip_lines: int = 0, decode: bool = True, encoding: str = "utf-8", errors: str = "ignore", return_path: bool = False, **fmtparams) -> IterDataPipe: ...
    # Functional form of 'CSVDictParserIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def parse_csv_as_dict(self, *, skip_lines: int = 0, decode: bool = True, encoding: str = "utf-8", errors: str = "ignore", return_path: bool = False, **fmtparams) -> IterDataPipe: ...
    # Functional form of 'JsonParserIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def parse_json_files(self, **kwargs) -> IterDataPipe: ...
    # Functional form of 'TarArchiveReaderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def read_from_tar(self, mode: str = "r:*", length: int = -1) -> IterDataPipe: ...
    # Functional form of 'XzFileReaderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def read_from_xz(self, length: int = -1) -> IterDataPipe: ...
    # Functional form of 'ZipArchiveReaderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def read_from_zip(self, length: int = -1) -> IterDataPipe: ...
    # Functional form of 'LineReaderIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def readlines(self, *, skip_lines: int = 0, strip_newline: bool = True, decode: bool = False, encoding="utf-8", errors: str = "ignore", return_path: bool = True) -> IterDataPipe: ...
    # Functional form of 'Rows2ColumnarIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def rows2columnar(self, column_names: List[str] = None) -> IterDataPipe: ...
    # Functional form of 'FSSpecSaverIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def save_by_fsspec(self, mode: str = "w", filepath_fn: Optional[Callable] = None) -> IterDataPipe: ...
    # Functional form of 'IoPathSaverIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def save_by_iopath(self, mode: str = "w", filepath_fn: Optional[Callable] = None, *, pathmgr=None) -> IterDataPipe: ...
    # Functional form of 'SaverIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def save_to_disk(self, mode: str = "w", filepath_fn: Optional[Callable] = None) -> IterDataPipe: ...
    # Functional form of 'IterKeyZipperIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def zip_with_iter(self, ref_datapipe: IterDataPipe, key_fn: Callable, ref_key_fn: Optional[Callable] = None, keep_key: bool = False, buffer_size: int = 10000, merge_fn: Optional[Callable] = None) -> IterDataPipe: ...
    # Functional form of 'MapKeyZipperIterDataPipe' - This DataPipe is only available through the 'torchdata' library.
    def zip_with_map(self, map_datapipe: MapDataPipe, key_fn: Callable, merge_fn: Optional[Callable] = None) -> IterDataPipe: ...

IterDataPipe = IterableDataset

class DFIterDataPipe(IterableDataset): ...

class TensorDataset(Dataset[Tuple[Tensor, ...]]):
    tensors: Tuple[Tensor, ...]
    def __init__(self, *tensors: Tensor) -> None: ...
    def __getitem__(self, index: Any): ...
    def __len__(self): ...

class ConcatDataset(Dataset[T_co]):
    datasets: List[Dataset[T_co]]
    cumulative_sizes: List[int]
    @staticmethod
    def cumsum(sequence: Any): ...
    def __init__(self, datasets: Iterable[Dataset]) -> None: ...
    def __len__(self): ...
    def __getitem__(self, idx: Any): ...
    @property
    def cummulative_sizes(self): ...

class ChainDataset(IterableDataset):
    datasets: Any = ...
    def __init__(self, datasets: Iterable[Dataset]) -> None: ...
    def __iter__(self) -> Any: ...
    def __len__(self): ...

class Subset(Dataset[T_co]):
    dataset: Dataset[T_co]
    indices: Sequence[int]
    def __init__(self, dataset: Dataset[T_co], indices: Sequence[int]) -> None: ...
    def __getitem__(self, idx: Any): ...
    def __len__(self): ...

def random_split(dataset: Dataset[T], lengths: Sequence[int], generator: Optional[Generator]=...) -> List[Subset[T]]: ...
