{"bc1b1e8253": {"title": "fixing mkldnn_linear & backward with silent error (#51713)", "body": "Summary:\nmkldnn_linear & mkldnn_linear_backward_input gives wrong result when weight is non contiguous.\n\nIssue exposed in PR https://github.com/pytorch/pytorch/issues/51613\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51713\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26282319\n\nPulled By: ngimel\n\nfbshipit-source-id: 96516e10c9dc72c30dac278fce09b746aa5f51b2", "pr_number": "51713", "files_changed": ["aten/src/ATen/native/mkldnn/Linear.cpp", "test/test_mkldnn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c034e0750c": {"title": "[QNNPACK, Sparsity] Code refactoring to allow for more generic block (#51118)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51118\n\nsparsity\n\nModify BCSR to pack generic block sparsity pattern.\nModify rest of the code to accommodate the change.\nThis is in preperation to support 8x1 sparsity.\n\nTest Plan:\nq8gemm-sparse-test\n\nImported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D26077767\n\nfbshipit-source-id: 7179975b07a1cb76ef26896701d782fb04638743", "pr_number": "51118", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/bench/q8gemm_sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/include/pack_block_sparse.h", "aten/src/ATen/native/quantized/cpu/qnnpack/include/pytorch_qnnpack.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/fully-connected-sparse.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/init.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/operator-run.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/pack_block_sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/operator.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/params.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-sparse-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/gemm-block-sparse-microkernel-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/q8gemm_sparse.cc"], "labels": ["Merged", "cla signed"]}, "6b2811f288": {"title": "[QNNPACK, Sparsity] Add 8x1 block sparse kernels for aarch32. (#51119)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51119\n\nAdds asm kernel for 8x1 block sparse kernel. Since ukernels is still\nproducing 4x8 blocks, similar to 1x4 sparsity pattern, we can use the\nsame prepacking kernel for activation. It does get a tiny bit hacky but\nallows us to reuse the kernel.\n\nTest Plan:\nq8gemm-sparse-test\nfully-connectest-sparse-test\n\nImported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D26077765\n\nfbshipit-source-id: cc087b0ff717a613906d442ea73680e785e0ecc2", "pr_number": "51119", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/bench/q8gemm_sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/fully-connected-sparse.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/init.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/operator-run.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/pack_block_sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/4x4-packA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/4x8c8x1-dq-packedA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/params.h", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/q8gemm_sparse.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-sparse-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/test/q8gemm_sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm_sparse/4x8c8x1-dq-packedA-aarch32-neon.S"], "labels": ["Merged", "cla signed"]}, "19753af6ea": {"title": "[QNNPACK Sparsity] Add aarch64 kernel of 8x1 sparsity (#51120)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51120\n\nAdds asm kernels for aarch64 using 8x1 sparsity. Also remove aarch32 8x4 prepacked kernels and 8x4 inline packed sse2 kernels.\n\nTest Plan:\nq8gemm-sparse-test\n\nImported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D26077766\n\nfbshipit-source-id: 29793d30a47b8f4084daf8950d925dc804d3dc59", "pr_number": "51120", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt", "aten/src/ATen/native/quantized/cpu/qnnpack/bench/q8gemm_sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/src/fully-connected-sparse.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/init.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/operator-run.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x4c1x4-dq-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x4c1x4-dq-packedA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x4c1x4-dq-sse2.c", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x8c8x1-dq-packedA-aarch64-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/qnnpack/q8gemm_sparse.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/fully-connected-sparse-operator-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/gemm-block-sparse-microkernel-tester.h", "aten/src/ATen/native/quantized/cpu/qnnpack/test/q8gemm_sparse.cc", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm_sparse/8x4c1x4-dq-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm_sparse/8x4c1x4-dq-packedA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm_sparse/8x4c1x4-dq-sse2.c", "aten/src/ATen/native/quantized/cpu/qnnpack/wrappers/q8gemm_sparse/8x8c8x1-dq-packedA-aarch64-neon.S"], "labels": ["Merged", "cla signed"]}, "215d9daceb": {"title": "Refactor internal methods into debugging utilities (#51737)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51737\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26288613\n\nPulled By: ansley\n\nfbshipit-source-id: 4504b1af5be7a200c1a6a376d432d7224eb8a796", "pr_number": "51737", "files_changed": ["test/test_fx.py", "torch/fx/experimental/graph_manipulation.py", "torch/fx/graph.py", "torch/fx/node.py"], "labels": ["Merged", "cla signed", "fx"]}, "9a964ce89b": {"title": "Enables backend preprocessing to take place outside of the backend interface (#51757)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51757\n\nEnables backend preprocessing to take place outside of the backend interface.\n\nWhat's new:\n* A new definition for backend preprocessing (i.e. BackendPreprocessFunction).\n* Registration of the backend's PyTorchBackendInterface interface implementation is augmented to take the BackendPreprocessFunction.\n* A new registry is created to handle the BackendPreprocessFunction functions, using the backend's name as key.\n* When a BackendPreprocessFunction is used, the PyTorchBackendInterface's \"preprocess\" method is not added to the LoweredModule. Instead, the BackendPreprocessFunction is called and its output used to set the LoweredModule's __processed_module.\n\nWhy?:\nThese changes are needed to avoid forcing backend preprocessing to be part of the LoweredModule, and in the future be able to eliminate \"preprocess\" from the PyTorchBackendInterface.\nThis is important for Mobile use cases where \"preprocess\" can take the bulk of the compilation process, and thus contain code dependencies that we do not want to bring (or cannot bring) to the Mobile binary.\n\nWhat didn't change:\n* Everything is backwards compatible:\n** The existing \"preprocess\" method in PyTorchBackendInterface is still there.\n** When backend registration is done without the BackendPreprocessFunction, as before, things work the same way: \"preprocess\" is added to LoweredModule, and invoked through the module's instance of the backend interface.\n\nLonger term, the plan is to refactor existing users to move to the new backend registration.\nghstack-source-id: 121190883\n\nTest Plan:\nUpdated existing tests (test_backend.py) to use the new registration mechanism.\nVerified test ran and passed (in my OSS build).\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26261042\n\nfbshipit-source-id: 0dc378acd5f2ab60fcdc01f7373616d1db961e61", "pr_number": "51757", "files_changed": ["test/cpp/jit/test_backend.cpp", "test/custom_backend/custom_backend.cpp", "test/custom_backend/custom_backend.h", "torch/csrc/jit/backends/backend.h", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_init.cpp", "torch/csrc/jit/backends/backend_interface.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "8a9090219e": {"title": "[pyper] register aten::index_out (#51742)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51742\n\nRegister aten::index_out with StaticRuntime\n\nTest Plan:\n```\nMKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --c2_weights=/data/users/ansha/tmp/adfinder/models/c2_local_ro_weight_data.pb --c2_inputs=/data/users/ansha/tmp/adfinder/models/c2_local_ro_input_data.pb --pred_net=/data/users/ansha/tmp/adfinder/models/c2_local_ro_net.pb --c2_sigrid_transforms_opt=1 --c2_apply_nomnigraph_passes=1 --c2_use_memonger=1 --scripted_model=/data/users/ansha/tmp/adfinder/models2/210494966_0.predictor.disagg.local_ro.pt --pt_inputs=/data/users/ansha/tmp/adfinder/models/local_wrapped_input_data.pt --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=true --compare_results=0 --iters=30000 --warmup_iters=10000 --num_threads=1 --do_profile=1 --benchmark_c2_predictor=0\n```\n\nBefore total ms/iter: 0.699626\nBefore: 0.0277974 ms.    4.03198%. aten::index (5 nodes)\n\nAfter total ms/iter: 0.696739\nAfter: 0.0254255 ms.    3.67315%. aten::index (5 nodes)\n\nReviewed By: hlu1\n\nDifferential Revision: D26261215\n\nfbshipit-source-id: b59ebd5ccd33478a9fbc4629a0075fec597a05cb", "pr_number": "51742", "files_changed": ["aten/src/ATen/native/IndexingUtils.h", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "fa70168804": {"title": "Add metacompile of Ternary if (#51789)", "body": "Summary:\nFixes issue: https://github.com/pytorch/pytorch/issues/49728\n========\nTernary if operation fails in Torchscript when the condition variable is annotated as Final.\n\nTests:\n=======\npytest -k test_ternary_static_if test/test_jit.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51789\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D26278969\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 27d1383290211503188428fb2e8b7749f59ba16e", "pr_number": "51789", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6488b2bc3a": {"title": "Revert D26282829: [pytorch][PR] Adding support for CUDA 11.2 in our nightly build matrix", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26282829 (https://github.com/pytorch/pytorch/commit/fb07aca7b05edfb9d41bca950425e70d0b29bcbf)\n\nOriginal commit changeset: b15380e5c44a\n\nfbshipit-source-id: 18f86e766ed9ec58da32167584bb5e4e2c87a639", "pr_number": null, "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/config.yml", ".circleci/scripts/binary_linux_test.sh", ".circleci/scripts/setup_ci_environment.sh"], "labels": []}, "4968227058": {"title": "add shape inference for Int8GenQuantParamsMinMax", "body": "Summary: As titleed\n\nTest Plan: successful test flow with A* setup: f245569242\n\nReviewed By: anurag16\n\nDifferential Revision: D25966283\n\nfbshipit-source-id: ef9945d5039933df44c2c3c26ca149f47538ff31", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": []}, "0c313564af": {"title": "Backward through sparse_coo_tensor (#50361)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/49683\n\nThis PR  solves Backward through sparse_coo_tensor bug by implementing a `sparse_mask_helper` function for n-dimensional sparse tensor for CPU and CUDA which is used to reimplement `sparse_constructor_values_backward` function.\n\nThis `sparse_mask` function was implemented before for  backward  sparse-sparse matmul. However,  the algorithm is little different  because in this case it should be applyable not only for matrices but for n-dimensional tensors. Thankfully it was not quite hard to extend and now both share the same code base.\n\nNote that  no new tests are required because now the backward for sparse-sparse matmul now uses the new `sparse_mask_helper`.\n\nngimel, mruberry - kindly review this.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50361\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26270483\n\nPulled By: ngimel\n\nfbshipit-source-id: ee4bda49ff86e769342674b64d3c4bc34eae38ef", "pr_number": "50361", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseMatMul.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseMatMul.cu", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": ["Merged", "cla signed", "module: autograd", "module: sparse", "open source", "triaged"]}, "bce4c82f0d": {"title": "[C2] Add TypeAndShape Inference logic for ReduceMean (#51828)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51828\n\nAs desc.\n\nTest Plan: Unit-tests.\n\nDifferential Revision: D26293844\n\nfbshipit-source-id: 2eb2a694c439b794ad7c134409e2b8926aabc91f", "pr_number": "51828", "files_changed": ["caffe2/operators/reduce_ops.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "79832f3d77": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D26309565\n\nfbshipit-source-id: b20d37ea90304052cef9b4dc359a5bd726d7fda7", "pr_number": null, "files_changed": ["torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": []}, "c89f15ec6d": {"title": "Reland nightlies 11.2 (#51874)", "body": "Summary:\nCherry-picked commits from https://github.com/pytorch/pytorch/issues/51611.\n\nRelanding after https://github.com/pytorch/pytorch/issues/51864 should fix failing CUDA tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51874\n\nReviewed By: malfet\n\nDifferential Revision: D26313173\n\nPulled By: janeyx99\n\nfbshipit-source-id: 02250abb526cc7400bc2d9bbb146e8210ccd4b40", "pr_number": "51874", "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/config.yml", ".circleci/scripts/binary_linux_test.sh"], "labels": ["Merged", "cla signed"]}, "7467f90b13": {"title": "Report test time regressions (#50171)", "body": "Summary:\nThis is a followup to https://github.com/pytorch/pytorch/issues/49190. Vaguely speaking, the goals are to make it easy to identify test time regressions introduced by PRs. Eventually the hope is to use this information to edit Dr CI comments, but this particular PR just does the analysis and prints it to stdout, so a followup PR would be needed to edit the actual comments on GitHub.\n\n**Important:** for uninteresting reasons, this PR moves the `print_test_stats.py` file.\n\n- *Before:* `test/print_test_stats.py`\n- *After:* `torch/testing/_internal/print_test_stats.py`\n\nNotes on the approach:\n\n- Just getting the mean and stdev for the total job time of the last _N_ commits isn't sufficient, because e.g. if `master` was broken 5 commits ago, then a lot of those job times will be much shorter, breaking the statistics.\n- We use the commit history to make better estimates for the mean and stdev of individual test (and suite) times, but only when the test in that historical commit is present and its status matches that of the base commit.\n- We list all the tests that were removed or added, or whose status changed (e.g. skipped to not skipped, or vice versa), along with time (estimate) info for that test case and its containing suite.\n- We don't list tests whose time changed a lot if their status didn't change, because there's a lot of noise and it's unclear how to do that well without too many false positives.\n- We show a human-readable commit graph that indicates exactly how many commits are in the pool of commits that could be causing regressions (e.g. if a PR has multiple commits in it, or if the base commit on `master` doesn't have a report in S3).\n- We don't show an overall estimate of whether the PR increased or decreased the total test job time, because it's noisy and it's a bit tricky to aggregate stdevs up from individual tests to the whole job level. This might change in a followup PR.\n- Instead, we simply show a summary at the bottom which says how many tests were removed/added/modified (where \"modified\" means that the status changed), and our best estimates of the mean times (and stdevs) of those changes.\n- Importantly, the summary at the bottom is only for the test cases that were already shown in the more verbose diff report, and does not include any information about tests whose status didn't change but whose running time got much longer.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50171\n\nTest Plan:\nTo run the unit tests:\n```\n$ python test/test_testing.py\n$ python test/print_test_stats.py\n```\n\nTo verify that this works, check the [CircleCI logs](https://app.circleci.com/pipelines/github/pytorch/pytorch/258628/workflows/9cfadc34-e042-485e-b3b3-dc251f160307) for a test job run on this PR; for example:\n- pytorch_linux_bionic_py3_6_clang9_test\n\nTo test locally, use the following steps.\n\nFirst run an arbitrary test suite (you need to have some XML reports so that `test/print_test_stats.py` runs, but we'll be ignoring them here via the `--use-json` CLI option):\n```\n$ DATA_DIR=/tmp\n$ ARBITRARY_TEST=testing\n$ python test/test_$ARBITRARY_TEST.py --save-xml=$DATA_DIR/test/test_$ARBITRARY_TEST\n```\nNow choose a commit and a test job (it has to be on `master` since we're going to grab the test time data from S3, and [we only upload test times to S3 on the `master`, `nightly`, and `release` branches](https://github.com/pytorch/pytorch/pull/49645)):\n```\n$ export CIRCLE_SHA1=c39fb9771d89632c5c3a163d3c00af3bef1bd489\n$ export CIRCLE_JOB=pytorch_linux_bionic_py3_6_clang9_test\n```\nDownload the `*.json.bz2` file(s) for that commit/job pair:\n```\n$ aws s3 cp s3://ossci-metrics/test_time/$CIRCLE_SHA1/$CIRCLE_JOB/ $DATA_DIR/ossci-metrics/test_time/$CIRCLE_SHA1/$CIRCLE_JOB --recursive\n```\nAnd feed everything into `test/print_test_stats.py`:\n```\n$ bzip2 -kdc $DATA_DIR/ossci-metrics/test_time/$CIRCLE_SHA1/$CIRCLE_JOB/*Z.json.bz2 | torch/testing/_internal/print_test_stats.py --compare-with-s3 --use-json=/dev/stdin $DATA_DIR/test/test_$ARBITRARY_TEST\n```\nThe first part of the output should be the same as before this PR; here is the new part, at the end of the output:\n\n- https://pastebin.com/Jj1svhAn\n\nReviewed By: walterddr\n\nDifferential Revision: D26232345\n\nPulled By: samestep\n\nfbshipit-source-id: b687b1737519d2eed68fbd591a667e4e029de509", "pr_number": "50171", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "mypy.ini", "test/print_test_stats.py", "test/test_testing.py", "tools/code_coverage/package/oss/init.py", "torch/testing/_internal/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "d454a84bab": {"title": "derivatives.yaml cleanup + restore codegen code forgotten in refactor (#51721)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51721\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26285908\n\nPulled By: albanD\n\nfbshipit-source-id: 3130736be9146eaee3a8e80be59a66eb2180d536", "pr_number": "51721", "files_changed": ["tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py"], "labels": ["Merged", "cla signed"]}, "b97a040f71": {"title": "ENH: toggle TORCH_WARN_ONCE to TORCH_WARN for tests (#48560)", "body": "Summary:\nToward fixing https://github.com/pytorch/pytorch/issues/47624\n\n~Step 1: add `TORCH_WARN_MAYBE` which can either warn once or every time in c++, and add a c++ function to toggle the value.\nStep 2 will be to expose this to python for tests. Should I continue in this PR or should we take a different approach: add the python level exposure without changing any c++ code and then over a series of PRs change each call site to use the new macro and change the tests to make sure it is being checked?~\n\nStep 1: add a python and c++ toggle to convert TORCH_WARN_ONCE into TORCH_WARN so the warnings can be caught in tests\nStep 2: add a python-level decorator to use this toggle in tests\nStep 3: (in future PRs): use the decorator to catch the warnings instead of `maybeWarnsRegex`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48560\n\nReviewed By: ngimel\n\nDifferential Revision: D26171175\n\nPulled By: mruberry\n\nfbshipit-source-id: d83c18f131d282474a24c50f70a6eee82687158f", "pr_number": "48560", "files_changed": ["c10/util/Exception.cpp", "c10/util/Exception.h", "docs/source/torch.rst", "test/test_torch.py", "torch/_C/__init__.pyi.in", "torch/__init__.py", "torch/csrc/Module.cpp", "torch/overrides.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit", "open source", "triaged"]}, "0dd1d60d54": {"title": "[JIT] Remove Dropout during Frozon Optimization (#51589)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51589\n\nDropout operators are only needed in training. Remove them for frozen models.\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D26214259\n\nfbshipit-source-id: 3ab05869e1e1f6c57498ba62bf40944f7c2189aa", "pr_number": "51589", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_graph_optimizations.cpp", "torch/csrc/jit/passes/remove_dropout.cpp", "torch/csrc/jit/passes/remove_dropout.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "649e683255": {"title": "Fix torch.nonzero type annotation (#51635)", "body": "Summary:\nThe overloads are a little tricky here. It's important that the overloads are such that it's unambiguous what\n`torch.nonzero(x)` will resolve to - so just specify defaults for one of the overloads. Also, `out` is left out of the second overload\nbecause a non-None value for `out` is not valid in combination with `as_tuple=True`.\n\nCloses gh-51434\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51635\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26279203\n\nPulled By: walterddr\n\nfbshipit-source-id: 8459c04fc9fbf7fc5f31b3f631aaac2f98b17ea6", "pr_number": "51635", "files_changed": ["tools/pyi/gen_pyi.py", "torch/_C/_VariableFunctions.pyi.in", "torch/_C/__init__.pyi.in"], "labels": ["Merged", "cla signed", "module: typing", "open source", "triaged"]}, "1aaddd83a5": {"title": "don't set the same C++ and C standards twice (#51832)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51832\n\nReviewed By: izdeby\n\nDifferential Revision: D26312660\n\nPulled By: ezyang\n\nfbshipit-source-id: 7d646cd106397e70bca0050d0aa30eb62b085cee", "pr_number": "51832", "files_changed": ["CMakeLists.txt"], "labels": ["Merged", "cla signed", "merge-this-please", "open source"]}, "21dccbca62": {"title": "Revert D26232345: [pytorch][PR] Report test time regressions", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26232345 (https://github.com/pytorch/pytorch/commit/7467f90b131ab28340d1b6c1f565fc4291424af1)\n\nOriginal commit changeset: b687b1737519\n\nfbshipit-source-id: 10a031c5500b083f7c82f2ae2743b671c5a07bff", "pr_number": null, "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "mypy.ini", "test/print_test_stats.py", "test/test_testing.py", "tools/code_coverage/package/oss/init.py", "torch/testing/_internal/print_test_stats.py"], "labels": []}, "d9e6750759": {"title": "fix multi_output_kernel (#51827)", "body": "Summary:\nWith zasdfgbnm's help and with his small TensorIterator kernel repro https://github.com/zasdfgbnm/tensoriterator we've found a workaround for what looks like a compiler bug in multi_output_kernel that manifests itself with cuda 10.2 and cuda 11 when there is a non-trivial OffsetCalculator.\nIt looks like those nvcc versions cannot handle inheritance in device structs, so instead of inheriting `multi_outputs_unroll` from `unroll` we make it independent.\ncc vkuzo, haichuan-fb I verified that reverting https://github.com/pytorch/pytorch/issues/49315 to bring back multi_output_kernel and running `test_learnable_backward_per_channel_cuda` test passes, but I didn't do it in this PR - can you take it up as a follow-up?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51827\n\nReviewed By: izdeby\n\nDifferential Revision: D26305559\n\nPulled By: ngimel\n\nfbshipit-source-id: 1168e7c894d237a954abfd1998eaad54f0ce40a7", "pr_number": "51827", "files_changed": ["aten/src/ATen/native/cuda/MemoryAccess.cuh", "test/quantization/test_workflow_module.py"], "labels": ["Merged", "cla signed"]}, "6fa5e96f2e": {"title": "remove unnecessary BoxedKernelWrapper specialization now that ops are all c10-full (#50963)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50963\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D26026665\n\nPulled By: bdhirsh\n\nfbshipit-source-id: ef6e515f7dae5052538789e5b75dc551b4ce3b11", "pr_number": "50963", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/impl/boxing.h"], "labels": ["Merged", "cla signed"]}, "41bab9a4b6": {"title": "Plumbing dispatch keys through the dispatcher (#49354)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49354\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D25614042\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 269a75e9a3ac518aa63bff2cafbd47ed2c4ff780", "pr_number": "49354", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.cpp", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/impl/boxing.h", "aten/src/ATen/core/boxing/impl/kernel_stackbased_test.cpp", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/boxing/impl/test_helpers.h", "aten/src/ATen/core/dispatch/CppSignature.h", "aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/op_registration/op_registration.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "c10/core/DispatchKeySet.h", "c10/test/util/Metaprogramming_test.cpp", "c10/test/util/TypeList_test.cpp", "c10/util/Metaprogramming.h", "c10/util/TypeList.h", "tools/autograd/gen_trace_type.py", "tools/codegen/gen.py", "torch/library.h"], "labels": ["Merged", "cla signed"]}, "b9acfcddeb": {"title": "Support mypy ignore annotation with particular rule specified (#51675)", "body": "Summary:\nPreviously TorchScript allows a ignore-all type check suppression rule that looks like\n```\ncode code code  # type: ignore\n```\n\nBut a more common use case is\n```\ncode code code  # type: ignore[specific-rule]\n```\nThis PR allows the more common use case\n\nFixes https://github.com/pytorch/pytorch/issues/48643\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51675\n\nReviewed By: ansley\n\nDifferential Revision: D26304870\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 0ac9ee34f0219c86e428318a69484d5aa3ec433f", "pr_number": "51675", "files_changed": ["test/test_jit.py", "torch/jit/annotations.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "d90911adf9": {"title": "fix AdaptiveAveragePooling crash problem for non support input (#51443)", "body": "Summary:\nFor none support input, we should not do check in a parallel region, this PR will first do the dtype check, and then do parallel for.\nFixes https://github.com/pytorch/pytorch/issues/51352.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51443\n\nReviewed By: izdeby\n\nDifferential Revision: D26305584\n\nPulled By: ngimel\n\nfbshipit-source-id: 6faa3148af5bdcd7246771c0ecb4db2b31ac82c6", "pr_number": "51443", "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling3d.cpp", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "159c48b19b": {"title": "Fix triplet margin loss and reciprocal docs (#51650)", "body": "Summary:\nReciprocal: the note should be placed after the formula\n\nTriplet-margin-loss (before):\n![image](https://user-images.githubusercontent.com/13428986/106784863-cb3eb780-661a-11eb-8372-07b51e4cb2d4.png)\nAfter:\n![image](https://user-images.githubusercontent.com/13428986/106784948-e5789580-661a-11eb-890c-6185aab96e54.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51650\n\nReviewed By: izdeby\n\nDifferential Revision: D26314151\n\nPulled By: soulitzer\n\nfbshipit-source-id: d7574e64e96a41a515231ba7e1008de8b2f292aa", "pr_number": "51650", "files_changed": ["torch/_torch_docs.py", "torch/nn/modules/loss.py"], "labels": ["Merged", "cla signed"]}, "97e35858ec": {"title": "[Resubmit] Add compare_set operation and test to TCPStore (#51815)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51815\n\nThis is resubmission of #51593, already approved.\n\nTest Plan: Imported from OSS\n\nReviewed By: izdeby\n\nDifferential Revision: D26316875\n\nPulled By: H-Huang\n\nfbshipit-source-id: d81cb131ef6b9e2ebaee32bb505dfc11235bc29d", "pr_number": "51815", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/StoreTestCommon.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "58eb23378f": {"title": "Clean up usage of torch._six partially (#49785)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/42919\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49785\n\nReviewed By: mruberry\n\nDifferential Revision: D25963833\n\nPulled By: bugra\n\nfbshipit-source-id: 11c90d6b8d3f206c9d0a4d8621b773beb10c6ba2", "pr_number": "49785", "files_changed": ["test/run_test.py", "test/test_autograd.py", "test/test_cuda.py", "test/test_dataloader.py", "test/test_jit.py", "test/test_reductions.py", "torch/_jit_internal.py", "torch/_six.py", "torch/_utils.py", "torch/autograd/gradcheck.py", "torch/cuda/amp/autocast_mode.py", "torch/cuda/amp/grad_scaler.py", "torch/cuda/nccl.py", "torch/distributed/optim/zero_redundancy_optimizer.py", "torch/jit/_recursive.py", "torch/jit/_script.py", "torch/jit/annotations.py", "torch/nn/modules/container.py", "torch/nn/modules/utils.py", "torch/nn/quantized/modules/utils.py", "torch/onnx/utils.py", "torch/optim/optimizer.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/jit_utils.py", "torch/utils/data/_utils/collate.py", "torch/utils/data/_utils/pin_memory.py", "torch/utils/data/_utils/worker.py", "torch/utils/data/dataloader.py", "torch/utils/data/sampler.py"], "labels": ["Merged", "better-engineering", "cla signed", "oncall: jit", "open source", "triaged"]}, "1e70b4bb73": {"title": "Add GH Actions CI to build nightly Docker and push to GitHub Container Registry (#51755)", "body": "Summary:\nCurrently PyTorch repository provides Dockerfile to build Docker with nightly builds, but it doesn't have CI to actually build those Dockers.\nThis PR adds a GitHub action workflow to create PyTorch nightly build Docker and publish them to GitHub Container Registry.\nAlso, add \"--always\" option to the `git describe --tags` command that generates the Docker image tag.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51755\n\nTest Plan: Manually trigger the workflow build in the GitHub Actions web UI.\n\nReviewed By: seemethere\n\nDifferential Revision: D26320180\n\nPulled By: xuzhao9\n\nfbshipit-source-id: e00b472df14f5913cab9b06a41e837014e87f1c7", "pr_number": "51755", "files_changed": [".github/scripts/build_publish_nightly_docker.sh", ".github/workflows/push_nightly_docker_ghcr.yml", "docker.Makefile"], "labels": ["Merged", "cla signed", "module: docker"]}, "9e4f3b89c4": {"title": "[Gradient Compression] Add register_comm_hook API to DDP communication hooks documentation page (#51846)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51846\n\n`register_comm_hook` method is defined in DistributedDataParallel module, but it is not covered in `distributed.rst`. Since it's closely related to DDP communication hook, add the docstrings to `ddp_comm_hooks.rst` instead of a reference.\n\nScreenshot:\n\n{F370425625}\nghstack-source-id: 121278173\n\nTest Plan:\nview locally\n\npython_doc_test:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/271234/workflows/dc0b443d-8a62-4334-9b42-800c33a68553/jobs/10770636\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26298191\n\nfbshipit-source-id: 32e0685fd3c935cf9a2d129e6c520a94aa3e3817", "pr_number": "51846", "files_changed": ["docs/source/ddp_comm_hooks.rst"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "21ef248fb8": {"title": "[reland] Report test time regressions (#50171)", "body": "Summary:\nThis is a followup to https://github.com/pytorch/pytorch/issues/49190. Vaguely speaking, the goals are to make it easy to identify test time regressions introduced by PRs. Eventually the hope is to use this information to edit Dr CI comments, but this particular PR just does the analysis and prints it to stdout, so a followup PR would be needed to edit the actual comments on GitHub.\n\n**Important:** for uninteresting reasons, this PR moves the `print_test_stats.py` file.\n\n- *Before:* `test/print_test_stats.py`\n- *After:* `torch/testing/_internal/print_test_stats.py`\n\nNotes on the approach:\n\n- Just getting the mean and stdev for the total job time of the last _N_ commits isn't sufficient, because e.g. if `master` was broken 5 commits ago, then a lot of those job times will be much shorter, breaking the statistics.\n- We use the commit history to make better estimates for the mean and stdev of individual test (and suite) times, but only when the test in that historical commit is present and its status matches that of the base commit.\n- We list all the tests that were removed or added, or whose status changed (e.g. skipped to not skipped, or vice versa), along with time (estimate) info for that test case and its containing suite.\n- We don't list tests whose time changed a lot if their status didn't change, because there's a lot of noise and it's unclear how to do that well without too many false positives.\n- We show a human-readable commit graph that indicates exactly how many commits are in the pool of commits that could be causing regressions (e.g. if a PR has multiple commits in it, or if the base commit on `master` doesn't have a report in S3).\n- We don't show an overall estimate of whether the PR increased or decreased the total test job time, because it's noisy and it's a bit tricky to aggregate stdevs up from individual tests to the whole job level. This might change in a followup PR.\n- Instead, we simply show a summary at the bottom which says how many tests were removed/added/modified (where \"modified\" means that the status changed), and our best estimates of the mean times (and stdevs) of those changes.\n- Importantly, the summary at the bottom is only for the test cases that were already shown in the more verbose diff report, and does not include any information about tests whose status didn't change but whose running time got much longer.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50171\n\nTest Plan:\nTo run the unit tests:\n```\n$ python test/test_testing.py\n$ python test/print_test_stats.py\n```\n\nTo verify that this works, check the [CircleCI logs](https://app.circleci.com/pipelines/github/pytorch/pytorch/258628/workflows/9cfadc34-e042-485e-b3b3-dc251f160307) for a test job run on this PR; for example:\n- pytorch_linux_bionic_py3_6_clang9_test\n\nTo test locally, use the following steps.\n\nFirst run an arbitrary test suite (you need to have some XML reports so that `test/print_test_stats.py` runs, but we'll be ignoring them here via the `--use-json` CLI option):\n```\n$ DATA_DIR=/tmp\n$ ARBITRARY_TEST=testing\n$ python test/test_$ARBITRARY_TEST.py --save-xml=$DATA_DIR/test/test_$ARBITRARY_TEST\n```\nNow choose a commit and a test job (it has to be on `master` since we're going to grab the test time data from S3, and [we only upload test times to S3 on the `master`, `nightly`, and `release` branches](https://github.com/pytorch/pytorch/pull/49645)):\n```\n$ export CIRCLE_SHA1=c39fb9771d89632c5c3a163d3c00af3bef1bd489\n$ export CIRCLE_JOB=pytorch_linux_bionic_py3_6_clang9_test\n```\nDownload the `*.json.bz2` file(s) for that commit/job pair:\n```\n$ aws s3 cp s3://ossci-metrics/test_time/$CIRCLE_SHA1/$CIRCLE_JOB/ $DATA_DIR/ossci-metrics/test_time/$CIRCLE_SHA1/$CIRCLE_JOB --recursive\n```\nAnd feed everything into `test/print_test_stats.py`:\n```\n$ bzip2 -kdc $DATA_DIR/ossci-metrics/test_time/$CIRCLE_SHA1/$CIRCLE_JOB/*Z.json.bz2 | torch/testing/_internal/print_test_stats.py --compare-with-s3 --use-json=/dev/stdin $DATA_DIR/test/test_$ARBITRARY_TEST\n```\nThe first part of the output should be the same as before this PR; here is the new part, at the end of the output:\n\n- https://pastebin.com/Jj1svhAn\n\nReviewed By: malfet, izdeby\n\nDifferential Revision: D26317769\n\nPulled By: samestep\n\nfbshipit-source-id: 1ba06cec0fafac77f9e7341d57079543052d73db", "pr_number": "50171", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "mypy-strict.ini", "mypy.ini", "test/print_test_stats.py", "test/test_testing.py", "tools/code_coverage/package/oss/init.py", "torch/testing/_internal/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "dac730af11": {"title": "Warn if mypy version doesn't match CI (#51799)", "body": "Summary:\nThis PR adds a local [`mypy` plugin](https://mypy.readthedocs.io/en/stable/extending_mypy.html#extending-mypy-using-plugins) that warns if you accidentally run `mypy` using a version that doesn't match [the version we install for CI](https://github.com/pytorch/pytorch/blob/6045663f391e9bebac31e006a98c1dd381792936/.circleci/docker/common/install_conda.sh#L117), since this trips people up sometimes when `mypy` gives errors in some versions (see https://github.com/pytorch/pytorch/issues/51513) but not others.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51799\n\nTest Plan:\nTo check that this doesn't break our `mypy` test(s) when you have the correct version installed:\n```\npython test/test_type_hints.py\n```\nTo check that this does indeed warn when you have an incorrect `mypy` version installed, switch to a different version (e.g. 0.782), and run the above command or either of these:\n```\nmypy\nmypy --config-file=mypy-strict.ini\n```\nYou should get the following message on stderr:\n```\nYou are using mypy version 0.782, which is not supported\nin the PyTorch repo. Please switch to mypy version 0.770.\n\nFor example, if you installed mypy via pip, run this:\n\n    pip install mypy==0.770\n\nOr if you installed mypy via conda, run this:\n\n    conda install -c conda-forge mypy=0.770\n```\n\nReviewed By: janeyx99\n\nDifferential Revision: D26282010\n\nPulled By: samestep\n\nfbshipit-source-id: 7b423020d0529700dea8972b27afa2d7068e1b12", "pr_number": "51799", "files_changed": ["mypy-strict.ini", "mypy.ini", "mypy_plugins/check_mypy_version.py"], "labels": ["Merged", "cla signed"]}, "7b9ca54ecf": {"title": "Reset checkpoint_valid flag when error happens during function execution (#51746)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/37874, https://github.com/pytorch/pytorch/issues/51743\n\nUses RAII to manage the flag so that it gets reset properly on exception\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51746\n\nReviewed By: izdeby\n\nDifferential Revision: D26319619\n\nPulled By: soulitzer\n\nfbshipit-source-id: ea1235438ba516f99195c83fa23d5880f9977c93", "pr_number": "51746", "files_changed": ["test/test_autograd.py", "torch/autograd/__init__.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/utils/checkpoint.py"], "labels": ["Merged", "cla signed"]}, "2303c244fc": {"title": "skip a second call to shouldUseRecordFunction for BackendSelect ops (#50891)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50891\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D25999514\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 8a6c17ab502fe463cf3fb38a1e555c64bc5556f0", "pr_number": "50891", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "034a007ad8": {"title": "Remind about AutoNonVariableTypeMode in error message. (#51655)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51655\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D26228508\n\nPulled By: ailzhang\n\nfbshipit-source-id: f5f48fde3611c84cc6473b77824ebf9dffbb4453", "pr_number": "51655", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.cpp"], "labels": ["Merged", "cla signed"]}, "8ab22a080b": {"title": "Build pytorch_android using Gradle wrapper. (#51067)", "body": "Summary:\n[Here](https://docs.gradle.org/current/userguide/gradle_wrapper.html), there is the following description.\n`The recommended way to execute any Gradle build is with the help of the Gradle Wrapper`\n\nI took a little time to prepare Gradle for `pytorch_android` build. (version etc.)\n\nI think using Gradle wrapper will make `pytorch_android` build more seamless.\n\nGradle wrapper version: 4.10.3\n\nhttps://github.com/pytorch/pytorch/blob/250c71121b8ac2ef1899a2414c939d4d45fc2be4/.circleci/scripts/build_android_gradle.sh#L13\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51067\n\nReviewed By: izdeby\n\nDifferential Revision: D26315718\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: f8077d7b28dc0b03ee48bcdac2f5e47d9c1f04d9", "pr_number": "51067", "files_changed": ["android/.gitignore", "android/common.sh", "android/gradle/wrapper/gradle-wrapper.jar", "android/gradle/wrapper/gradle-wrapper.properties", "android/gradlew", "android/gradlew.bat"], "labels": ["Merged", "cla signed", "module: android", "oncall: mobile", "open source", "triaged"]}, "482b94ae51": {"title": "move RoutedDecoder Dataset to DataPipe (#51704)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51704\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D26245910\n\nPulled By: glaringlee\n\nfbshipit-source-id: 91e3c9f8a6c1209c1a1a752ba29a80dbd9bf4119", "pr_number": "51704", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/routeddecoder.py", "torch/utils/data/datapipes/utils/decoder.py"], "labels": ["Merged", "cla signed"]}, "015cabf82a": {"title": "move GroupByFilename Dataset to DataPipe (#51709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51709\n\nMove GroupByFilename Dataset to DataPipe\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D26263585\n\nPulled By: glaringlee\n\nfbshipit-source-id: 00e3e13b47b89117f1ccfc4cd6239940a40d071e", "pr_number": "51709", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/groupbykey.py"], "labels": ["Merged", "cla signed"]}, "d5a2429c24": {"title": "Fix flake8 failures (#51963)", "body": "Summary:\nFixes flake8 failures in test_autograd.py by using `gradcheck` from `torch.testing._internal.common_utils` rather than directly from`torch.autograd.gradcheck`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51963\n\nReviewed By: albanD\n\nDifferential Revision: D26339107\n\nPulled By: malfet\n\nfbshipit-source-id: 63e0f12df16b70e394097ad88852984c1848a9e6", "pr_number": "51963", "files_changed": ["test/test_autograd.py"], "labels": ["Merged", "cla signed"]}, "d61d8d886b": {"title": "correct value argument name for Tensor.index_fill_ docs (#51763)", "body": "Summary:\nThe name of \"val\" is inconsistent with the rest of the API and also\ninconsistent with the underlying C++ implementation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51763\n\nTest Plan:\nUsed the following command to demonstrate incorrect docs before and\ncorrect docs after:\n  python -c 'import torch; print(torch.Tensor.index_fill_.__doc__)'\n\nFixes https://github.com/pytorch/pytorch/issues/51250\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26271273\n\nPulled By: dagitses\n\nfbshipit-source-id: 4897da80b639c54ca652d2111e13f26efe2646a0", "pr_number": "51763", "files_changed": ["torch/_tensor_docs.py"], "labels": ["Merged", "cla signed"]}, "35b3e16091": {"title": "[pytorch] Fix torch.nn.functional.normalize to be properly scriptable (#51909)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51909\n\nSeveral scenarios don't work when trying to script `F.normalize`, notably when you try to symbolically trace through it with using the default argument:\n\n```\nimport torch.nn.functional as F\nimport torch\nfrom torch.fx import symbolic_trace\n\ndef f(x):\n    return F.normalize(x)\n\ngm = symbolic_trace(f)\ntorch.jit.script(gm)\n```\nwhich leads to the error\n```\nRuntimeError:\n\nnormalize(Tensor input, float p=2., int dim=1, float eps=9.9999999999999998e-13, Tensor? out=None) -> (Tensor):\nExpected a value of type 'float' for argument 'p' but instead found type 'int'.\n:\ndef forward(self, x):\n    normalize_1 = torch.nn.functional.normalize(x, p = 2, dim = 1, eps = 1e-12, out = None);  x = None\n                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    return normalize_1\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26324308\n\nfbshipit-source-id: 30dd944a6011795d17164f2c746068daac570cea", "pr_number": "51909", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "42635c3e59": {"title": "Fix regex in collect_env.py for CUDA 11 (#51852)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51840\n\nManually tested with both CUDA 10.2.89 & 11.2.67.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51852\n\nReviewed By: izdeby\n\nDifferential Revision: D26326105\n\nPulled By: mrshenli\n\nfbshipit-source-id: 46fbe5f20c02bca982ce2ec6e62f7cc3a14fcc97", "pr_number": "51852", "files_changed": ["torch/utils/collect_env.py"], "labels": ["Merged", "cla signed", "open source"]}, "285e69a9cd": {"title": "[package] more reliable method for determining standard library-ness (#51694)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51694\n\nWe implicitly extern standard library modules. Our method of determining\nwhether a module is in the standard library is a little unreliable. In\nparticular, I'm seeing lots of flaky errors on windows/mac CI when I\nstart doing more complicated packaging tests.\n\nI looked into the best ways to do this, turns out there's no reliable\nway, so tools that need to do this generally just parse the Python docs\nfor a listing and save it. I took `isort`'s lists and called it a day.\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D26243751\n\nPulled By: suo\n\nfbshipit-source-id: 48c685cd45ae847fe986bcb9f39106e0c3361cdc", "pr_number": "51694", "files_changed": ["torch/package/_stdlib.py", "torch/package/exporter.py"], "labels": ["Merged", "cla signed"]}, "85b25257ff": {"title": "[package] Use custom persistent_load in PackageImporter (#51595)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51595\n\nRight now `PackageExporter` defines its own `persistent_id` but\n`PackageImporter` uses the one defined in `torch.serialization`. I have\nsome downstream plans to customize this so this PR just splits it out.\n\nNot to fear! I know this introduces some duplication and potential for\ndifferent behavior between `torch.save` and `torch.package`, but I have\nplans to re-unify them soon.\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D26211578\n\nPulled By: suo\n\nfbshipit-source-id: 48a2ccaefb2525e1498ad68b75c46d9de3d479b7", "pr_number": "51595", "files_changed": ["torch/package/importer.py"], "labels": ["Merged", "cla signed"]}, "c357f8b826": {"title": "[package] make torch.package produce unified format (#51826)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51826\n\nLooks like this:\n```\nresnet.pt\n\u251c\u2500\u2500 .data  # Data folder named so it can't clash with torch.package codemodules.\n\u2502   \u2502      # Names/extensions automatically added to avoid namingconflicts.\n\u2502   \u251c\u2500\u2500 94286146172688.storage   # tensor data\n\u2502   \u251c\u2500\u2500 94286146172784.storage\n\u2502   \u251c\u2500\u2500 extern_modules           # torch.package metadata\n\u2502   \u251c\u2500\u2500 version                  # version metadata\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 model  # package pickled model created w/\n\u2502   \u2502      # exporter.save_pickel('model','model.pkl', resnet_model)\n\u2502   \u2514\u2500\u2500 model.pkl\n\u2514\u2500\u2500 torchvision  # all code dependencies for packaged picked\n    \u2514\u2500\u2500 models   # models are captured as source files\n            \u251c\u2500\u2500 resnet.py\n                    \u2514\u2500\u2500 utils.py\n```\n\nSince `version` is hardcoded in our zip reader/writer implementation,\nadd it as an option that defaults to \"version\" but accepts other\nlocations for putting the version metadata.\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D26295649\n\nPulled By: suo\n\nfbshipit-source-id: 2d75feeb7de0f78196b4d0b6e2b814a7d58bd1dd", "pr_number": "51826", "files_changed": ["caffe2/serialize/inline_container.cc", "caffe2/serialize/versions.h", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/init.cpp", "torch/package/exporter.py", "torch/package/importer.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "20fe2e12d6": {"title": "typo (#48887)", "body": "Summary:\na small grammar fix\n\njspisak - thank you!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48887\n\nReviewed By: malfet, zhangguanheng66\n\nDifferential Revision: D25358638\n\nPulled By: brianjo\n\nfbshipit-source-id: 3b805b54df3410f8770e1c6ddc569b26661cece4", "pr_number": "48887", "files_changed": ["torch/cuda/__init__.py"], "labels": ["Merged", "cla signed", "open source"]}, "8c09cc6475": {"title": "Remove android toolchain in Windows CircleCI image (#51405)", "body": "Summary:\nFixes #{issue number}\nIt can spare nearly 10 GB of disk space.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51405\n\nReviewed By: izdeby\n\nDifferential Revision: D26325768\n\nPulled By: janeyx99\n\nfbshipit-source-id: d9208c59dfd17d7bb529291821c5f1779666ac6f", "pr_number": "51405", "files_changed": [".circleci/scripts/binary_windows_build.sh"], "labels": ["Merged", "ci/all", "cla signed", "open source", "triaged"]}, "5dd1568aa3": {"title": "[ROCm] skip more magma tests (#51915)", "body": "Summary:\nAdditional magma tests have been identified as failing after integrating hipMAGMA into the ROCm builds.  Skipping is necessary until they can be fixed properly.  This is blocking migration of ROCm CI to 4.0.1.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51915\n\nReviewed By: izdeby\n\nDifferential Revision: D26326404\n\nPulled By: malfet\n\nfbshipit-source-id: 558cce66f216f404c0316ab036e2e5637fc99798", "pr_number": "51915", "files_changed": ["test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "b3fda95fe7": {"title": "Add LazyBatchNormXd (#51862)", "body": "Summary:\nSame diff with https://github.com/pytorch/pytorch/issues/51548 (cc. albanD)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51862\n\nReviewed By: izdeby\n\nDifferential Revision: D26312289\n\nPulled By: albanD\n\nfbshipit-source-id: 9cdec0e0c9021c33d10d85010978c7fa5cb4dc60", "pr_number": "51862", "files_changed": ["docs/source/nn.rst", "test/test_nn.py", "torch/jit/_recursive.py", "torch/nn/__init__.py", "torch/nn/modules/__init__.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/lazy.py", "torch/nn/modules/module.py", "torch/nn/parameter.py", "torch/nn/parameter.pyi"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "141f615161": {"title": "Support torch.type (#51904)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51904\n\nFixes issue: #25433\n\n=========\nMakes tensor.type(dtype) scriptable\n\nTest:\n======\npython test/test_jit.py -v TestJit.test_script_tensor_type\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26331503\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: d9188999fee601a8402fdc4d9052dee4e0d529d5", "pr_number": "51904", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/builtin_functions.cpp", "torch/csrc/jit/frontend/sugared_value.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "1e171f024b": {"title": "Fix warnings in TensorShape (#51642)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51642\n\nCompiling currently gives:\n```\nan 13 16:46:39 In file included from ../aten/src/ATen/native/TensorShape.cpp:12:\nJan 13 16:46:39 ../aten/src/ATen/native/Resize.h:37:24: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:39     if (new_size_bytes > self->storage().nbytes()) {\nJan 13 16:46:39         ~~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~~~~~~~~~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:32:24: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long long') [-Wsign-compare]\nJan 13 16:46:39   for (size_t i = 0; i < shape_tensor.numel(); ++i) {\nJan 13 16:46:39                      ~ ^ ~~~~~~~~~~~~~~~~~~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:122:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:39   for (int64_t i = 0; i < tensors.size(); i++) {\nJan 13 16:46:39                       ~ ^ ~~~~~~~~~~~~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:162:21: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:39   for (int i = 0; i < tensors.size(); i++) {\nJan 13 16:46:39                   ~ ^ ~~~~~~~~~~~~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:300:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:39   for (int64_t i = 0; i < s1.size(); ++i) {\nJan 13 16:46:39                       ~ ^ ~~~~~~~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:807:21: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:39     TORCH_CHECK(dim < self_sizes.size());\nJan 13 16:46:39                 ~~~ ^ ~~~~~~~~~~~~~~~~~\nJan 13 16:46:39 ../c10/util/Exception.h:361:31: note: expanded from macro 'TORCH_CHECK'\nJan 13 16:46:39   if (C10_UNLIKELY_OR_CONST(!(cond))) {                                 \\\nJan 13 16:46:39                               ^~~~\nJan 13 16:46:39 ../c10/util/Exception.h:244:47: note: expanded from macro 'C10_UNLIKELY_OR_CONST'\nJan 13 16:46:39 #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\nJan 13 16:46:39                                               ^\nJan 13 16:46:39 ../c10/macros/Macros.h:173:65: note: expanded from macro 'C10_UNLIKELY'\nJan 13 16:46:39 #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\nJan 13 16:46:39                                                                 ^~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:855:24: warning: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'const int64_t' (aka 'const long long') [-Wsign-compare]\nJan 13 16:46:39   for (size_t i = 0; i < num_blocks; ++i) {\nJan 13 16:46:39                      ~ ^ ~~~~~~~~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:2055:23: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:39     for (int i = 0; i < vec.size(); i++) {\nJan 13 16:46:39                     ~ ^ ~~~~~~~~~~\nJan 13 16:46:39 ../aten/src/ATen/native/TensorShape.cpp:2100:25: warning: comparison of integers of different signs: 'int64_t' (aka 'long long') and 'size_t' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:39   for (int64_t i = 0; i < src.size(); ++i) {\n```\nThis fixes issues with loop iteration variable types\n\nTest Plan: Sandcastle tests\n\nReviewed By: ngimel\n\nDifferential Revision: D25935136\n\nfbshipit-source-id: a5da4af16bb8045cc16ab1c78b8e0f2bb3ae64bd", "pr_number": "51642", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "fc314350ad": {"title": "Make RebatchingBuffer compatible with auto shape inference", "body": "Summary: no-op to operator behavior, resolve https://fburl.com/wte0v7tf\n\nTest Plan: buck test\n\nReviewed By: huangyi1979\n\nDifferential Revision: D26333212\n\nfbshipit-source-id: d237e8caf5977bc19fcced6aeedc6464fc905457", "pr_number": null, "files_changed": ["caffe2/core/blob_serialization.cc", "caffe2/proto/caffe2.proto"], "labels": []}, "0ec00c1292": {"title": "[docs] Add docs for storage and tensors for quantized Tensor (#51817)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51817\n\nTest Plan: Imported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D26292464\n\nPulled By: jerryzh168\n\nfbshipit-source-id: c5992deda4af949de4ea2e40edee8f22bd59b9e1", "pr_number": "51817", "files_changed": ["docs/source/storage.rst", "docs/source/tensors.rst"], "labels": ["Merged", "cla signed"]}, "8fab33f942": {"title": "Fix the lifetime of PyTensorType (#51649)", "body": "Summary:\nMake sure that `PyTensorType` objects are always available during the shutdown.\nSee https://github.com/pytorch/pytorch/issues/42125#issuecomment-772397319 for a more in-depth explanation of why it's needed.\n\nFixes https://github.com/pytorch/pytorch/issues/42125.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51649\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26256843\n\nPulled By: ezyang\n\nfbshipit-source-id: 4d1dac75e063f0bdc65e0784140641fc4beb8616", "pr_number": "51649", "files_changed": ["torch/csrc/tensor/python_tensor.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2f2b170068": {"title": "[Pytorch Mobile] Only preserve bundled input helpers for forward if they exist (#51884)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51884\n\nit is now possible to bundle inputs and not bundle them for forward. This is ok and so we need to account for that.\nghstack-source-id: 121266667\n\nTest Plan: Manually bundle inputs for a function not named forward. Call optimize_for_mobile and make sure the functions are still there. {P173289878}\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26304558\n\nfbshipit-source-id: 79f82d9de59c70b76f34e01f3d691107bf40e7bc", "pr_number": "51884", "files_changed": ["torch/utils/mobile_optimizer.py"], "labels": ["Merged", "cla signed"]}, "0410cba23e": {"title": "[FX] make map_arg require a callable (#51907)", "body": "Summary:\nThis makes something like: `map_arg(lambda x: x, [Node(), Node()])` throw an error (before it would silently return `lambda x: x`)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51907\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26323916\n\nPulled By: jansel\n\nfbshipit-source-id: f56ebcf9a3af47546d75603567025163f1fb8454", "pr_number": "51907", "files_changed": ["torch/fx/node.py"], "labels": ["Merged", "cla signed", "fx"]}, "7e54a64828": {"title": "[C2] Add shape inference logic for ColwiseMax operator. (#51914)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51914\n\nAs desc.\n\nTest Plan: Unit-test.\n\nReviewed By: intermilan\n\nDifferential Revision: D26299115\n\nfbshipit-source-id: 9c80236f843e907476da1747dcd623c85147fa90", "pr_number": "51914", "files_changed": ["caffe2/operators/reduction_ops.cc", "caffe2/python/operator_test/reduction_ops_test.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "12d85b536e": {"title": "Fixing Softmax bench. (#51898)", "body": "Summary:\nFixes and enables the microbenchmark for Softmax.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51898\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D26333189\n\nPulled By: navahgar\n\nfbshipit-source-id: be0934e413c4f6728593f896e53a0b31f1657e52", "pr_number": "51898", "files_changed": ["benchmarks/tensorexpr/__main__.py", "benchmarks/tensorexpr/pt_engine.py", "benchmarks/tensorexpr/softmax.py"], "labels": ["Merged", "cla signed"]}, "e964d77fca": {"title": "[pytorch] recast infer_type error and amend with name and item that failed inferring", "body": "Summary:\nWhen type inference fails when JITing torchscript module, the error message does not give any implication where the error fails. For example:  \"Cannot create dict for key type 'int?', only int, float, complex, Tensor and string keys are supported\".\n\nThis adds the variable name and item to the error message.\n\nReviewed By: ajaech\n\nDifferential Revision: D26327483\n\nfbshipit-source-id: d8c85e7550258d7c56530f5826ff9683ca8b2b94", "pr_number": null, "files_changed": ["torch/jit/_recursive.py"], "labels": []}, "104371e1dc": {"title": "[DataLoader] Implement FilterIterDataPipe (#51783)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51783\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26277688\n\nPulled By: ejguan\n\nfbshipit-source-id: 25ed7da9da88c030b29627142c2f04fed26cdcda", "pr_number": "51783", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/selecting.py"], "labels": ["Merged", "cla signed"]}, "9eb70c3c78": {"title": "[DataLoader] Rename Callable to Map IterDataPipe (#51879)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51879\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26314775\n\nPulled By: ejguan\n\nfbshipit-source-id: ee77909eae97092155ed6a6c794540e68a04d754", "pr_number": "51879", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/callable.py", "torch/utils/data/datapipes/iter/selecting.py"], "labels": ["Merged", "cla signed"]}, "1921b244f6": {"title": "[DataLoader] Rename files of functional datapipes (#51880)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51880\n\nUsing the reference of [iter-tools](https://more-itertools.readthedocs.io/en/stable/api.html) to rename files based on the functionality.\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26314776\n\nPulled By: ejguan\n\nfbshipit-source-id: e97bac047a0fa808676cd6f3a9202109d17f81ca", "pr_number": "51880", "files_changed": ["torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/batch.py", "torch/utils/data/datapipes/iter/combinatorics.py", "torch/utils/data/datapipes/iter/grouping.py", "torch/utils/data/datapipes/iter/sampler.py"], "labels": ["Merged", "cla signed"]}, "c6b4fc8a90": {"title": "[ROCm] add 4.0.1 docker image (#51507)", "body": "Summary:\nAdd a ROCm 4.0.1 docker image for CI. Keep the 3.10 image.\nKeep the 3.9 image until the 3.9 image is no longer needed.\nPlan is to keep two ROCm versions at a time.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51507\n\nReviewed By: seemethere\n\nDifferential Revision: D26350348\n\nPulled By: malfet\n\nfbshipit-source-id: 6230278343ee48f19e96067180590beab96b17cc", "pr_number": "51507", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "3af7b673ef": {"title": "Let child CUDAFuture wait for parent CUDAFuture's CUDAEvents (#51820)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51820\n\nIf the child cannot extract tensors from returned IValue, the\ncurrent child CUDAFuture won't wait for anything. In this case,\nif the `wait()` wasn't called on the parent Future, streams are\nnot synchronized, and it is possible that parent Future's CUDA\nops have not been added to streams yet.\n\nThis commit adds a `markCompletedWithDataPtrs()` to `ivalue::Future`,\nand RPC uses this API to pass Message tensor dataPtrs to the\n`PyObject` Future when marking it as completed.\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D26324068\n\nPulled By: mrshenli\n\nfbshipit-source-id: 3d838754f6daabad5cd9fb8953e4360196d110bb", "pr_number": "51820", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/cuda/CUDAFuture.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "475278f1c0": {"title": "[FX] Make some modifications to limitation section (#51928)", "body": "Summary:\n![](https://i.imgur.com/P0Tq4xR.jpg)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51928\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26329664\n\nPulled By: Chillee\n\nfbshipit-source-id: 94fd7b03ca53f48b1e4633a462c6e02bb0fd2f3c", "pr_number": "51928", "files_changed": ["docs/source/fx.rst"], "labels": ["Merged", "cla signed"]}, "d5a9627f10": {"title": "[PyTorch] Re-order TensorImpl fields to save a word (#50920)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50920\n\nThere was a hole left after previous changes.\nghstack-source-id: 120714378\n\nTest Plan: static_assert still passes.\n\nReviewed By: ezyang\n\nDifferential Revision: D26008763\n\nfbshipit-source-id: c3830328835e28a0d06c833172ac60457049824b", "pr_number": "50920", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "d4e84b0c07": {"title": "[FX] Fix leaf modules in Transformer (#51998)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51998\n\nTest Plan: Imported from OSS\n\nReviewed By: Chillee\n\nDifferential Revision: D26352087\n\nPulled By: jamesr66a\n\nfbshipit-source-id: ad8abc6507d4ea95fd3c99b226d1b40c3e9e64cf", "pr_number": "51998", "files_changed": ["test/test_fx.py", "torch/fx/interpreter.py"], "labels": ["Merged", "cla signed", "fx"]}, "256f93fb0f": {"title": "[FX][EZ] Fix tuple type annotations (#52010)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52010\n\nTest Plan: Imported from OSS\n\nReviewed By: ansley\n\nDifferential Revision: D26355481\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 27bbc5d8949beb68663f2e1e7963bec9afbef0cc", "pr_number": "52010", "files_changed": ["test/test_fx.py", "torch/fx/interpreter.py"], "labels": ["Merged", "cla signed", "fx"]}, "d23cb94098": {"title": "[FX] Generalize dict key check in `create-arg` (#51927)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51927\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26329655\n\nPulled By: jamesr66a\n\nfbshipit-source-id: a15e7d9564551521af12a8fde1c7524856f0cbc2", "pr_number": "51927", "files_changed": ["test/test_fx.py", "torch/fx/proxy.py"], "labels": ["Merged", "cla signed", "fx"]}, "18e0a61388": {"title": "add more logging fields that can be set in construction time (#51260)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51260\n\nadd more logging fields to DDPLoggingData, including param stats, bucket stats, environment variables, nccl version, data type\nghstack-source-id: 121260224\n\nTest Plan: unit tests\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26118245\n\nfbshipit-source-id: ba48b7a11340bda1f5f3b24c8603545d346361e9", "pr_number": "51260", "files_changed": ["c10/util/Logging.h", "tools/build_variables.bzl", "torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/__init__.py", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5a9bac58be": {"title": "Automated submodule update: FBGEMM (#52014)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/884fb257abcb9d38fd3981e258d24614d70934b3\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52014\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: mrshenli\n\nDifferential Revision: D26357567\n\nfbshipit-source-id: a9f239c9d3273d04ee15fb052b2bf4f25477814b", "pr_number": "52014", "files_changed": ["third_party/fbgemm"], "labels": ["Merged", "cla signed", "open source"]}, "929b91a24d": {"title": "ns_eager: rename Logger I/O var names to logger_cls (#51359)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51359\n\n`Logger` is the name of the base Logger class.  It's confusing that\nit is also used as a variable name, which can represent this class\nor its subclasses.  Renaming to `logger_cls` to make it clearer.\n\nTest Plan:\n```\npython test/test_quantization.py TestEagerModeNumericSuite\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D26149577\n\nfbshipit-source-id: a9c12f9446f66e5c683ab054b2a94aeb0cf9cc8a", "pr_number": "51359", "files_changed": ["torch/quantization/_numeric_suite.py"], "labels": ["Merged", "cla signed"]}, "768662913a": {"title": "Migrate masked_fill__cuda to ATen (#51404)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/49543\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51404\n\nReviewed By: mrshenli\n\nDifferential Revision: D26329833\n\nPulled By: ngimel\n\nfbshipit-source-id: 510988888fad015239ab4766eb391a89b742130b", "pr_number": "51404", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LegacyDefinitions.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCTensorMasked.cuh", "aten/src/THC/THCTensorMath.h", "aten/src/THC/generated/THCTensorMaskedBFloat16.cu", "aten/src/THC/generated/THCTensorMaskedBool.cu", "aten/src/THC/generated/THCTensorMaskedByte.cu", "aten/src/THC/generated/THCTensorMaskedChar.cu", "aten/src/THC/generated/THCTensorMaskedDouble.cu", "aten/src/THC/generated/THCTensorMaskedFloat.cu", "aten/src/THC/generated/THCTensorMaskedHalf.cu", "aten/src/THC/generated/THCTensorMaskedInt.cu", "aten/src/THC/generated/THCTensorMaskedLong.cu", "aten/src/THC/generated/THCTensorMaskedShort.cu", "aten/src/THC/generic/THCTensorMasked.cu", "aten/src/THC/generic/THCTensorMasked.h", "test/test_torch.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "5f9fb93c14": {"title": "[model loading] Add max_batch_size override for batch size exploration", "body": "Summary: Currently batch_size is determined on modeling side. Add a flag caffe2_predictor_disagg_acc_max_batch_size_override to explore different batch_size during inference.\n\nTest Plan:\nreplayer test\nset caffe2_predictor_disagg_acc_max_batch_size_override=32 on both server and client side.\n\nReviewed By: khabinov\n\nDifferential Revision: D26318568\n\nfbshipit-source-id: 4fa79e2087a5f7f7670988aec7e5b41e63f9980b", "pr_number": null, "files_changed": ["caffe2/opt/custom/in_batch_broadcast.cc"], "labels": []}, "0620c96fd6": {"title": "Back out \"Revert D26009829: Optimize relu on cpu using clamp_min\" (#51819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51819\n\nOriginal commit changeset: 3e945b438fb8\n\nOne does not simply change the patterns of aten op calls\nghstack-source-id: 121379333\n\nTest Plan: CI\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26291736\n\nfbshipit-source-id: b819ac013c0438cc2f70daed7d7f2ef8fdc12ab7", "pr_number": "51819", "files_changed": ["aten/src/ATen/native/Activation.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "74082f0d6f": {"title": "[te][llvm] Generate arithmetic vs logical right shift as appropriate (#51749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51749\n\nFollowing in the mode of C++, we probably want to distinguish when\nit's appropriate to do arithmetic vs. logical right shift.\n\n> For negative a, the value of a >> b is implementation-defined (in most\n> implementations, this performs arithmetic right shift, so that the result\n> remains negative).\n\nIf you look at what clang does, if `a` is unsigned, a logical shift is\ngenerated; if signed, an arithmetic shift.  Let's do the same here.  This turns\nout to be useful for, e.g., implementing transcendental function\napproximations.\nghstack-source-id: 121366317\n\nTest Plan:\nAdded Byte (unsigned) and Char (signed) right-shift tests to\ntest_llvm.\n\nReviewed By: asuhan\n\nDifferential Revision: D26245856\n\nfbshipit-source-id: 260ee9bf4b032b9ce216f89acbc273cde0ed688c", "pr_number": "51749", "files_changed": ["test/cpp/tensorexpr/test_llvm.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ff73be7e45": {"title": "[te] Introduce likely/unlikely CompareSelect hint (#51751)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51751\n\nSimilar in spirit to the `__builtin_expect` C intrinsic, it's useful\nto be able to hint the expected branch direction in a tensor expression.  Using\nthis flag has a few effects on codegen:\n\n- The CompareSelect is generated using conditional branches, rather than selects\n- The conditional branches are strongly hinted (like, 100000:1) in the indicated direction\n- A vectorized hinted CompareSelect computes its condition in parallel with a\n  mask \"reduction\" (e.g. a bitcast from `<i1 x 8>` to `<i*>`).  In AVX terms\n  this sequence might look like:\n```\nvpcmpgtd %ymm0, %ymm1, %ymm2\nvmovmskps %ymm2, %eax\n```\n\nThe motivating case for this addition is an attempt I'm making to replicate\nfast transcendentals using tensor expressions.  Floating-point numbers have\nlots of special cases (denormals, inf, nan) that need special handling, and\nit's convenient to be able to punt that handling off to a slow path while\nkeeping the fast path nice and tight.\nghstack-source-id: 121366315\n\nTest Plan:\nI'm not sure how to test this (except I can tell you it works for\nthe `log` implementation I'm working on right now).  It would be nice to plumb\nthe LLIR/ASM output through programmatically so it can be used in FileCheck.\nMaybe I'll do that in another diff?\n\nReviewed By: asuhan\n\nDifferential Revision: D26246401\n\nfbshipit-source-id: 900f7fa0520010fb9931d6e3efc8680a51f8d844", "pr_number": "51751", "files_changed": ["torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "2e35fe9535": {"title": "[te] Implement log approximation using the VML approach (#51752)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51752\n\nUsing a straight power series approximation with enough terms gives\nprecision down to the denormal range, and avoids the fp division used in the\nsleef approach.  This is nice because recent CPUs have dual pipelined fma units,\nso we can compute 16 logarithms in parallel; whereas there's usually only one\nFP divider and it has a fairly high latency/low throughput.\nghstack-source-id: 121392347\n\nTest Plan:\nOn my avx2+fma broadwell:\n```\n---------------------------------------------------------------------------\nBenchmark                    Time           CPU Iterations UserCounters...\n---------------------------------------------------------------------------\nlog_nnc_sleef/64           178 ns        178 ns    3933565 log/s=358.993M/s\nlog_nnc_sleef/512         1286 ns       1285 ns     559459 log/s=398.354M/s\nlog_nnc_sleef/8192       19366 ns      19364 ns      36619 log/s=423.053M/s\nlog_nnc_sleef/32768      79288 ns      79286 ns       8718 log/s=413.287M/s\n\nlog_nnc_fast/64             92 ns         92 ns    7644990 log/s=696.939M/s\nlog_nnc_fast/512           483 ns        483 ns    1426802 log/s=1059.49M/s\nlog_nnc_fast/8192         7519 ns       7514 ns      95319 log/s=1090.23M/s\nlog_nnc_fast/32768       31344 ns      31338 ns      22397 log/s=1045.62M/s\n\nlog_nnc_vml/64              88 ns         88 ns    7923812 log/s=728.469M/s\nlog_nnc_vml/512            454 ns        454 ns    1521437 log/s=1.12739G/s\nlog_nnc_vml/8192          6763 ns       6763 ns     103264 log/s=1.21136G/s\nlog_nnc_vml/32768        26565 ns      26564 ns      23609 log/s=1.23354G/s\n\nlog_aten/64                418 ns        418 ns    1651401 log/s=153.117M/s\nlog_aten/512               801 ns        801 ns     875857 log/s=638.923M/s\nlog_aten/8192             6877 ns       6872 ns     100840 log/s=1.19208G/s\nlog_aten/32768           26989 ns      26988 ns      26268 log/s=1.21416G/s\n```\n\nReviewed By: bwasti, zheng-xq\n\nDifferential Revision: D26246400\n\nfbshipit-source-id: dae47ee6baeab1a813ec4d4440748164051aed3d", "pr_number": "51752", "files_changed": ["benchmarks/cpp/tensorexpr/bench_approx.cpp", "test/cpp/tensorexpr/test_approx.cpp", "torch/csrc/jit/tensorexpr/expr.cpp", "torch/csrc/jit/tensorexpr/expr.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "602434bcbe": {"title": "[te] Benchmark vml-based logit (#51771)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51771\n\nThis benchmarks an NNC implementation of logit based on VML's log\nimplementation.\n\nIt's a modest improvement over the sleef algorithm, but seems to be a bit\nslower than aten (at larger sizes), and I'm not totally sure why, since you'd\nthink a fused logit kernel would be better than doing clamp/sub/div, followed\nby log.  And yet...\n\nNote that it's important to vectorize this kernel by 16, even on an 8-wide AVX2\nmachine; I suspect that it's needed to give the scheduler enough freedom to\nfill up both FMA pipes to avoid stalling on fpdiv or (maybe) memory.\nghstack-source-id: 121392349\n\nTest Plan:\n```\n-----------------------------------------------------------------------------\nBenchmark                      Time           CPU Iterations UserCounters...\n-----------------------------------------------------------------------------\nlogit_nnc_sleef/64           483 ns        483 ns    1452336 logit/s=132.469M/s\nlogit_nnc_sleef/512         3019 ns       3019 ns     228059 logit/s=169.577M/s\nlogit_nnc_sleef/8192       71427 ns      71424 ns       9662 logit/s=114.695M/s\nlogit_nnc_sleef/32768     307062 ns     306722 ns       2406 logit/s=106.833M/s\n\nlogit_nnc_fast/64            147 ns        147 ns    4408910 logit/s=434.908M/s\nlogit_nnc_fast/512           781 ns        781 ns     881230 logit/s=655.53M/s\nlogit_nnc_fast/8192        12519 ns      12518 ns      55626 logit/s=654.421M/s\nlogit_nnc_fast/32768       50530 ns      50526 ns      10000 logit/s=648.536M/s\n\nlogit_nnc_vml/64             125 ns        125 ns    5551460 logit/s=511.603M/s\nlogit_nnc_vml/512            733 ns        733 ns     938444 logit/s=698.955M/s\nlogit_nnc_vml/8192         11282 ns      11280 ns      61610 logit/s=726.23M/s\nlogit_nnc_vml/32768        45051 ns      44991 ns      15473 logit/s=728.325M/s\n\nlogit_aten/64                450 ns        449 ns    1599269 logit/s=142.429M/s\nlogit_aten/512              1055 ns       1054 ns     665538 logit/s=485.595M/s\nlogit_aten/8192            10865 ns      10864 ns      64152 logit/s=754.032M/s\nlogit_aten/32768           42106 ns      42103 ns      16477 logit/s=778.287M/s\n\nlogit_caffe2/64              233 ns        233 ns    2952127 logit/s=274.761M/s\nlogit_caffe2/512            1795 ns       1795 ns     393354 logit/s=285.177M/s\nlogit_caffe2/8192          29924 ns      29923 ns      23225 logit/s=273.77M/s\nlogit_caffe2/32768        123899 ns     123893 ns       5642 logit/s=264.487M/s\n```\n\nReviewed By: bwasti\n\nDifferential Revision: D26272325\n\nfbshipit-source-id: b9771a96e0150685506dbc625e7894e81c93a688", "pr_number": "51771", "files_changed": ["benchmarks/cpp/tensorexpr/bench_approx.cpp"], "labels": ["Merged", "cla signed"]}, "9c0caf0384": {"title": "Adding support for comparing two bool varibales (#51844)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51844\n\nFixes issue #48174\n\n=========\n\nAdds support to compare two bool variables\n\nTest:\n======\npython test/test_jit.py -k test_compare_two_bool_inputs\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26353694\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 41af5ba3e4075ed7a21595b10e388a7302aa1fce", "pr_number": "51844", "files_changed": ["test/test_jit.py", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "594a66d778": {"title": "Warn about floor_divide performing incorrect rounding (#50281) (#50281)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50281\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51745\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nPulled By: mruberry\n\nDifferential Revision: D26257855\n\nfbshipit-source-id: e5d497cf07b0c746838ed081c5d0e82fb4cb701b", "pr_number": "50281", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "test/jit/test_save_load.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_binary_ufuncs.py", "test/test_jit.py", "test/test_ops.py", "test/test_sparse.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/csrc/jit/frontend/builtin_functions.cpp", "torch/nn/functional.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset7.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py", "torch/testing/__init__.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "3cf78395cb": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D26364039\n\nfbshipit-source-id: 750eb64b22cd84cf99d6595970c10f3aa6037f0b", "pr_number": null, "files_changed": ["torch/csrc/jit/frontend/sugared_value.cpp"], "labels": []}, "8b0cb5ede3": {"title": "OpInfo: Added clamp and trunc tests with aliases (#51167)", "body": "Summary:\nDescription:\n- Added clamp, trunc tests with aliases\n- Added tests for aliases for asin(h), acos(h), etc\n- fixed 'fix' alias implementation\n- fixed annotations in test_jit_alias_remapping\n- updated native_functions.yaml aliases guidelines\n\nBlocked by https://github.com/pytorch/pytorch/issues/50368\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51167\n\nReviewed By: gchanan\n\nDifferential Revision: D26245753\n\nPulled By: mruberry\n\nfbshipit-source-id: e17b657f0515139735a8a677b1ae284904f98aef", "pr_number": "51167", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_op_aliases.py", "test/test_ops.py", "test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c1b7ca8062": {"title": "Early terminate CUDA on common_utils TestCases (#50914)", "body": "Summary:\nThis is a follow up on https://github.com/pytorch/pytorch/issues/49869.\n\nPreviously CUDA early termination only happens for generic test classes that extends from `DeviceTypeTestBase`. However, JIT test cases which extends from common_utils.TestCase cannot benefit from the early termination.\n\nThis change moves the early termination logic into common_utils.TestCase class.\n- all tests extended from common_utils.TestCase now should early terminate if CUDA assert occurs.\n- For TestCases that extends from common_device_type.DeviceTypeTestBase, still only do torch.cuda.synchronize() when RTE is thrown.\n- For TestCases extends common_utils.TestCase, regardless of whether a test case uses GPU or not, it will always synchronize CUDA as long as `torch.cuda.is_initialize()` returns true.\n- Disabling this on common_distributed.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50914\n\nReviewed By: malfet\n\nDifferential Revision: D26019289\n\nPulled By: walterddr\n\nfbshipit-source-id: ddc7c1c0d00db4d073a6c8bc5b7733637a7e77d1", "pr_number": "50914", "files_changed": ["test/test_testing.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "Reverted", "cla signed", "oncall: distributed"]}, "50e6f0fdb6": {"title": "Add benchmark for torch.nn.functional.interpolate", "body": "Summary:\nThis diff adds a new microbencharmk for the\n `torch.nn.functional.interpolate` operator, using OpBench\n\nTest Plan:\n```\n[nicolashug@59262.od ~/fbsource/fbcode/caffe2/benchmarks/operator_benchmark/pt (39207820)]$ buck run //caffe2/benchmarks/operator_benchmark/pt:interpolate_test -- --tag_filter short\nStarting new Buck daemon...\nBuck daemon started.\nParsing buck files: finished in 06:30.7 min\nCreating action graph: finished in 33.9 sec\nBuilding: finished in 02:53.4 min (100%) 24224/24224 jobs, 24224 updated\n  Total time: 09:58.2 min\n/data/sandcastle/boxes/fbsource/fbcode/buck-out/dev/gen/caffe2/benchmarks/operator_benchmark/pt/interpolate_test#link-tree/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True\nForward Execution Time (us) : 510.818\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False\nForward Execution Time (us) : 684.324\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue\n# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True\nForward Execution Time (us) : 33791.970\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse\n# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False\nForward Execution Time (us) : 50120.585\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue\n# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True\nForward Execution Time (us) : 37668.089\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse\n# Input: input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False\nForward Execution Time (us) : 56869.472\n```\n\nReviewed By: fmassa\n\nDifferential Revision: D26225318\n\nfbshipit-source-id: 7757296192e630c42a6e4913c5c1d93af11d286d", "pr_number": null, "files_changed": ["benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/pt/interpolate_test.py"], "labels": []}, "c4a8f0ceaa": {"title": "[torch script] Add pure list producing ops to alias analysis (#51999)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51999\n\nas in title\n\nTest Plan: waiting on CI for now\n\nReviewed By: eellison\n\nDifferential Revision: D26349297\n\nfbshipit-source-id: bd5574ed1f8448ba18a6fda4bdc45f45d8b158e9", "pr_number": "51999", "files_changed": ["torch/csrc/jit/ir/alias_analysis.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "03e82f7944": {"title": "Use CUDA 11.2 for nightly docker build. (#51990)", "body": "Summary:\nSet CUDA_VERSION to 11.2.0 since Nvidia name their docker image on Ubuntu 18.04 to be nvidia/cuda:11.2.0-cudnn8-devel-ubuntu18.04.\n\nNote that cudatoolkit 11.2.0 is not yet on [conda](https://repo.anaconda.com/pkgs/main/linux-64/), and we need to wait for that before merging this PR.\n\n- https://hub.docker.com/r/nvidia/cuda/\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51990\n\nReviewed By: samestep\n\nDifferential Revision: D26371193\n\nPulled By: xuzhao9\n\nfbshipit-source-id: 76915490dc30ddb03ceeeadb3c45a6c02b60401e", "pr_number": "51990", "files_changed": [".github/scripts/build_publish_nightly_docker.sh", "Dockerfile", "docker.Makefile"], "labels": ["Merged", "cla signed"]}, "b7b944a319": {"title": "Avoid TensorPipe agent spamming logs when unable to guess IP address (#51784)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51784\n\nThe TensorPipe agent mimics Gloo when trying to guess the most reasonable IP address to bind to. When that fails, it prints a warning to inform the user. It turns out, we were attempting to guess the address a lot of times (I counted at least 18: 1 for the UV transport, 1 for the IBV transport, 16 for the multiplexed UV channel) and thus they might all print that same identical warning message. That's noisy. Since the outcome of all these guesses will be the same (unless the system config changes underneath, which is unlikely) we can just do it once, print the warning (at most) once, cache the result and reuse it over and over.\n\nAlso, we used to have two identical but distinct ways of doing this, one provided by the UV transport and one by the IBV one. TensorPipe offers both methods because backends are modular and independent. However PyTorch always requires the UV one to be present, hence we can always rely on the UV helpers, and avoid using the IBV ones.\nghstack-source-id: 121121275\n\nTest Plan: Look at the CircleCI logs, I think I saw this situation happening there.\n\nReviewed By: mrshenli\n\nDifferential Revision: D26275838\n\nfbshipit-source-id: 8a2ffc40d80388bdca32dbcfed16f28a0a6177a3", "pr_number": "51784", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a1c67b0763": {"title": "Silence harmless error logs of TensorPipe agent during shutdown (#51785)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51785\n\nThe TensorPipe pipes do not really support a \"graceful\" shutdown: if one side is expecting data (i.e., it has scheduled a readDescriptor call) and the other side closes, the former will receive an error. Such an error will not even be predictable, as it depends on the backend: some may detect this and report it \"well\" (through an EOFError), others may not be able to tell this apart from a failure and report it as such.\n\nThis meant that during shutdown some of these errors would fire and thus the agent would log them as warning. We did add a note that these were expected under some conditions, so that users wouldn't be alarmed, but it was still a far-from-ideal experience.\n\nIn principle we could build a \"protocol\" on top of these pipes to \"agree\" on a graceful shutdown, and this was the plan to solve this. However, it was rather complicated to implement.\n\nHere I am proposing a quicker, but perhaps hackier, solution, which re-uses the already existing graceful shutdown \"protocol\" of the agent (i.e., the `join` method) to put the agent in a special state in which it will silence all errors due to a remote shutting down.\n\nSuch a check cannot happen in the `shutdown` method, because that's also used in case of ungraceful shutdown (in which case I believe we'd still want to display errors). Since it needs to make sure that all participants have transitioned to this new state before any of them can continue (as otherwise one of them may close its pipes before another one has realized that this is now expected), we need to perform a barrier. Hence the ideal place for it is the `join` method, where we're already doing a lot of gang-wide synchronization. Since the `join` method isn't only called during shutdown, we need to make sure we only switch the agent to this state when it's the last call to join, and we do so by adding a new optional argument to it (which will be ignored by all agents except the TensorPipe one).\n\nI realize this isn't the prettiest solution, and since it changes the agent's API it's worth discussing it carefully. Let me know what you think!\nghstack-source-id: 121131940\n\nTest Plan: Run on CircleCI, where this occurred quite a bit, and check the logs.\n\nReviewed By: mrshenli\n\nDifferential Revision: D26276137\n\nfbshipit-source-id: 69ef14fe10908e80e627d9b4505352e482089cc8", "pr_number": "51785", "files_changed": ["torch/_C/_distributed_rpc.pyi", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/testing/init.cpp", "torch/distributed/rpc/api.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ce8ba5f3bc": {"title": "Fix test time history report if no ancestor report (#52054)", "body": "Summary:\nThis fixes an issue (currently blocking https://github.com/pytorch/pytorch/issues/51905) where the test time regression reporting step will fail if none of the most recent `master` ancestors have any reports in S3 (e.g. if a new job is added).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52054\n\nTest Plan:\n```\npython test/test_testing.py\n```\n\nReviewed By: walterddr\n\nDifferential Revision: D26369507\n\nPulled By: samestep\n\nfbshipit-source-id: 4c4e1e290cb943ce8fcdadacbf51d66b31c3262a", "pr_number": "52054", "files_changed": ["test/test_testing.py", "torch/testing/_internal/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "bd6248106b": {"title": "Keep alive graph when creating iterators from it (#51951)", "body": "Summary:\nPreviously, the graph might have been delete while Python still has iterators, leading to segfaults.\n\nThis does not fully work for iterators from Nodes and Blocks as they may be invalidated when the owning graph goes out of scope. I will look into these separately.\n\nFixes https://github.com/pytorch/pytorch/issues/50454\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51951\n\nReviewed By: mrshenli\n\nDifferential Revision: D26352629\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 67299b6cbf1ac7ab77f8703a0ca8f1162e03fcd4", "pr_number": "51951", "files_changed": ["test/jit/test_python_bindings.py", "torch/csrc/jit/python/python_ir.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "9b8d414a9c": {"title": "update sccache wrapper to accommodate new sccache for macos build (#51357)", "body": "Summary:\nBefore I change sccache to point to the newer version in the S3 bucket, this PR makes sure the new sccache wrapper works.\n\nThis PR previously tested a newer version of sccache for macos build jobs. Last sccache used is over a year old. The results of using both are different, but the speed isn't too impacted, see below.\n\nWith newer sccache and alternate wrapper script from this PR: https://app.circleci.com/pipelines/github/pytorch/pytorch/271777/workflows/b5c6a75e-781a-4c0f-8c99-ff2cbe1e877c/jobs/10808567\n\nWith old sccache: https://app.circleci.com/pipelines/github/pytorch/pytorch/271875/workflows/962079ce-e146-482e-b493-c99004f8d89a/jobs/10805680\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51357\n\nReviewed By: walterddr\n\nDifferential Revision: D26373266\n\nPulled By: janeyx99\n\nfbshipit-source-id: ac5ccc512039379af6111b92a5ce37c5268dfdfe", "pr_number": "51357", "files_changed": [".jenkins/pytorch/macos-build.sh"], "labels": ["Merged", "cla signed"]}, "a1b8f3d4b6": {"title": "Replace CUDA 11.1 Linux CI with CUDA 11.2 (#51905)", "body": "Summary:\nAdding 11.2 to CI with BUILD_SPLIT_CUDA enabled.\n\nDisabled the following tests as they were failing in test_optim.py:\ntest_adadelta\ntest_adam\ntest_adamw\ntest_multi_tensor_optimizers\ntest_rmsprop\n\n(Issue tracking that is here: https://github.com/pytorch/pytorch/issues/51992)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51905\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26368575\n\nPulled By: janeyx99\n\nfbshipit-source-id: 31612c7d04d51afb3f18956e43dc7f7db8a91749", "pr_number": "51905", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".jenkins/pytorch/test.sh", "test/test_optim.py"], "labels": ["Merged", "ci/all", "cla signed"]}, "d0fd41dcfe": {"title": "Add size op in nnapi serializer (#52026)", "body": "Summary:\nserializer didn't support aten::size\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52026\n\nTest Plan: Torchvision Mobilenetv2 [script](https://pytorch.org/tutorials/prototype/nnapi_mobilenetv2.html) works. [Test](https://github.com/pytorch/pytorch/commit/ecfed07cc599b7c95e98d692984d47cbee769a85) to be merged after [this PR](https://github.com/pytorch/pytorch/pull/47521/files) is merged\n\nReviewed By: dreiss\n\nDifferential Revision: D26363133\n\nPulled By: axitkhurana\n\nfbshipit-source-id: 772a6bea62bca69f8bba19c25c582a1734a70eb1", "pr_number": "52026", "files_changed": ["torch/backends/_nnapi/serializer.py"], "labels": ["Merged", "cla signed"]}, "9f1f5636d7": {"title": "Revert D26019289: [pytorch][PR] Early terminate CUDA on common_utils TestCases", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26019289 (https://github.com/pytorch/pytorch/commit/c1b7ca80627896f8fd54791f10c245f5c4ac81c1)\n\nOriginal commit changeset: ddc7c1c0d00d\n\nfbshipit-source-id: 6902d03fa06cda5d03191846bc4dd98af501b594", "pr_number": null, "files_changed": ["test/test_testing.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_utils.py"], "labels": []}, "410ef1335a": {"title": "[JIT] Add buffer/parameter metadata test to test_save_load.py (#49594)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49594\n\n**Summary**\nThis commit adds a unit test to `test_save_load.py` that checks that\nsaving and loading a module preserves metadata about which module\nattributes are parameters and buffers. The hooks that are currently used\nto automatically check serialization of every function and module in the\nunit tests check that the archive produced by saving and loading and\nsaving again are the same and that the type tags for the actual IValues\nrepresenting the module match before saving and after loading. However,\nthese tests do not check that buffer and parameter metadata was not\nlost or destroyed during serialization.\n\n**Test Plan**\nRan the new unit test.\n\nTest Plan: Imported from OSS\n\nReviewed By: xw285cornell\n\nDifferential Revision: D25730603\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 06a202935d9e0654cb1966c34f54707f0a28a331", "pr_number": "49594", "files_changed": ["test/jit/test_save_load.py"], "labels": ["cla signed", "oncall: jit"]}, "5431d87c3e": {"title": "[JIT] Use `is_buffer` in `BufferPolicy::valid` (#49588)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49588\n\n**Summary**\n`BufferPolicy::valid` uses `!typ->is_parameter(i)` to check if an\nattribute is a buffer or not; it should use `type->is_buffer(i)` instead.\nIt also removes a forward compatibility gate in `python_print.cpp` that\nhas prevented the preservation of buffer metadata during serialization\nin fbcode. Without this, the first change (to `BufferPolicy`) does not\nwork correctly in fbcode.\n\n**Test Plan**\nIt is difficult to write an additional test that would have failed before this\ncommit because the two booleans `is_parameter` and `is_buffer` are never set\nto `true` at the same time.\n\n**Fixes**\nThis commit fixes #48746.\n\nTest Plan: Imported from OSS\n\nReviewed By: xw285cornell\n\nDifferential Revision: D25633250\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: e727f8506f16d2e2b28f3d76a655f6528e7ac6cb", "pr_number": "49588", "files_changed": ["torch/csrc/jit/api/module.h"], "labels": ["cla signed", "oncall: jit"]}, "bff8194522": {"title": "Replace 11.1 with 11.2 on CI for Windows (#51598)", "body": "Summary:\nAdding CUDA 11.2 to Windows CI.\n\nDisabled tests:\n\nThe following ran into `CUDA error: misaligned address` for CUDA 11.2: (issue linked below)\n`test_where_scalar_valid_combination_cuda_complex128` in test_torch.py\n`test_sgn_complex_cuda` in test_autograd.py\n\nThe following ran into `CUDA error: too many resources requested for launch` for CUDA 11.2: (https://github.com/pytorch/pytorch/issues/52002)\ntest_EmbeddingBag_per_sample_weights_and_new_offsets_cuda_int64_float64\ntest_EmbeddingBag_per_sample_weights_and_offsets_cuda_int64_float64\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51598\n\nReviewed By: mrshenli\n\nDifferential Revision: D26344965\n\nPulled By: janeyx99\n\nfbshipit-source-id: 3c9a4ed16d748969e96593220ec0a9f33e1ffcef", "pr_number": "51598", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/windows_cuda_install.sh", ".circleci/scripts/windows_cudnn_install.sh", ".jenkins/pytorch/win-test.sh", "test/test_autograd.py", "test/test_nn.py", "test/test_torch.py"], "labels": ["ci/all"]}, "fa325d7c9f": {"title": "Use `sum_integers` and `multiply_integers` (#51146)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51146\n\nTest Plan: Sandcastle tests\n\nReviewed By: ngimel\n\nDifferential Revision: D25903430\n\nfbshipit-source-id: 329c14018c9e5192864eed88a8ed0a5068ff1c69", "pr_number": "51146", "files_changed": ["aten/src/ATen/BatchedFallback.cpp", "aten/src/ATen/TensorUtils.cpp", "aten/src/ATen/Utils.cpp", "aten/src/ATen/Utils.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/Fill.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/Resize.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/cuda/CuFFTPlanCache.h", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/ATen/native/cuda/ScanKernels.cu", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cudnn/RNN.cpp", "aten/src/ATen/native/group_norm.cpp", "aten/src/ATen/native/layer_norm.h", "aten/src/ATen/native/metal/MetalPrepackOpRegister.cpp", "aten/src/ATen/native/metal/MetalTensor.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/MPSImage+Tensor.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm", "aten/src/ATen/native/mkl/SpectralOps.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/quantized/cpu/qnormalization.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/utils/Factory.cpp", "aten/src/ATen/native/vulkan/Vulkan.cpp", "aten/src/ATen/native/vulkan/VulkanOps.cpp", "aten/src/ATen/native/vulkan/ops/Tensor.cpp", "aten/src/ATen/native/vulkan/ops/Tensor.h", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/test/scalar_tensor_test.cpp", "c10/core/TensorImpl.h", "caffe2/contrib/fakelowp/batch_matmul_fp16_fake_op.h", "caffe2/contrib/tensorrt/tensorrt_op_trt.cc", "caffe2/core/qtensor.h", "caffe2/operators/cosh_op.cu", "caffe2/operators/elementwise_mul_gradient_op.cc", "caffe2/operators/pool_op.cu", "caffe2/operators/reduce_ops.cc", "caffe2/utils/math/reduce.cc", "caffe2/utils/math/utils.cc", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/jit/passes/frozen_conv_folding.cpp", "torch/csrc/jit/serialization/export.cpp", "torch/lib/c10d/Utils.hpp"], "labels": ["cla signed", "fb-exported", "oncall: distributed", "oncall: jit"]}, "4add8502c3": {"title": "inlining a function that i noticed were hot during previous benchmarking (#50848)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50848\n\nI noticed that the call overhead from `Tensor::device()` for ~1-2% of instruction counts depending on the microbenchmark\n\nSome nice looking instruction count wins https://www.internalfb.com/intern/paste/P164529004/\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D25984136\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 0e54f2afe78caeb5a03abbb15e9197556acfeca1", "pr_number": "50848", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.cpp", "c10/util/FunctionRef.h"], "labels": ["cla signed"]}, "705fa7e964": {"title": "[Usability] Capture argument names for traced functions and modules (#51775)", "body": "Summary:\nPreviously `torch.jit.trace` relies on AutoGrad hooks to infer name of tensors in computation, including those of function/method arguments. This often doesn't work out because:\n\n- These names often do not exist\n- Tracer uses argument name of first tensor operation on each tensor as inferred argument names. These tensor operations have programmatically-generated names like `argument_1`\n\nThis PR extracts argument names directly from Python functions and pass them down to tracer, which then assigns them to correct graph inputs. This way, we always have the correct argument names captured in IR.\n\nThis is useful for both debugging and supporting using `InterfaceType` to represent traced modules.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51775\n\nReviewed By: izdeby\n\nDifferential Revision: D26273105\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 934a385041137dc3731bb6fa8657b11532fed9e5", "pr_number": "51775", "files_changed": ["test/expect/TestTensorBoard.test_pytorch_graph.expect", "test/jit/test_jit_utils.py", "test/jit/test_tracer.py", "test/test_jit.py", "torch/_C/__init__.pyi.in", "torch/_jit_internal.py", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tracer.h", "torch/csrc/jit/python/python_tracer.cpp", "torch/csrc/jit/python/python_tracer.h", "torch/csrc/jit/python/script_init.cpp", "torch/jit/_trace.py", "torch/testing/_internal/jit_utils.py"], "labels": ["cla signed", "oncall: jit"]}, "de334e6a2f": {"title": "fast-path is_complex() in the dispatcher (#50054)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50054\n\nTest Plan: Imported from OSS\n\nReviewed By: swolchok\n\nDifferential Revision: D25760987\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 24666d3b86df6799ebbc478fdcdcaa445daff439", "pr_number": "50054", "files_changed": ["aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h"], "labels": ["cla signed", "module: complex"]}, "d7ea0fe75a": {"title": "[testing] Add OpInfo for rad2deg and deg2rad (#51283)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/50006\n\nWe should probably add aliases for these operators to be consistent with NumPy names i.e. `np.degrees` and `np.radians`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51283\n\nReviewed By: ngimel\n\nDifferential Revision: D26171163\n\nPulled By: mruberry\n\nfbshipit-source-id: 1869604ed400820d95f6ff50a0e3cba1de1ffa84", "pr_number": "51283", "files_changed": ["test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "open source"]}, "faaff0cd9b": {"title": "[caffe2 and pytorch] use new sparse adagrad JIT'ed function in fbgemm", "body": "Summary: To consider small delay between fbgemm and caffe2/pytorch repo, we are taking multiple steps. In this diff, we use new interface with temp name.\n\nTest Plan: CI\n\nReviewed By: dskhudia\n\nDifferential Revision: D26231909\n\nfbshipit-source-id: 83ceb3e12026d459532ef54459ac125b5625d644", "pr_number": null, "files_changed": ["caffe2/sgd/adagrad_op.h"], "labels": []}, "9653161fb4": {"title": "bump nightlies to 1.9.0 (#51891)", "body": "Summary:\nsimilar to https://github.com/pytorch/pytorch/pull/45696\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51891\n\nReviewed By: izdeby\n\nDifferential Revision: D26318646\n\nPulled By: seemethere\n\nfbshipit-source-id: 757194845c758a24eed2d0550866ba890e7a0b58", "pr_number": "51891", "files_changed": [".circleci/scripts/binary_populate_env.sh", "android/README.md", "android/test_app/app/build.gradle", "torch/csrc/onnx/onnx.h", "version.txt"], "labels": ["Merged", "cla signed", "module: ci", "releng"]}, "39aa3db62b": {"title": "use make_shared and make_unique and clean unneeded code (#51829)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51829\n\nReviewed By: izdeby\n\nDifferential Revision: D26306098\n\nPulled By: smessmer\n\nfbshipit-source-id: 4f6c0469c68f044c0bfe0925fcf7b030a25d15e2", "pr_number": "51829", "files_changed": ["caffe2/core/nomnigraph/Representations/NeuralNet.cc", "caffe2/core/nomnigraph/include/nomnigraph/Representations/ControlFlow.h", "caffe2/core/nomnigraph/include/nomnigraph/Representations/NeuralNet.h", "caffe2/core/nomnigraph/include/nomnigraph/Support/Pointer.h", "caffe2/core/nomnigraph/include/nomnigraph/Transformations/SubgraphMatcher.h", "caffe2/core/nomnigraph/tests/NeuralNetTest.cc", "caffe2/core/stats.cc", "caffe2/opt/converter.cc", "caffe2/opt/converter.h", "caffe2/opt/custom/converter.cc", "caffe2/opt/device.cc", "caffe2/opt/device_test.cc", "caffe2/opt/distributed.cc", "caffe2/python/pybind_state_nomni.cc"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "4ab0ef36a4": {"title": "change back to multiple_outputs_gpu_kernel for learnable fake per-channel quantization (#52017)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52017\n\nChange back to multiple_outputs_gpu_kernel for per-channel quantization backward c++/cuda implementations (for diff D24479735 (https://github.com/pytorch/pytorch/commit/0c60922fb0614132433779ad45ab8f30783db2ae))\nghstack-source-id: 121409281\n\nTest Plan:\n## Unit Test:\n`buck test mode/dev-nosan -c fbcode.platform=platform009 //caffe2/test:quantization -- -v TestFakeQuantize`\n\n## Benchmark Test: (checkout f3980d1d678e)\n`buck run mode/opt //caffe2/benchmarks/operator_benchmark/pt:quantization_test -- --operators FakeQuantizePerTensorOpBenchmark`\n\n`buck run mode/opt //caffe2/benchmarks/operator_benchmark/pt:quantization_test -- --operators FakeQuantizePerChannelOpBenchmark`\n\n### In **microseconds** (`1e-6` second),\ninput size: [1, 3, 256, 256]\n|                           | C++ Kernel | Non-backprop C++ Kernel |\n|---------------------------|---------------|------------|-------------------------|---|\n| Per Tensor CPU Forward    | 1372.123                | 1365.981 |\n| Per Tensor Cuda Forward   | 84.586                 | 27.205|\n| Per Channel CPU Forward   | 2306.668                | 2299.991|\n| Per Channel Cuda Forward  | 154.742                 | 135.219 |\n| Per Tensor CPU Backward   | 2544.617               | 581.268|\n| Per Tensor Cuda Backward   | 304.529                 | 137.335|\n| Per Channel CPU Backward   | 2582.783               |582.088 |\n| Per Channel Cuda Backward  | 474.265                | 134.082|\n\ninput size: [1, 3, 512, 512]\n\n|                           | C++ Kernel | Non-backprop C++ Kernel |\n|---------------------------|---------------|------------|-------------------------|---|\n| Per Tensor CPU Forward    | 5426.244                | 5726.440 |\n| Per Tensor Cuda Forward   | 85.834                 | 26.871|\n| Per Channel CPU Forward   | 9125.913                | 9118.152|\n| Per Channel Cuda Forward  | 159.599                 | 145.117 |\n| Per Tensor CPU Backward   | 14020.830               | 2214.864|\n| Per Tensor Cuda Backward  | 285.525                 | 131.302|\n| Per Channel CPU Backward  | 14801.976               |2104.345 |\n| Per Channel Cuda Backward | 513.025                | 120.222|\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26357325\n\nfbshipit-source-id: f42e3803258b0f6b418eab1301b5e5a466671859", "pr_number": "52017", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_affine.h", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp"], "labels": ["Merged", "cla signed"]}, "fd41ed1cce": {"title": "Fix flaky TestTrainingLoop - TestE2ETensorPipe (#51939)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51939\n\nTestTrainingLoop - TestE2ETensorPipe was flaky since there would still\nbe inflight background RPCs running as we performed the assertions. This\nresulted in these assertions failing since we didn't wait for all RPCs on the\nagent to finish.\n\nTo resolve this issue, in this PR we join() and shutdown() the RPC agent to\nensure no further RPCs are done. Then we assertion the map sizes to ensure no\nleaks occurred.\n\nIn addition to this, added messageIdToTimeout map to lookup the appropriate\ntimeout for a messageId. This ensures we remove the appropriate entry from the\nmap. The previous solution was passing the expirationTime through the lambda,\nbut it is not guaranteed the lambda would read the response of the request we\njust sent out.\nghstack-source-id: 121412604\n\nTest Plan:\n1) unit tests\n2) waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26331585\n\nfbshipit-source-id: a41e0534d7d4dfd240446e661e5541311931c7d7", "pr_number": "51939", "files_changed": ["test/cpp/rpc/test_e2e_tensorpipe.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "bc856b49d4": {"title": "Add support for constants to fx_glow (#52094)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52094\n\nPull Request resolved: https://github.com/pytorch/glow/pull/5329\n\nNested constants are created as placeholders by the graph_splitter used in the partitioner. So we change them back to get_attr nodes before serializing the graph.\n\nReviewed By: jfix71\n\nDifferential Revision: D26375577\n\nfbshipit-source-id: 66631aadd6f5b8826ffa0a1e70176fbcaa7431d5", "pr_number": "52094", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "7763c127cd": {"title": "[PyTorch] move aten::dict to lite interpreter (#52032)", "body": "Summary:\nAs title, this operator is needed by [DeepLabV3 model](https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html) used in Image Segmentation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52032\n\nTest Plan:\nImported from OSS\n\n1. CI\n2. Get the pr (https://github.com/pytorch/pytorch/pull/51419), build pytorch_android (`BUILD_LITE_INTERPRETER=1  ./scripts/build_pytorch_android.sh x86`), run ImageSegmentation app on emulator.\n{F371671630}\n\nReviewed By: dhruvbird\n\nDifferential Revision: D26365389\n\nPulled By: cccclai\n\nfbshipit-source-id: bd4c2bd2be83ed6bd3a4cd35eddb98c11a20e245", "pr_number": "52032", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "fb2693a632": {"title": "Use bool/float instead of np.bool/np.float (#52103)", "body": "Summary:\nThis is causing type hint test errors on the latest numpy:\n\n```\ntorch/testing/_internal/common_quantized.py:38: error: Module has no attribute \"float\"; maybe \"float_\", \"cfloat\", or \"float64\"?  [attr-defined]\ntorch/testing/_internal/common_methods_invocations.py:758: error: Module has no attribute \"bool\"; maybe \"bool_\" or \"bool8\"?  [attr-defined]\n```\n\nRuntime-wise, there's also a deprecation warning:\n\n```\n__main__:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n```\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52103\n\nReviewed By: suo\n\nDifferential Revision: D26401210\n\nPulled By: albanD\n\nfbshipit-source-id: a7cc12ca402c6645473c98cfc82caccf161160c9", "pr_number": "52103", "files_changed": ["torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_quantized.py"], "labels": ["Merged", "cla signed", "open source"]}, "0de7a4582e": {"title": "Fix Pytorch docker image name by adding the registry prefix (#52089)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52089\n\nTest Plan:\nManually trigger the CI\n\nFixes the [nightly docker pipeline failure](https://github.com/pytorch/pytorch/actions?query=workflow%3A%22Build+PyTorch+nightly+Docker+image+and+push+to+GitHub+Container+Registry%22)\n\nReviewed By: albanD\n\nDifferential Revision: D26390660\n\nPulled By: xuzhao9\n\nfbshipit-source-id: 5259fe35ffd154fc6684753f358ec5a63f31428f", "pr_number": "52089", "files_changed": [".github/scripts/build_publish_nightly_docker.sh", ".github/workflows/push_nightly_docker_ghcr.yml"], "labels": ["Merged", "cla signed"]}, "aa2fede201": {"title": "Fix autograd when `inputs` contains tensors without materialized grad_fn (#51940)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/39784\nAt the time the issue was filed, there was only issue (1) below.\n\nThere are actually now two issues here:\n1. We always set all inputs passed in through `inputs` arg as `needed = True` in exec_info. So if we pass in an input that has a grad_fn that is not materialized, we create an entry of exec_info with nullptr as key with `needed = True`. Coincidentally, when we perform simple arithmetic operations, such as \"2 * x\", one of the next edges of mul is an invalid edge, meaning that its grad_fn is also nullptr. This causes the discovery algorithm to set all grad_fns that have a path to this invalid_edge as `needed = True`.\n2. Before the commit that enabled the engine skipped the dummy node, we knew that root node is always needed, i.e., we hardcode `exec_info[&graph_root]=true`. The issue was that this logic wasn't updated after the code was updated to skip the graph root.\n\nTo address (1), instead of passing in an invalid edge if an input in `inputs` has no grad_fn, we create a dummy grad_fn. This is done in both python and cpp entry points. The alternative is to add logic for both backward() and grad() cases to check whether the grad_fn is nullptr and set needed=false in that case (the .grad() case would be slightly more complicated than the .backward() case here).\n\nFor (2), we perform one final iteration of the discovery algorithm so that we really know whether we need to execute the graph root.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51940\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26369529\n\nPulled By: soulitzer\n\nfbshipit-source-id: 14a01ae7988a8de621b967a31564ce1d7a00084e", "pr_number": "51940", "files_changed": ["test/cpp/api/autograd.cpp", "test/test_autograd.py", "torch/csrc/autograd/autograd.cpp", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/python_engine.cpp"], "labels": ["Merged", "cla signed"]}, "425a5dc3f7": {"title": "[DataLoader] Modify SamplerIDP signature (#52104)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52104\n\nMake the API of `SamplerIterDataPipe` more reasonable with `sampler_args` and `sampler_kwargs`.\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26401494\n\nPulled By: ejguan\n\nfbshipit-source-id: ee5b5c414782d0880b12968bc9c8aa470b753f6a", "pr_number": "52104", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/combinatorics.py"], "labels": ["Merged", "cla signed"]}, "10d407647f": {"title": "[PyTorch] Reduce template expansion in call_functor_with_args_from_stack (#51313)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51313\n\nThe problem here is similar to the one described in\nhttps://devblogs.microsoft.com/cppblog/build-throughput-series-more-efficient-template-metaprogramming/\nin that we are iterating over an integer seqeunce of length N, where N\nis the number of argument types to our function, and specializing\n`TypeListAt` (which we call `element_t`) for each Ith element of the\ntypelist, which instantiates O(I) template specializations, for a\ntotal of O(N^2).\n\nThe solution is also similar: we iterate over the typelist\ndirectly. Unlike in the blog post, we do also need the index in the\nsequence, so we retain the index_sequence.\nghstack-source-id: 121363464\n\nTest Plan:\nInspect -ftime-trace output for RegisterCPU.cpp.\n\nBefore: P168220187\nAfter: P168220294\n\nwe can see that we spend less time instantiating\ncall_functor_with_args_from_stack and spend a similar amount of time\ncompiling it. The win is modest, but it's a win and I've already\nwritten it so I'm sending it out. (I was hoping it would reduce\ncompilation time for make_boxed_from_unboxed_functor.)\n\nReviewed By: bhosmer\n\nDifferential Revision: D26136784\n\nfbshipit-source-id: c91a523486e3019bd21dcd03e51a58aa25aa0981", "pr_number": "51313", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h"], "labels": ["Merged", "cla signed"]}, "c931c29120": {"title": "[PyTorch][easy] Fix TODOs in CppFunction constructors (#51315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51315\n\nThe TODOs said to remove this wrapper, and it seems that it can be removed easily.\nghstack-source-id: 121363465\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D26137147\n\nfbshipit-source-id: f1e5971dca071f37400d77cc823214527e4231bc", "pr_number": "51315", "files_changed": ["torch/library.h"], "labels": ["Merged", "cla signed"]}, "c4eb22009e": {"title": "Drop some Python 2 compatibility code (#51769)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51769\n\nRemove some Python 2 compatibility code that otherwise causes errors to\nbe reported from static type checkers.\n\nStatic type checkers complain that the old Python 2 modules and\nfunctions referenced by this code do not exist.  Given that Python 2\nsupport is entirely deprecated now we can simply remove the\ncompatibility code.\nghstack-source-id: 121313191\n\nTest Plan:\nWas able to get Pyre to successfully type check the `caffe2/python`\ndirectory with this and some other changes.\n\nReviewed By: Tianshu-Bao\n\nDifferential Revision: D26271723\n\nPulled By: simpkins\n\nfbshipit-source-id: fec8a09466be6867388832380480aafd36616aa1", "pr_number": "51769", "files_changed": ["caffe2/python/data_workers.py", "caffe2/python/hypothesis_test.py", "caffe2/python/models/download.py", "caffe2/python/models/seq2seq/translate.py"], "labels": ["Merged", "cla signed"]}, "81b9aa743b": {"title": "[pytorch] Update caffe2/python to eliminate Pyre errors (#52083)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52083\n\nThis makes minor fixes in `caffe2/python` to address all errors currently\nreported by Pyre.\n\nI update the code to fix errors when doing so looked simple and safe,\nand added `pyre-fixme` comments in other places.\nghstack-source-id: 121109695\n\nTest Plan: Confirmed that Pyre no longer reports errors under `caffe2/python`\n\nDifferential Revision: D26272279\n\nfbshipit-source-id: b1eb19d323b613f23280ce9c71e800e874ca1162", "pr_number": "52083", "files_changed": ["caffe2/python/brew.py", "caffe2/python/gradient_check_test.py", "caffe2/python/layer_test_util.py", "caffe2/python/layers/tags.py", "caffe2/python/mint/app.py", "caffe2/python/models/resnet_test.py", "caffe2/python/models/shufflenet_test.py", "caffe2/python/operator_test/layer_norm_op_test.py", "caffe2/python/operator_test/mpi_test.py", "caffe2/python/operator_test/scale_op_test.py", "caffe2/python/predictor/predictor_exporter.py", "caffe2/python/schema.py", "caffe2/python/workspace.py", "caffe2/python/workspace_test.py"], "labels": ["Merged", "cla signed"]}, "0bc7b9843b": {"title": "use sccache 2.15 over the outdated sccache (#52095)", "body": "Summary:\nChange macos build job on CI to use newer sccache.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52095\n\nReviewed By: walterddr\n\nDifferential Revision: D26406024\n\nPulled By: janeyx99\n\nfbshipit-source-id: a40da4acd4c01af16d30269e67c7015aff54503a", "pr_number": "52095", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["Merged", "cla signed"]}, "497b772547": {"title": "Add custom implementation for `csqrt` if libc++ is used (#52018)", "body": "Summary:\nlibc++ implements csqrt using polar form of the number, which results in higher numerical error, if `arg` is close to 0, pi/2, pi, 3pi/4\n\nFixes https://github.com/pytorch/pytorch/issues/47500\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52018\n\nReviewed By: walterddr\n\nDifferential Revision: D26359947\n\nPulled By: malfet\n\nfbshipit-source-id: 8c9f4dc45948cb29c43230dcee9b030c2642d981", "pr_number": "52018", "files_changed": ["c10/test/util/complex_test_common.h", "c10/util/complex_math.cpp", "c10/util/complex_math.h", "test/test_unary_ufuncs.py"], "labels": ["cla signed", "module: complex"]}, "517185f946": {"title": "test_lc_1d: Increase deadline to 5 seconds (#52013)", "body": "Summary:\nIncreasing the deadline as to avoid\nflakiness of the test on ROCM.\n\nSigned-off-by: Roy, Arindam <rarindam@gmail.com>\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52013\n\nReviewed By: albanD\n\nDifferential Revision: D26360209\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1ddc7062c5ff7c980233d22844073de9fb7dcbb3", "pr_number": "52013", "files_changed": ["caffe2/python/operator_test/locally_connected_op_test.py"], "labels": ["cla signed", "module: rocm", "open source"]}, "b6806308ac": {"title": "typo in docs ddp_comm_hooks.rst (#51986)", "body": "Summary:\nFixes a typo in ddp_comm_hooks.rst\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51986\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26360314\n\nPulled By: mrshenli\n\nfbshipit-source-id: 50349501c53823cbcbad0f72be7c6ac9d51a4120", "pr_number": "51986", "files_changed": ["docs/source/ddp_comm_hooks.rst"], "labels": ["Merged", "cla signed", "open source"]}, "76c6e12a5c": {"title": "Minor spelling updates (#52149)", "body": "Summary:\nAdd space between 'e.g.' and 'build'\n'pacakge'->'package'\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52149\n\nReviewed By: osalpekar\n\nDifferential Revision: D26405824\n\nPulled By: malfet\n\nfbshipit-source-id: 386390d3f31a9fc268b05902b9dca1deeaf626f9", "pr_number": "52149", "files_changed": ["docs/source/distributed.rst"], "labels": ["cla signed", "oncall: distributed"]}, "4c93a79a04": {"title": "[Dist Profiling] Support shape recording for profiling collectives (#51822)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51822\n\nAdds support for shape recording for profiling distributed collectives, for nccl/gloo backends. Added\nboth cpp and python tests to ensure that shapes are recorded properly. Note that we don't add `ProcessGroupNCCLTest`s since they need to be modified to support single process per device and > 1 world size.\nghstack-source-id: 121507509\n\nTest Plan: CI\n\nReviewed By: mrzzd\n\nDifferential Revision: D26291739\n\nfbshipit-source-id: 5f7bd54d8c36d17a4a29e172b25266ca3dbd8fbd", "pr_number": "51822", "files_changed": ["torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["cla signed", "oncall: distributed"]}, "ea8aadf4b6": {"title": "Use self-hosted runner for nightly docker build CI. (#52148)", "body": "Summary:\nThe GitHub-hosted runner has maximum 14 GB disk space, which is not enough to host the nightly Docker build.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52148\n\nTest Plan: CI workflow\n\nReviewed By: samestep\n\nDifferential Revision: D26406295\n\nPulled By: xuzhao9\n\nfbshipit-source-id: 18a0dff45613649d6c15b8e1e9ca85042f648afd", "pr_number": "52148", "files_changed": [".github/workflows/push_nightly_docker_ghcr.yml"], "labels": ["cla signed"]}, "8908874003": {"title": "Gh/taylorrobie/import timer fbcode (#52124)", "body": "Summary:\n`torch.__config__._cxx_flags` gets called on import, but this means that Timer can't be used if it fails. (Even just the wall time parts.) This is needlessly restrictive.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52124\n\nReviewed By: albanD\n\nDifferential Revision: D26395917\n\nPulled By: robieta\n\nfbshipit-source-id: 4336a77dba131f80d386368ef715eed63c1cbcb4", "pr_number": "52124", "files_changed": ["torch/utils/benchmark/utils/cpp_jit.py"], "labels": ["cla signed"]}, "deb74edb28": {"title": "Add script to display history for a single test across multiple jobs over time (#52000)", "body": "Summary:\nAdapted from this gist: https://gist.github.com/malfet/1c34f261a28ae7af61210174394eaece\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52000\n\nTest Plan: Example shell session here: https://pastebin.com/HYgWZBFB\n\nReviewed By: walterddr\n\nDifferential Revision: D26372191\n\nPulled By: samestep\n\nfbshipit-source-id: cdc9a27e1b4a0b3123a70e693b17d524e7c6cb95", "pr_number": "52000", "files_changed": ["mypy-strict.ini", "tools/README.md", "tools/test_history.py"], "labels": ["cla signed"]}, "4c58be4573": {"title": "[StaticRuntime] Clean up input references (#51952)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51952\n\nStaticRuntime should not hold owning refs of inputs after inference is finished. This diff adds a pass to clean them up and unit tests to enforce the check.\n\nWill clean up output tensors in separate diffs.\n\nTest Plan:\n```\nbuck test //caffe2/benchmarks/static_runtime:static_runtime_cpptest\nbuck test mode/opt-clang caffe2/caffe2/fb/predictor:ptvsc2_predictor_bench_test\n```\n\nReviewed By: bwasti\n\nDifferential Revision: D26331506\n\nfbshipit-source-id: d395a295ada9de3033d0ea05d1dbab62d879a03b", "pr_number": "51952", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/impl.cpp"], "labels": ["cla signed", "fb-exported", "oncall: jit"]}, "70a805a286": {"title": "[ROCm] skip one more magma test that is flaky (#52064)", "body": "Summary:\nSkipped hipMAGMA tests are tracked in https://github.com/pytorch/pytorch/issues/51303.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52064\n\nReviewed By: albanD\n\nDifferential Revision: D26406745\n\nPulled By: walterddr\n\nfbshipit-source-id: 2405ea06e03450eb22177c2c8b12a366cfbdaa93", "pr_number": "52064", "files_changed": ["test/test_linalg.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "6385c13630": {"title": "[vulkan] Efficient gemm implementation (#49609)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49609\n\nTest Plan: Imported from OSS\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D26209677\n\nPulled By: SS-JIA\n\nfbshipit-source-id: 773a944559bf0deb3cf3e233d833220a12f9f2ab", "pr_number": "49609", "files_changed": ["aten/src/ATen/native/vulkan/api/Context.cpp", "aten/src/ATen/native/vulkan/api/Context.h", "aten/src/ATen/native/vulkan/glsl/addmm.glsl", "aten/src/ATen/native/vulkan/glsl/mean.glsl", "aten/src/ATen/native/vulkan/glsl/mean2d.glsl", "aten/src/ATen/native/vulkan/glsl/mm.glsl", "aten/src/ATen/native/vulkan/ops/Add.cpp", "aten/src/ATen/native/vulkan/ops/Clamp.cpp", "aten/src/ATen/native/vulkan/ops/Convolution.cpp", "aten/src/ATen/native/vulkan/ops/Mean.cpp", "aten/src/ATen/native/vulkan/ops/Mm.cpp", "aten/src/ATen/native/vulkan/ops/Mul.cpp", "aten/src/ATen/native/vulkan/ops/Pool.cpp", "aten/src/ATen/native/vulkan/ops/Tensor.cpp", "aten/src/ATen/native/vulkan/ops/Upsample.cpp", "aten/src/ATen/test/vulkan_api_test.cpp"], "labels": ["Merged", "cla signed"]}, "ac2bdf553e": {"title": "update build_host_protoc command for macos cross compilation (#50922)", "body": "Summary:\nCurrently, adding a cross compile build is failing on CI due to a cmake builtin compiler check that does not pass due to cross compiling the host protoc library.\n\nSetting the CMAKE_TRY_COMPILE_TARGET_TYPE flag should fix it. (Based on this [SOF answer](https://stackoverflow.com/questions/53633705/cmake-the-c-compiler-is-not-able-to-compile-a-simple-test-program).)\n\nTo test that this works, please run: `CMAKE_OSX_ARCHITECTURES=arm64 USE_MKLDNN=OFF USE_NNPACK=OFF USE_QNNPACK=OFF USE_PYTORCH_QNNPACK=OFF BUILD_TEST=OFF python setup.py install` from a Mac x86_64 machine with Xcode12.3 (anything with MacOS 11 SDK).\n\nThen, you can check that things were compiled for arm by running `lipo -info <file>` for any file in the `build/lib` directory.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50922\n\nReviewed By: malfet\n\nDifferential Revision: D26355054\n\nPulled By: janeyx99\n\nfbshipit-source-id: 919f3f9bd95d7c7bba6ab3a95428d3ca309f8ead", "pr_number": "50922", "files_changed": ["CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "22b12179db": {"title": "[PyTorch] Make TORCH_INTERNAL_ASSERT use torchCheckFail too (#52086)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52086\n\nI previously fixed TORCH_CHECK in D25481308 (https://github.com/pytorch/pytorch/commit/7d406b4a0751afdc2bd20d7be0920986178b41ae), but didn't cover TORCH_INTERNAL_ASSERT. No reason not to fix it too.\nghstack-source-id: 121456574\n\nTest Plan:\nRun framework overhead benchmarks.\nRun build size check for igios.\n\nAdindexer benchmark looks encouraging.\n\nBefore:\n```\nI0210 11:10:59.974778 2570617 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0548625. Iters per second: 18227.4\nI0210 11:11:07.591706 2570617 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0677804. Iters per second: 14753.5\nI0210 11:11:07.637014 2570617 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.35653. Iters per second: 157.319\nI0210 11:11:14.592409 2572700 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0543933. Iters per second: 18384.6\nI0210 11:11:22.158799 2572700 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0673752. Iters per second: 14842.3\nI0210 11:11:22.204160 2572700 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.37655. Iters per second: 156.825\nI0210 11:11:29.233793 2573079 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0541586. Iters per second: 18464.3\nI0210 11:11:36.726284 2573079 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0666658. Iters per second: 15000.2\nI0210 11:11:36.774489 2573079 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.36777. Iters per second: 157.041\nI0210 11:11:43.799113 2573238 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0535797. Iters per second: 18663.8\nI0210 11:11:51.433924 2573238 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0679261. Iters per second: 14721.9\nI0210 11:11:51.479207 2573238 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.34747. Iters per second: 157.543\nI0210 11:11:58.492782 2573599 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0548257. Iters per second: 18239.6\nI0210 11:12:06.072979 2573599 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0674848. Iters per second: 14818.2\nI0210 11:12:06.118813 2573599 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.34473. Iters per second: 157.611\n\n```\n\nAfter:\n```\nI0210 11:13:00.267062 2577288 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0531031. Iters per second: 18831.3\nI0210 11:13:07.591711 2577288 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0651389. Iters per second: 15351.8\nI0210 11:13:07.636951 2577288 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.25168. Iters per second: 159.957\nI0210 11:13:14.497283 2580005 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0524907. Iters per second: 19051\nI0210 11:13:21.814965 2580005 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0650504. Iters per second: 15372.7\nI0210 11:13:21.861150 2580005 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.32074. Iters per second: 158.209\nI0210 11:13:28.775005 2580166 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0528345. Iters per second: 18927\nI0210 11:13:36.041087 2580166 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0646226. Iters per second: 15474.5\nI0210 11:13:36.087904 2580166 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.38721. Iters per second: 156.563\nI0210 11:13:43.223469 2580706 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0534523. Iters per second: 18708.3\nI0210 11:13:50.603958 2580706 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.065639. Iters per second: 15234.8\nI0210 11:13:50.649281 2580706 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.24524. Iters per second: 160.122\nI0210 11:13:57.490873 2580904 BlackBoxPredictorBenchLib.cpp:384] C2 run finished. Milliseconds per iter: 0.0529411. Iters per second: 18888.9\nI0210 11:14:04.745435 2580904 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 0.0644963. Iters per second: 15504.8\nI0210 11:14:04.790006 2580904 PyTorchPredictorBenchLib.cpp:209] PyTorch run finished. Milliseconds per iter: 6.22258. Iters per second: 160.705\n```\n\nLooks like a pretty clear win (though it seems to have helped C2 as well). I checked with perf stat as well and it looks like a 1.9% CPU cycles win:\n\nbefore:\n```\n    35,313,858,645      cycles                    #    1.989 GHz                      ( +-  0.32% )  (99.98%)\n         17,750.69 msec task-clock                #    0.999 CPUs utilized            ( +-  0.33% )\n    70,524,321,763      instructions              #    2.00  insn per cycle           ( +-  0.52% )  (99.98%)\n```\n\nafter:\n```\n    34,628,390,377      cycles                    #    1.988 GHz                      ( +-  0.41% )  (99.98%)\n         17,416.59 msec task-clock                #    0.999 CPUs utilized            ( +-  0.41% )\n    70,800,211,396      instructions              #    2.04  insn per cycle           ( +-  0.11% )  (99.98%)\n```\n\nReviewed By: ezyang\n\nDifferential Revision: D26372806\n\nfbshipit-source-id: 817c7e61741334bb3ac33b617f9628309959b9c3", "pr_number": "52086", "files_changed": ["c10/util/Exception.h"], "labels": ["Merged", "cla signed"]}, "ba7a2f6513": {"title": "Add debug helper function to check target property (#52093)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52093\n\n# Summary\nThe previous debug if statement only prints the file list, but it's not clear whether the target includes the file list correctly. This function can examine the target so it's more accurate. This pr includes changes:\n1. Add a file `DebugHelper.cmake` with `print_target_properties` function.\n2. Replace the debug if statement `if(FALSE)` by adding magical variable `PRINT_CMAKE_DEBUG_INFO` and applying the variable accordingly.\n\nNote: previous debug if statement output example:\n```\n-- CPU sources:\n--   /Users/chenlai/pytorch/aten/src/ATen/BatchedFallback.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/BatchedTensorImpl.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/BatchingRegistrations.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/CPUGeneratorImpl.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/Context.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/DLConvertor.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/DynamicLibrary.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/ExpandUtils.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/LegacyTHFunctionsCPU.cpp\n--   /Users/chenlai/pytorch/aten/src/ATen/MemoryOverlap.cpp\n...\n-- GPU sources:\n-- CPU include:\n--   /Users/chenlai/pytorch/build_android/caffe2/aten/src/TH\n--   /Users/chenlai/pytorch/aten/src/TH\n--   /Users/chenlai/pytorch/aten/src\n--   /Users/chenlai/pytorch/build_android/caffe2/aten/src\n...\n-- GPU include:\n--   /Users/chenlai/pytorch/build_android/caffe2/aten/src/TH\n--   /Users/chenlai/pytorch/aten/src/TH\n--   /Users/chenlai/pytorch/build_android/caffe2/aten/src/TH\n--   /Users/chenlai/pytorch/aten/src/TH\n```\n\n# Test plan\nSet `PRINT_CMAKE_DEBUG_INFO` to true by adding `DPRINT_CMAKE_DEBUG_INFO` in `./scripts/build_pytorch_android.sh`, run `./scripts/build_pytorch_android.sh x86`\n\n`print_target_properties(torch)` shows\n```\ntorch ANDROID_ARCH = x86\ntorch ANDROID_STL_TYPE = c++_static\ntorch ARCHIVE_OUTPUT_DIRECTORY = /Users/chenlai/pytorch/build_android_x86/lib\ntorch AUTOGEN_ORIGIN_DEPENDS = ON\ntorch AUTOMOC_COMPILER_PREDEFINES = ON\ntorch AUTOMOC_MACRO_NAMES = Q_OBJECT;Q_GADGET;Q_NAMESPACE;Q_NAMESPACE_EXPORT\ntorch AUTOMOC_PATH_PREFIX = OFF\ntorch BINARY_DIR = /Users/chenlai/pytorch/build_android_x86/caffe2\ntorch BINARY_DIR = /Users/chenlai/pytorch/build_android_x86/caffe2\ntorch BUILD_WITH_INSTALL_RPATH = FALSE\ntorch CXX_STANDARD = 14\ntorch C_STANDARD = 11\ntorch IMPORTED = FALSE\ntorch IMPORTED_GLOBAL = FALSE\ntorch INCLUDE_DIRECTORIES = /Users/chenlai/pytorch/build_android_x86/aten/src;/Users/chenlai/pytorch/aten/src;/Users/chenlai/pytorch/build_android_x86;/Users/chenlai/pytorch;/Users/chenlai/pytorch/third_party/XNNPACK/include;/Users/chenlai/Library/Android/sdk/ndk/21.3.6528147/sources/third_party/vulkan/src/common;/Users/chenlai/pytorch/cmake/../third_party/eigen;/Users/chenlai/pytorch/cmake/../third_party/pybind11/include\ntorch INCLUDE_DIRECTORIES = /Users/chenlai/pytorch/build_android_x86/aten/src;/Users/chenlai/pytorch/aten/src;/Users/chenlai/pytorch/build_android_x86;/Users/chenlai/pytorch;/Users/chenlai/pytorch/third_party/XNNPACK/include;/Users/chenlai/Library/Android/sdk/ndk/21.3.6528147/sources/third_party/vulkan/src/common;/Users/chenlai/pytorch/cmake/../third_party/eigen;/Users/chenlai/pytorch/cmake/../third_party/pybind11/include\ntorch INCLUDE_DIRECTORIES = /Users/chenlai/pytorch/build_android_x86/aten/src;/Users/chenlai/pytorch/aten/src;/Users/chenlai/pytorch/build_android_x86;/Users/chenlai/pytorch;/Users/chenlai/pytorch/third_party/XNNPACK/include;/Users/chenlai/Library/Android/sdk/ndk/21.3.6528147/sources/third_party/vulkan/src/common;/Users/chenlai/pytorch/cmake/../third_party/eigen;/Users/chenlai/pytorch/cmake/../third_party/pybind11/include\ntorch INSTALL_RPATH = $ORIGIN\ntorch INSTALL_RPATH_USE_LINK_PATH = TRUE\ntorch INTERFACE_LINK_LIBRARIES = torch_cpu_library\ntorch ISPC_HEADER_SUFFIX = _ispc.h\ntorch LIBRARY_OUTPUT_DIRECTORY = /Users/chenlai/pytorch/build_android_x86/lib\ntorch LINK_LIBRARIES = torch_cpu_library\ntorch NAME = torch\ntorch PCH_INSTANTIATE_TEMPLATES = ON\ntorch PCH_WARN_INVALID = ON\ntorch POSITION_INDEPENDENT_CODE = TRUE\ntorch RUNTIME_OUTPUT_DIRECTORY = /Users/chenlai/pytorch/build_android_x86/bin\ntorch SKIP_BUILD_RPATH = FALSE\ntorch SOURCES = /Users/chenlai/pytorch/build_android_x86/empty.cpp\ntorch SOURCE_DIR = /Users/chenlai/pytorch/caffe2\ntorch SOURCE_DIR = /Users/chenlai/pytorch/caffe2\ntorch TYPE = STATIC_LIBRARY\ntorch TYPE = STATIC_LIBRARY\ntorch UNITY_BUILD_BATCH_SIZE = 8\ntorch UNITY_BUILD_MODE = BATCH\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D26377725\n\nPulled By: cccclai\n\nfbshipit-source-id: dbe21ad533759f33711a0ce5328205bbcd5cf0f3", "pr_number": "52093", "files_changed": ["caffe2/CMakeLists.txt", "cmake/DebugHelper.cmake"], "labels": ["Merged", "cla signed"]}, "578f0a04c7": {"title": "fix torch.nn.parallel.scatter_gather.gather to handle NamedTuples and handle moving output to CPU (#51104)", "body": "Summary:\nFixes #{[50510](https://github.com/pytorch/pytorch/issues/50510)}\n\nAllows ```torch.nn.parallel.scatter_gather.gather``` to accept a list of NamedTuples as input and returns a NamedTuple whose elements are tensors. I added the author's fix using the ```is_namedtuple``` function.\n\nWhile testing this fix, I encountered a deprecation warning instructing me to use ```'cpu'``` instead of ```-1``` to move the outputs to the CPU. However, doing this causes an assertion error in the ```_get_device_index``` function. I solved this by handling the CPU case in the affected ```forward``` function.\nrohan-varma\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51104\n\nReviewed By: albanD\n\nDifferential Revision: D26395578\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 6e98c9ce1d9f1725973c18d24a6554c1bceae465", "pr_number": "51104", "files_changed": ["test/test_cuda.py", "torch/nn/parallel/_functions.py", "torch/nn/parallel/scatter_gather.py"], "labels": ["Merged", "cla signed", "module: data parallel", "module: nn", "module: scatter functions", "open source", "triaged"]}, "dc25c90cfc": {"title": "Check kernel launches in caffe2/aten/src/THCUNN (#52172)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52172\n\nTest Plan: Sandcastle tests\n\nReviewed By: xush6528\n\nDifferential Revision: D26408802\n\nfbshipit-source-id: 4470203087bfedaf5825e5d63f3b9de25dd50161", "pr_number": "52172", "files_changed": ["aten/src/THCUNN/generic/ClassNLLCriterion.cu", "aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "aten/src/THCUNN/generic/MultiMarginCriterion.cu", "aten/src/THCUNN/generic/RReLU.cu", "aten/src/THCUNN/generic/SpatialClassNLLCriterion.cu", "aten/src/THCUNN/generic/SpatialDepthwiseConvolution.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "db6e0c7c0e": {"title": "Replace a platform.system() check with sys.platform (#51766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51766\n\nCheck if we are on Windows using `sys.platform` rather than\n`platform.system()`.  Even though `platform.system()` is more modern, it\nhas a few downsides: this performs a runtime check of the platform type,\nwhich has non-zero overhead.  On Linux it actually executes the separate\n`/bin/uname` process.  On the other hand `sys.platform` is determined\nwhen the Python interpreter is compiled, so this is a simple hard-coded\nstring.\n\nBecause it is a runtime check, `platform.system()` checks also cannot be\nanalyzed by static type checkers like Pyre and Mypy.  These type\ncheckers do understand `sys.platform` checks, and can correctly avoid\ncomplaining about code paths that use platform-specific modules and\nfunctions.  e.g., they can avoid complaining about `ctypes.WinDLL` not\nexisting on Linux if its use is guarded by a `sys.platform` check.\nghstack-source-id: 121107705\n\nTest Plan: Ran tests on Linux, and will check CI test results.\n\nReviewed By: mraway\n\nDifferential Revision: D26271724\n\nPulled By: simpkins\n\nfbshipit-source-id: b86e427e4ceec0324464ba4bc88b95d5813172d0", "pr_number": "51766", "files_changed": ["caffe2/python/__init__.py"], "labels": ["Merged", "cla signed"]}, "e4203c4306": {"title": "Automated submodule update: FBGEMM (#52129)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/4d203256ba36b4f4db523ae74ac285a3e31b36da\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52129\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: jianyuh\n\nDifferential Revision: D26393870\n\nPulled By: jspark1105\n\nfbshipit-source-id: 6cf01c45c8768f453c9fac5f8af6813db0549083", "pr_number": "52129", "files_changed": ["third_party/fbgemm"], "labels": ["Merged", "cla signed", "open source"]}, "0c9d72b5e1": {"title": "[StaticRuntime] Clean up output references and remove dead code (#51991)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51991\n\n- Clean up references of outputs, including Tuples/Lists, by using move semantics\n- Clean up references of elements in output Tuples/Lists by adding them to `unmanaged_values_` in MemoryPlanner. Check for corner case of Tuple/List element being inputs.\n- Modify unit tests to check for use_counts of outputs\n- Clean up dead code. A bit overlap with D25592967, but shouldn't be a problem.\n\nThis diff does not try to fix the alias problem with the MemoryPlanner.\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\n```\nbuck test //caffe2/benchmarks/static_runtime:static_runtime_cpptest\nbuck test mode/opt-clang caffe2/caffe2/fb/predictor:ptvsc2_predictor_bench_test\n```\n\nReviewed By: bwasti\n\nDifferential Revision: D26333953\n\nfbshipit-source-id: cadc0595ad6ab754c4f1f7a5a3733b2c16b3102f", "pr_number": "51991", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["Merged", "Reverted", "cla signed", "fb-exported", "oncall: jit"]}, "992d251c39": {"title": "Revert D26333953: [StaticRuntime] Clean up output references and remove dead code", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26333953 (https://github.com/pytorch/pytorch/commit/0c9d72b5e11e9d3e706eaf1c07406a0cebfe27c7)\n\nOriginal commit changeset: cadc0595ad6a\n\nfbshipit-source-id: 75d0b33099342653cd8867b129139325789aee6c", "pr_number": null, "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": []}, "d22f700f9e": {"title": "Link torch_global_deps to libtbb.so if USE_TBB is enabled (#51741)", "body": "Summary:\nSome distributions of MKL such as the one in the Conda default channel have an implicit dependency to TBB even though they do not list it explicitly in their ELF dynamic section (DT_NEEDED). Pre-loading torch_global_deps into a process that uses such an MKL distribution fails with an unresolved symbol error due to missing libtbb.so. This code change forces torch_global_deps to load libtbb.so into the process to avoid such issues.\n\nMore over although we distribute our own TBB build, it is a widely-used third-party library and the same global namespace treatment rules should apply to it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51741\n\nReviewed By: malfet\n\nDifferential Revision: D26261214\n\nPulled By: cbalioglu\n\nfbshipit-source-id: 94491275f8ec82d5917695e57dd766a10da92726", "pr_number": "51741", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "e8ab58bfc7": {"title": "[reland] Early terminate CUDA on common_utils TestCases (#52126)", "body": "Summary:\nTake 2 of https://github.com/pytorch/pytorch/issues/50914\nThis change moves the early termination logic into common_utils.TestCase class.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52126\n\nTest Plan: CI with ci-all tag\n\nReviewed By: malfet\n\nDifferential Revision: D26391762\n\nPulled By: walterddr\n\nfbshipit-source-id: a149ecc47ccda7f2795e107fb95915506ae060b4", "pr_number": "52126", "files_changed": ["test/test_testing.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "ci/all", "cla signed"]}, "1795398c24": {"title": "Updates rounding_mode documentation to remove \"true\" (#52202)", "body": "Summary:\nIn design review the use of the word \"true\" for a \"rounding mode\" which actually performed no rounding was, understandably, considered confusing. This PR updates the documentation to remove references to \"true.\" The signatures for torch.div and torch.divide are updated to reflect the future behavior where rounding_mode=None will be the default.\n\nThis is slightly inaccurate. Today when rounding mode is not specified it is effectively None, but users cannot actually specify rounding_mode=None today. That change was considered too disruptive to the 1.8 branch cut process.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52202\n\nReviewed By: gchanan\n\nDifferential Revision: D26424979\n\nPulled By: mruberry\n\nfbshipit-source-id: db3cc769c0d9c6d7e42bfad294073c99fa9168d9", "pr_number": "52202", "files_changed": ["tools/pyi/gen_pyi.py", "torch/_tensor_docs.py", "torch/_torch_docs.py"], "labels": ["Merged", "cla signed"]}, "de54510f15": {"title": "Check kernel launches in caffe2/caffe2/image (#52173)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52173\n\nTest Plan: Sandcastle tests\n\nReviewed By: xush6528\n\nDifferential Revision: D26408885\n\nfbshipit-source-id: f90a00199a73487cb9134f20c58975b134a0117b", "pr_number": "52173", "files_changed": ["caffe2/image/transform_gpu.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "05b60921ae": {"title": "[iOS][PyTorch][OSS] fix iOS nightly build (#52197)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52197\n\nD26187854 (https://github.com/pytorch/pytorch/commit/6045663f391e9bebac31e006a98c1dd381792936) added `from typing_extensions import Literal` to `tools/codegen/gen.py` whereas `typing_extensions` was not installed while building iOS binary\n\nWas reproduced here: https://app.circleci.com/pipelines/github/pytorch/pytorch/273256/workflows/a1c66866-87ad-4ace-a0f7-f8c17524091c/jobs/10882828\n\nghstack-source-id: 121621817\n\nTest Plan:\nCreated a PR to trigger the nightly build which also includes the fix.\nhttps://github.com/pytorch/pytorch/pull/52195\nThe nightly build was successful: https://app.circleci.com/pipelines/github/pytorch/pytorch/273262/workflows/ed7a0f14-2b48-4599-877f-45271473dd86/jobs/10883042\n\n{F372504913}\n\nReviewed By: linbinyu\n\nDifferential Revision: D26420298\n\nfbshipit-source-id: d88c9203473def936aaf1c1756c3c926d087a959", "pr_number": "52197", "files_changed": [".circleci/scripts/binary_ios_build.sh"], "labels": ["Merged", "cla signed"]}, "fa0a049d4e": {"title": "Add a make_tempdir() utility function to the TestCase base class (#51762)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51762\n\nUpdate test_util.py to add a `make_tempdir()` function to the `TestCase`\nclass.  The main advantage of this function is that the temporary\ndirectory will be automatically cleaned up when the test case finishes,\nso that test case does not need to worry about manually cleaning up this\ndirectory.\n\nThis also prefixes the directory name with `caffe2_test.` so that it is\nmore obvious where the temporary directories came from if they are ever\nleft behind after a crashed or killed test process.\n\nThis updates the tests in `operator_test/load_save_test.py` to use this\nnew function, so they no longer have to perform their own manual cleanup\nin each test.\n\nTest Plan: python caffe2/python/operator_test/load_save_test.py\n\nReviewed By: mraway\n\nDifferential Revision: D26271178\n\nPulled By: simpkins\n\nfbshipit-source-id: 51175eefed39d65c03484482e84923e5f39a4768", "pr_number": "51762", "files_changed": ["caffe2/python/operator_test/load_save_test.py", "caffe2/python/test_util.py"], "labels": ["Merged", "cla signed"]}, "0dc0cb1d8d": {"title": "Enable FP16 sparse regularizer", "body": "Summary: Previously there was no regularizer implemented for fp16 sparse features. Add regularizer support here using the Float16SparseNormalize implemented in this stack.\n\nTest Plan:\nbuck test //caffe2/caffe2/python:regularizer_test\n\nIn f248648705, we can see there is the operator `Float16SparseNormalize`.\n\n{F356635445}\n\nReviewed By: bigrabithong\n\nDifferential Revision: D24042567\n\nfbshipit-source-id: 5e0065f8c10b8748daffa8a54a6bf8f461460b18", "pr_number": null, "files_changed": ["caffe2/python/regularizer.py", "caffe2/python/regularizer_test.py"], "labels": []}, "49c8be516e": {"title": "Add ARM64 cross-compilation build on OS X (#49751)", "body": "Summary:\nTests cross-compilation of ARM64 architecture in MacOS CI.\n\nThis should be merged after PR https://github.com/pytorch/pytorch/issues/50243 and https://github.com/pytorch/pytorch/issues/50922 (adding a fix).\n\nThe reason we pin the wheel to be version 0.36.2 is because lower versions cannot handle c38 as a tag for the wheel.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49751\n\nReviewed By: albanD\n\nDifferential Revision: D26411133\n\nPulled By: janeyx99\n\nfbshipit-source-id: 00a5cf597aee10adea1547579270cb3b38732563", "pr_number": "49751", "files_changed": [".circleci/cimodel/data/simple/macos_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".jenkins/pytorch/macos-build.sh", ".jenkins/pytorch/macos-common.sh"], "labels": ["Merged", "cla signed"]}, "7b21c6be67": {"title": "[Dist Profiling] Enable profiling for gloo send/recv (#52004)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52004\n\nEnables profiling of p2p collectives for Gloo. Modified/added relevant unittests.\nghstack-source-id: 121507511\n\nTest Plan: CI\n\nReviewed By: mrzzd\n\nDifferential Revision: D26347164\n\nfbshipit-source-id: f4d1c474fccf40d5776fc13c4add7a053ea08960", "pr_number": "52004", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a8321855ad": {"title": "Check kernel launches in caffe2/aten/src/THC (#52174)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52174\n\nTest Plan: Sandcastle tests\n\nReviewed By: ngimel\n\nDifferential Revision: D26408837\n\nfbshipit-source-id: deecd2e856946d1adbc985c13db110c06de6f3df", "pr_number": "52174", "files_changed": ["aten/src/THC/THCSleep.cu", "aten/src/THC/generic/THCTensorTopK.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "b2d8f0a431": {"title": "[pytorch][bot] update mobile op deps (#52110)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52110\n\nLLVM_DIR=/usr ANALYZE_TORCH=1 tools/code_analyzer/build.sh\ncp build_code_analyzer/work/torch_result.yaml tools/code_analyzer/default_op_deps.yaml\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26419138\n\nPulled By: ljk53\n\nfbshipit-source-id: 26bf00036b19ad18a9cf06111df4d9fe32e5feab", "pr_number": "52110", "files_changed": ["tools/code_analyzer/default_op_deps.yaml"], "labels": ["Merged", "cla signed"]}, "2900cf2b94": {"title": "Refactor autograd discovery code (#52057)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/34067 by using https://github.com/pytorch/pytorch/issues/34426 by hczhu\nIn addition to removing the unnecessary any() we do also:\n - Get rid of the outer loop since graph_root also needs to be checked\n - Update psuedo code description so it matches what the code does\n - Add some comments explaining the difference between assigning `info.needed_` and `info.captures_` in terms of how that affects discovery\n - [edit: another benefit is that exec_info entries are no longer created for all reachable nodes]\n\nThis PR is on top of https://github.com/pytorch/pytorch/issues/51940, so once that lands rebasing on top of master should get rid of the extra commits and changes\n\nI'm not sure if this change will bring a lot of performance gains, but the main benefit is that the code is easier to read.\n\nTrivial graph:\n```\ntorch.autograd.grad(a*b, [a, b], gradient)\nsetup:\n  a = torch.rand((2, 2), requires_grad=True)\n  b = torch.rand((2, 2), requires_grad=True)\n  gradient = torch.ones(2, 2)\n\nTimer before:\n  15.45 us\nTime after:\n  14.33 us\n1 measurement, 10000 runs , 1 thread\n\nInstructions after:\n                           All          Noisy symbols removed\n    Instructions:      8271213                    8193169\n    Baseline:             4244                       3838\nInstructions before:\n                           All          Noisy symbols removed\n    Instructions:      8142843                    8054463\n    Baseline:             4280                       3838\n100 runs per measurement, 1 thread\n```\nSmall graph:\n```\ntorch.autograd.grad((b*a.exp()+a*b.exp()).sum(), (a, b))\nsetup:\n  a = torch.rand((2, 2), requires_grad=True)\n  b = torch.rand((2, 2), requires_grad=True)\n\nTime before:\n  52.25 us\nTime after:\n  50.80 us\n1 measurement, 10000 runs , 1 thread\n\nInstruction count before:\n                           All          Noisy symbols removed\n    Instructions:     25601257                   25518229\n    Baseline:             4228                       3838\nInstruction count after:\n                           All          Noisy symbols removed\n    Instructions:     25606533                   25522797\n    Baseline:             4228\n100 runs per measurement, 1 thread\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52057\n\nReviewed By: ngimel\n\nDifferential Revision: D26432207\n\nPulled By: soulitzer\n\nfbshipit-source-id: beef68344d66e9e286378e31e3311ba43c25c749", "pr_number": "52057", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["Merged", "cla signed"]}, "bfe6e23209": {"title": "Early version of fx graph matcher for NS (#51588)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51588\n\nEarly version of utility to match nodes between graph A and graph B, for Numerical Suite for FX graph mode quantization.\n\nThe main goal of this utility is to reliably match the nodes of graph A to the nodes of graph B, and throw an easy to read error message.  This will be used in future PRs to create the APIs for matching activations.  It also could potentially be used to match weights.\n\nTest Plan:\nFor now, we have bare bones test coverage on some toy models, and a single torchvision model.\n\n```\npython test/test_quantization.py TestFXGraphMatcher\n```\n\nFuture PRs will add more testing.\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D26403093\n\nfbshipit-source-id: 60e318d51e6fefe65265488c4967629d946048ef", "pr_number": "51588", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "test/test_quantization.py", "torch/quantization/ns/__init__.py", "torch/quantization/ns/graph_matcher.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "37622db76a": {"title": "ns for fx - stubs of the three APIs (compare weights, activations, activations with shadow) (#51669)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51669\n\nAdds the basic functionality for the three Numeric Suite core APIs to work on FX models:\n1. comparing weights\n2. comparing activations, with same input fed to both models\n3. comparing activations, with nodes of A shadowing nodes of B\n\nNote: there are a lot of TODOs in the code, and some/most of the APIs and implementation details may change as we iterate.  This is just the first PR.\n\nTest Plan:\nWe have unit test coverage for all of the APIs, for now this is with toy models:\n\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26403094\n\nfbshipit-source-id: 9752331d4ae0105346d3da309b13c895b593b450", "pr_number": "51669", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "test/test_quantization.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/quantization/ns/utils.py", "torch/quantization/ns/weight_utils.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "1657d59641": {"title": "Walk Python AST to check for unsupported attribute type annotations (#51805)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51805\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D26418589\n\nPulled By: ansley\n\nfbshipit-source-id: c13e9096dcfa242d158ebf1ae4f86ef6c46ff0ec", "pr_number": "51805", "files_changed": ["test/jit/test_scriptmod_ann.py", "test/test_jit.py", "torch/jit/_check.py", "torch/jit/_recursive.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4cc10563e7": {"title": "Customize traceback for calls to symbolically-traced code (#51648)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51648\n\nThe following code will throw during the call to `traced(5)`:\n```python\nclass M(nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.W = torch.nn.Parameter(torch.randn(5))\n\n    def forward(self, x):\n        return torch.dot(self.W, x)\n\ntraced = fx.symbolic_trace(M())\ntraced(5)\n```\n\nTraceback before:\n```\nTraceback (most recent call last):\n  File \"test/tinytest.py\", line 26, in <module>\n    traced(5)\n  File \"/home/ansley/local/pytorch/torch/fx/graph_module.py\", line 338, in wrapped_call\n    return self._cls_call(self, *args, **kwargs)\n  File \"/home/ansley/local/pytorch/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"<eval_with_key_0>\", line 4, in forward\nTypeError: dot(): argument 'tensor' (position 2) must be Tensor, not int\n```\n\nTraceback after:\n```\nTraceback (most recent call last):\n  File \"/home/ansley/local/pytorch/torch/fx/graph_module.py\", line 338, in wrapped_call\n    return torch.nn.Module.__call__(self, *args, **kwargs)\n  File \"/home/ansley/local/pytorch/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"<eval_with_key_1>\", line 4, in forward\n    dot_1 = torch.dot(w, x);  w = x = None\nTypeError: dot(): argument 'tensor' (position 2) must be Tensor, not int\n\nCall using an FX-traced Module, line 4 of the traced Module\u2019s generated forward function:\n    w = self.W\n    dot_1 = torch.dot(w, x);  w = x = None\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    relu_1 = dot_1.relu();  dot_1 = None\n\n    return relu_1\n```\n\n(Note that the same `TypeError` is thrown despite modifying the traceback.)\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26424005\n\nPulled By: ansley\n\nfbshipit-source-id: 368f46ba81fb3111bd09654825bb2ac5595207d1", "pr_number": "51648", "files_changed": ["test/test_fx.py", "torch/fx/graph_module.py", "torch/testing/_internal/jit_utils.py"], "labels": ["Merged", "cla signed", "fx"]}, "388c38505c": {"title": "[Metal] Add concat op for metal", "body": "Summary: Add concat op to enable models such as SqueezeNet.\n\nTest Plan:\nTest on device:\n```\narc focus2 pp-ios\n```\nTest on mac\n```\nbuck test pp-macos\n```\n\nReviewed By: xta0\n\nDifferential Revision: D26029029\n\nfbshipit-source-id: b0d621f2069a722f0770218c435b22feac4fb873", "pr_number": null, "files_changed": ["aten/src/ATen/native/metal/MetalAten.mm", "aten/src/ATen/native/metal/MetalShaders.h", "aten/src/ATen/native/metal/mpscnn/MPSCNN.h", "aten/src/ATen/native/metal/mpscnn/MPSCNN.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.h", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": []}, "71d0b5632b": {"title": "Add SqueezeNet to PyTorch Playground", "body": "Summary: Add support for SqueezeNet in the PyTorch Playground test app\n\nTest Plan:\n```\narc focus2 pp-ios\n```\n\nReviewed By: xta0\n\nDifferential Revision: D26083960\n\nfbshipit-source-id: a0d753eefa431f2f9e377f082c564370d6774c0b", "pr_number": null, "files_changed": ["aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.h", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": []}, "4ab86c87a2": {"title": "[caffe2 and pytorch] replace temp name of new sparse adagrad JIT'ed function in fbgemm (#52193)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52193\n\nIn this step, we replace the temp name and use the old interface name with new behavior\n\nTest Plan: CI\n\nReviewed By: dskhudia\n\nDifferential Revision: D26232170\n\nfbshipit-source-id: 60233f98fe91a15c3c834bf6fde1b185269dd2b6", "pr_number": "52193", "files_changed": ["caffe2/sgd/adagrad_op.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "a184ef8df5": {"title": "[TorchScript] C++ interface of to_<backend> (#51797)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51797\n\nThe C++ API, ```codegen_backend_module``` is added to ```to_<backend>```. Python related stuffs are decoupled in this function. It can be used from both C++ and python.\n\n* Tests\nPython: The existing ```test_backends.py```, which calls the C++ API under the hood.\nC++: The end-to-end test of ```jit.BackendTest.ToBackend``` is added in ```test_backend.cpp```. The original class definitions in this file is moved to ```test_backend_lib.cpp```\n\nghstack-source-id: 121687464\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: raziel\n\nDifferential Revision: D26280518\n\nfbshipit-source-id: fd466e4b448847ce64010a3297fff0b5760c5280", "pr_number": "51797", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_lib.cpp", "tools/build_variables.bzl", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_init.cpp"], "labels": ["Merged", "Reverted", "cla signed", "oncall: jit"]}, "96fd5d87f7": {"title": "Add `dict()` constructor (#51934)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51934\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26418199\n\nPulled By: ansley\n\nfbshipit-source-id: 524f6d9d29ee1fa1b7c5e80ada82e577f47089dc", "pr_number": "51934", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_list_dict.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "87c0b6bffc": {"title": "[RPC] Move confirmation future in rref context to jit future (#51695)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51695\n\nAs part of the plan to completely eliminate torch/csrc/utils/future.h,\nwe are converting this to JitFuture (c10::ivalue::Future).\nghstack-source-id: 121695708\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D26238873\n\nfbshipit-source-id: 92bad1a349964ce8a9a80e2d1cf68f293cbe411c", "pr_number": "51695", "files_changed": ["torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8ff5a46c32": {"title": "[RPC] waitForThreadLocalRRefs returns jitFuture (#51696)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51696\n\nModify this API to use JitFuture.\nghstack-source-id: 121695707\n\nTest Plan: Ci\n\nReviewed By: mrshenli\n\nDifferential Revision: D26239132\n\nfbshipit-source-id: 15c0c349a79e660fe4862e1d99176989f8159bf4", "pr_number": "51696", "files_changed": ["torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "846755af2f": {"title": "Remove unused include in TensorIteratorDynamicCasting.h (#51824)", "body": "Summary:\nIn the past, this file included `thrust/complex.h` because the `thrust::complex` --> `c10::complex` migration was not done. Today, this task has been done for a while but seems that this include was not deleted.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51824\n\nReviewed By: albanD\n\nDifferential Revision: D26417144\n\nPulled By: ngimel\n\nfbshipit-source-id: 1fff5b8d50f0b34c963a7893cbb0599895823105", "pr_number": "51824", "files_changed": ["aten/src/ATen/native/TensorIteratorDynamicCasting.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "490eb3e735": {"title": "Add 3D depthwise seperable convolution (#51027)", "body": "Summary:\nBecause this pull request (https://github.com/pytorch/pytorch/issues/40801) becomes an important part of recent 3D models, brings significant improvement in speed, and also have been open for a while. So I decided to resolve the previous review comment and modify it a bit so that it can be merged into the latest version of Pytorch.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51027\n\nReviewed By: albanD\n\nDifferential Revision: D26414116\n\nPulled By: ngimel\n\nfbshipit-source-id: 562c099f4d7f6d603a9c2f2e2a518bc577b0d8ee", "pr_number": "51027", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cuda/DepthwiseConv3d.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_nn.py", "tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed", "module: convolution", "open source", "triaged"]}, "52e6ef8b53": {"title": "[TensorExpr] Add another test for ExternalCalls. (#52162)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52162\n\nThis test demonstrates how external calls can interoperate with other\ntensor computations and between themselves.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D26410813\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 8180164013b43f613d53620d1b249e0af769ae8e", "pr_number": "52162", "files_changed": ["test/cpp/tensorexpr/test_external_calls.cpp"], "labels": ["Merged", "cla signed"]}, "b8f3a658f9": {"title": "Do not include \"DynamicLibrary.h\" into a top-level header (#52182)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52182\n\nDynamicLibrary provides a very specific functionality, so there is no need to exposes it to every project depending on `ATen.h`\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D26417404\n\nPulled By: malfet\n\nfbshipit-source-id: f8318cacb07dcc8b2f95984f88ea1df4e5369b8b", "pr_number": "52182", "files_changed": ["aten/src/ATen/ATen.h", "torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/codegen/fuser/cpu/fused_kernel.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "357e5baf7e": {"title": "Extend DynamcLibrary constructor to support alternative library name (#52183)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52183\n\nThis allows one to load library that can exist on the system under different names.\nCurrently, this functionality is Linux only, as on Windows shared libraries are not renamed by `auditwheel`\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D26417405\n\nPulled By: malfet\n\nfbshipit-source-id: d327e2565b26cf5b7214e7978862f56e02cad7c6", "pr_number": "52183", "files_changed": ["aten/src/ATen/DynamicLibrary.cpp", "aten/src/ATen/DynamicLibrary.h"], "labels": ["Merged", "cla signed"]}, "73de98204d": {"title": "[JIT] Add static method support for TorchBind (#51177)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51177\n\n**Summary**\nThis commit adds support for static methods to TorchBind. Just like\npybind, the API for declaring a static method is `def_static(...)`. A\nstatic method must be called on the class directly, and can be called\nboth in Python as well as TorchScript.\n\nSupport for static methods is implemented in a manner similar to that of\ninstance methods. Registered static functions are wrapped in a layer of\nunboxing logic, their schemas are inferred using templates and\nmetaprogramming, and they are added to the `ClassType` object\ncorresponding to the TorchBind class on which they are registered.\nScriptClass has been extended to support a `__getattr__` function so\nthat static methods of TorchBind classes can be invoked in Python. The\nimplementation of `__getattr__` returns `ScriptClassFunctionPtr`, a\nversion of `StrongFunctionPtr` without a compilation unit (since the\nfunctions of a TorchBind class live inside the TorchBind registry).\nWithin TorchScript, TorchBind static functions are desugared in\n`PythonClassValue::attr` by looking them up on the class type of the\n`PythonClassValue` instance.\n\n**Test Plan**\nThis commit adds a unit test that tests a simple static method on a\nTorchBind class.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26356942\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 1b6a9bc2e5f3e22071ad78e331a0201fbbf7ab30", "pr_number": "51177", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/cpp/jit/test_custom_class_registrations.cpp", "test/jit/test_torchbind.py", "torch/csrc/jit/passes/inliner.cpp", "torch/csrc/jit/python/python_custom_class.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/custom_class.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4949eea0ff": {"title": "[StaticRuntime] Clean up output references and remove dead code (#52237)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52237\n\nRedo D26331506 (https://github.com/pytorch/pytorch/commit/4c58be4573dbc9274a9edd57edd0166397141e7b). Get rid of `nodiscard` which broke OSS CI.\n\n- Clean up references of outputs, including Tuples/Lists, by using move semantics\n- Clean up references of elements in output Tuples/Lists by adding them to `unmanaged_values_` in MemoryPlanner. Check for corner case of Tuple/List element being inputs.\n- Modify unit tests to check for use_counts of outputs\n- Clean up dead code. A bit overlap with D25592967, but shouldn't be a problem.\n\nThis diff does not try to fix the alias problem with the MemoryPlanner.\n\nReviewed By: swolchok\n\nDifferential Revision: D26432539\n\nfbshipit-source-id: e08990e4066c1ce69ad5274860851d012b7be411", "pr_number": "52237", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "eaddadd4f7": {"title": "Revert D26403094: ns for fx - stubs of the three APIs (compare weights, activations, activations with shadow)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26403094 (https://github.com/pytorch/pytorch/commit/37622db76a102cee7df67bab8d228780fef0874d)\n\nOriginal commit changeset: 9752331d4ae0\n\nfbshipit-source-id: f0a32d443a29b25af33d90420dfd1bada40c917c", "pr_number": null, "files_changed": ["test/quantization/test_numeric_suite_fx.py", "test/test_quantization.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/quantization/ns/utils.py", "torch/quantization/ns/weight_utils.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "1903b32c35": {"title": "Directly Return when Numel == 0 for WeightedSum and ScatterWeightedSum", "body": "Summary:\nCurrent Caffe2 operators WeightedSum and ScatterWeightedSum will enforce that the first input is not empty; otherwise they will raise error. However, in some cases we will have 0 batch size in training and eval. For example, when training and eval current AF and AI OC models, we will filter out the search ads in data pipeline, which might cause 0 batch size in some iterations. As a result, if the models are using Dper3 modules that contains WeightedSum or ScatterWeightedSum (e.g., HistogramBinningCalibration module), they will occasionally fail in training or eval.\n\nTo address this issue, we revise the implementation of WeightedSum and ScatterWeightedSum so that we will directly return when their first inputs are empty without failing the operators.\n\nTest Plan:\nWe tested the code change by building a Dper3 backend canary package. All the jobs for AF and AI OC succeeded with the modified Caffe2 operators:\n\nf251058001\nf251058142\nf251058332\n\nTo compare, all the jobs with identical model configs but with the canary package built from master failed:\n\nf250993908\nf250994106\nf250994174\n\nReviewed By: chenshouyuan, huayuli00\n\nDifferential Revision: D26444645\n\nfbshipit-source-id: 1c2f81a078810e3ef3c17c133a715090dee2c0ff", "pr_number": null, "files_changed": ["caffe2/operators/utility_ops.h"], "labels": []}, "cd46ee6175": {"title": "Revert D26280518: [TorchScript] C++ interface of to_<backend>", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26280518 (https://github.com/pytorch/pytorch/commit/a184ef8df5861167138432acbe91373e0fd83b9a)\n\nOriginal commit changeset: fd466e4b4488\n\nfbshipit-source-id: e4def49703ab525c063b8cc5d11296b9cc614fbb", "pr_number": null, "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_lib.cpp", "tools/build_variables.bzl", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_init.cpp"], "labels": []}, "df837d0384": {"title": "Use the libc++ detection instead of clang detection around std:isinfinite (#52164)", "body": "Summary:\nFixes #52163\n\nThe libc++ vs libstdc++ detection in the pre-processor is taken from https://stackoverflow.com/questions/31657499/how-to-detect-stdlib-libc-in-the-preprocessor\n\nNote that in our case `std:isinfinite` presents means that we don't need to import any additional headers to guarantee the `_LIBCPP_VERSION` presents for the `libc++`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52164\n\nReviewed By: albanD\n\nDifferential Revision: D26413108\n\nPulled By: malfet\n\nfbshipit-source-id: 515e258d6758222c910ababf5172c3a275aff08f", "pr_number": "52164", "files_changed": ["aten/src/ATen/native/cpu/MultinomialKernel.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "68e2a8c420": {"title": "Reenable test_nn tests for Windows (#52051)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52002\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52051\n\nReviewed By: ngimel\n\nDifferential Revision: D26409749\n\nPulled By: janeyx99\n\nfbshipit-source-id: 5fa76d4fff8cf0fe2130c925fde9dffd0d1e7172", "pr_number": "52051", "files_changed": ["aten/src/ATen/native/cuda/EmbeddingBag.cu", "test/test_nn.py"], "labels": ["Merged", "ci/all", "cla signed"]}, "4df8e774e6": {"title": "[ROCm] warn unsupported PYTORCH_CUDA_FUSER_DISABLE_FMA (#50508)", "body": "Summary:\nnvcc's `--fmad=false` is not valid for the HIP compiler.  Upcoming ROCm releases will start treating unrecognized compiler flags as an error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50508\n\nReviewed By: albanD\n\nDifferential Revision: D25920291\n\nPulled By: mrshenli\n\nfbshipit-source-id: c0ff3b74dd07f3d0661ba29efafaab291ef3621c", "pr_number": "50508", "files_changed": ["torch/csrc/jit/codegen/cuda/executor_utils.cpp"], "labels": ["Merged", "cla signed", "module: cuda", "module: rocm", "oncall: jit", "open source"]}, "b01b7ea4f3": {"title": "store artifacts for windows binary build (#52239)", "body": "Summary:\nBetter debugging: allows you to download the final package for binary windows builds\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52239\n\nReviewed By: agolynski\n\nDifferential Revision: D26463613\n\nPulled By: janeyx99\n\nfbshipit-source-id: ffb0ec044be23286b8975b9a6d2f90d05c2aff9c", "pr_number": "52239", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml"], "labels": ["Merged", "ci/all", "cla signed", "module: ci", "module: windows"]}, "0019a20a2b": {"title": "[WIP] Add a `_flush_compilation_cache` for testing (#52001)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52001\n\nReviewed By: eellison\n\nDifferential Revision: D26371876\n\nPulled By: Krovatkin\n\nfbshipit-source-id: db773d7124916bad31e80bdd7bb9b4170060977b", "pr_number": "52001", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "71d5a8ea62": {"title": "[nnc] Benchmark inference batchnorm (#52251)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52251\n\nBatchnorm in inference is just a bunch of pointwise ops.  NNC\nshould be able to do a good job of this, and indeed it does.  For fun\nI've included a fused BN->Relu (although the real fusion fun would be\nConv->BN->Relu...).\n\n```\n---------------------------------------------------------------------------------------\nBenchmark                                Time           CPU Iterations UserCounters...\n---------------------------------------------------------------------------------------\nBatchNorm/ATen/1/64/112/112         252886 ns     252875 ns       2785 GB/s=25.3981G/s\nBatchNorm/ATen/1/256/14/14           12145 ns      12145 ns      55347 GB/s=33.0525G/s\nBatchNorm/ATen/1/128/28/28           18919 ns      18918 ns      37749 GB/s=42.437G/s\nBatchNorm/ATen/1/64/56/56            61434 ns      61433 ns      11315 GB/s=26.1363G/s\nBatchNorm/ATen/1/512/7/7             11924 ns      11923 ns      59070 GB/s=16.8327G/s\nBatchNorm/ATen/5/64/112/112        1873321 ns    1873292 ns        382 GB/s=17.1424G/s\nBatchNorm/ATen/5/256/14/14           83470 ns      83459 ns       8538 GB/s=24.0483G/s\nBatchNorm/ATen/5/128/28/28          157521 ns     157520 ns       4440 GB/s=25.4829G/s\nBatchNorm/ATen/5/64/56/56           314675 ns     314670 ns       2235 GB/s=25.513G/s\nBatchNorm/ATen/5/512/7/7             48129 ns      48128 ns      14582 GB/s=20.851G/s\n\nBatchNorm/NNC/1/64/112/112          249454 ns     249428 ns       2802 GB/s=25.749G/s\nBatchNorm/NNC/1/256/14/14             9321 ns       9321 ns      74573 GB/s=43.066G/s\nBatchNorm/NNC/1/128/28/28            16874 ns      16873 ns      40999 GB/s=47.5797G/s\nBatchNorm/NNC/1/64/56/56             59276 ns      59275 ns      12047 GB/s=27.0878G/s\nBatchNorm/NNC/1/512/7/7               3452 ns       3452 ns     202610 GB/s=58.1394G/s\nBatchNorm/NNC/5/64/112/112         1820201 ns    1820038 ns        373 GB/s=17.6439G/s\nBatchNorm/NNC/5/256/14/14            78429 ns      78420 ns       8871 GB/s=25.5935G/s\nBatchNorm/NNC/5/128/28/28           155214 ns     155202 ns       4514 GB/s=25.8635G/s\nBatchNorm/NNC/5/64/56/56            311454 ns     311449 ns       2163 GB/s=25.7768G/s\nBatchNorm/NNC/5/512/7/7              26853 ns      26851 ns      25283 GB/s=37.3735G/s\n\nBatchNorm/ATenRelu/1/64/112/112     378879 ns     378849 ns       1844 GB/s=16.9528G/s\nBatchNorm/ATenRelu/1/256/14/14       16707 ns      16705 ns      41391 GB/s=24.029G/s\nBatchNorm/ATenRelu/1/128/28/28       30235 ns      30235 ns      23060 GB/s=26.5529G/s\nBatchNorm/ATenRelu/1/64/56/56        91164 ns      91160 ns       7662 GB/s=17.6132G/s\nBatchNorm/ATenRelu/1/512/7/7         14681 ns      14681 ns      46088 GB/s=13.6707G/s\nBatchNorm/ATenRelu/5/64/112/112    2864060 ns    2863566 ns        243 GB/s=11.2142G/s\nBatchNorm/ATenRelu/5/256/14/14      118376 ns     118367 ns       5907 GB/s=16.9561G/s\nBatchNorm/ATenRelu/5/128/28/28      237893 ns     237873 ns       2936 GB/s=16.8749G/s\nBatchNorm/ATenRelu/5/64/56/56       472452 ns     472386 ns       1479 GB/s=16.9949G/s\nBatchNorm/ATenRelu/5/512/7/7         61389 ns      61379 ns      11442 GB/s=16.3496G/s\n\nBatchNorm/NNCRelu/1/64/112/112      248378 ns     248341 ns       2812 GB/s=25.8618G/s\nBatchNorm/NNCRelu/1/256/14/14         9965 ns       9964 ns      76013 GB/s=40.2861G/s\nBatchNorm/NNCRelu/1/128/28/28        16153 ns      16153 ns      43343 GB/s=49.7004G/s\nBatchNorm/NNCRelu/1/64/56/56         58761 ns      58757 ns      12095 GB/s=27.3265G/s\nBatchNorm/NNCRelu/1/512/7/7          10529 ns      10529 ns      66590 GB/s=19.0625G/s\nBatchNorm/NNCRelu/5/64/112/112     1799001 ns    1798757 ns        362 GB/s=17.8527G/s\nBatchNorm/NNCRelu/5/256/14/14        78252 ns      78246 ns       8974 GB/s=25.6504G/s\nBatchNorm/NNCRelu/5/128/28/28       154940 ns     154923 ns       4483 GB/s=25.9102G/s\nBatchNorm/NNCRelu/5/64/56/56        312329 ns     312324 ns       2244 GB/s=25.7046G/s\nBatchNorm/NNCRelu/5/512/7/7          51203 ns      51199 ns      13559 GB/s=19.6004G/s\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D26440786\n\nPulled By: bertmaher\n\nfbshipit-source-id: 7d3f7bf6eee4c37736e9875d31ae1b483af9fb6f", "pr_number": "52251", "files_changed": ["benchmarks/cpp/tensorexpr/CMakeLists.txt", "benchmarks/cpp/tensorexpr/bench_batchnorm.cpp"], "labels": ["Merged", "cla signed"]}, "b887c30980": {"title": "Out version for sum (#52225)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52225\n\nSupported out version for sum for SR\n\nTest Plan:\nbuck test caffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nsum node runtime before out version (1000 time run): 3558us\n\nsum node runtime after out version (1000 time run): 2173 us\n\nReviewed By: ajyu\n\nDifferential Revision: D26259744\n\nfbshipit-source-id: bc6a1231353d79a96d45f1cdc676e78a92469d85", "pr_number": "52225", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "f3f72b5c6b": {"title": "when BUILD_SPLIT_CUDA=ON, create dummy torch_cuda (#52305)", "body": "Summary:\nMakes dummy torch_cuda target to maintain better backwards compatibility.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52305\n\nTest Plan:\nRun `export BUILD_SPLIT_CUDA=ON && python setup.develop`.\nWhen it's done building, run `ls -lah` within `build/lib` to check that `libtorch_cuda.so` exists and is the same size as `libtorch.so`.\n\nReviewed By: walterddr\n\nDifferential Revision: D26463915\n\nPulled By: janeyx99\n\nfbshipit-source-id: 2b4cb8ee49bd75e11dc89d94b5956917b1800df1", "pr_number": "52305", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "63206ada8f": {"title": "Adding back CUDA 11.1 CI (#52171)", "body": "Summary:\n- Does not disable current CUDA 11.2 CI jobs\n- Does not reenable tests disabled for CUDA 11.2\n- Removes some unused docker images\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52171\n\nReviewed By: malfet\n\nDifferential Revision: D26461533\n\nPulled By: janeyx99\n\nfbshipit-source-id: e0e23117498320e72f2cbca547981c5894b48b68", "pr_number": "52171", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/scripts/windows_cuda_install.sh", ".circleci/scripts/windows_cudnn_install.sh"], "labels": ["Merged", "ci/all", "cla signed"]}, "52af23b912": {"title": "Update PyBind to official v2.6.2 tag (#52304)", "body": "Summary:\nThis moves PyBind module from pre-2.6.2 to an official 2.6.2 release hash.\nSee https://github.com/pybind/pybind11/releases/tag/v2.6.2\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52304\n\nReviewed By: samestep\n\nDifferential Revision: D26463177\n\nPulled By: malfet\n\nfbshipit-source-id: 6c6c5d0a4ff0c3f399370194e90dc8295fdd4bb2", "pr_number": "52304", "files_changed": ["third_party/pybind11"], "labels": ["Merged", "cla signed"]}, "4e36891e4f": {"title": "Temporary disable cat tests on MacOS due to Sandcastle failure", "body": "Summary: The `cat` op tests pass on device and local MacOS, but will fail during Sandcastle runs. Disabling them for now while we investigate why they fail in Sandcastle.\n\nTest Plan: `buck test //fbobjc/Apps/Internal/PyTorchPlaygroundMac:PyTorchPlaygroundMacTests`\n\nReviewed By: xta0\n\nDifferential Revision: D26468606\n\nfbshipit-source-id: 440369bb68641060fa98dbf37fb8825ee56083e0", "pr_number": null, "files_changed": ["aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": []}, "059c564ba4": {"title": "[DataLoader] Fix module import (#52224)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52224\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26429871\n\nPulled By: ejguan\n\nfbshipit-source-id: fcf2e5435658ecb92af1079def953b08cebb1f7f", "pr_number": "52224", "files_changed": ["test/test_datapipe.py", "torch/utils/data/__init__.py", "torch/utils/data/datapipes/__init__.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/utils/decoder.py"], "labels": ["Merged", "cla signed"]}, "9409a3a39b": {"title": "Check kernel launches in caffe2/operators (#52240)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52240\n\nTest Plan: Sandcastle tests\n\nReviewed By: xush6528\n\nDifferential Revision: D26408330\n\nfbshipit-source-id: 60779ba0e38c8f90e0e341c8faa2661e631112dd", "pr_number": "52240", "files_changed": ["caffe2/operators/abs_op.cu", "caffe2/operators/accuracy_op.cu", "caffe2/operators/acos_op.cu", "caffe2/operators/affine_channel_op.cu", "caffe2/operators/arg_ops.cu", "caffe2/operators/asin_op.cu", "caffe2/operators/atan_op.cu", "caffe2/operators/batch_gather_ops.cu", "caffe2/operators/batch_moments_op.cu", "caffe2/operators/batch_permutation_op.cu", "caffe2/operators/batch_sparse_to_dense_op.cu", "caffe2/operators/boolean_mask_ops.cu", "caffe2/operators/boolean_unmask_ops.cu", "caffe2/operators/bucketize_op.cu", "caffe2/operators/cast_op.cu", "caffe2/operators/cbrt_op.cu", "caffe2/operators/ceil_op.cu", "caffe2/operators/channel_backprop_stats_op.cu", "caffe2/operators/channel_shuffle_op.cu", "caffe2/operators/channel_stats_op.cu", "caffe2/operators/channelwise_conv3d_op_cudnn.cu", "caffe2/operators/clip_op.cu", "caffe2/operators/cos_op.cu", "caffe2/operators/cosh_op.cu", "caffe2/operators/cosine_embedding_criterion_op.cu", "caffe2/operators/cross_entropy_op.cu", "caffe2/operators/cube_op.cu", "caffe2/operators/deform_conv_op.cu", "caffe2/operators/depthwise_3x3_conv_op_cudnn.cu", "caffe2/operators/distance_op.cu", "caffe2/operators/dropout_op.cu", "caffe2/operators/elementwise_div_op.cu", "caffe2/operators/elementwise_linear_op.cu", "caffe2/operators/elementwise_mul_op.cu", "caffe2/operators/elementwise_ops.cu", "caffe2/operators/elu_op.cu", "caffe2/operators/erf_op.cu", "caffe2/operators/filler_op.cu", "caffe2/operators/find_op.cu", "caffe2/operators/floor_op.cu", "caffe2/operators/gather_op.cuh", "caffe2/operators/gelu_op.cu", "caffe2/operators/generate_proposals_op.cu", "caffe2/operators/generate_proposals_op_util_nms_gpu.cu", "caffe2/operators/glu_op.cu", "caffe2/operators/group_norm_op.cu", "caffe2/operators/gru_unit_op_gpu.cu", "caffe2/operators/half_float_ops.cu", "caffe2/operators/hard_sigmoid_op.cu", "caffe2/operators/instance_norm_op.cu", "caffe2/operators/integral_image_op.cu", "caffe2/operators/layer_norm_op.cu", "caffe2/operators/leaky_relu_op.cu", "caffe2/operators/lengths_tile_op.cu", "caffe2/operators/local_response_normalization_op.cu", "caffe2/operators/logit_op.cu", "caffe2/operators/lp_pool_op.cu", "caffe2/operators/lstm_unit_op_gpu.cu", "caffe2/operators/margin_ranking_criterion_op.cu", "caffe2/operators/max_pool_with_index.cu", "caffe2/operators/minmax_ops.cu", "caffe2/operators/mod_op.cu", "caffe2/operators/moments_op.cu", "caffe2/operators/multi_class_accuracy_op.cu", "caffe2/operators/normalize_ops.cu", "caffe2/operators/one_hot_ops.cu", "caffe2/operators/pack_segments.cu", "caffe2/operators/pad_op_gpu.cu", "caffe2/operators/piecewise_linear_transform_op.cu", "caffe2/operators/pool_op.cu", "caffe2/operators/pow_op.cu", "caffe2/operators/prelu_op.cu", "caffe2/operators/reciprocal_op.cu", "caffe2/operators/reduce_front_back_max_ops.cu", "caffe2/operators/reduce_front_back_sum_mean_ops.cu", "caffe2/operators/reduce_ops.cu", "caffe2/operators/reduction_ops.cu", "caffe2/operators/relu_n_op.cu", "caffe2/operators/relu_op.cu", "caffe2/operators/replace_nan_op.cu", "caffe2/operators/resize_3d_op.cu", "caffe2/operators/resize_op.cu", "caffe2/operators/reverse_packed_segs_op.cu", "caffe2/operators/rmac_regions_op.cu", "caffe2/operators/rms_norm_op.cu", "caffe2/operators/rnn/recurrent_network_op_gpu.cu", "caffe2/operators/roi_align_gradient_op.cu", "caffe2/operators/roi_align_op.cu", "caffe2/operators/roi_align_rotated_gradient_op.cu", "caffe2/operators/roi_align_rotated_op.cu", "caffe2/operators/roi_pool_op.cu", "caffe2/operators/rsqrt_op.cu", "caffe2/operators/scale_blobs_op.cu", "caffe2/operators/segment_reduction_op_gpu.cu", "caffe2/operators/selu_op.cu", "caffe2/operators/sequence_ops.cu", "caffe2/operators/sigmoid_op.cu", "caffe2/operators/sin_op.cu", "caffe2/operators/sinh_op.cu", "caffe2/operators/slice_op.cu", "caffe2/operators/softmax_ops.cu", "caffe2/operators/softplus_op.cu", "caffe2/operators/softsign_op.cu", "caffe2/operators/space_batch_op_gpu.cu", "caffe2/operators/sparse_to_dense_op.cu", "caffe2/operators/spatial_batch_norm_op_impl.cuh", "caffe2/operators/stump_func_op.cu", "caffe2/operators/swish_op.cu", "caffe2/operators/tan_op.cu", "caffe2/operators/tanh_op.cu", "caffe2/operators/thresholded_relu_op.cu", "caffe2/operators/tile_op.cu", "caffe2/operators/top_k.cu", "caffe2/operators/unique_ops.cu", "caffe2/operators/upsample_op.cu", "caffe2/operators/utility_ops.cu", "caffe2/operators/weighted_sample_op.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "4156588365": {"title": "[nnc] Allow 1 ulp tolerance in log approximation (#52165)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52165\n\nApparently bitwise identicality is too high a bar (I'm seeing\ndifferences at this level depending on the HW platform, e.g.,\nBroadwell is bitwise accurate but Skylake is 1ulp off).  But anyways\nVML is accurate to 1 ulp, so let's allow that.\nghstack-source-id: 121815001\n\nTest Plan: test_approx\n\nReviewed By: asuhan\n\nDifferential Revision: D26408079\n\nfbshipit-source-id: 46cbd1487c72ae7bc40567f2f72ed2b919707d0d", "pr_number": "52165", "files_changed": ["test/cpp/tensorexpr/test_approx.cpp"], "labels": ["Merged", "cla signed"]}, "fa393b56e7": {"title": "[static runtime] use NNC to generate logit, relu and tanh (#52322)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52322\n\ndiff BS=1\n```\nC2 run finished. Milliseconds per iter: 0.0564008. Iters per second: 17730.3\nPyTorch run finished. Milliseconds per iter: 0.0677778. Iters per second: 14754.1\n```\ndiff BS=20\n```\nC2 run finished. Milliseconds per iter: 0.51086. Iters per second: 1957.48\nPyTorch run finished. Milliseconds per iter: 0.510077. Iters per second: 1960.49\n```\n\nmaster BS=1\n```\nC2 run finished. Milliseconds per iter: 0.0567362. Iters per second: 17625.4\nPyTorch run finished. Milliseconds per iter: 0.0706478. Iters per second: 14154.7\n```\n\nmaster BS=20\n```\nC2 run finished. Milliseconds per iter: 0.510943. Iters per second: 1957.17\nPyTorch run finished. Milliseconds per iter: 0.516338. Iters per second: 1936.72\n```\n\nReviewed By: bertmaher\n\nDifferential Revision: D25407106\n\nfbshipit-source-id: 08595ba5e4be59e2ef95fb9b24da7e7671692395", "pr_number": "52322", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "bfc7e28188": {"title": "reland - ns for fx - stubs of the three APIs (compare weights, activations, activations with shadow) (#52302)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52302\n\nAdds the basic functionality for the three Numeric Suite core APIs to work on FX models:\n1. comparing weights\n2. comparing activations, with same input fed to both models\n3. comparing activations, with nodes of A shadowing nodes of B\n\nNote: there are a lot of TODOs in the code, and some/most of the APIs and implementation details may change as we iterate.  This is just the first PR.\n\nTest Plan:\nWe have unit test coverage for all of the APIs, for now this is with toy models:\n\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26463013\n\nPulled By: vkuzo\n\nfbshipit-source-id: e454115099ad18e4037d3c54986951cdffcab367", "pr_number": "52302", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "test/test_quantization.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/quantization/ns/utils.py", "torch/quantization/ns/weight_utils.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "bb9e0c625e": {"title": "[nnc] Add dummy reference to llvm::cfg::Update<BasicBlock> (#52321)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52321\n\nWe're seeing undefined references to this function in coverage builds.\nI don't even know why the toolchain is trying to look for it, because it's not\nactually used in our code anywhere.\n\nObviously dropping in a dummy reference is a workaround more than a real\nsolution, but I'd like to get the coverage build back online.\nghstack-source-id: 121818432\n\nTest Plan: `buck build mode/dbgo-cov //caffe2/test/...`\n\nReviewed By: asuhan\n\nDifferential Revision: D26467484\n\nfbshipit-source-id: 4de8d950b03d0818ffc317fc1bed9be8cf470352", "pr_number": "52321", "files_changed": ["torch/csrc/jit/tensorexpr/llvm_jit.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "35b0560ea2": {"title": "Automated submodule update: FBGEMM (#52255)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/7f3baec49699a1a45c87c72ada8fae0e91069b72\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52255\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: jspark1105\n\nDifferential Revision: D26443031\n\nfbshipit-source-id: 9e2758c73a15e7d2b5aefa5bc38270404cb5862a", "pr_number": "52255", "files_changed": ["third_party/fbgemm"], "labels": ["Merged", "cla signed", "open source"]}, "8c185e62f9": {"title": "torchvision hipify revamp fix (#51453)", "body": "Summary:\nThe torchvision build error from hipify revamp, \"KeyError: '/usr/include/libpng16/png.h'\" is fixed in this PR\n\nDescription:\n\nTraceback (most recent call last):\n  File \"setup.py\", line 471, in <module>\n    ext_modules=get_extensions(),\n  File \"setup.py\", line 329, in get_extensions\n    extra_compile_args=extra_compile_args\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/cpp_extension.py\", line 892, in CUDAExtension\n    is_pytorch_extension=True,\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py\", line 978, in hipify\n    clean_ctx=clean_ctx)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py\", line 212, in preprocess\n    hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py\", line 175, in preprocess_file_and_save_result\n    hip_clang_launch, is_pytorch_extension, clean_ctx, show_progress)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py\", line 792, in preprocessor\n    output_source = RE_ANGLE_HEADER.sub(mk_repl('#include <{0}>', False), output_source)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/hipify/hipify_python.py\", line 785, in repl\n    value = HIPIFY_FINAL_RESULT[header_filepath][\"hipified_path\"]\nKeyError: '/usr/include/libpng16/png.h'\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51453\n\nReviewed By: agolynski\n\nDifferential Revision: D26459979\n\nPulled By: fmassa\n\nfbshipit-source-id: f653f55fd34c71314e6c6682217f84b2d1e49335", "pr_number": "51453", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["Merged", "cla signed", "module: vision", "open source", "triaged"]}, "f235c65a2b": {"title": "[TorchScript] C++ interface of to_<backend> (Re-land) (#52340)", "body": "Summary:\nThis is a re-land off https://github.com/pytorch/pytorch/pull/51797 with fix for spurious libcuda dependency\n\nFix limits the scope of `no-as-needed` linker flag to just `jitbackend_test`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52340\n\nReviewed By: agolynski, iseeyuan\n\nDifferential Revision: D26476168\n\nPulled By: malfet\n\nfbshipit-source-id: f909428af82182b3bffd020ca18cca7a9b5846b6", "pr_number": "52340", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_lib.cpp", "tools/build_variables.bzl", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "680c4ce1dd": {"title": "[PyTorch] Avoid some extra intrusive_ptr<Tuple> copies in Unpickler (#51902)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51902\n\nThese seem like straightforward improvements. (I don't have measurements; feel free to reject if you're skeptical)\nghstack-source-id: 121278775\n\nTest Plan: CI\n\nReviewed By: qizzzh\n\nDifferential Revision: D26322438\n\nfbshipit-source-id: d393a32cc34bb68bc4f804f4b1cc5a8af27763c9", "pr_number": "51902", "files_changed": ["torch/csrc/jit/serialization/unpickler.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6e1a5b1196": {"title": "[PyTorch] Use real if constexpr behind macro in hot template (#51368)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51368\n\nThis seems to noticably reduce build times, at least for\nRegisterCPU.cpp. It makes sense that a compiler builtin would be\nfaster than simulating the same builtin with templates.\n\nIdentified with templight.\nghstack-source-id: 121378959\n\nTest Plan:\nConfirmed this speeds up RegisterCPU.cpp optimized build by\nsimply running builds under `time(1)`:\n\nprevious diff: [50.53, 50.41, 50.57, 50.67, 50.94]\nmean: 50.6 std: 0.179\n\nthis diff: [45.71, 45.89, 46.21, 48.51, 45.84]\nmean: 46.4 std: 1.05\n\nReviewed By: bhosmer\n\nDifferential Revision: D26154964\n\nfbshipit-source-id: 62ee2f5a872007db032dfebf7ad4d1b6e1ce63d1", "pr_number": "51368", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h"], "labels": ["Merged", "cla signed"]}, "4501b52fe5": {"title": "Benchmark for torch.ops.quantized.linear_prepack_fp16 operator (#52229)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52229\n\nCreate benchmarks for\ntorch.ops.quantized.linear_prepack_fp16  and torch.ops.quantized.linear_unpack_fp16 operators\n\nBenchmark for these operators are written in the same format as the other benchmarks for other operators.\n\nTest Plan:\nlinear_prepack_fp16 test was successfully run with various parameters:\n\nSample test run output:\n ----------------------------------------\n PyTorch/Caffe2 Operator Micro-benchmarks\n ----------------------------------------\n Tag : long\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M8_N32_K256_cpu\n Input: M: 8, N: 32, K: 256, device: cpu\nForward Execution Time (us) : 14.002\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M8_N32_K512_cpu\n Input: M: 8, N: 32, K: 512, device: cpu\nForward Execution Time (us) : 14.114\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M8_N64_K256_cpu\n Input: M: 8, N: 64, K: 256, device: cpu\nForward Execution Time (us) : 19.355\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M8_N64_K512_cpu\n Input: M: 8, N: 64, K: 512, device: cpu\nForward Execution Time (us) : 19.056\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M128_N32_K256_cpu\n Input: M: 128, N: 32, K: 256, device: cpu\nForward Execution Time (us) : 115.963\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M128_N32_K512_cpu\n Input: M: 128, N: 32, K: 512, device: cpu\nForward Execution Time (us) : 116.259\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M128_N64_K256_cpu\n Input: M: 128, N: 64, K: 256, device: cpu\nForward Execution Time (us) : 229.336\n\n Benchmarking PyTorch: linear_prepack_fp16\n Mode: Eager\n Name: linear_prepack_fp16_M128_N64_K512_cpu\n Input: M: 128, N: 64, K: 512, device: cpu\nForward Execution Time (us) : 220.016\n\nlinear_unpack_fp16 test was successfully run with identical parameters\n\nReviewed By: b-koopman\n\nDifferential Revision: D26403343\n\nfbshipit-source-id: 11a98e56177952b94f291006975b0b719f48d1b9", "pr_number": "52229", "files_changed": ["benchmarks/operator_benchmark/pt/linear_prepack_fp16_test.py", "benchmarks/operator_benchmark/pt/linear_unpack_fp16_test.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "a6e94d274f": {"title": "[Pytorch] Add python binding to use mobile cpu allocator. (#52323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52323\n\nUsing default cpu allocator for ops executed on qnnpack backend will result in\nasan failures with heap overflow since qnnpack (and xnnpack) can access input\nbeyond their and/beginning.\n\nHere we are enabling this feature specifically to enable dynamic sparse linear op test\nusing qnnpack engine. In dynamic linear op, the fp32 bias is not packed and\nhence can result in out-of-bound access.\n\nTest Plan: test_set_default_mobile_cpu_allocator.py\n\nReviewed By: z-a-f\n\nDifferential Revision: D26263481\n\nfbshipit-source-id: a49227cac7e6781b0db4a156ca734d7671972d9f", "pr_number": "52323", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "test/run_test.py", "test/test_set_default_mobile_cpu_allocator.py", "torch/csrc/Module.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "87ebaa4eb1": {"title": "[Pytorch, Sparsity] Integrate sparse qnnpack operator in framework", "body": "Summary:\nAdd QNNPACK specific packed params for sparse linear.\nAdd sparse linear dynamic op with appropriate registration.\nAdd python side LinearDynamic module for sparsity.\nAdd tests to validate sparse linear qnnpack kernels.\nNote that since these test are mostly run on x86 platform and\ngiven that 1x4 sparse kernels are implemented both in sse and arm,\nLinearDynamic at the moment defaults to 1x4 pattern.\nPlan is to add another diff that will allow a global override for 8x1 pattern\nsuch that prepare/convert flow can work for exporting model for mobile.\n\nTest Plan: buck run caffe2/torch/fb/model_optimization:sparsity_test\n\nReviewed By: z-a-f\n\nDifferential Revision: D26263480\n\nfbshipit-source-id: 04ab60aec624d1ecce8cfb38b79c7e94f501cdf6", "pr_number": null, "files_changed": ["torch/testing/_internal/common_quantized.py"], "labels": []}, "d8bb932245": {"title": "Support AST rewriting for submodules (#52297)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52297\n\nBefore, an `nn.Module` with submodules would fail AST rewriting with `TypeError: 'RewrittenModule' object does not support item assignment`. (Try the `test_ast_rewriter_reassigns_submodules` test case on `master`.) This PR fixes the issue as well as adding additional test cases\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26483820\n\nPulled By: ansley\n\nfbshipit-source-id: 757e898dc2b0a67daf2bd039d555b85f4e443322", "pr_number": "52297", "files_changed": ["test/test_fx.py", "torch/fx/experimental/rewriter.py"], "labels": ["Merged", "cla signed", "fx"]}, "99619ea3b7": {"title": "Automated submodule update: FBGEMM (#52354)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/c52008892732932d63829802403b54f551d1560f\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52354\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: ejguan\n\nDifferential Revision: D26484989\n\nfbshipit-source-id: c9ccce0141be49c57b80e14992f842364bb18a00", "pr_number": "52354", "files_changed": ["third_party/fbgemm"], "labels": ["Merged", "cla signed", "open source"]}, "a8885ee7e6": {"title": "[BE][typing] add caffe2/torch proto stubs (1 of 2) (#52341)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52341\n\nAdd type stubs for caffe2 protos and scripts for generating them.\n\nIt's worth calling special attention to the following. In order to make `DeviceType`s like `CPU`, `CUDA`, etc. directly accessible from the `caffe2_pb2` module, they are currently freedom-patched into it in `caffe2/python/__init__.py`. This is not ideal: it would be better if these were autogenerated when the protobuf definitions were created by using `allow_alias = true` in the `DeviceTypeProto` definition in `caffe2.proto`.\n\nHowever, it is impossible to do this currently without significant effort. The issue is that the generated proto constants would conflict with various constants defined in the C++ caffe2 codebase in `caffe2_pb.h`. We cannot simply remove these constants and replace them with the caffe2 DeviceTypeProto constants, because a huge portion of code expects `at::DeviceType` constants defined in `core/DeviceType.h` (apparently duplicated to avoid having to figure out how to autogenerate the protobuf definitions using cmake for ATen).\n\nInstead, we make a best-effort to add additional definitions in `caffe2_pb2.py` by looking for any freedom-patched constants in `caffe2/python/__init__.py` and making sure they have corresponding stubs in the pyi (see `gen_proto_typestubs_helper.py`).\n\nTest Plan: Make sure CI is green; we're just adding stubs.\n\nReviewed By: d4l3k\n\nDifferential Revision: D26331875\n\nfbshipit-source-id: 2eea147e5bf393542f558ff8cf6385c47624b770", "pr_number": "52341", "files_changed": ["caffe2/proto/caffe2_legacy_pb2.pyi", "caffe2/proto/caffe2_pb2.pyi", "caffe2/proto/gen_proto_typestubs.sh", "caffe2/proto/gen_proto_typestubs_helper.py", "caffe2/proto/hsm_pb2.pyi", "caffe2/proto/metanet_pb2.pyi", "caffe2/proto/predictor_consts_pb2.pyi", "caffe2/proto/prof_dag_pb2.pyi", "caffe2/proto/torch_pb2.pyi"], "labels": ["Merged", "cla signed", "fb-exported"]}, "975d9f2551": {"title": "Mypy fixes for pytorch master (#52090)", "body": "Summary:\nThis PR adds fixes mypy issues on the current pytorch main branch. In special, it replaces occurrences of `np.bool/np.float` to `np.bool_/np.float64`, respectively:\n\n```\ntest/test_numpy_interop.py:145: error: Module has no attribute \"bool\"; maybe \"bool_\" or \"bool8\"?  [attr-defined]\ntest/test_numpy_interop.py:159: error: Module has no attribute \"float\"; maybe \"float_\", \"cfloat\", or \"float64\"?  [attr-defined]\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52090\n\nReviewed By: walterddr\n\nDifferential Revision: D26469596\n\nPulled By: malfet\n\nfbshipit-source-id: e55a5c6da7b252469e05942e0d2588e7f92b88bf", "pr_number": "52090", "files_changed": ["caffe2/contrib/aten/aten_test.py", "test/test_numpy_interop.py", "test/test_type_hints.py", "torch/cuda/amp/autocast_mode.py", "torch/utils/tensorboard/summary.py"], "labels": ["Merged", "cla signed", "module: typing", "open source", "triaged"]}, "c442776f3c": {"title": "[PyTorch] Debug-gate static_assert in KernelFunction::makeFromUnboxedFunctor (#51367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51367\n\nTemplight said that this assertion was taking about 5% of build time for RegisterCPU.cpp (a hopefully-representative example I picked to shorten my iteration cycle).\n\nI've debug-gated it on the grounds that 1) we at least try to build\neverything in debug mode and 2) optimized builds presumably take\nlonger in general, so we can more afford to pay the build time cost in\ndebug builds.\n\nThe win is not entirely clear; please see the test plan for details.\nghstack-source-id: 121378960\n\nTest Plan:\n1) Built RegisterCPU.cpp with -ftime-trace before and after. It doesn't seem to call out any difference in the details, but the overall time is stably down more like 10% (55s before and 49s after).\n2) Did a full rebuild of aten-cpu with -ftime-trace before and\nafter. No significant difference in build times shown (it says *after*\nis a regression, but it's using wall-time data and the machine is\nloaded during builds so there's some noise).\n3) Re-profiled with Templight.\n\nBefore:\n\n{F366557311}\n\nAfter:\n\n{F366557501}\n\nNot sure what to conclude overall. A known problem with templight is that template instantiations form more of a dependency graph than a tree because they're cached internally, so eliminating the first caller of a template may just move the time to another caller. However, it looks like we have actually reduced is_functor traffic.\n\nUPDATE: I don't think that the -ftime-trace measurement was reliable; it seems to skew running times. I built this diff vs its base 5 times and measured the CPU (\"user\") time each time. Results (in seconds):\n\nprevious diff: [51.97, 50.54, 50.49, 52.89, 51.61]\nmean: 51.5 std: 0.906\n\nthis diff: [50.53, 50.41, 50.57, 50.67, 50.94]\nmean: 50.6 std: 0.179\n\nReviewed By: ezyang\n\nDifferential Revision: D26153793\n\nfbshipit-source-id: 9a66912c1b2b068f453e78be57454e4e62b7107b", "pr_number": "51367", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction_impl.h"], "labels": ["Merged", "cla signed"]}, "a9f5e7229e": {"title": "[PyTorch] Remove reference_cast in make_boxed_from_unboxed_functor (#51319)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51319\n\nWe were going out of our way to accommodate `IValue::to<Tensor>` returning a copy of the inner Tensor. `IValue::toTensor` is capable of returning a reference without copying, so if we use it directly, we can allow kernels that want to take `Tensor &` to do so!\nAs a bonus, we get reduced build times.\nghstack-source-id: 121378961\n\nTest Plan:\nRely on CI for correctness.\nProfiled build time with -ftime-trace for RegisterCPU.cpp using an extracted build invocation.\n\nBefore: P168244900\n\nAfter: P168245014\n\nNote reduced time spent compiling make_boxed_from_unboxed_functor.\n\nI also ran the AdIndexer benchmark (https://fb.quip.com/ztERAYjuzdlr) with static runtime disabled and batch size 1 to see how big the effect on boxed call performance was (any kernels that take `Tensor&` or `const Tensor&` should now actually save a refcount bump). Looks like it was roughly 1% better:\n\nBefore: 124-125 usec/iter\nAfter: 122-123 usec/iter\n\nReviewed By: bhosmer\n\nDifferential Revision: D26138549\n\nfbshipit-source-id: b0f830527da360c542c815bef2f7e1692615b32a", "pr_number": "51319", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "torch/custom_class_detail.h"], "labels": ["Merged", "cla signed"]}, "b2aa63f17c": {"title": "[PyTorch] Fix return value of IValue::to for Tensor/String (#51463)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51463\n\nWe can make the return type of the `to()` template match the return type of toFoo() by using the same technique we use for `list_element_to_const_ref`. Also simplifies `list_element_to_const_ref`.\nghstack-source-id: 121363468\n\nTest Plan:\nCI\n\nbuilt and ran AdIndexer benchmark w/ batch size 1 under perf stat\n--repeat 5 to make sure it didn't regress\n\nReviewed By: bhosmer\n\nDifferential Revision: D26163848\n\nfbshipit-source-id: b8563263b9f9fa5311c7d7cedc89e28bc5badda0", "pr_number": "51463", "files_changed": ["aten/src/ATen/core/List.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/core/ivalue_to.h"], "labels": ["Merged", "cla signed"]}, "0e2520baae": {"title": "[PyTorch] Don't read 1 char per iteration in Unpickler::readString (#51901)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51901\n\nIt's much more efficient to read multiple chars with 1 memcpy than to call `read<char>` multiple times.\nghstack-source-id: 121278774\n\nTest Plan:\nRun WireSerializerBench before/after for small tensors:\n\n```\n/tmp/WireSerializerBench.Reader --real_data /mnt/homedir/hwwang/test_serialized_api_request --real_pytorch_api_request --bm_regex '[Ss]mall'\n```\n\nBefore:\n```\nDeSerializeWire(Small)                                       7.65us  130.65K\nDeSerializeWire(small_Zstd)                      100.49%     7.62us  131.29K\nDeSerializeWire(small_Snappy)                    100.49%     7.62us  131.29K\nDeSerializeWireIValue(Small)                      82.89%     9.23us  108.30K\nDeSerializeWireIValue(small_Zstd)                 82.87%     9.24us  108.27K\nDeSerializeWireIValue(small_Snappy)               82.33%     9.30us  107.57K\nDeSerializeC2ToBlob(small_NoCompress)           1150.28%   665.39ns    1.50M\nDeSerializeC2ToBlob(small_Zstd)                 1149.70%   665.72ns    1.50M\nDeSerializeC2ToBlob(small_Zstd_Fast)            1150.94%   665.00ns    1.50M\nDeSerializeC2ToBlob(Small_Snappy)               1151.70%   664.57ns    1.50M\nDeSerializeC2ToString(small)                    9297.81%    82.32ns   12.15M\n```\n\nAfter:\n```\nDeSerializeWire(Small)                                       6.86us  145.84K\nDeSerializeWire(small_Zstd)                      100.52%     6.82us  146.60K\nDeSerializeWire(small_Snappy)                    100.13%     6.85us  146.03K\nDeSerializeWireIValue(Small)                      83.94%     8.17us  122.42K\nDeSerializeWireIValue(small_Zstd)                 84.00%     8.16us  122.50K\nDeSerializeWireIValue(small_Snappy)               84.53%     8.11us  123.28K\nDeSerializeC2ToBlob(small_NoCompress)           1019.48%   672.58ns    1.49M\nDeSerializeC2ToBlob(small_Zstd)                 1020.03%   672.23ns    1.49M\nDeSerializeC2ToBlob(small_Zstd_Fast)            1020.59%   671.85ns    1.49M\nDeSerializeC2ToBlob(Small_Snappy)               1020.30%   672.05ns    1.49M\nDeSerializeC2ToString(small)                    7709.63%    88.94ns   11.24M\n```\n\nSecond run after to demonstrate it wasn't just variance:\n\n```\nDeSerializeWire(Small)                                       6.92us  144.57K\nDeSerializeWire(small_Zstd)                       99.24%     6.97us  143.47K\nDeSerializeWire(small_Snappy)                     99.58%     6.95us  143.97K\nDeSerializeWireIValue(Small)                      84.83%     8.15us  122.63K\nDeSerializeWireIValue(small_Zstd)                 84.72%     8.16us  122.49K\nDeSerializeWireIValue(small_Snappy)               84.59%     8.18us  122.29K\nDeSerializeC2ToBlob(small_NoCompress)           1031.03%   670.89ns    1.49M\nDeSerializeC2ToBlob(small_Zstd)                 1030.64%   671.14ns    1.49M\nDeSerializeC2ToBlob(small_Zstd_Fast)            1013.39%   682.57ns    1.47M\nDeSerializeC2ToBlob(Small_Snappy)               1013.95%   682.19ns    1.47M\nDeSerializeC2ToString(small)                    8155.98%    84.81ns   11.79M\n```\n\nBy the way, this gets us closer to deserialization parity for the real data sample included in D26049387:\n\nbaseline:\n```\nDeSerializeWire(RealData)                                    7.34ms   136.24\nDeSerializeWire(RealData_Zstd)                    99.95%     7.34ms   136.17\nDeSerializeWire(RealData_Snappy)                 100.09%     7.33ms   136.36\nDeSerializeWireIValue(RealData)                   82.69%     8.88ms   112.65\nDeSerializeWireIValue(RealData_Zstd)              82.76%     8.87ms   112.75\nDeSerializeWireIValue(RealData_Snappy)            82.68%     8.88ms   112.64\nDeSerializeC2ToBlob(RealData_NoCompress)         116.87%     6.28ms   159.23\nDeSerializeC2ToBlob(RealData_Zstd)               117.33%     6.26ms   159.85\nDeSerializeC2ToBlob(RealData_Zstd_Fast)          117.38%     6.25ms   159.91\nDeSerializeC2ToBlob(RealData_Snappy)             117.61%     6.24ms   160.23\nDeSerializeC2ToString(RealData)                 4571.81%   160.55us    6.23K\n```\n\nwith this diff:\n```\nDeSerializeWire(RealData)                                    6.57ms   152.17\nDeSerializeWire(RealData_Zstd)                   100.17%     6.56ms   152.43\nDeSerializeWire(RealData_Snappy)                 100.09%     6.57ms   152.31\nDeSerializeWireIValue(RealData)                   83.06%     7.91ms   126.40\nDeSerializeWireIValue(RealData_Zstd)              83.16%     7.90ms   126.54\nDeSerializeWireIValue(RealData_Snappy)            83.22%     7.90ms   126.64\nDeSerializeC2ToBlob(RealData_NoCompress)         104.02%     6.32ms   158.29\nDeSerializeC2ToBlob(RealData_Zstd)               103.46%     6.35ms   157.43\nDeSerializeC2ToBlob(RealData_Zstd_Fast)          104.64%     6.28ms   159.23\nDeSerializeC2ToBlob(RealData_Snappy)             104.65%     6.28ms   159.25\nDeSerializeC2ToString(RealData)                 4051.03%   162.22us    6.16K\n```\n\nReviewed By: qizzzh\n\nDifferential Revision: D26321083\n\nfbshipit-source-id: 92d45e760580bb290078ddac84128174daef0e55", "pr_number": "51901", "files_changed": ["torch/csrc/jit/serialization/unpickler.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e36a900e89": {"title": "[tools] Use anonymous access to access S3 bucket (#52338)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52338\n\nReviewed By: samestep\n\nDifferential Revision: D26475415\n\nPulled By: malfet\n\nfbshipit-source-id: a96e8868b11e9e7691daa54ff2baef4446605ba7", "pr_number": "52338", "files_changed": ["tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "324c6aada1": {"title": "BFloat16: enable prepacked weights's inference (#48922)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48922\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D25537188\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ab6eb1ba8cffb5ba9d00d05db8ef616628f8c932", "pr_number": "48922", "files_changed": ["aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/mkldnn/Linear.cpp", "aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp", "aten/src/ATen/native/mkldnn/Normalization.cpp", "aten/src/ATen/native/mkldnn/Pooling.cpp", "aten/src/ATen/native/mkldnn/Relu.cpp", "aten/src/ATen/native/mkldnn/Utils.h", "test/test_mkldnn.py", "torch/utils/mkldnn.py"], "labels": ["Merged", "cla signed", "open source"]}, "cbede834d4": {"title": "[JIT] Add support for default argument values to Torchbind (#51253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51253\n\n**Summary**\nThis commit adds support to Torchbind for specifying default values for\narguments of custom class methods.\n\n**Test Plan**\nThis commit adds a unit test to `test_torchbind.py` that exercises this\nfeature.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D26131529\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 68bc86b045dd2f03ba41e1a116081a6eae6ba9ff", "pr_number": "51253", "files_changed": ["test/cpp/jit/test_custom_class_registrations.cpp", "test/jit/test_torchbind.py", "torch/custom_class.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "72d1ccd3ca": {"title": "Revert D26263480: [Pytorch, Sparsity] Integrate sparse qnnpack operator in framework", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26263480 (https://github.com/pytorch/pytorch/commit/87ebaa4eb190d1da0ce3b839c59d015cb53f9a62)\n\nOriginal commit changeset: 04ab60aec624\n\nfbshipit-source-id: ad7690eebdc4b2782c2c94b5bbadbde4ef7c0627", "pr_number": null, "files_changed": ["torch/testing/_internal/common_quantized.py"], "labels": []}, "4305609d66": {"title": "Fix complex acos edge cases (#52287)", "body": "Summary:\nUse `std::acos` even when avx2 is available\nAdd slow but accurate implementation of complex arc cosine based on\nW. Kahan \"Branch Cuts for Complex Elementary Functions\" paper, where\ncacos(z).re = 2*atan2(sqrt(1-z).re(), sqrt(1+z).re())\ncacos(z).im = asinh((sqrt(conj(1+z))*sqrt(1-z)).im())\n\nFixes https://github.com/pytorch/pytorch/issues/42952\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52287\n\nReviewed By: walterddr\n\nDifferential Revision: D26455027\n\nPulled By: malfet\n\nfbshipit-source-id: a81ce1ba4953eff4d3c2a265ef9199896a67b240", "pr_number": "52287", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_complex_float.h", "c10/util/complex_math.cpp", "c10/util/complex_math.h", "test/test_unary_ufuncs.py"], "labels": ["Merged", "cla signed", "module: complex"]}, "059ee85ca4": {"title": "[PyTorch] Devirtualize TensorImpl::storage() (#51050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51050\n\nSubclasses want to be able to make storage() calls throw, so\nwe find some free space in TensorImpl to add a flag that they can set\nto make that happen without making storage() virtual. It should still\nbe inlineable.\nghstack-source-id: 121819684\n\nTest Plan:\nCompared `perf stat` on 1M iterations on AdIndexer benchmark before/after\n\nBefore:\n```\n         74,483.15 msec task-clock                #    0.999 CPUs utilized            ( +-  0.14% )\n            16,637      context-switches          #    0.223 K/sec                    ( +- 11.97% )\n                 3      cpu-migrations            #    0.000 K/sec                    ( +-  7.20% )\n           107,085      page-faults               #    0.001 M/sec                    ( +-  2.39% )\n   147,356,440,831      cycles                    #    1.978 GHz                      ( +-  0.14% )  (50.06%)\n   278,678,430,378      instructions              #    1.89  insn per cycle           ( +-  0.01% )  (50.05%)\n    43,540,698,177      branches                  #  584.571 M/sec                    ( +-  0.01% )  (50.05%)\n       141,028,843      branch-misses             #    0.32% of all branches          ( +-  1.00% )  (50.05%)\n\n```\n\nAfter:\n```\n         74,178.77 msec task-clock                #    0.999 CPUs utilized            ( +-  0.31% )\n            17,125      context-switches          #    0.231 K/sec                    ( +-  3.41% )\n                 3      cpu-migrations            #    0.000 K/sec\n           109,535      page-faults               #    0.001 M/sec                    ( +-  1.04% )\n   146,803,364,372      cycles                    #    1.979 GHz                      ( +-  0.30% )  (50.03%)\n   277,726,600,254      instructions              #    1.89  insn per cycle           ( +-  0.02% )  (50.03%)\n    43,299,659,815      branches                  #  583.720 M/sec                    ( +-  0.03% )  (50.03%)\n       130,504,094      branch-misses             #    0.30% of all branches          ( +-  1.14% )  (50.03%)\n\n```\n\nLooks like approximately 0.3% instruction count win (and similarly for cycles, but that's within noise).\n\nReviewed By: ezyang\n\nDifferential Revision: D26013815\n\nfbshipit-source-id: 07939957929070e18b9981d492d8279c9bb33c55", "pr_number": "51050", "files_changed": ["aten/src/ATen/BatchedTensorImpl.cpp", "aten/src/ATen/BatchedTensorImpl.h", "aten/src/ATen/OpaqueTensorImpl.h", "aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/native/metal/MetalTensorImpl.h", "aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.h", "aten/src/ATen/quantized/QTensorImpl.cpp", "aten/src/ATen/quantized/QTensorImpl.h", "aten/src/ATen/test/basic.cpp", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/UndefinedTensorImpl.cpp", "c10/core/UndefinedTensorImpl.h"], "labels": ["Merged", "cla signed"]}, "6dabe0b291": {"title": "[Dist Profiling] Enable dist profiling for DDP (gloo only) (#52031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52031\n\nCloses https://github.com/pytorch/pytorch/issues/52020\nEnsures that we can profile collectives in DDP by propagating the profiler threadLocalState appropriately. As described in the above issue, before this wouldn't work as the profiler would only be enabled on the main thread.\nghstack-source-id: 121818080\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26356192\n\nfbshipit-source-id: 0158b5833a3f857a0b4b2943ae3037e9d998dfd1", "pr_number": "52031", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "440fddf07b": {"title": "Remove unnecessary statement in `capture_stderr` (#52366)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52366\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26489602\n\nPulled By: ansley\n\nfbshipit-source-id: dd0db0a631840b5efd5dc48887fbf724781c6be4", "pr_number": "52366", "files_changed": ["torch/testing/_internal/jit_utils.py"], "labels": ["Merged", "cla signed"]}, "edf8130e9e": {"title": "[PyTorch] Add set_data_ptr_noswap & use where possible (#52244)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52244\n\n`StorageImpl::set_data_ptr` returns the old pointer and thus has to do extra\nwork. Found because `std::swap<at::DataPtr>` was showing up in\nprofiling, although at < 1%.\nghstack-source-id: 121795131\n\nTest Plan:\nRun AdIndexer benchmark under `perf stat`.\n\nBefore:\n```\n         17,990.01 msec task-clock                #    0.998 CPUs utilized            ( +-  0.43% )\n             6,550      context-switches          #    0.364 K/sec                    ( +- 31.42% )\n                 3      cpu-migrations            #    0.000 K/sec                    ( +-  7.14% )\n           103,820      page-faults               #    0.006 M/sec                    ( +-  2.47% )\n    35,610,511,494      cycles                    #    1.979 GHz                      ( +-  0.40% )  (50.03%)\n    71,651,045,779      instructions              #    2.01  insn per cycle           ( +-  0.07% )  (50.02%)\n    11,679,947,910      branches                  #  649.246 M/sec                    ( +-  0.10% )  (50.03%)\n        69,088,927      branch-misses             #    0.59% of all        branches          ( +-  0.24% )  (50.06%\n```\n\nAfter:\n```\n         17,896.20 msec task-clock                #    0.999 CPUs utilized            ( +-  0.24% )\n             4,011      context-switches          #    0.224 K/sec                    ( +- 27.77% )\n                 3      cpu-migrations            #    0.000 K/sec\n           100,350      page-faults               #    0.006 M/sec                    ( +-  1.58% )\n    35,418,702,208      cycles                    #    1.979 GHz                      ( +-  0.23% )  (50.05%)\n    71,449,334,935      instructions              #    2.02  insn per cycle           ( +-  0.09% )  (50.03%)\n    11,652,819,899      branches                  #  651.134 M/sec                    ( +-  0.12% )  (50.04%)\n        69,744,411      branch-misses             #    0.60% of all branches          ( +-  0.53% )  (50.06%)\n```\n\nCycles difference is within the noise, but it looks like we have an\n0.28% instruction count win, which is outside the noise (and fits with\nintuition that this should be better).\n\nReviewed By: hlu1\n\nDifferential Revision: D26437297\n\nfbshipit-source-id: bf0fceccf6ad78f1497b03ccb4cdfd1a21c6846c", "pr_number": "52244", "files_changed": ["aten/src/ATen/cpp_custom_type_hack.h", "aten/src/THC/THCStorage.cpp", "c10/core/Storage.h", "c10/core/StorageImpl.h", "c10/core/TensorImpl.h", "torch/csrc/jit/runtime/static/impl.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "51c28e4d7e": {"title": "[ROCm] enable fft tests (#51581)", "body": "Summary:\nThis PR enable some failing unit tests for fft in pytorch on ROCM.\n\nThe reason these tests were failing was due to hipfft clobbering inputs causing a mismatch in tests that were checking that applying ffts and their reverse would get you back to the input.\n\nWe solve this by cloning the input using an existing flag on the ROCM platform.\n\nThere PR doesnot enable all fft tests. There are other issues that need to be resolved before that can happen.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51581\n\nReviewed By: ejguan\n\nDifferential Revision: D26489344\n\nPulled By: seemethere\n\nfbshipit-source-id: 472fce8e514adcf91e7f46a686cbbe41e91235a9", "pr_number": "51581", "files_changed": ["aten/src/ATen/native/cuda/CuFFTPlanCache.h", "test/test_spectral_ops.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "6c875f17ca": {"title": "Enable PyTorch_QNNPACK for Apple Silicon builds (#52308)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52308\n\nReviewed By: janeyx99\n\nDifferential Revision: D26488223\n\nPulled By: malfet\n\nfbshipit-source-id: ecc3925f3374ad4a9e8f740b007bf6f3b23d8e51", "pr_number": "52308", "files_changed": [".jenkins/pytorch/macos-build.sh"], "labels": ["Merged", "cla signed"]}, "e7f28d4241": {"title": "[PyTorch Mobile] Restructure DispatchStub::operator() code to move template independent code into an external method (#51403)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51403\n\nTurns out this isn't a new idea. swolchok posted about this a while ago and this was discussed in the composability group.\n\nLinks to posts:\n* Template Hoisting: https://fb.workplace.com/groups/llvm.gcc/permalink/2321250667923535/\n* C++: Most of the code in a template should depend on the template parameter(s): https://fb.workplace.com/groups/2088132188069398/permalink/2224983771050905/\nghstack-source-id: 121873716\n\nTest Plan: Results in a 10KiB size reduction on fbios. Will re-run BSB for igios.\n\nReviewed By: swolchok\n\nDifferential Revision: D25859327\n\nfbshipit-source-id: 915abebb2643f8ac9a901f3b4d79c63f4bbb5fee", "pr_number": "51403", "files_changed": ["aten/src/ATen/native/DispatchStub.cpp", "aten/src/ATen/native/DispatchStub.h"], "labels": ["Merged", "cla signed"]}, "5003d417d4": {"title": "[PyTorch Mobile] Outline DispatchStub::get_call_ptr() (#51908)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51908\n\nAs suggested by swolchok. The idea is to outline `DispatchStub::get_call_ptr()` and also not have it specialized per instantiation of the `DispatchStub` class since it results in size bloat for mobile.\n\nghstack-source-id: 121873712\n\nTest Plan:\nBuild + Circle CI.\n\n### lightspeed\n\n```\nD26324255-V8 (https://www.internalfb.com/intern/diff/D26324255/?dest_number=121462800)\n\nmessenger-experimental-optimized-device: Succeeded\nChange in Download Size for arm64 + 3x assets variation: -13.0 KiB\nChange in Uncompressed Size for arm64 + 3x assets variation: -51.4 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:325359338899206@base/bsb:325359338899206@diff/\n```\n\n### igios\n\n```\nD26324255-V8 (https://www.internalfb.com/intern/diff/D26324255/?dest_number=121462800)\n\nigios: Succeeded\nChange in Download Size for arm64 + 3x assets variation: -9.2 KiB\nChange in Uncompressed Size for arm64 + 3x assets variation: -23.4 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:823799391811488@base/bsb:823799391811488@diff/\n```\n\n### fbios-pika\n\n```\nD26324255-V8 (https://www.internalfb.com/intern/diff/D26324255/?dest_number=121462800)\n\nfbios-pika: Succeeded\nChange in Download Size for arm64 + 3x assets variation: -8.0 KiB\nChange in Uncompressed Size for arm64 + 3x assets variation: -22.7 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:1345469719167068@base/bsb:1345469719167068@diff/\n```\n\nReviewed By: swolchok\n\nDifferential Revision: D26324255\n\nfbshipit-source-id: 61aba8687f4c1b742fa9d9d917a026abc8d9c328", "pr_number": "51908", "files_changed": ["aten/src/ATen/native/DispatchStub.cpp", "aten/src/ATen/native/DispatchStub.h"], "labels": ["Merged", "cla signed"]}, "f6e0f5b85a": {"title": "[typing] ignore mypy false positives in aten_test.py (#52370)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52370\n\nAfter adding .pyi stubs for torch / caffe2 protos, there were some mypy false positives (https://github.com/pytorch/pytorch/pull/52341). We tell mypy to ignore the offending file here.\n\nTest Plan: Let CI run.\n\nReviewed By: malfet, dzhulgakov\n\nDifferential Revision: D26490302\n\nfbshipit-source-id: 87cdfd7419efdc7abece9ca975a464201732b7a0", "pr_number": "52370", "files_changed": ["mypy.ini"], "labels": ["Merged", "cla signed", "fb-exported"]}, "f7a3634466": {"title": "[WIP][FX] Normalize torch.nn.functional calls (#51816)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51816\n\nTest Plan: Imported from OSS\n\nReviewed By: Chillee\n\nDifferential Revision: D26290764\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 9c05ff1b7c6f0ab8a13516f7cc2fe279980ebe5d", "pr_number": "51816", "files_changed": ["test/test_fx_experimental.py", "torch/fx/experimental/normalize.py"], "labels": ["Merged", "cla signed", "fx"]}, "7e2becb70f": {"title": "[PyTorch] Reduce copy/move in c10::ivalue::from (#52324)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52324\n\n`c10::ivalue::from` took its parameter by value. `List` has\nan expensive move ctor (it has to copy the type shared_ptr) and dtor\n(it has to decref the type, which isn't null), so it's better to avoid\nintermediate List objects in function parameters.\nghstack-source-id: 121807292\n\nTest Plan:\nProfiled AdIndexer benchmark; time spent in push_outputs is\ndown from 0.5% to 0.23%.\nComparing assembly for\n`c10::impl::push_outputs<c10::List<at::Tensor>, false>::call`, we went\nfrom 4 List move ctor calls and 5 ~intrusive_ptr calls to 2 move ctor\ncalls and 3 dtor calls, respectively.\n\nReviewed By: bhosmer\n\nDifferential Revision: D26471093\n\nfbshipit-source-id: 7b2c5e8d391a428f2b4d895717a43123c8d7a054", "pr_number": "52324", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "79e10ce97b": {"title": "[PyTorch] Construct IValue from List without copies in args (#52325)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52325\n\nList's move constructor is comparatively expensive (copies the type) and so is its destructor (has to destroy the type, which isn't null). So, it's best not to create intermediate `List` objects in function parameters. Copy elision won't save us here; it's not allowed to! (see https://en.cppreference.com/w/cpp/language/copy_elision)\nghstack-source-id: 121807291\n\nTest Plan:\nProfile AdIndexer benchmark. Time spent in push_outputs is\ndown from 0.2% to 0.01%.\nInspecting assembly for\n`c10::impl::push_outputs<c10::List<at::Tensor>,false>::call`\nshows that we have gone from 2 List move ctor calls and 3\n~instrusive_ptr dtor calls to 0 calls and 1 call, respectively.\n\nReviewed By: bhosmer\n\nDifferential Revision: D26471092\n\nfbshipit-source-id: 412a85fcc36d141fb91710c7855df24c137813a9", "pr_number": "52325", "files_changed": ["aten/src/ATen/core/List.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "2775ff4a47": {"title": "[BE] Decorate unused functions with C10_UNUSED (#52378)", "body": "Summary:\nThis suppresses repeated warnings for every file that includes vec256 or\nmath.h:\n```\n../aten/src/ATen/native/Math.h:1095:15: warning: unused function 'calc_igamma' [-Wunused-function]\nc10::BFloat16 calc_igamma<c10::BFloat16>(c10::BFloat16 a, c10::BFloat16 x) {\n              ^\n../aten/src/ATen/native/Math.h:1100:11: warning: unused function 'calc_igamma' [-Wunused-function]\nc10::Half calc_igamma<c10::Half>(c10::Half a, c10::Half x) {\n          ^\n../aten/src/ATen/native/Math.h:1105:15: warning: unused function 'calc_igammac' [-Wunused-function]\nc10::BFloat16 calc_igammac<c10::BFloat16>(c10::BFloat16 a, c10::BFloat16 x) {\n              ^\n../aten/src/ATen/native/Math.h:1110:11: warning: unused function 'calc_igammac' [-Wunused-function]\nc10::Half calc_igammac<c10::Half>(c10::Half a, c10::Half x) {\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52378\n\nReviewed By: walterddr\n\nDifferential Revision: D26492412\n\nPulled By: malfet\n\nfbshipit-source-id: c570c9beb9915c96fca297e0b88d0291937d3132", "pr_number": "52378", "files_changed": ["aten/src/ATen/cpu/vec256/vec256.h", "aten/src/ATen/native/Math.h"], "labels": ["Merged", "cla signed"]}, "2b202667c1": {"title": "[1/N] CPU pointwise optimization: Add a benchmark for Relu", "body": "Summary: As title\n\nTest Plan:\nBuilding: finished in 01:58.4 min (100%) 16761/16761 jobs, 16761 updated\n  Total time: 02:32.3 min\nRun on (24 X 2394.45 MHz CPU s)\n2021-02-16 21:29:30\n----------------------------------------------------------------------------------------------------\nBenchmark                                             Time           CPU Iterations UserCounters...\n----------------------------------------------------------------------------------------------------\nrelu_nnc/64                                        1738 ns       1738 ns     410535 log/s=36.8257M/s\nrelu_nnc/512                                       1708 ns       1708 ns     408678 log/s=299.711M/s\nrelu_nnc/8192                                      3297 ns       3297 ns     214362 log/s=2.48499G/s\nrelu_nnc/32768                                    10725 ns      10722 ns      61032 log/s=3.05603G/s\nlog_nnc_sleef/64                                   2076 ns       2075 ns     326248 log/s=30.8436M/s\nlog_nnc_sleef/512                                  3070 ns       3069 ns     230616 log/s=166.81M/s\nlog_nnc_sleef/8192                                22214 ns      22210 ns      31251 log/s=368.849M/s\nlog_nnc_sleef/32768                               85835 ns      85824 ns       8366 log/s=381.804M/s\nlog_nnc_fast/64                                    1852 ns       1852 ns     379123 log/s=34.5532M/s\nlog_nnc_fast/512                                   2456 ns       2456 ns     299463 log/s=208.503M/s\nlog_nnc_fast/8192                                 10953 ns      10952 ns      69894 log/s=747.957M/s\nlog_nnc_fast/32768                                35424 ns      35422 ns      19986 log/s=925.08M/s\nlog_nnc_vml/64                                     2361 ns       2361 ns     356220 log/s=27.1063M/s\nlog_nnc_vml/512                                    2218 ns       2218 ns     313444 log/s=230.857M/s\nlog_nnc_vml/8192                                   8420 ns       8420 ns      81594 log/s=972.912M/s\nlog_nnc_vml/32768                                 29484 ns      29484 ns      21701 log/s=1.1114G/s\nlog_aten/64                                       15970 ns      15970 ns      44401 log/s=4.00742M/s\nlog_aten/512                                      18344 ns      18344 ns      41056 log/s=27.9114M/s\nlog_aten/8192                                     24894 ns      24893 ns      27414 log/s=329.084M/s\nlog_aten/32768                                    29129 ns      29125 ns      22477 log/s=1.12508G/s\nlogit_nnc_sleef/64                                 2379 ns       2379 ns     261168 logit/s=26.8981M/s\nlogit_nnc_sleef/512                                5778 ns       5774 ns     114009 logit/s=88.6757M/s\nlogit_nnc_sleef/8192                              57268 ns      57236 ns      12429 logit/s=143.127M/s\nlogit_nnc_sleef/32768                            216356 ns     216344 ns       3026 logit/s=151.462M/s\nlogit_nnc_fast/64                                  2178 ns       2173 ns     282306 logit/s=29.4565M/s\nlogit_nnc_fast/512                                 2955 ns       2943 ns     202527 logit/s=173.95M/s\nlogit_nnc_fast/8192                               14836 ns      14835 ns      46794 logit/s=552.192M/s\nlogit_nnc_fast/32768                              53999 ns      53997 ns      12842 logit/s=606.846M/s\nlogit_nnc_vml/64                                   2132 ns       2132 ns     335874 logit/s=30.018M/s\nlogit_nnc_vml/512                                  3029 ns       3029 ns     250988 logit/s=169.058M/s\nlogit_nnc_vml/8192                                13264 ns      13263 ns      53504 logit/s=617.655M/s\nlogit_nnc_vml/32768                               49395 ns      48284 ns      14526 logit/s=678.654M/s\nlogit_aten/64                                     88180 ns      86690 ns       9270 logit/s=738.261k/s\nlogit_aten/512                                    54682 ns      54489 ns      10000 logit/s=9.3964M/s\nlogit_aten/8192                                  170878 ns     164357 ns       6965 logit/s=49.8427M/s\nlogit_aten/32768                                 452291 ns     434638 ns       3967 logit/s=75.3915M/s\nlogit_caffe2/64                                   30170 ns      29902 ns      24686 logit/s=2.14029M/s\nlogit_caffe2/512                                 203517 ns     201201 ns       3570 logit/s=2.54472M/s\nlogit_caffe2/8192                               3199528 ns    3157098 ns        220 logit/s=2.59479M/s\nlogit_caffe2/32768                             12520838 ns   12504846 ns         56 logit/s=2.62042M/s\ntanh_nnc_fast/64                                   1979 ns       1977 ns     309745 tanh/s=32.3752M/s\ntanh_nnc_fast/512                                  2331 ns       2331 ns     300937 tanh/s=219.636M/s\ntanh_nnc_fast/8192                                 8323 ns       8323 ns      83601 tanh/s=984.26M/s\ntanh_nnc_fast/32768                               30767 ns      30766 ns      23024 tanh/s=1065.06M/s\ntanh_aten/64                                      17181 ns      17180 ns      36818 tanh/s=3.72522M/s\ntanh_aten/512                                     19071 ns      19036 ns      37243 tanh/s=26.8968M/s\ntanh_aten/8192                                    53542 ns      52006 ns      16268 tanh/s=157.521M/s\ntanh_aten/32768                                  619869 ns     587600 ns       1000 tanh/s=55.7658M/s\ntanh_caffe2/64                                     9668 ns       9654 ns      70926 tanh/s=6.62919M/s\ntanh_caffe2/512                                   70409 ns      70409 ns       9881 tanh/s=7.27184M/s\ntanh_caffe2/8192                                1179098 ns    1179011 ns        644 tanh/s=6.9482M/s\ntanh_caffe2/32768                               4384300 ns    4382613 ns        156 tanh/s=7.47682M/s\nBatchNorm/ATen/1/64/112/112                    23186429 ns   23183715 ns         27 GB/s=277.028M/s\nBatchNorm/ATen/1/256/14/14                      1772907 ns    1770636 ns        394 GB/s=226.703M/s\nBatchNorm/ATen/1/128/28/28                      3069417 ns    3069229 ns        232 GB/s=261.569M/s\nBatchNorm/ATen/1/64/56/56                       6367276 ns    6367190 ns        111 GB/s=252.173M/s\nBatchNorm/ATen/1/512/7/7                        1334734 ns    1334373 ns        516 GB/s=150.411M/s\nBatchNorm/ATen/5/64/112/112                   131727903 ns  131721364 ns          7 GB/s=243.792M/s\nBatchNorm/ATen/5/256/14/14                      7879002 ns    7874672 ns         85 GB/s=254.873M/s\nBatchNorm/ATen/5/128/28/28                     15561373 ns   15269781 ns         42 GB/s=262.877M/s\nBatchNorm/ATen/5/64/56/56                      29169722 ns   29107393 ns         24 GB/s=275.812M/s\nBatchNorm/ATen/5/512/7/7                        5042006 ns    5028687 ns        100 GB/s=199.559M/s\nBatchNorm/NNC/1/64/112/112                      3303598 ns    3271058 ns        188 GB/s=1.96344G/s\nBatchNorm/NNC/1/256/14/14                        330641 ns     326644 ns       2033 GB/s=1.22889G/s\nBatchNorm/NNC/1/128/28/28                        498706 ns     497894 ns       1131 GB/s=1.61242G/s\nBatchNorm/NNC/1/64/56/56                        1116910 ns    1114768 ns        641 GB/s=1.44033G/s\nBatchNorm/NNC/1/512/7/7                          163380 ns     163351 ns       3493 GB/s=1.22867G/s\nBatchNorm/NNC/5/64/112/112                     16392078 ns   16386427 ns         41 GB/s=1.95971G/s\nBatchNorm/NNC/5/256/14/14                       1133781 ns    1133369 ns        674 GB/s=1.77086G/s\nBatchNorm/NNC/5/128/28/28                       2053208 ns    2053211 ns        276 GB/s=1.95503G/s\nBatchNorm/NNC/5/64/56/56                        3874949 ns    3874734 ns        165 GB/s=2.07193G/s\nBatchNorm/NNC/5/512/7/7                          653665 ns     651498 ns       1236 GB/s=1.54033G/s\nBatchNorm/ATenRelu/1/64/112/112                36878892 ns   36100523 ns         22 GB/s=177.907M/s\nBatchNorm/ATenRelu/1/256/14/14                  6404318 ns    5544976 ns        100 GB/s=72.3913M/s\nBatchNorm/ATenRelu/1/128/28/28                  5897059 ns    5735509 ns        106 GB/s=139.973M/s\nBatchNorm/ATenRelu/1/64/56/56                  10075458 ns    9965146 ns         62 GB/s=161.125M/s\nBatchNorm/ATenRelu/1/512/7/7                    2680507 ns    2662541 ns        254 GB/s=75.3806M/s\nBatchNorm/ATenRelu/5/64/112/112               145738113 ns  144253693 ns          5 GB/s=222.612M/s\nBatchNorm/ATenRelu/5/256/14/14                 13582519 ns   13427209 ns         65 GB/s=149.476M/s\nBatchNorm/ATenRelu/5/128/28/28                 22747138 ns   22627185 ns         31 GB/s=177.401M/s\nBatchNorm/ATenRelu/5/64/56/56                  53609692 ns   52936728 ns         15 GB/s=151.656M/s\nBatchNorm/ATenRelu/5/512/7/7                   11378314 ns   11083777 ns         65 GB/s=90.5395M/s\nBatchNorm/NNCRelu/1/64/112/112                  3154436 ns    3148939 ns        193 GB/s=2.03958G/s\nBatchNorm/NNCRelu/1/256/14/14                    337341 ns     337163 ns       1926 GB/s=1.19055G/s\nBatchNorm/NNCRelu/1/128/28/28                    505570 ns     505569 ns       1231 GB/s=1.58794G/s\nBatchNorm/NNCRelu/1/64/56/56                     903452 ns     903421 ns        659 GB/s=1.77728G/s\nBatchNorm/NNCRelu/1/512/7/7                      158521 ns     158321 ns       3781 GB/s=1.2677G/s\nBatchNorm/NNCRelu/5/64/112/112                 15488210 ns   15480019 ns         41 GB/s=2.07446G/s\nBatchNorm/NNCRelu/5/256/14/14                   1149186 ns    1148963 ns        649 GB/s=1.74683G/s\nBatchNorm/NNCRelu/5/128/28/28                   2011589 ns    2011424 ns        320 GB/s=1.99564G/s\nBatchNorm/NNCRelu/5/64/56/56                    3776274 ns    3776060 ns        161 GB/s=2.12607G/s\nBatchNorm/NNCRelu/5/512/7/7                      699762 ns     699582 ns        975 GB/s=1.43446G/s\nBM_CompileSwish                                30471825 ns   30470017 ns         24\nBM_CompileSwishLLVMOnly                        27479624 ns   27473475 ns         25\nFusedOverhead                                    196219 ns     196195 ns       3342\nUnfusedOverhead                                  220210 ns     220119 ns       3302\nGemm/Torch/128/128/128                           115526 ns     115343 ns       7414 GFLOPS=36.3637G/s\nGemm/TensorExprNoopt/128/128/128                3155851 ns    3155706 ns        210 GFLOPS=1.32912G/s\nGemm/TensorExprTile32x32/128/128/128             124454 ns     124452 ns       5774 GFLOPS=33.7021G/s\nGemm/TensorExprTile4x16/128/128/128              174408 ns     174366 ns       3987 GFLOPS=24.0546G/s\nGemm/TensorExprTile4x16VecUnroll/128/128/128      72949 ns      72948 ns       9028 GFLOPS=57.4974G/s\nGemm/TensorExprTile4x16Cache/128/128/128          73237 ns      73234 ns       9501 GFLOPS=57.2726G/s\nReduce1D/Torch/16777216                       426865265 ns  426853756 ns          2 BYTES=157.217M/s\nReduce1D/Naive/16777216                       132347709 ns  132343710 ns          5 BYTES=507.08M/s\nReduce1D/NativeRfactor/16777216               234668375 ns  234664682 ns          3 BYTES=285.978M/s\nReduce1D/TeNaive/16777216                      20468304 ns   20467906 ns         34 BYTES=3.27874G/s\nReduce1D/TeSplitTail/16777216                  20378995 ns   20378678 ns         34 BYTES=3.29309G/s\nReduce1D/TeSplitMask/16777216                  20371783 ns   20371260 ns         36 BYTES=3.29429G/s\nReduce1D/TeRfactorV2/16777216                   8235908 ns    8235723 ns         84 BYTES=8.14851G/s\n\nCPU info:\n\nRunning ```sudo lshw -class processor```. Get 24 CPUs with identical architecture as follows:\n\n  *-cpu:0\n       description: CPU\n       product: Intel Core Processor (Broadwell)\n       vendor: Intel Corp.\n       physical id: 400\n       bus info: cpu@0\n       version: 6.61.2\n       slot: CPU 0\n       size: 2GHz\n       capacity: 2GHz\n       width: 64 bits\n       capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp x86-64 constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat\n       configuration: cores=1 enabledcores=1 microcode=1 threads=1\n\nReviewed By: bwasti\n\nDifferential Revision: D26275048\n\nfbshipit-source-id: 3de669f622eb8cd328787caa878dc0c05de600a5", "pr_number": null, "files_changed": ["benchmarks/cpp/tensorexpr/bench_approx.cpp", "torch/csrc/jit/tensorexpr/expr.h"], "labels": []}, "76af821c36": {"title": "[PyTorch] \"Fix\" wrong-looking move in TensorImpl (#52344)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52344\n\nThis line is a bug-prone use of std::move combined with a reference to the moved-from parameter in the same series of function call arguments. This is normally a problem because the order of evaluation is undefined -- if the move happens before the call to `storage.device()`, we may have problems. It is not a problem here because we are merely forwarding from one `Storage&&` parameter to another.\nghstack-source-id: 121837267\n\nTest Plan: See no clang-tidy/HowToEven warning on the diff, I hope\n\nReviewed By: bhosmer\n\nDifferential Revision: D26436550\n\nfbshipit-source-id: da85d79be854ff42c5a0cab9649ba82295816eca", "pr_number": "52344", "files_changed": ["c10/core/TensorImpl.cpp"], "labels": ["Merged", "cla signed"]}, "b9f051db9f": {"title": "Add type hints for the _import_c_extension module (#51767)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51767\n\nThe `_import_c_extension.py` finds the right C extension library to use,\nand then simply re-exports all of the symbols that it defines.\n\nThis adds a `_import_c_extension.pyi` file with type hints to let type\ncheckers like Pyre and Mypy know the names of the symbols that will be\nre-exported from the C extension.\n\nThis does not define all of the symbols provided by the C extension,\nbut does define all of the symbols necessary to make type checkers happy\nabout other code in the `caffe2/python` directory.\nghstack-source-id: 121916324\n\nTest Plan:\nWas able to have Pyre successfully type check the `caffe2/python`\ndirectory with this stub file plus a few other changes.\n\nConfirmed that all of the dependent projects affected by this report no new\npyre issues in sandcastle.\n\nRan `python test/test_type_hints.py` in the PyTorch github repository and\nconfirmed it also passes.\n\nDifferential Revision: D26271726\n\nPulled By: simpkins\n\nfbshipit-source-id: 6dbadcf02e0b2cc44a9e3cdabe9291c1250959b4", "pr_number": "51767", "files_changed": ["caffe2/python/_import_c_extension.pyi"], "labels": ["Merged", "cla signed"]}, "70bed6a55a": {"title": "Removes deprecated preprocess method from the backend interface (#52258)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52258\n\nRemoves deprecated preprocess method from the backend interface.\n\nPreprocessing logic should be now registered along with the backend interface (i.e. PyTorchBackendInterface) via the BackendPreprocessFunction.\n\nAlso refactored internal dependencies.\nghstack-source-id: 121704837\n\nTest Plan:\nValidates all related tests pass:\n\nbuck test mode/dev //caffe2/test/cpp/jit:jit -- --exact 'caffe2/test/cpp/jit:jit - BackendTest.ToBackend'\n\npython test/test_jit.py TestBackends\n\n===== Glow\n\nbuck test mode/dev //glow/fb/torch_glow/tests:TorchGlowBackendTests\n\nbuck test mode/dev //glow/fb/torch_glow/tests:torch_glow_backend_tests\n\nReviewed By: jackm321\n\nDifferential Revision: D26443479\n\nfbshipit-source-id: afdc51ae619ced293d10c7a6a12f3530e4c4e53c", "pr_number": "52258", "files_changed": ["test/cpp/jit/test_backend_lib.cpp", "test/custom_backend/custom_backend.h", "torch/csrc/jit/backends/backend.h", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_interface.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "908ba05a06": {"title": "[Pytorch] Add python binding to use mobile cpu allocator. (#52376)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52376\n\nUsing default cpu allocator for ops executed on qnnpack backend will result in\nasan failures with heap overflow since qnnpack (and xnnpack) can access input\nbeyond their and/beginning.\n\nHere we are enabling this feature specifically to enable dynamic sparse linear op test\nusing qnnpack engine. In dynamic linear op, the fp32 bias is not packed and\nhence can result in out-of-bound access.\n\nTest Plan: CI\n\nReviewed By: z-a-f\n\nDifferential Revision: D26491943\n\nfbshipit-source-id: bcc2485e957c7abdef0853c36f6e0f876c20cee3", "pr_number": "52376", "files_changed": ["torch/_C/__init__.pyi.in"], "labels": ["Merged", "cla signed", "fb-exported"]}, "08b95e3c48": {"title": "[Pytorch, Sparsity] Integrate sparse qnnpack operator in framework (#52377)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52377\n\nAdd QNNPACK specific packed params for sparse linear.\nAdd sparse linear dynamic op with appropriate registration.\nAdd python side LinearDynamic module for sparsity.\nAdd tests to validate sparse linear qnnpack kernels.\nNote that since these test are mostly run on x86 platform and\ngiven that 1x4 sparse kernels are implemented both in sse and arm,\nLinearDynamic at the moment defaults to 1x4 pattern.\nPlan is to add another diff that will allow a global override for 8x1 pattern\nsuch that prepare/convert flow can work for exporting model for mobile.\n\nTest Plan: buck run caffe2/torch/fb/model_optimization:sparsity_test\n\nReviewed By: z-a-f\n\nDifferential Revision: D26491944\n\nfbshipit-source-id: b98839b4c62664e1fabbb0cbeb2e5c1bd5903b4d", "pr_number": "52377", "files_changed": ["torch/testing/_internal/common_quantized.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "27d89057f8": {"title": "[caffe2] fix deserialization of unknown tensor data_type values (#52411)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52411\n\nThe `TensorDeserializer` code previously did not correctly handle unknown\n`data_type` values.  It attempted to deserialize the data as floats, rather\nthan recognizing that it did not understand the data type and erroring out.\n\nGoogle protobuf will never return unknown values for enum fields.  If an\nunknown value is found in serialized data, the protobuf code discards it.\nAs a result `has_data_type()` will return false, but `get_data_type()` will\nsimply return the default value, which happens to be set to `FLOAT`.  As a\nresult if we ever encounter a serialized blob with an unknown data type the\nprevious code would incorrectly think the data type was `FLOAT`.\n\nThis fixes the code to check if the `data_type` value is present before\nreading it.\nghstack-source-id: 121915981\n\nTest Plan:\nIncluded a unit test that verifies this behavior.  Confirmed that without this\nfix the code proceeded with the float deserialization code path.  When\ndeserializing int32_t data it fortunately did fail later due to an unexpected\nfield length check, but this isn't guaranteed to be the case.  In some cases\nit potentially could incorrectly succeed and return wrong data.\n\nReviewed By: mraway\n\nDifferential Revision: D26375502\n\nfbshipit-source-id: 4f84dd82902e18df5e693f4b28d1096c96de7916", "pr_number": "52411", "files_changed": ["caffe2/core/blob_serialization.cc", "caffe2/core/serialization_test.cc"], "labels": ["Merged", "cla signed"]}, "f7aa88b400": {"title": "[caffe2] Explicitly define all DataTypes in python/core.py (#51768)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51768\n\nThis updates python/core.py to explicitly define all of the `DataType`\nvalues rather than dynamically defining them at runtime from the\n`caffe2_pb2` values.\n\nThis allows type checkers like Pyre and Mypy to see the members of the\n`DataType` class.  Otherwise the type checkers report errors such as\n`\"core.DataType\" has no attribute \"INT64\"`.\n\nThis code does keep a run-time check that all of the data types defined\nby `caffe2_pb2.proto` are defined correctly in this file.  This way if\nsomeone does add a new type to `caffe2_pb2.proto` it should be very\nquickly apparent that this file needs to be updated and kept in sync.\nghstack-source-id: 121936201\n\nTest Plan:\nConfirmed that various caffe2/python tests still pass.\nVerified that this allows many `pyre-fixme` comments to be removed in\ndownstream projects, and that Pyre is still clean for these projects.\n\nReviewed By: jeffdunn\n\nDifferential Revision: D26271725\n\nPulled By: simpkins\n\nfbshipit-source-id: f9e95795de60aba67d7d3872d0c141ed82ba8e39", "pr_number": "51768", "files_changed": ["caffe2/python/core.py"], "labels": ["Merged", "cla signed"]}, "eaad002cf6": {"title": "[PyTorch] s/__attribute__((__noinline__))/__attribute__((noinline))/ (#52362)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52362\n\nAFAICT, it is documented to be the latter and not the former.\nGCC: https://gcc.gnu.org/onlinedocs/gcc/Common-Function-Attributes.html#Common-Function-Attributes\nClang: https://clang.llvm.org/docs/AttributeReference.html\n\nBoth versions work in the oldest and newest GCC & Clang versions on Godbolt: https://godbolt.org/z/s6f4PW\n\nSo why change?\n1) lack of underscores matches the documentation\n2) AMD HIP defines `__noinline__` as a macro, which doesn't play well with the underscore version.\nSee https://github.com/ROCm-Developer-Tools/HIP/blob/2080cc113a2d767352b512b9d24c0620b6dee790/include/hip/hcc_detail/host_defines.h#L54\nghstack-source-id: 121875424\n\nTest Plan: Rely on existing CI\n\nReviewed By: bhosmer\n\nDifferential Revision: D26488991\n\nfbshipit-source-id: 6cfcdfd41c58170659e263cd519ac5359ffd5d46", "pr_number": "52362", "files_changed": ["c10/macros/Macros.h"], "labels": ["Merged", "cla signed"]}, "f1e004b954": {"title": "Fix compiler warning for MathConstants.h (#52123)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52123\n\nCompiler currently complains:\n```\ncaffe2/c10/util/MatchConstants.h(18): warning: calling a constexpr __host__ function(\"from_bits\") from a __host__ __device__ function(\"pi\") is not allowed.\n```\nThis diff extirpates the warning\n\nTest Plan: Sandcastle tests\n\nReviewed By: xush6528\n\nDifferential Revision: D26379485\n\nfbshipit-source-id: ab4821119cba8c43fd1d5788c4632d0613529ec8", "pr_number": "52123", "files_changed": ["c10/util/BFloat16.h"], "labels": ["Merged", "cla signed"]}, "f6321977e9": {"title": "Fix shape inference for multiple outputs with different output dtypes (#52417)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52417\n\nWhen we have multiple outputs, previously, we will set `infered_data_type` to the first output and stick to it. This is not correct if we have more outputs that has different dtype. The fix here will revert `infered_data_type` back to previous value (`UNDEFINED`) so that we can still enter the conditional check and set the right dtype for second and more outputs.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/fb/operators:infer_bound_shape_op_test\n```\n\nReviewed By: khabinov\n\nDifferential Revision: D26502161\n\nfbshipit-source-id: 647f0106a5785dc156fddfc196ac67001602fda8", "pr_number": "52417", "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "7a408c7290": {"title": "[complex] `masked_fill`: Complex Autograd support and update masked_scatter skips. (#52035)", "body": "Summary:\nNow that `masked_fill` CUDA is migrated, skips on masked_scatter can be removed.\n\nReference: https://github.com/pytorch/pytorch/issues/33152\n\n**Note**:\n\nHave decreased the shape of Tensor for `masked_scatter` from (M, M) -> (S, S) and so on.\n\nWith shapes of M : **96.53s**\n```\ntest/test_ops.py ........................................ssssssssssss........................ssssssssssss........................                                                   [100%]\n\n=============================================================== 88 passed, 24 skipped, 7981 deselected in 96.53s (0:01:36) ================================================================\n```\n\nWith shapes of S : **46.53s**\n```\ntest/test_ops.py ........................................ssssssssssss........................ssssssssssss........................                                                   [100%]\n\n==================================================================== 88 passed, 24 skipped, 7981 deselected in 46.53s =====================================================================\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52035\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26369476\n\nPulled By: anjali411\n\nfbshipit-source-id: 7a79d5a609b0019f8fe9ce6452924dd33390dce1", "pr_number": "52035", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "tools/autograd/gen_variable_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed", "complex_autograd", "module: complex", "open source"]}, "de9016007a": {"title": "[PyTorch][easy] Coalesce string literals in data_ptr error message (#52379)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52379\n\nThere's no reason to create and concatenate multiple string literals here when we could just combine them.\nghstack-source-id: 121890478\n\nTest Plan: builds\n\nReviewed By: ilia-cher\n\nDifferential Revision: D26492399\n\nfbshipit-source-id: a9e611a5b7ce5c1255135f3a0db12cc765b29a87", "pr_number": "52379", "files_changed": ["aten/src/ATen/templates/TensorMethods.cpp"], "labels": ["Merged", "cla signed"]}, "f6a6814a4f": {"title": "Dont look at reduction output args when computing mem dependencies (#52170)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52170\n\nghstack-source-id: 121813073\n\nTest Plan: unit tests\n\nReviewed By: navahgar\n\nDifferential Revision: D26411735\n\nfbshipit-source-id: 31c35af80d4f3e06df17ec65e4c91f604fc8745a", "pr_number": "52170", "files_changed": ["torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "62d5f60ad2": {"title": "Avoid using ReduceOp->output_args() in rfactor (#52177)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52177\n\nI'm trying to get rid of `output_args` for reductions, because they\nshouldn't be necessary; it's reducing over its reduction axis, why\ndoes it need to know where its output is going?\n\nRfactor is probably the trickiest place where we use output_args, but\nit looks like it's mostly just carrying around the location of the\nstore, so use that instead.\nghstack-source-id: 121813072\n\nTest Plan:\nbuild/bin/test_tensorexpr && build/bin/tensorexpr_bench\n\nImported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26420548\n\nfbshipit-source-id: aeab564c6113fa02eabb14c9b70c7edfd05b264d", "pr_number": "52177", "files_changed": ["torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a788c2d777": {"title": "[nnc] Remove output_args from ReduceOp (#52187)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52187\n\nReduceOp doesn't need to track the indices that its result will be written into.\nghstack-source-id: 121813075\n\nTest Plan:\ntest_tensorexpr, tensorexpr_bench\n\nImported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D26420575\n\nfbshipit-source-id: 7afcfa611515334e36de8039722011687f3b61e4", "pr_number": "52187", "files_changed": ["test/cpp/tensorexpr/test_reductions.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/reduction.cpp", "torch/csrc/jit/tensorexpr/reduction.h", "torch/csrc/jit/tensorexpr/var_substitutor.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ac121165e2": {"title": "Remove ReduceOp::accumulator (#52196)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52196\n\nA reduction does not need to know the buffer into which its\nresult will be written.  This change gets us closer to being able to\ncreate reductions inside Compute, where we have access to the tensor\naxes.\nghstack-source-id: 121813071\n\nTest Plan: test_tensorexpr\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D26420107\n\nPulled By: bertmaher\n\nfbshipit-source-id: c8d8a99649adfd6de56fe53a728f5aa034a84f13", "pr_number": "52196", "files_changed": ["test/cpp/tensorexpr/test_reductions.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_visitor.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/reduction.cpp", "torch/csrc/jit/tensorexpr/reduction.h", "torch/csrc/jit/tensorexpr/var_substitutor.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "8228086e64": {"title": "[static runtime] Use VML-inspired logarithm with NNC, tweak scheduling (#52423)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52423\n\nNNC has a new logarithm implementation that closely matches the\nperformance of VML (see D26246400 (https://github.com/pytorch/pytorch/commit/2e35fe953553247d8a22fc38b039374e426f13b8)).  Using this in the NNC generated kernel for\nlogit increases the win slightly.\nghstack-source-id: 121953008\n\nTest Plan:\n```\ncaffe2=0 bs=20 scripts/bwasti/static_runtime/run.sh\n```\n\nReviewed By: bwasti\n\nDifferential Revision: D26291426\n\nfbshipit-source-id: c5c3933732c6ade5235f23d6dc71410170a6c749", "pr_number": "52423", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7a67a7a396": {"title": "[static runtime] Generate sigmoid with NNC (#52424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52424\n\nNNC has a fast sigmoid on par with aten.  Using it for static runtime\nlets us skip dispatch overhead.\nghstack-source-id: 121953787\n\nTest Plan:\n```\ncaffe2=0 batch=1 run.sh\n```\n\nReviewed By: bwasti\n\nDifferential Revision: D26291425\n\nfbshipit-source-id: a2ad79765dacee352625f0e5322e871556e0ca9e", "pr_number": "52424", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "9cf6be6b3e": {"title": "Fix torch.nn.functional.interpolate microbenchmark for non-4D inputs", "body": "Summary: This diff fixes the `interpolate` microbenchmark for non-4D inputs, which are not supported by the `bilinear` mode\n\nTest Plan:\n5D and 3D:\n\n```\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,16,320,320)_output_size(8,256,256)\n# Input: input_size: (1, 3, 16, 320, 320), output_size: (8, 256, 256)\nForward Execution Time (us) : 221008.660\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(4,512,320)_output_size(256,)\n# Input: input_size: (4, 512, 320), output_size: (256,)\nForward Execution Time (us) : 9727.900\n\n```\n\n4D\n```\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True\nForward Execution Time (us) : 375.181\n\n```\n\nReviewed By: fmassa\n\nDifferential Revision: D26486678\n\nfbshipit-source-id: 5d476afba3f35da9f8b86db16e21505bdb00888b", "pr_number": null, "files_changed": ["benchmarks/operator_benchmark/pt/interpolate_test.py"], "labels": []}, "60518d10f6": {"title": "[deploy] torch::deploy API (#51754)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51754\n\nThis API allows you to manage multiple python interpreters in a single\nprocess to deploy PyTorch models packaged with torch.package.\n\ntorch/csrc/deploy/deploy.h contains the API definition\ntorch/csrc/deploy/test_deploy.cpp has some examples.\n\nNotes:\n* mutex is added to PyTorchStreamReader to make it safe to use from multiple threads at once.\n* USE_DEPLOY is only true for the special libtorch_deployinterpreter.so library, when enabled\n  we use a hash table to maintain PyObject <> at::Tensor mappping rather than the internal pointer\n  in Tensor since >1 interpreter may have a reference to the tensor.\n* serialization.py has some additional functions for creating pickle objects\n  but keeping storages in memory for use transfering tensors between interpreters\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab\n\nDifferential Revision: D26329468\n\nPulled By: zdevito\n\nfbshipit-source-id: d75f4ebb9a27f1d911179d9996041bcb3ca04a07", "pr_number": "51754", "files_changed": [".gitignore", ".jenkins/pytorch/test.sh", "caffe2/serialize/inline_container.cc", "caffe2/serialize/inline_container.h", "torch/CMakeLists.txt", "torch/_deploy.py", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/deploy/CMakeLists.txt", "torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/example/benchmark.cpp", "torch/csrc/deploy/example/examples.py", "torch/csrc/deploy/example/generate_examples.py", "torch/csrc/deploy/example/gpu_wrapper.py", "torch/csrc/deploy/example/trace_simple.py", "torch/csrc/deploy/interpreter/CMakeLists.txt", "torch/csrc/deploy/interpreter/hide_symbols.script", "torch/csrc/deploy/interpreter/interpreter.cpp", "torch/csrc/deploy/interpreter/interpreter.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.h", "torch/csrc/deploy/interpreter/test_main.cpp", "torch/csrc/deploy/test_deploy.cpp", "torch/csrc/jit/python/init.cpp", "torch/deploy.h", "torch/package/importer.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "758aa45563": {"title": "Revert D26369476: [pytorch][PR] [complex] `masked_fill`: Complex Autograd support and update masked_scatter skips.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26369476 (https://github.com/pytorch/pytorch/commit/7a408c72907c4a05fadd2b0ffa2735cafe09ec94)\n\nOriginal commit changeset: 7a79d5a609b0\n\nfbshipit-source-id: f0011f40962ccbcd8e7c19bd727e1e49cf2ec0c4", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "tools/autograd/gen_variable_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "c8b3686a3e": {"title": "Make bias in lazy modules lazy and avoid create empty tensors (#52212)", "body": "Summary:\nSome minor improvement for lazy modules introduced in https://github.com/pytorch/pytorch/issues/44538, https://github.com/pytorch/pytorch/issues/47350 and https://github.com/pytorch/pytorch/issues/51548.\n\nThis PR mainly turn the bias to `UninitializedParameter` and instead of creating empty tensors like\n```python\nself.bias = Parameter(torch.Tensor(0))\nself.bias = UninitializedParameter()\n```\nI think it would be better to\n```python\nself.register_parameter('bias', None)\nself.bias = UninitializedParameter()\n```\n\nIn addition, I change the constructor of the `LazyBatchNorm` from\n```python\nself.running_mean = UninitializedBuffer()\n```\nto\n```python\nself.register_buffer('running_mean', UninitializedBuffer())\n```\nas the original one would not change the underlying `self._buffers`.\n\nThank you for your time on reviewing this PR :).\n\nGently ping albanD, mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52212\n\nReviewed By: jbschlosser\n\nDifferential Revision: D26504508\n\nPulled By: albanD\n\nfbshipit-source-id: 7094d0bb4fa9e2a40a07b79d350ea12a6ebfd080", "pr_number": "52212", "files_changed": ["test/test_nn.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/linear.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ba77b8d84e": {"title": "[PyTorch][easy] Make shared empty string static instead of thread_local (#52220)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52220\n\nD21268320 (https://github.com/pytorch/pytorch/commit/d068a456d3ad05e483738954436038a79221f7be) made this thread_local, but I don't think it was necessary to do so.\nghstack-source-id: 121877050\n\nTest Plan: CI\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D26378724\n\nfbshipit-source-id: 7f17b5cff42983ea8f5be1bd254de01bf8db9a0e", "pr_number": "52220", "files_changed": ["c10/util/StringUtil.h"], "labels": ["Merged", "cla signed"]}, "efbb854ed8": {"title": "[PyTorch] Avoid std::string in TORCH_CHECK when possible (#52221)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52221\n\nThe previous code forced a `std::string` to be created even when the default message or a user-provided string literal message was used. Now it's not forced and we don't need an outlined lambda in those cases either.\nghstack-source-id: 121877056\n\nTest Plan:\nCompare assembly for\n\n```\n#include <c10/util/Exception.h>\n\nvoid f(bool b) {\n  TORCH_CHECK(b, \"message\");\n}\n\nvoid g(bool b) {\n  TORCH_CHECK(b);\n}\n\nvoid h(bool b) {\n  TORCH_CHECK(b, \"message\", random());\n}\n```\n\nbefore/after in fbcode optimized build.\n\nBefore: P174696735\nAfter: P174696840\n\nFor `f()` and `g()`, we go from a call to an outlined lambda that did a bunch of `std::string` creation to a load of a string constant before calling `torchCheckFail`. This is a clear improvement.\n\nFor `h()`, results are mixed: we save a bunch of *extra* string goop in the outlined lambda and instead call `c10::detail::_str_wrapper` directly. This is good for overall size. However, we no longer outline the call to `random()`, which is less than ideal. I hope to recover the ability to fully outline the `random()` call in future diffs; this is just thorny enough that I don't want to cram even more into one diff.\n\nAdded automated test to make sure `TORCH_CHECK` and `TORCH_INTERNAL_ASSERT` only evaluate their arguments once.\n\nProfiled AdIndexer mergenet benchmark in perf to check that `IValue::toTensor` is still getting inlined.\n\nReviewed By: bhosmer\n\nDifferential Revision: D26380783\n\nfbshipit-source-id: 288860772423994ac739a8f33e2c09f718e8dd38", "pr_number": "52221", "files_changed": ["c10/test/util/exception_test.cpp", "c10/util/Exception.h", "c10/util/StringUtil.h"], "labels": ["Merged", "cla signed"]}, "3978ffb37a": {"title": "NS for FX: add test for a simple sparsenn model (#52092)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52092\n\nAdds a very simple toy sparsenn model, and enables\nits inspection with the new NS APIs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_sparsenn_compare_activations\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_sparsenn_shadow\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26403095\n\nfbshipit-source-id: 3c3650aca47186deb32f2b3f1d87a0716d1ad9d1", "pr_number": "52092", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "test/test_quantization.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "d903106bad": {"title": "[wip] ns for fx: add support for subgraph matching (#52130)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52130\n\nWe have patterns like (F.linear, F.relu) which need to match\nto (toq.linear_relu).  So, we need to match subgraphs.\n\nThis PR does the following:\n* defines a \"subgraph\" as (start_node, end_node). The current assumption\nis that subgraphs are simple, there is always a path from start_node to\nend_node, and we can ignore any non-input args/kwargs of these nodes\nfor the purposes of matching and copying things. An example one node\nsubgraph is (F.linear, F.linear).  An example two node subgraph\nis (F.linear, F.relu).\n* changes the matching logic to iterate over subgraphs instead of nodes\n* changes the NS core APIs to use subgraph pairs instead of node pairs:\n1. for weights, we match on the start node\n2. for unshadowed activations, we observe the end nodes\n3. for shadowed activations, we copy the subgraph of a to graph c\n\nTODO(before review) write up better, not ready for review yet\n\nTest Plan:\nTODO before land: better test plan\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26403092\n\nfbshipit-source-id: e49aaad4b02b8d60589435848bee422b8f41937a", "pr_number": "52130", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "a937d1cb16": {"title": "ns for fx: make weights comparison work on N models (#52356)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52356\n\nRefactor the NS for FX compare weights API to be able to\nwork on N models and do arbitrary matching strategies.\n\nWe factor out a util which takes a model and a list of\nnodes to extract weights for, with names to give the extracted\nweights.  The user can then call this util with a set\nof nodes and names created in any way they want.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26487271\n\nfbshipit-source-id: 0c2172a1b33d47565004a307aff14d205671add7", "pr_number": "52356", "files_changed": ["torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "ad9746456e": {"title": "ns for fx: make unshadowed activation comparison work for N models (#52357)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52357\n\nRefactor the NS for FX compare unshadowed activations API to be able\nto work on N models and do arbitrary matching strategies.\n\nWe factor out a util which takes a model and a list of\nnodes to extract weights for, with names to give the extracted\nweights. The user can then call this util with a set\nof nodes and names created in any way they want.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26487270\n\nfbshipit-source-id: 1372ef07b5f3ddc7cebdfb2dee0221a2facd0527", "pr_number": "52357", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "c7b0005831": {"title": "Enhance Tensor.unflatten to support -1 as the inferred size (#51955)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51719, https://github.com/pytorch/pytorch/issues/28142\n\n**Change**\n- Update `torch.Tensor.unflatten` to support users pass`-1` as the inferred size for both tensors and named tensors.\n- Examples of using `-1` in the `unflatten` function are added to the docs.\n- Fix the rendered issue of original `unflatten` docs by removing a blank line between its example section.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51955\n\nReviewed By: agolynski\n\nDifferential Revision: D26467198\n\nPulled By: zou3519\n\nfbshipit-source-id: 6a3ede25561223187273796427ad0cb63f125364", "pr_number": "51955", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "test/test_namedtensor.py", "test/test_torch.py", "torch/tensor.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b52e2e6045": {"title": "[BE] _get_torch_cuda_version should return tuple (#52409)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52409\n\nReviewed By: jbschlosser, glaringlee\n\nDifferential Revision: D26513924\n\nPulled By: walterddr\n\nfbshipit-source-id: ee18ef357c326c5ad344d80c59821cc2b8814734", "pr_number": "52409", "files_changed": ["test/test_sparse.py", "torch/testing/_internal/common_cuda.py", "torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed"]}, "983347fa25": {"title": "Allow broadcasting against lerp weights. (#52319)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52319\n\nFixes: https://github.com/pytorch/pytorch/issues/52254\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26488411\n\nPulled By: gchanan\n\nfbshipit-source-id: 60eb471609986584c4235ba7f263581e988e7642", "pr_number": "52319", "files_changed": ["aten/src/ATen/native/Lerp.cpp", "aten/src/ATen/native/cuda/Lerp.cu", "test/test_binary_ufuncs.py", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "9e54532947": {"title": "[PyTorch Mobile] 15KiB size reduction by reducing MaxTypeIndex from 256 to 32 (#51881)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51881\n\n`MaxTypeIndex` controls the size of the array\n\n```\ndetail::TypeMetaData* TypeMeta::typeMetaDatas() {\n  static detail::TypeMetaData instances[MaxTypeIndex + 1]\n```\n\nin `typeid.cpp`.\n\nIn practice, I have seen that this array doesn't hold more than 18 elements once the PyTorch library has been initialized (in mobile unit tests). I couldn't find situations where elements may be added to this array post library initialization.\n\nThere is a runtime check to prevent array overflow, so reducing the size of the storage shouldn't come at any additional risk from the perspective of loss in visibility of errors.\n\nThe fact that this array is staically allocated ends up using a bunch of space in the binary (potentially to initialize the trailing elements?). I'm somewhat surprised but this. However, this change registered a 15KiB size win on both fbios as well as igios.\n\nFound this when I was looking at a bloaty run that I shared with smessmer on friday: https://www.internalfb.com/intern/everpaste/?handle=GLXImQisHOfT74EBAKw47V3ktuAzbsIXAAAB\n\nI initially thought that the methods being passed in to the constructor of `detail::TypeMetaData` were causing the size increase, but only later relaized the issue after reading the folloing helpful comment:\n\n```\n    // The remainder of the array is padded with TypeMetaData blanks.\n    // The first of these is the entry for ScalarType::Undefined.\n    // The rest are consumed by CAFFE_KNOWN_TYPE entries.\n```\nghstack-source-id: 121875657\n\nTest Plan:\nSandcastle runs + the following BSB runs.\n\n### igios\n\n```\nD26299594-V1 (https://www.internalfb.com/intern/diff/D26299594/?dest_number=121221891)\n\nigios: Succeeded\nChange in Download Size for arm64 + 3x assets variation: +596 B\nChange in Uncompressed Size for arm64 + 3x assets variation: -15.8 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:443632243487886@base/bsb:443632243487886@diff/\n```\n\n### fbios\n\n```\nD26299594-V1 (https://www.internalfb.com/intern/diff/D26299594/?dest_number=121221891)\n\nfbios: Succeeded\nChange in Download Size for arm64 + 3x assets variation: +104 B\nChange in Uncompressed Size for arm64 + 3x assets variation: -15.7 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:169233698063125@base/bsb:169233698063125@diff/\n```\n\nReviewed By: raziel, iseeyuan\n\nDifferential Revision: D26299594\n\nfbshipit-source-id: 9a78c03da621fbc25a1d8087376628bccc8dbfda", "pr_number": "51881", "files_changed": ["c10/util/typeid.h"], "labels": ["Merged", "Reverted", "cla signed"]}, "8f3ed60d3e": {"title": "enable mkldnn conv2d backward to support mkldnn tensor input (#48994)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48994\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D25537189\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: d81d247798fad3815b735468d66ef9d62c07ef77", "pr_number": "48994", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "test/test_mkldnn.py", "torch/csrc/autograd/functions/accumulate_grad.h"], "labels": ["Merged", "cla signed", "open source"]}, "c7a70eec1b": {"title": "Make LLVM the default backend for TE (#52314)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52264\n\nWhen CPU fusion is enabled without LLVM support in PyTorch, it causes huge slowdown (> 50x). This PR makes the LLVM backend the default backend for TE. Now, an error will be reported if CPU fusion is enabled without LLVM support, to avoid this performance regression.\n\nThis PR also updates the tests to not use LLVM, so that the old flow is continued. This is necessary because tests run in CI do not have LLVM.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52314\n\nReviewed By: ejguan\n\nDifferential Revision: D26491294\n\nPulled By: navahgar\n\nfbshipit-source-id: 74561db1207da805d6d28039450db046ba2988fb", "pr_number": "52314", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp", "test/jit/test_profiler.py", "test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/testing/_internal/jit_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "49c90648d3": {"title": "[iOS GPU] Fix max_pool_2d (#52431)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52431\n\nThe previous implementation was missing the padding information. Thus is not correct.\nghstack-source-id: 121950755\n\nTest Plan:\n- `buck test pp-macos`\n- CircleCI\n\nReviewed By: SS-JIA\n\nDifferential Revision: D26508482\n\nfbshipit-source-id: b28b99c399c4f1390a5cc4f023e470eed0f8c073", "pr_number": "52431", "files_changed": ["aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.h", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": ["Merged", "cla signed"]}, "22adea04df": {"title": "Revert D26299594: [PyTorch Mobile] 15KiB size reduction by reducing MaxTypeIndex from 256 to 32", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26299594 (https://github.com/pytorch/pytorch/commit/9e54532947e4ff06e4265c61e0f9bf3577ef54f7)\n\nOriginal commit changeset: 9a78c03da621\n\nfbshipit-source-id: 2be1149539892447872eb3289f3fdef0ac92c090", "pr_number": null, "files_changed": ["c10/util/typeid.h"], "labels": []}, "e0b6252de0": {"title": "[ROCm] Enable test_ddp_hooks.py test cases (#52403)", "body": "Summary:\nRe-enabling these test cases for ROCm because they are passing.\n\njeffdaily\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52403\n\nReviewed By: jbschlosser, SciPioneer\n\nDifferential Revision: D26516727\n\nPulled By: malfet\n\nfbshipit-source-id: 6c70805eda39b0aadfbeb30a527af3906d2da867", "pr_number": "52403", "files_changed": ["test/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "bb7e07ce8e": {"title": "[glow] Extending AOT config with two more fields (#5359)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/glow/pull/5359\n\nReviewed By: ChunliF\n\nDifferential Revision: D26468908\n\nfbshipit-source-id: 16c4f4215f302c023d75c204b999f23ed6254aa1", "pr_number": null, "files_changed": ["caffe2/proto/caffe2.proto"], "labels": []}, "65f6e665e6": {"title": "Improvements for FX tracer (#52232)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52232\n\nPull Request resolved: https://github.com/pytorch/glow/pull/5327\n\nReviewed By: gcatron\n\nDifferential Revision: D26355583\n\nfbshipit-source-id: f062e0b3a9cadf1584738bed85e9964b9a63efaf", "pr_number": "52232", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "d8b28579c3": {"title": "Add NNC support for aten::hardtanh (a hot operation in mobilenet v2/v3) (#52394)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52394\n\nTest Plan:\nImported from OSS\n\ntest/test_tensorexpr.py\ntest/test_jit_fuser_te.py\n\nReviewed By: bertmaher\n\nDifferential Revision: D26497856\n\nPulled By: huiguoo\n\nfbshipit-source-id: 8558f89826cad250da6f970bfc49384f2b9d7ee0", "pr_number": "52394", "files_changed": ["test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "83feaebfc3": {"title": "Add support for pow (#52374)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/18627\nAdds pow support for JIT\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52374\n\nTest Plan: python test/test_jit.py -k test_torch_pow\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26515596\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 0c25a8eba8ed93291c5e447e863edac2a35b61fb", "pr_number": "52374", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3adc8f8cf7": {"title": "Enable min & max for Float16 & BFloat16 (#51244)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50790.\n\nAdded `min()` & `max()` support for `Float16` & `BFloat16`.\nCUDA already supported these ops on `Float16`, so the other three combinations had to be enabled.\n`OpInfo`s for `min` & `max` were also added, and their sample inputs were removed from `method_tests()`.\n\n### MORE INFO\nThe (slightly) long-term goal is to add dispatch for `min()` & `max()` related operations on CPU & CUDA for `Float16` & `BFloat16`,\nwherever they aren't present already:\n1. `amin()`\n2. `argmax()`\n3. `amax()`\n4. `argmin()`\n5. `torch._aminmax()`\n6. `torch.clamp()` on CPU. Was already supported on CUDA\n7. `min()` (in this PR)\n8. `max()` (in this PR)\n9. `minimum()`\n10. `maximum()`\n\nI'll submit separate PRs for the other ops.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51244\n\nReviewed By: jbschlosser\n\nDifferential Revision: D26503455\n\nPulled By: anjali411\n\nfbshipit-source-id: c32247f214e9272ca2e4322a23337874e737b140", "pr_number": "51244", "files_changed": ["aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "test/test_reductions.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "4ee5bc74d3": {"title": "[DataLoader] Change signature of Functional DataPipe (#52458)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52458\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26523282\n\nPulled By: ejguan\n\nfbshipit-source-id: c7358fc351f859617754a27b8a701d11ada5d61a", "pr_number": "52458", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/callable.py", "torch/utils/data/datapipes/iter/combinatorics.py", "torch/utils/data/datapipes/iter/grouping.py", "torch/utils/data/datapipes/iter/selecting.py"], "labels": ["Merged", "cla signed"]}, "c29e279f72": {"title": "[DDP] unittest for when params arent used in backward pass (#52384)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52384\n\nAdds a simple UT with unittest that we can modify when we enable DDP backward without needing all parameters to get gradient.\nghstack-source-id: 122001930\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26482479\n\nfbshipit-source-id: c80bdeea7cf9db35390e385084ef28d64ed239eb", "pr_number": "52384", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "0c46b6b3f6": {"title": "[DDP] Enhance warning for find_unused_params (#52385)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52385\n\nThis warning should specify that we did not find unused params in the\n_forward_ pass, which is when we log this warning. This is to avoid confusion\nwhen we get an error because not all outputs were used to compute loss, which\nalso raises an error about unused parameters (to be fixed in the next diff)\nghstack-source-id: 122001929\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26494136\n\nfbshipit-source-id: d9b41732ea7e5e31b899d590d311080e3dc56682", "pr_number": "52385", "files_changed": ["torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c75fa39b6c": {"title": "add stats that can only be collected at runtime (#51386)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51386\n\nadd stats such as rebuilt bucket stats, unused parameter stats and performance stats to ddp logging data\n\n1. gpu time stats are not collected for single process multiple devices in this diff, as that requires events are created and recorded on multiple devices\n2. use at::cuda::event API for safer calls\n3. events may not be created in autograd hook if hook is not triggered in user's codes, e.g., users runs in non-sync mode in some iterations. So we checked events are created or not before synchronizing, also skipped invalid results.\n4. users may not set device upfront, so explicitly set proper device before creating events in our prepare_forward() and prepare_backward() calls\n\nghstack-source-id: 121933566\n\nTest Plan: unit tests\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26158645\n\nfbshipit-source-id: ce5f15187802eba76accb980449be68902c10178", "pr_number": "51386", "files_changed": ["c10/util/Logging.h", "test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d0795ab358": {"title": "log newly added construction and runtime stats at randomly selected iterations (#51394)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51394\n\nlog newly added construction and runtime stats at randomly selected iterations\nghstack-source-id: 121934040\n\nTest Plan: unit tests\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26161885\n\nfbshipit-source-id: add6e02c1a03e6f74f08b9a9aecf90fa81631d60", "pr_number": "51394", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3b11822825": {"title": "[RPC] Refactor rref_context to not use utils::Future (#51697)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51697\n\nRefactors the rest of rref_context, specifically pendingOwners map and `getOwnerRRef` to use JitFuture.\nghstack-source-id: 122037611\n\nTest Plan: CI\n\nReviewed By: wanchaol\n\nDifferential Revision: D26243268\n\nfbshipit-source-id: ab8874c8253274e8fe50dcd7291e0655a8f3f1df", "pr_number": "51697", "files_changed": ["test/cpp/rpc/e2e_test_base.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "df3d1d9378": {"title": "[RPC] delete torch/csrc/utils/future.h (#51698)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51698\n\nCompletely eliminates torch::utils::Future as we are now full relying on JitFuture.\nghstack-source-id: 122037612\n\nTest Plan: CI\n\nReviewed By: kiukchung\n\nDifferential Revision: D26243735\n\nfbshipit-source-id: 95010a730f9d35e618f74c5f9de482738cd57c15", "pr_number": "51698", "files_changed": ["torch/csrc/distributed/rpc/message.h", "torch/csrc/utils/future.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7ca9776874": {"title": "Fixed _out variants of linear algebra functions (#51560)", "body": "Summary:\nThis PR modifies the behavior of `_out` variants to match the description here https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch\nWith this PR result and input tensors must be on the same device and have the same \"type kind\".\n\nI skipped `qr` and `eig` in this process as they require a bit more work.\n\nFunctions that can use the provided storage directly do so. If `result` is not empty and not in the batched column-major format or does not have the same type as input then we have to allocate a temporary tensor and copy it.\n\nTODO:\n\n- [x] Add more tests for same device and valid safe dtype\n- [x] Move inv and solve changes to separate PRs https://github.com/pytorch/pytorch/pull/51968, https://github.com/pytorch/pytorch/pull/51977\n\nRef. https://github.com/pytorch/pytorch/issues/42666\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51560\n\nReviewed By: albanD\n\nDifferential Revision: D26400734\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: a6201ed7e919c1670c6ff3ef60217d1dbfb72e67", "pr_number": "51560", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebraUtils.h", "test/test_linalg.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source", "triaged"]}, "b71215a909": {"title": "Revert D26515596: [pytorch][PR] Add support for pow", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26515596 (https://github.com/pytorch/pytorch/commit/83feaebfc3d825575d5cce0aed54a99edfe148c1)\n\nOriginal commit changeset: 0c25a8eba8ed\n\nfbshipit-source-id: 1a206f0b2923d922911fdaa5448a4e3a844ac5c4", "pr_number": null, "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": []}, "93c4067f25": {"title": "[BE] Cleanup UnaryOpsKernel.cpp (#52444)", "body": "Summary:\nDelete unused `dispatchtypes` of `IMPLEMENT_FLOAT_KERNEL` and `IMPLEMENT_COMPLEX_KERNEL`\nMove common part of above-mentioned macros into `IMPLEMENT_ITERATOR_LAMBDA` macro\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52444\n\nReviewed By: walterddr\n\nDifferential Revision: D26517032\n\nPulled By: malfet\n\nfbshipit-source-id: f03f89602f14fb513c66f3f2a96596e4c1e4cd16", "pr_number": "52444", "files_changed": ["aten/src/ATen/native/cpu/UnaryOpsKernel.cpp"], "labels": ["Merged", "cla signed"]}, "dbeda994db": {"title": "Update FindvecLib.cmake for macOS 10.14, 10.15 and Big Sur (#51288)", "body": "Summary:\nWhen compiling libtorch on macOS there is the option to use the `vecLib` BLAS library from Apple's (Accelerate)[https://developer.apple.com/documentation/accelerate] framework. Recent versions of macOS have changed the location of veclib.h, this change adds the new locations to `FindvecLib.cmake`\n\nTo test run the following command:\n```\nBLAS=vecLib python setup.py install --cmake --cmake-only\n```\n\nThe choice of BLAS library is confirmed in the output:\n```\n-- Trying to find preferred BLAS backend of choice: vecLib\n-- Found vecLib: /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Accelerate.framework/Versions/Current/Frameworks/vecLib.framework/Versions/Current/Headers\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51288\n\nReviewed By: jbschlosser\n\nDifferential Revision: D26531136\n\nPulled By: malfet\n\nfbshipit-source-id: ce86807ccbf66973f33b3acb99b7f40cfd182b9b", "pr_number": "51288", "files_changed": ["cmake/Modules/FindvecLib.cmake"], "labels": ["Merged", "cla signed", "module: build", "module: osx", "open source", "triaged"]}, "8094e4844d": {"title": "[ROCm] Enable test_jit_c10.py tests for ROCm (#52410)", "body": "Summary:\nRe-enabling these test cases for ROCm because they are passing.\n\njeffdaily\n\nSigned-off-by: Kyle Chen <kylechen@amd.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52410\n\nReviewed By: glaringlee\n\nDifferential Revision: D26516757\n\nPulled By: malfet\n\nfbshipit-source-id: 49921ee724a50f19afd8e6884a5f3ecd9291fa5c", "pr_number": "52410", "files_changed": ["test/distributed/test_jit_c10d.py"], "labels": ["Merged", "cla signed", "module: rocm", "oncall: distributed", "open source", "triaged"]}, "5fda3b094c": {"title": "Add conj OpInfo and fix out inconsistency (#52059)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\nFixes: https://github.com/pytorch/pytorch/issues/51949\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52059\n\nReviewed By: ailzhang\n\nDifferential Revision: D26373800\n\nPulled By: anjali411\n\nfbshipit-source-id: d2c92263a690072c0f23cb60885be42eebea48c6", "pr_number": "52059", "files_changed": ["aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryComplexKernels.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "9699c703c2": {"title": "Stable sort for the CPU take 2. (#51790)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/38681.\nA duplicate of https://github.com/pytorch/pytorch/pull/50052 created to become importable to the fb internal tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51790\n\nReviewed By: agolynski\n\nDifferential Revision: D26279045\n\nPulled By: glaringlee\n\nfbshipit-source-id: 348e171dee9c370a76002b65d0c82c329f57a421", "pr_number": "51790", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/CompositeRandomAccessorCommon.h", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/Sorting.h", "aten/src/ATen/native/cpu/SortingKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/TensorCompare.cpp", "test/test_sort_and_select.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "bc6852c192": {"title": "Change TCPStore world_size and is_master to be optional (#51809)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51809\n\nChanges to TCPStore which will make world_size and is_master optional parameters for initialization.\n\nAPI before change:\n```python\n# arguments: host_name, port, world_size, is_master, timeout=300s\nserver_store = dist.TCPStore(\"127.0.0.1\", 0, 2, True)\nclient_store = dist.TCPStore(\"127.0.0.1\", 0, 2, False)\n```\n\nAPI after change:\n```python\n# arguments: host_name, port, world_size=-1, is_master=False, timeout=300s\nserver_store = dist.TCPStore(\"127.0.0.1\", 0, is_master=True)\nclient_store = dist.TCPStore(\"127.0.0.1\", 0)\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D26461770\n\nPulled By: H-Huang\n\nfbshipit-source-id: 5b2157029c73e8706e158cd49ecce60c9f3a7f41", "pr_number": "51809", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "76e8324370": {"title": "[package] rename ex/importer.py to package_ex/importer.py (#52320)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52320\n\nas title\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D26468416\n\nPulled By: suo\n\nfbshipit-source-id: 890eecea76426918daff900402fbcbc149e48535", "pr_number": "52320", "files_changed": ["test/test_package.py", "torch/_deploy.py", "torch/package/__init__.py", "torch/package/exporter.py", "torch/package/importer.py", "torch/package/package_exporter.py", "torch/package/package_importer.py"], "labels": ["Merged", "cla signed"]}, "d5ac929b62": {"title": "[package] Introduce Importer to manage module namespace collisions. (#51975)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51975\n\nSee comments in code.\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D26340592\n\nPulled By: suo\n\nfbshipit-source-id: 61b16bafad15e19060710ad2d8487c776d672847", "pr_number": "51975", "files_changed": ["test/test_package.py", "torch/_deploy.py", "torch/package/__init__.py", "torch/package/_custom_import_pickler.py", "torch/package/importer.py", "torch/package/package_exporter.py", "torch/package/package_importer.py"], "labels": ["Merged", "cla signed"]}, "752d808fa0": {"title": "Trace linear as aten::linear (#51897)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/pull/51613 made `torch.nn.functional.linear` compile as `aten::linear`, extend the same behavior with tracing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51897\n\nReviewed By: albanD\n\nDifferential Revision: D26320711\n\nPulled By: eellison\n\nfbshipit-source-id: a26d3c37323a0706313c6ebb210bad60eec6a64b", "pr_number": "51897", "files_changed": ["test/expect/TestTensorBoard.test_pytorch_graph.expect", "test/jit/test_onnx_export.py", "test/jit/test_tracer.py", "test/onnx/expect/TestOperators.test_linear.expect", "test/onnx/test_pytorch_onnx_onnxruntime.py", "tools/autograd/gen_trace_type.py", "torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "08017f4598": {"title": "Add explicit cudart_static dependency for cublas_static (#52509)", "body": "Summary:\nFixes following error during static linking, by enforcing that cudart dependency is put after cublasLt\n```\n/usr/bin/ld: /usr/local/cuda/lib64/libcublasLt_static.a(libcublasLt_static.a.o): undefined reference to symbol 'cudaStreamWaitEvent@libcudart.so.11.0'\n/usr/local/cuda/lib64/libcudart.so: error adding symbols: DSO missing from command line\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52509\n\nReviewed By: janeyx99\n\nDifferential Revision: D26547622\n\nPulled By: malfet\n\nfbshipit-source-id: 4e17f18cf0ab5479a549299faf2583a79fbda4b9", "pr_number": "52509", "files_changed": ["cmake/public/cuda.cmake"], "labels": ["Merged", "cla signed"]}, "bcd77cece4": {"title": "[JIT] Display an error message when with item is not an object (#52335)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52335\n\n**Summary**\n`with` statements can only be used with objects that have `__enter__`\nand `__exit__` defined. At present, any attempt to use an expression\nthat returns something that is not an instance of a class type results\nin a cryptic internal assert failure instead of a useful error message.\nThis is because the code that generates IR for `with` statements uses\n`Type::expect` as if it were `Type::cast`; that is, as if it returns\n`nullptr` on failure.\n\nThis commit fixes this issue by checking the `kind()` of the type of the\nexpression used as the with item before calling `expect<ClassType>()` on\nit.\n\n**Test Plan**\nThis commit adds a unit test to `test_with_errors` to test this case.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D26504909\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 92d108e0c010370fd45131a57120f50c0b85c401", "pr_number": "52335", "files_changed": ["test/jit/test_with.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "28e3dfdcca": {"title": "[JIT] Allow __exit__ to have a return value (#52336)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52336\n\n**Summary**\nIn Python, the boolean interpretation of the return value of `__exit__` of objects that are used as context managers with `with` statements is used to determine\nwhether or not to propagate exceptions thrown inside the body of the with\nstatement. This latter feature is not possible to add to TorchScript at\nthe moment, but the current requirement that `__exit__` not have any\nreturn values can make it difficult to script a context manager whose\n`__exit__` *does* have a return value.\n\nAccordingly, this commit removes the requirement that `__exit__` must\nnot have any return value. TorchScript does not interpret this return\nvalue in the same way Python does (or at all), but this should make it\neasier to share context managers between eager mode and script code.\n\n**Test Plan**\nThis commit adds a return value to one of the context managers used in\n`TestWith`.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D26504910\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 2ab635a24d111ac25df4e361b716be8fada5128e", "pr_number": "52336", "files_changed": ["test/jit/test_with.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c9c4b871a5": {"title": "[pytorch] reintroduce static dispatch (#51957)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51957\n\nThis is a simplified version of #51554.\n\nCompared to #51554, this version only supports statically dispatching to\na specific backend. The benefit is that it skipped the dispatch key\ncomputation logic thus has less framework overhead. The downside is that\nif input tensors do not match the specified backend it will throw error\ninstead of falling back to regular dispatch.\n\nSample code:\n```\nTensor empty(IntArrayRef size, TensorOptions options, c10::optional<MemoryFormat> memory_format) {\n    return at::cpu::empty(size, options, memory_format);\n}\n\n// aten::conj(Tensor(a) self) -> Tensor(a)\nTensor conj(const Tensor & self) {\n    return at::math::conj(self);\n}\n\n// aten::conj.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\nTensor & conj_out(Tensor & out, const Tensor & self) {\n    return at::cpu::conj_out(out, self);\n}\n\n// aten::conj.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\nTensor & conj_outf(const Tensor & self, Tensor & out) {\n    return at::cpu::conj_out(out, self);\n}\n\n// aten::_conj(Tensor self) -> Tensor\nTensor _conj(const Tensor & self) {\n    return at::defaultbackend::_conj(self);\n}\n```\n\nFor ops without the specific backend dispatch, it will throw error:\n```\n// aten::_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank) -> bool\nbool _use_cudnn_ctc_loss(const Tensor & log_probs, const Tensor & targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank) {\n    TORCH_CHECK(false, \"Static dispatch does not support _use_cudnn_ctc_loss for CPU.\");\n}\n```\n\nDifferential Revision: D26337857\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nPulled By: ljk53\n\nfbshipit-source-id: a8e95799115c349de3c09f04a26b01d21a679364", "pr_number": "51957", "files_changed": [".circleci/cimodel/data/simple/mobile_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/build-mobile.sh", "CMakeLists.txt", "aten/src/ATen/templates/Functions.cpp", "aten/src/ATen/templates/TensorMethods.cpp", "cmake/Codegen.cmake", "test/mobile/custom_build/build.sh", "tools/codegen/api/translate.py", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "b6ed05130e": {"title": "Adding a flag to enable CPU fusion in benchmarks (#48612)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/48612\n\nTest Plan: python -m benchmarks.tensorexpr --device cpu --mode fwd --jit_mode trace --cpu_fusion element\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D26548643\n\nPulled By: navahgar\n\nfbshipit-source-id: adb537818d77c9b6b0fe434ae6d963a5f348ad24", "pr_number": "48612", "files_changed": ["benchmarks/tensorexpr/__main__.py"], "labels": ["Merged", "Stale", "cla signed"]}, "44ff79d849": {"title": "Automatically set BUILD_SPLIT_CUDA for cpp exts (#52503)", "body": "Summary:\nFixes https://github.com/pytorch/vision/pull/3418#issuecomment-781673110\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52503\n\nReviewed By: malfet\n\nDifferential Revision: D26546857\n\nPulled By: janeyx99\n\nfbshipit-source-id: a100b408e7cd28695145a1dda7f2fa081bb7f21f", "pr_number": "52503", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed", "open source"]}, "b6cf17deee": {"title": "[reland][complex] `masked_fill`: Complex Autograd support and update masked_scatter skips. (#52483)", "body": "Summary:\nReland https://github.com/pytorch/pytorch/issues/52035\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52483\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D26545097\n\nPulled By: anjali411\n\nfbshipit-source-id: f154c239183279be381a7393a8226778b36148bb", "pr_number": "52483", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "tools/autograd/gen_variable_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "complex_autograd", "module: complex", "open source", "triaged"]}, "d6755934fa": {"title": "[PyTorch] Make c10::str(const char*) return const char* (#52222)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52222\n\n`c10::str()` is often used with variadic macros. It can be more efficient to get a C string out if you put a C string in, like if you are able to defer std::string creation to an outlined function or even never do it at all. Meanwhile, there is an implicit conversion from const char* to std::string, so users who expected a std::string will still make one.\nghstack-source-id: 121877052\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: bhosmer\n\nDifferential Revision: D26419663\n\nfbshipit-source-id: 400bef71e6a0004b5914f5f511ea0e04e0d7599b", "pr_number": "52222", "files_changed": ["c10/util/StringUtil.h"], "labels": ["Merged", "cla signed"]}, "566f7c79d3": {"title": "[c10] Take advantage of c10::str optis for simple CAFFE_ENFORCE (#52223)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52223\n\nAfter the previous diffs, `c10::str()` will return a\n`CompileTimeEmptyString` when passed 0 arguments and a `const char*` when\npassed 1 `const char *` argument. We can take advantage of this to\noutline further std::string creation from CAFFE_ENFORCE.\nghstack-source-id: 121877053\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nCompare assembly for\n```\n#include <c10/util/Logging.h>\n\nvoid f(bool b) {\n  CAFFE_ENFORCE(b);\n}\n\nvoid g(bool b) {\n  CAFFE_ENFORCE(b, \"message\");\n}\n\nvoid h(bool b) {\n  CAFFE_ENFORCE(b, \"message\", random());\n}\n```\n\nbefore & after this diff.\n\nbefore: P174902847\nafter: P174902912\n\nf & g are clearly much improved, and h is about the same.\n\n(I tried measuring caffe2 perf on the AdIndexer MergeNet benchmark, but didn't see a win, which makes sense because the change is small.)\n\nReviewed By: bhosmer\n\nDifferential Revision: D26405181\n\nfbshipit-source-id: c51a9e459ae7d9876494a83ade6f6fe725619512", "pr_number": "52223", "files_changed": ["c10/util/Logging.cpp", "c10/util/Logging.h"], "labels": ["Merged", "cla signed"]}, "7cd9892f83": {"title": "[PyTorch] Sync TORCH_INTERNAL_ASSERT optis with TORCH_CHECK (#52226)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52226\n\nThis gets TORCH_INTERNAL_ASSERT to parity with TORCH_CHECK in terms of optimization for 0 or 1 argument.\nghstack-source-id: 121877054\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nCompare generated assembly for\n```\n#include <c10/util/Exception.h>\n\nvoid f(bool b) {\n  TORCH_INTERNAL_ASSERT(b, \"message\");\n}\n\nvoid g(bool b) {\n  TORCH_INTERNAL_ASSERT(b);\n}\n\nvoid h(bool b) {\n  TORCH_INTERNAL_ASSERT(b, \"message\", random());\n}\n```\n\nbefore/after this diff.\nBefore: P174916324\nAfter: P174916411\n\nBefore, f and g called out to outlined lambdas to build\nstd::strings. After, they load string constants and call\ntorchInternalAssertFail. Similarly, h calls random() and c10::detail::_str_wrapper() inline and then calls out to torchInternalAssertFail. As with D26380783 (https://github.com/pytorch/pytorch/commit/efbb854ed8a9df35c0ea896c2d216ab3da10d677), I hope to solve the problem of outlining the random & _str_wrapper calls separately.\n\nProfile AdIndexer benchmark & verify that toTensor() is still inlined (it calls TORCH_INTERNAL_ASSERT with an integer argument, like `h` above).\n\nReviewed By: bhosmer\n\nDifferential Revision: D26410575\n\nfbshipit-source-id: f82ffec8d302c9a51f7a82c65bc698fab01e1765", "pr_number": "52226", "files_changed": ["c10/util/Exception.cpp", "c10/util/Exception.h"], "labels": ["Merged", "cla signed"]}, "db33afbf9f": {"title": "Change cmake to allow building with MLC kick-off build (#51326)", "body": "Summary:\n- Allows build process to build with MLC enabled if subrepo folder mlc is in path and we can link against ML Compute on macOS BigSur\n- To build with MLC enabled you will need to clone the mlc repo inside the pytorch repository.\n- We need both this change and https://github.com/pytorch/pytorch/pull/50634 on pytorch/pytorch to enable the `mlc` device.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51326\n\nReviewed By: glaringlee\n\nDifferential Revision: D26533138\n\nPulled By: malfet\n\nfbshipit-source-id: 0baa06b4eb2d62dbfc0f6fc922096cb0db1cc7d1", "pr_number": "51326", "files_changed": ["CMakeLists.txt", "caffe2/CMakeLists.txt", "torch/CMakeLists.txt", "torch/csrc/Module.cpp", "torch/csrc/utils/variadic.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "09516d2d0c": {"title": "Reenables skipped tests for all CUDA versions except 11.2 (#52359)", "body": "Summary:\nThis PR adds functionality to skip a test based on CUDA version.\n\nThis way, we can be more specific when skipping a test, such as when the test only fails for a particular CUDA version.\n\nThis allows us to add back the skipped tests for CUDA 11.2 for other CUDA versions, such as 10.1 and 11.1.\n\nI tested this locally (by using 11.0 instead of 11.2), but will run all the CI to make sure it works.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52359\n\nReviewed By: walterddr\n\nDifferential Revision: D26487951\n\nPulled By: janeyx99\n\nfbshipit-source-id: 45c71cc6105ffd9985054880009cf68ea5ef3f6a", "pr_number": "52359", "files_changed": ["test/test_autograd.py", "test/test_optim.py", "test/test_torch.py", "torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "ci/all", "cla signed"]}, "8fe6d17847": {"title": "Moving 11.2 CI to master only (#52536)", "body": "Summary:\nMoves the 11.2 linux and windows builds to master only.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52536\n\nReviewed By: walterddr\n\nDifferential Revision: D26557925\n\nPulled By: janeyx99\n\nfbshipit-source-id: 28b8018112c159e2ae259dc00884c17796951c90", "pr_number": "52536", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": ["Merged", "cla signed"]}, "a3e693789f": {"title": "[qunat][graphmode][fx] Enable test for non quantized input for cat (#52414)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52414\n\nWhen the input is not quantized, we'll still quantize cat as requested by the qconfig, even though\nit might be slower\n\nTest Plan: Imported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D26503554\n\nfbshipit-source-id: 29d7c136711a12c124791c10ae436b61c1407668", "pr_number": "52414", "files_changed": ["test/quantization/test_quantize_fx.py"], "labels": ["Merged", "cla signed"]}, "ef8d17e112": {"title": "[DDP] Separate error messages for unused params in forward and not all outputs (#52391)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52391\n\nThere are 2 ways DDP can throw the exception refactored here -\n1) Unused params in the forward pass. We provide `find_unused_parameters=True` for this.\n2) All params used in fwd pass, but not all outputs used in loss computation. There are a few workarounds for this but we do not provide native support.\n\nPreviously, these 2 issues were combined into 1 error message but that has historically resulted in confusion, with users reporting getting this error even when they enable `find_unused_parameters=True` (which they expect to fix this error). As a result there is additional churn to debug these issues because the true cause (1) vs (2) is not known.\n\nThis commit helps to fix the issue by separating out the 2 error messages depending on if we ran with unused parameter detection or not. Hopefully this should make the error message much more clear and actionable.\n\nerror msg with `find_unused_params=True`:\n```\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. Since `find_unused_parameters=True` is enabled, this likely  means that not all `forward` outputs participate in computing loss. You can fix this by making sure all `forward` function outputs participate in calculating loss.\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n```\nerror msg without `find_unused_params` specified:\n```\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\nmaking sure all `forward` function outputs participate in calculating loss.\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n```\nghstack-source-id: 122097900\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26496688\n\nfbshipit-source-id: 4a9eeeda10293da13d94a692d10cb954e4506d7c", "pr_number": "52391", "files_changed": ["torch/lib/c10d/reducer.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "72f9b3c8d5": {"title": "[StaticRuntime] Add function to check for memory leak (#52342)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52342\n\nReviewed By: yinghai\n\nDifferential Revision: D26420826\n\nfbshipit-source-id: 4023f80fadd21e192afa485d96acd37c845146be", "pr_number": "52342", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "1c64f862f6": {"title": "Update vec_mergee operand specifiers (_vecb) (#52091)", "body": "Summary:\nPatch needed in order to build on ppc64le with compiler g++V7.  (w/o fix, only works on minimum compiler V8).\n\nFixes https://github.com/pytorch/pytorch/issues/51592\n\nTo be clear, credit where due:\nI tested this patch on a ppc64 RHEL container using gcc/g++ 7.4 compiler to ensure a complete pytorch build was successful -- and it was. However, I do not take credit for this patch.  I found and reported the issue, but the full brainpower to identify the cause of the error and the appropriate solution and thus the credit for this fix truly belongs to quickwritereader (and I am just helping with the legwork to integrate it after having tested it).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52091\n\nReviewed By: ejguan\n\nDifferential Revision: D26494943\n\nPulled By: glaringlee\n\nfbshipit-source-id: 0babdb460db5047c54144f724466b77dd2d8a364", "pr_number": "52091", "files_changed": ["aten/src/ATen/cpu/vec256/vsx/vec256_complex_float_vsx.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "bb34fd6191": {"title": "[DataLoader] Fix util ImportError (#52459)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52459\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26523941\n\nPulled By: ejguan\n\nfbshipit-source-id: 8cd84982348687cf84fe5e821f51fbac43a783fa", "pr_number": "52459", "files_changed": ["torch/utils/data/datapipes/utils/decoder.py"], "labels": ["Merged", "cla signed"]}, "53373a8e8c": {"title": "remove deprecated function (#52426)", "body": "Summary:\nIn order to remove the annoying compilation warnings\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52426\n\nReviewed By: glaringlee\n\nDifferential Revision: D26525718\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 0a46389bd8e27e77250ca9501125c6ffc4b5d45b", "pr_number": "52426", "files_changed": ["torch/csrc/jit/runtime/operator.h", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source", "triaged"]}, "d9161d6da3": {"title": "Optimize `setDebugName` time complexity (#52346)", "body": "Summary:\n`setDebugName` maintains an invariant that all debug names of values in same graph must be distinct. This is achieved by appending numeric suffixes to requested debug names. However, the implementation was slow (O(N^2)) when there are a lot of name conflicts. This PR fixes the problem by adding more book-keeping logic so that time complexity is brought down to O(1) on average.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52346\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D26564462\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 3260fc3b436f1b0bcb45fdd2d1ec759b5828263f", "pr_number": "52346", "files_changed": ["torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a62b0deae0": {"title": "[pytorch] make is_tracing scriptable (#49853)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49853\n\nfix https://github.com/pytorch/pytorch/issues/47379\n\nTest Plan: buck test mode/dev-nosan //caffe2/test:jit -- 'test_script_is_tracing'\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D25704315\n\nfbshipit-source-id: 33c09c5bc1f1b62ef254f58e18ab1e951dbd1790", "pr_number": "49853", "files_changed": ["test/test_jit.py", "torch/jit/_trace.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "15892a651f": {"title": "[PyTorch Mobile] Create compile time string for passing in to the exception message instead of 4 arguments that will be concatenated at runtime (#52303)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52303\n\nswolchok did some stellar work in D26372806 (https://github.com/pytorch/pytorch/commit/22b12179db15923007aaec80829766079bb0b9d1) (and friends) to simplify exception handling code-paths and outline uncommon code paths. In addition, non-inlined versions of exception handling functions were provided but only in case of specific cases where 1 (or 2?) arguments were passed in to the exception throwing macros.\n\nThis change hopes to take advantage of that infrastructure and only pass in a single `const char*` to `AT_ERROR` to leverage any current (or future) optimizations that may take place in this space.\n\nSince this isn't yet in production, it won't have a size impact. However, my guess is that it will be a significant size win once we turn on tracing based selective build since the exception code path will be present in every kernel function multiple times over since most dtypes will be unselected.\nghstack-source-id: 122149806\n\nTest Plan: Build + auto-generated unit tests for tracing based selective build.\n\nReviewed By: swolchok\n\nDifferential Revision: D26463089\n\nfbshipit-source-id: 349160a37d43d629249b92fa24f12b5bd128df1c", "pr_number": "52303", "files_changed": ["aten/src/ATen/Dispatch.h"], "labels": ["Merged", "cla signed"]}, "597c9f8b22": {"title": "fix zero_point rounding for _fake_quantize_learnable_per_channel_affine (#52290)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52290\n\n_fake_quantize_learnable_per_channel_affine should allow taking non-integer zero_point as input, and perform rounding and clamp before doing forward/backward. In this diff, we make _fake_quantize_learnable_per_channel_affine to round and clamp zero_point beforehand as in _fake_quantize_learnable_per_tensor_affine.\nghstack-source-id: 122148099\n\nTest Plan: `buck test mode/dev-nosan -c fbcode.platform=platform009 //caffe2/test:quantization -- test_learnable`\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26446342\n\nfbshipit-source-id: fc9b6832fa247cc9d41265eb4fd1575a2d2ed12c", "pr_number": "52290", "files_changed": ["aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "test/quantization/test_workflow_module.py"], "labels": ["Merged", "cla signed"]}, "65bfa1389d": {"title": "[PyTorch Mobile] Do not create a static variable in Dispatcher::singleton() (#52447)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52447\n\nCurrently, `Dispatcher::singleton()` is always inlined. Additionally, `Dispatcher::singleton()` contains a static variable, which means that the generated code calls `__cxa_guard_acquire` and `__cxa_guard_release` which help implement exactly once semantics for the initialization of the `static Dispatcher& s` variable. For `C10_MOBILE`, we should not create the additional static ref within the inlined function to save binary size since it results in a lot of additional code being generated by the compiler. The `Dispatcher::singleton()` method is called from the generated method stubs for all aten opertors that are code-generated and potentially also from other operators that hand off execution to the kernel function for the right backend via the PyTorch Dispatcher.\n\nThis is a classic space/time (efficiency) tradeoff, so feedback would be welcome. kimishpatel, I'll need your expertise in figuring out how to perf-test this change, specifically for mobile.\n\nHere's the godbolt link in case you wish to check out the generated code for a `static` variable within a function: https://godbolt.org/z/cdsG3v\n\n{F375631117}\nghstack-source-id: 121989311\n\nTest Plan:\nBuild + BSB\n\n### lightspeed-messenger\n\n*Divide the number below by 2*\n\n```\nD26507049-V1 (https://www.internalfb.com/intern/diff/D26507049/?dest_number=121944956)\n\nmessenger-experimental-optimized-device: Succeeded\nChange in Download Size for arm64 + 3x assets variation: -21.7 KiB\nChange in Uncompressed Size for arm64 + 3x assets variation: -65.4 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:243392763936025@base/bsb:243392763936025@diff/\n```\n\n### igios\n\n```\nD26507049-V1 (https://www.internalfb.com/intern/diff/D26507049/?dest_number=121944956)\n\nigios: Succeeded\nChange in Download Size for arm64 + 3x assets variation: -15.6 KiB\nChange in Uncompressed Size for arm64 + 3x assets variation: -34.3 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:882756935844095@base/bsb:882756935844095@diff/\n```\n\n### fbios-pika\n\n```\nD26507049-V1 (https://www.internalfb.com/intern/diff/D26507049/?dest_number=121944956)\n\nfbios-pika: Succeeded\nChange in Download Size for arm64 + 3x assets variation: -8.6 KiB\nChange in Uncompressed Size for arm64 + 3x assets variation: -29.1 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:832297083999539@base/bsb:832297083999539@diff/\n```\n\nReviewed By: swolchok\n\nDifferential Revision: D26507049\n\nfbshipit-source-id: 0d2f55ea2d42a0782fb69aabfa517f2ec60c8036", "pr_number": "52447", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["Merged", "cla signed"]}, "a935118c90": {"title": "Fix caffee2 to use MaybeAlign when using LLVM trunk", "body": "Summary: Trunk at 13 uses a different type for `CreateAlignedStore` and `CreateAlignedLoad` so updating usage here to reflect this.\n\nTest Plan:\nbuck build mode/opt-clang-thinlto sigrid/predictor/v2:sigrid_remote_predictor -c cxx.extra_cxxflags=\"-Wforce-no-error -fbracket-depth=300\" -c cxx.profile=\"fbcode//fdo/autofdo-bolt-compatible/sigrid/predictor/v2/sigrid_remote_predictor:autofdo-bolt-compatible\" -c cxx.modules=False\n\nPreviously:\ncaffe2/torch/csrc/jit/tensorexpr/llvm_codegen.cpp:1079:21: error: no matching member function for call to 'CreateAlignedLoad'\n      value_ = irb_.CreateAlignedLoad(vaddr, 4);\n               ~~~~~^~~~~~~~~~~~~~~~~\nthird-party-buck/platform009/build/llvm-fb/include/llvm/IR/IRBuilder.h:1681:13: note: candidate function not viable: no known conversion from 'int' to 'llvm::MaybeAlign' for 2nd argument\n  LoadInst *CreateAlignedLoad(Value *Ptr, MaybeAlign Align,\n\nNow:\nPasses\n\nDifferential Revision: D26562330\n\nfbshipit-source-id: dbf9ca5247ccd4351861995c2c5480a7cc55c202", "pr_number": null, "files_changed": ["torch/csrc/jit/tensorexpr/llvm_codegen.cpp"], "labels": []}, "14f7bf0629": {"title": "[PyTorch] update CMake to build libtorch lite (#51419)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51419\n\n## Summary\n\n1. Add an option `BUILD_LITE_INTERPRETER` in `caffe2/CMakeLists.txt` and set `OFF` as default.\n2. Update 'build_android.sh' with an argument to swtich `BUILD_LITE_INTERPRETER`, 'OFF' as default.\n3. Add a mini demo app `lite_interpreter_demo` linked with `libtorch` library, which can be used for quick test.\n\n## Test Plan\nBuilt lite interpreter version of libtorch and test with Image Segmentation demo app ([android version](https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation)/[ios version](https://github.com/pytorch/ios-demo-app/tree/master/ImageSegmentation))\n\n### Android\n1. **Prepare model**: Prepare the lite interpreter version of model by run the script below to generate the scripted model `deeplabv3_scripted.pt` and `deeplabv3_scripted.ptl`\n```\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscripted_module = torch.jit.script(model)\n# Export full jit version model (not compatible lite interpreter), leave it here for comparison\nscripted_module.save(\"deeplabv3_scripted.pt\")\n# Export lite interpreter version model (compatible with lite interpreter)\nscripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n\n```\n2. **Build libtorch lite for android**: Build libtorch for android for all 4 android abis (armeabi-v7a, arm64-v8a, x86, x86_64) `BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh`. This pr is tested on Pixel 4 emulator with x86, so use cmd `BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh x86` to specify abi to save built time. After the build finish, it will show the library path:\n```\n...\nBUILD SUCCESSFUL in 55s\n134 actionable tasks: 22 executed, 112 up-to-date\n+ find /Users/chenlai/pytorch/android -type f -name '*aar'\n+ xargs ls -lah\n-rw-r--r--  1 chenlai  staff    13M Feb 11 11:48 /Users/chenlai/pytorch/android/pytorch_android/build/outputs/aar/pytorch_android-release.aar\n-rw-r--r--  1 chenlai  staff    36K Feb  9 16:45 /Users/chenlai/pytorch/android/pytorch_android_torchvision/build/outputs/aar/pytorch_android_torchvision-release.aar\n```\n3. **Use the PyTorch Android libraries built from source in the ImageSegmentation app**: Create a folder 'libs' in the path, the path from repository root will be `ImageSegmentation/app/libs`. Copy `pytorch_android-release` to the path `ImageSegmentation/app/libs/pytorch_android-release.aar`. Copy 'pytorch_android_torchvision` (downloaded from [here](https://oss.sonatype.org/#nexus-search;quick~torchvision_android)) to the path `ImageSegmentation/app/libs/pytorch_android_torchvision.aar` Update the `dependencies` part of `ImageSegmentation/app/build.gradle` to\n```\ndependencies {\n    implementation 'androidx.appcompat:appcompat:1.2.0'\n    implementation 'androidx.constraintlayout:constraintlayout:2.0.2'\n    testImplementation 'junit:junit:4.12'\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.3.0'\n\n    implementation(name:'pytorch_android-release', ext:'aar')\n    implementation(name:'pytorch_android_torchvision', ext:'aar')\n\n    implementation 'com.android.support:appcompat-v7:28.0.0'\n    implementation 'com.facebook.fbjni:fbjni-java-only:0.0.3'\n}\n```\nUpdate `allprojects` part in `ImageSegmentation/build.gradle` to\n```\n\nallprojects {\n    repositories {\n        google()\n        jcenter()\n        flatDir {\n            dirs 'libs'\n        }\n    }\n}\n```\n4. **Update model loader api**: Update `ImageSegmentation/app/src/main/java/org/pytorch/imagesegmentation/MainActivity.java` by\n4.1 Add new import: `import org.pytorch.LiteModuleLoader;`\n4.2 Replace the way to load pytorch lite model\n```\n//            mModule = Module.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.pt\"));\n            mModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.ptl\"));\n```\n5. **Test app**: Build and run the ImageSegmentation app in Android Studio,\n![image](https://user-images.githubusercontent.com/16430979/107696279-9cea5900-6c66-11eb-8286-4d1d68abff61.png)\n\n### iOS\n1. **Prepare model**: Same as Android.\n2. **Build libtorch lite for ios** `BUILD_PYTORCH_MOBILE=1 IOS_PLATFORM=SIMULATOR BUILD_LITE_INTERPRETER=1   ./scripts/build_ios.sh`\n3. **Remove Cocoapods from the project**: run `pod deintegrate`\n4. **Link ImageSegmentation demo app with the custom built library**:\nOpen your project in XCode, go to your project Target\u2019s **Build Phases - Link Binaries With Libraries**, click the **+** sign and add all the library files located in `build_ios/install/lib`. Navigate to the project **Build Settings**, set the value **Header Search Paths** to `build_ios/install/include` and **Library Search Paths** to `build_ios/install/lib`.\nIn the build settings, search for **other linker flags**. Add a custom linker flag below\n```\n-all_load\n```\nFinally, disable bitcode for your target by selecting the Build Settings, searching for Enable Bitcode, and set the value to No.\n**\n\n5. Update library and api**\n5.1 Update `TorchModule.mm``\nTo use the custom built libraries the project, replace `#import <LibTorch/LibTorch.h>` (in `TorchModule.mm`) which is needed when using LibTorch via Cocoapods with the code below:\n\n```\n//#import <LibTorch/LibTorch.h>\n#include \"ATen/ATen.h\"\n#include \"caffe2/core/timer.h\"\n#include \"caffe2/utils/string_utils.h\"\n#include \"torch/csrc/autograd/grad_mode.h\"\n#include \"torch/script.h\"\n#include <torch/csrc/jit/mobile/function.h>\n#include <torch/csrc/jit/mobile/import.h>\n#include <torch/csrc/jit/mobile/interpreter.h>\n#include <torch/csrc/jit/mobile/module.h>\n#include <torch/csrc/jit/mobile/observer.h>\n```\n5.2 Update `ViewController.swift`\n```\n//        if let filePath = Bundle.main.path(forResource:\n//            \"deeplabv3_scripted\", ofType: \"pt\"),\n//            let module = TorchModule(fileAtPath: filePath) {\n//            return module\n//        } else {\n//            fatalError(\"Can't find the model file!\")\n//        }\n        if let filePath = Bundle.main.path(forResource:\n            \"deeplabv3_scripted\", ofType: \"ptl\"),\n            let module = TorchModule(fileAtPath: filePath) {\n            return module\n        } else {\n            fatalError(\"Can't find the model file!\")\n        }\n```\n\n### Unit test\nAdd `test/cpp/lite_interpreter`, with one unit test `test_cores.cpp` and a light model `sequence.ptl` to test `_load_for_mobile()`, `bc.find_method()` and `bc.forward()` functions.\n\n### Size:\n**With the change:**\nAndroid:\nx86: `pytorch_android-release.aar` (**13.8 MB**)\n\nIOS:\n`pytorch/build_ios/install/lib` (lib: **66 MB**):\n```\n(base) chenlai@chenlai-mp lib % ls -lh\ntotal 135016\n-rw-r--r--  1 chenlai  staff   3.3M Feb 15 20:45 libXNNPACK.a\n-rw-r--r--  1 chenlai  staff   965K Feb 15 20:45 libc10.a\n-rw-r--r--  1 chenlai  staff   4.6K Feb 15 20:45 libclog.a\n-rw-r--r--  1 chenlai  staff    42K Feb 15 20:45 libcpuinfo.a\n-rw-r--r--  1 chenlai  staff    39K Feb 15 20:45 libcpuinfo_internals.a\n-rw-r--r--  1 chenlai  staff   1.5M Feb 15 20:45 libeigen_blas.a\n-rw-r--r--  1 chenlai  staff   148K Feb 15 20:45 libfmt.a\n-rw-r--r--  1 chenlai  staff    44K Feb 15 20:45 libpthreadpool.a\n-rw-r--r--  1 chenlai  staff   166K Feb 15 20:45 libpytorch_qnnpack.a\n-rw-r--r--  1 chenlai  staff   384B Feb 15 21:19 libtorch.a\n-rw-r--r--  1 chenlai  staff    **60M** Feb 15 20:47 libtorch_cpu.a\n```\n`pytorch/build_ios/install`:\n```\n(base) chenlai@chenlai-mp install % du -sh *\n 14M\tinclude\n 66M\tlib\n2.8M\tshare\n```\n\n**Master (baseline):**\nAndroid:\nx86: `pytorch_android-release.aar` (**16.2 MB**)\n\nIOS:\n`pytorch/build_ios/install/lib` (lib: **84 MB**):\n```\n(base) chenlai@chenlai-mp lib % ls -lh\ntotal 172032\n-rw-r--r--  1 chenlai  staff   3.3M Feb 17 22:18 libXNNPACK.a\n-rw-r--r--  1 chenlai  staff   969K Feb 17 22:18 libc10.a\n-rw-r--r--  1 chenlai  staff   4.6K Feb 17 22:18 libclog.a\n-rw-r--r--  1 chenlai  staff    42K Feb 17 22:18 libcpuinfo.a\n-rw-r--r--  1 chenlai  staff   1.5M Feb 17 22:18 libeigen_blas.a\n-rw-r--r--  1 chenlai  staff    44K Feb 17 22:18 libpthreadpool.a\n-rw-r--r--  1 chenlai  staff   166K Feb 17 22:18 libpytorch_qnnpack.a\n-rw-r--r--  1 chenlai  staff   384B Feb 17 22:19 libtorch.a\n-rw-r--r--  1 chenlai  staff    78M Feb 17 22:19 libtorch_cpu.a\n```\n`pytorch/build_ios/install`:\n```\n(base) chenlai@chenlai-mp install % du -sh *\n 14M\tinclude\n 84M\tlib\n2.8M\tshare\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26518778\n\nPulled By: cccclai\n\nfbshipit-source-id: 4503ffa1f150ecc309ed39fb0549e8bd046a3f9c", "pr_number": "51419", "files_changed": ["CMakeLists.txt", "android/pytorch_android/CMakeLists.txt", "android/pytorch_android/build.gradle", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java", "aten/src/ATen/CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Summary.cmake", "scripts/build_android.sh", "scripts/build_ios.sh", "test/cpp/lite_interpreter_runtime/CMakeLists.txt", "test/cpp/lite_interpreter_runtime/light_model.ptl", "test/cpp/lite_interpreter_runtime/main.cpp", "test/cpp/lite_interpreter_runtime/sequence.ptl", "test/cpp/lite_interpreter_runtime/test_lite_interpreter_runtime.cpp", "tools/build_variables.bzl", "torch/csrc/autograd/profiler_legacy.cpp"], "labels": ["Merged", "ci/all", "cla signed"]}, "d819a21692": {"title": "Support any (#52360)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/18627\nAdds torch.any support for JIT\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52360\n\nTest Plan:\npython test/test_jit.py -k test_torch_any\npython test/test_jit.py -k test_any\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D26550626\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 36c2ae15e3bfb7b32bbf442818c879b0d2120cf1", "pr_number": "52360", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e677b71056": {"title": "Add support for pow (#52374)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/18627\nAdds pow support for JIT\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52374\n\nTest Plan: python test/test_jit.py -k test_torch_pow\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D26555070\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 0d325f09cf893e4ae50277a95a6b7ad67d94f342", "pr_number": "52374", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "d491fc6d48": {"title": "[PyTorch] Add comment to unify macro and rename one macro (#52573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52573\n\n## Summary\nAddress comments in https://github.com/pytorch/pytorch/pull/52540\n1. Add a comment to indicate that the macros `BUILD_LITE_INTERPRETER` and `C10_MOBILE` will be unified.\n2. Rename the macro `DBUILD_LITE_INTERPRETER` to `BUILD_LITE_INTERPRETER`\n\n## Test plan\n1. `MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ USE_CUDA=0 DEBUG=1 MAX_JOBS=16 BUILD_LITE_INTERPRETER=1  python setup.py develop`\n2. `/Users/chenlai/pytorch/cmake-build-debug/bin/test_lite_interpreter_runtime --gtest_filter=* --gtest_color=no`\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26572742\n\nPulled By: cccclai\n\nfbshipit-source-id: c8895fcfe8dd893f8157913f110e2ba025fc3955", "pr_number": "52573", "files_changed": ["caffe2/CMakeLists.txt", "torch/csrc/autograd/profiler_legacy.cpp"], "labels": ["Merged", "cla signed"]}, "d177654981": {"title": "[Take-2] [PyTorch Mobile] 15KiB size reduction by reducing MaxTypeIndex from 256 to 32 (#52466)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52466\n\n`MaxTypeIndex` controls the size of the array\n\n```\ndetail::TypeMetaData* TypeMeta::typeMetaDatas() {\n  static detail::TypeMetaData instances[MaxTypeIndex + 1]\n```\n\nin `typeid.cpp`.\n\nIn practice, I have seen that this array doesn't hold more than 18 elements once the PyTorch library has been initialized (in mobile unit tests). I couldn't find situations where elements may be added to this array post library initialization.\n\nThere is a runtime check to prevent array overflow, so reducing the size of the storage shouldn't come at any additional risk from the perspective of loss in visibility of errors.\n\nThe fact that this array is staically allocated ends up using a bunch of space in the binary (potentially to initialize the trailing elements?). I'm somewhat surprised but this. However, this change registered a 15KiB size win on both fbios as well as igios.\n\nFound this when I was looking at a bloaty run that I shared with smessmer on friday: https://www.internalfb.com/intern/everpaste/?handle=GLXImQisHOfT74EBAKw47V3ktuAzbsIXAAAB\n\nI initially thought that the methods being passed in to the constructor of `detail::TypeMetaData` were causing the size increase, but only later relaized the issue after reading the folloing helpful comment:\n\n```\n// The remainder of the array is padded with TypeMetaData blanks.\n// The first of these is the entry for ScalarType::Undefined.\n// The rest are consumed by CAFFE_KNOWN_TYPE entries.\n```\n\nThis change was originally reverted at https://www.internalfb.com/diff/D26525208 due to an ONNX test failure. Re-trying the change gated under `C10_MOBILE`.\nghstack-source-id: 122178181\n\nTest Plan:\nSandcastle runs + the following BSB runs.\n\n### igios\n\n```\nD26299594 (https://github.com/pytorch/pytorch/commit/9e54532947e4ff06e4265c61e0f9bf3577ef54f7)-V1 (https://www.internalfb.com/intern/diff/D26299594 (https://github.com/pytorch/pytorch/commit/9e54532947e4ff06e4265c61e0f9bf3577ef54f7)/?dest_number=121221891)\n\nigios: Succeeded\nChange in Download Size for arm64 + 3x assets variation: +596 B\nChange in Uncompressed Size for arm64 + 3x assets variation: -15.8 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:443632243487886@base/bsb:443632243487886@diff/\n```\n\n### fbios\n\n```\nD26299594 (https://github.com/pytorch/pytorch/commit/9e54532947e4ff06e4265c61e0f9bf3577ef54f7)-V1 (https://www.internalfb.com/intern/diff/D26299594 (https://github.com/pytorch/pytorch/commit/9e54532947e4ff06e4265c61e0f9bf3577ef54f7)/?dest_number=121221891)\n\nfbios: Succeeded\nChange in Download Size for arm64 + 3x assets variation: +104 B\nChange in Uncompressed Size for arm64 + 3x assets variation: -15.7 KiB\n\nMbex Comparison: https://our.intern.facebook.com/intern/mbex/bsb:169233698063125@base/bsb:169233698063125@diff/\n```\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26527921\n\nfbshipit-source-id: f019e5fd37e6caf24c58c6f144bedcda942d7164", "pr_number": "52466", "files_changed": ["c10/util/typeid.h"], "labels": ["Merged", "cla signed"]}, "c2b9283d4a": {"title": "[PyTorch Mobile] Use real if constexpr behind macro in hot template (copy D26154964 in a different setting) (#52420)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52420\n\nInspired by D26154964 (https://github.com/pytorch/pytorch/commit/6e1a5b1196aa0277a2113a4bca75b6e0f2b4c0c8), I'm basically going to just blindly copy the change that swolchok has made since it promises to reduce compile time, and who doesn't want faster compiles! I haven't actually checked if it has any impact on build time, but I have come to trust what swolchok does.\n\nIn addition, swolchok observed a size reduction with the change, which I assume happens when the `constexpr` is true since the lambda is invoked and possibly needs to be compiled in. When tracing based selective build is enabled, many many many of these will be enabled, and this will use valuable size. This change is required to get the maximum bang for our buck. In addition, I'll look into making the lambda not capture all arguments by ref via the ref-capture `[&]` directive.\n\nI can probably have an entire half's worth of impact by copying Scott's changes and mirroring them in other parts of the PyTorch codebase lol.\n\n#accep2ship\nghstack-source-id: 122178246\n\nTest Plan: Build\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26506634\n\nfbshipit-source-id: b91d5e4773ade292fddce8dddd7e5ba1e5afeb29", "pr_number": "52420", "files_changed": ["aten/src/ATen/Dispatch.h"], "labels": ["Merged", "cla signed"]}, "ee04cd9587": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D26575694\n\nfbshipit-source-id: afb40c6c12126b4f3cb8ea2ffc526b1d817f5471", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/operator.h"], "labels": []}, "c78a4a52d2": {"title": "remove unnecessary/dead code in upsample_nearest1d cuda kernel (#51916)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51916\n\nAfter getting ported to structured kernels, the vector overloads of `upsample_nearest1d` are DefaultBackend kernels, meaning they are backend agnostic. We can kill their CUDA-specific implementations.\n\nI also removed a few redundant checks in the cuda kernels that are now performed by the meta shape-checking function.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D26327749\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b5a17e14237fb36236d4079433f99c71cd3beef3", "pr_number": "51916", "files_changed": ["aten/src/ATen/native/cuda/UpSampleNearest1d.cu"], "labels": ["Merged", "cla signed"]}, "ed71cbdd39": {"title": "Revert PR 52483 \"[reland][complex] `masked_fill` (#52587)", "body": "Summary:\nRevert \"[reland][complex] `masked_fill`: Complex Autograd support update masked_scatter skips. (https://github.com/pytorch/pytorch/issues/52483)\"\n\nThis reverts commit b6cf17deeeb526b0dfee5434c96223debe62c506.\n\nReference: https://github.com/pytorch/pytorch/pull/52483#issuecomment-783023560\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52587\n\nReviewed By: anjali411\n\nDifferential Revision: D26579741\n\nPulled By: malfet\n\nfbshipit-source-id: 9b53c8aab51d844d0f65393609861a4ff72ef7bb", "pr_number": "52587", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "tools/autograd/gen_variable_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "d02a2bd5d1": {"title": "codegen'd API for redispatching (#52008)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52008\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D26356079\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 1fd34fbb4dbc48cc8390cad99e30e0d04fc75a4f", "pr_number": "52008", "files_changed": ["BUILD.bazel", "aten/src/ATen/templates/RedispatchFunctions.cpp", "aten/src/ATen/templates/RedispatchFunctions.h", "tools/codegen/api/types.py", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "014d2123a3": {"title": "Replace all AT_ASSERTM in ATen (#51677)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51677\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D26280321\n\nPulled By: bdhirsh\n\nfbshipit-source-id: cef273e45ba7167ae240b85410ca7a3913ad54b4", "pr_number": "51677", "files_changed": ["aten/src/ATen/cuda/detail/KernelUtils.h", "aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/test/test_assert.h"], "labels": ["Merged", "cla signed", "open source"]}, "4386a3803c": {"title": "Replace all ASSERTM in serialization (#51756)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51756\n\nTest Plan: Imported from OSS\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26280320\n\nPulled By: bdhirsh\n\nfbshipit-source-id: ddba1fe46b9f39234f010aac9cdf198e82727f84", "pr_number": "51756", "files_changed": ["torch/csrc/serialization.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "116d402200": {"title": "Skip handle_r_to_c for dot & vdot (#52474)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52455.\n\n**Summary:**\n`dot` & `vdot` operate on tensors of the same `dtype`,  so skip `handle_r_to_c` for them.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52474\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26570931\n\nPulled By: anjali411\n\nfbshipit-source-id: 07c6c50e3550e521d1807c519154b028d9168de7", "pr_number": "52474", "files_changed": ["tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed", "complex_autograd", "module: complex", "open source", "triaged"]}, "ad3319cbc2": {"title": "fractional_max_pool{2/3}d : Fix segfaults for incorrect kernel_size and output_size (#51626)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50967\n\nTODO:\n\n* [x] Add test for `fractional_max_pool3d` similar to `fractional_max_pool2d` (since there is no test for the same).\n\nNeeds Resolution:\n* [ ] ASAN failure on the newly added 3d variant test. https://app.circleci.com/pipelines/github/pytorch/pytorch/269483/workflows/8426b3b7-9a35-4032-a57a-729964a4a5ff/jobs/10673756\n* [ ] Failing gradcheck on MacOS. https://app.circleci.com/pipelines/github/pytorch/pytorch/269483/workflows/8426b3b7-9a35-4032-a57a-729964a4a5ff/jobs/10673101\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51626\n\nReviewed By: jbschlosser\n\nDifferential Revision: D26514064\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: e2cc57585dbc3a08c7f24591b202e0fabfd2a459", "pr_number": "51626", "files_changed": ["aten/src/ATen/native/FractionalMaxPool2d.cpp", "aten/src/ATen/native/FractionalMaxPool3d.cpp", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed", "module: internals", "open source", "triaged"]}, "03ae6d9903": {"title": "Remove useless _allgather_then_aggregate_hook (#52593)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52593\n\nThis hook is not used at all, and it probably can only be used for demonstrating that allgather is slower than allreduce, so it should never be used in practice.\n\nHowever, this hook and its helper function stay with the communication hook public APIs in the same file. It will be better to make the public API file as concise as possible.\n\nSince I don't think we will use this hook in the future, prefer deleting it to moving it to a separate file.\nghstack-source-id: 122180969\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26575318\n\nfbshipit-source-id: b258154a7c92e33236c34104bd79bc244ecdb158", "pr_number": "52593", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "783b5c0c9f": {"title": "op_whitelist -> op_allowlist (#52150)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52150\n\nRenames \"whitelist\" to \"allowlist\" to conform to company use standards, prevent critical errors raised by linters which detect the old usage, and to move toward more self-descriptive terminology.\n\nTest Plan: Sandcastle\n\nReviewed By: suo\n\nDifferential Revision: D26405520\n\nfbshipit-source-id: 9c3a41591d4e29c0197de9a8f5858c9c29271e26", "pr_number": "52150", "files_changed": ["aten/src/ATen/core/op_registration/op_allowlist.h", "aten/src/ATen/core/op_registration/op_allowlist_test.cpp", "aten/src/ATen/core/op_registration/op_whitelist.h", "aten/src/ATen/core/op_registration/op_whitelist_test.cpp", "test/cpp/jit/test_custom_operators.cpp", "torch/csrc/jit/runtime/operator.h", "torch/library.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "847d1d4d53": {"title": "add debug_flush_compilation_cache to `Method` (#52317)", "body": "Summary:\nForgot to add `debug_flush_compilation_cache ` to `Method` as well.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52317\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26583313\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 1b3e503950cc3314796aff53b3b8038d16767870", "pr_number": "52317", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/script_init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "09c56ef45e": {"title": "Remove DepTracker from LoopNest (#52405)", "body": "Summary:\nRemove the dependency tracker that works on Tensors, DepTracker, from LoopNest. This is essential to the goal of removing Tensors from LoopNest.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52405\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D26548621\n\nPulled By: navahgar\n\nfbshipit-source-id: b20f23d608c19ac71aebd31c14777d653eead36c", "pr_number": "52405", "files_changed": ["tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/analysis.cpp", "torch/csrc/jit/tensorexpr/analysis.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "067fd78f05": {"title": "add RECORD_FUNCTION to grad_sum_to_size (#52516)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52516\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26582645\n\nPulled By: Krovatkin\n\nfbshipit-source-id: f3aa7d959cc31fc6fd6f8a38c36488b01cc1a515", "pr_number": "52516", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6b8e670eb7": {"title": "[CI][IOS] Add lite interpreter ios build job (#52567)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52567\n\n## Summary\nAs title, add libtorch (lite) in ios x86 Em and\n\n## Test plan\nIn `config.yml`, remove `context: org-member`:\n```\n      - pytorch_ios_build:\n          build_environment: pytorch-ios-12.0.0-arm64_lite_interpreter_build\n          context: org-member\n          ios_arch: arm64\n          ios_platform: OS\n          lite_interpreter: \"1\"\n          name: pytorch_ios_12_0_0_arm64_lite_interpreter_build\n```\nThe build is:\n\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/276113/workflows/49fa2f6e-c978-424b-9177-bbe313955876/jobs/11050851\n\n**Build** step finishes successfully:\n![image](https://user-images.githubusercontent.com/16430979/108619899-d183b080-73dc-11eb-809d-a21f811cf821.png)\n\nIt fails **Run Build Test** because of missing `IOS_DEV_TEAM_ID`\n\nTest Plan: Imported from OSS\n\nReviewed By: xta0\n\nDifferential Revision: D26572842\n\nPulled By: cccclai\n\nfbshipit-source-id: 9d868ac7e94af37ef90212b754e91d98c0d20b30", "pr_number": "52567", "files_changed": [".circleci/cimodel/data/simple/ios_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["Merged", "cla signed"]}, "80240d0888": {"title": "update autograd kernels to use redispatch (#51363)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51363\n\nTest Plan: Imported from OSS\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26153580\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 5d7905d2c39c9bb7f219e703940ed3eef5230491", "pr_number": "51363", "files_changed": ["tools/autograd/gen_variable_type.py", "tools/autograd/templates/VariableType.cpp", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["Merged", "cla signed"]}, "947225cd1b": {"title": "update tracing codegen to use redispatch API (#52009)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52009\n\nTaking advantage of the new `redispatch` API to clean up the codegen'd tracing kernels. Instead of directly interacting with the Dispatcher, the tracing kernels now just call the `redispatch` API directly.\n\nOne small benefit to this: hopefully the compiler is more likely to inline `Dispatcher::redispatch()`, since it's now used in fewer call-sites. After this change, the only places it's used are:\n- the `redispatch` API (`RedispatchFunctions.cpp`)\n- BackendSelect kernels.\n\nOne small complication: the redispatch API doesn't interact too well with `manual_cpp_binding` ops currently. I put a note with some thoughts in the comments.\n\nExample tracing kernel before:\n```\nTensor add_Tensor(c10::DispatchKeySet ks, const Tensor & self, const\n  torch::jit::Node* node = nullptr;\n  std::shared_ptr<jit::tracer::TracingState> tracer_state;\n  if (jit::tracer::isTracing()) {\n    tracer_state = jit::tracer::getTracingState();\n    at::Symbol op_name;\n    op_name = jit::Symbol::fromQualString(\"aten::add\");\n    node = tracer_state->graph->create(op_name, /*num_outputs=*/0);\n    jit::tracer::recordSourceLocation(node);\n    jit::tracer::addInputs(node, \"self\", self);\n    jit::tracer::addInputs(node, \"other\", other);\n    jit::tracer::addInputs(node, \"alpha\", alpha);\n    tracer_state->graph->insertNode(node);\n\n    jit::tracer::setTracingState(nullptr);\n  }\n  static auto op = c10::Dispatcher::singleton()\n      .findSchemaOrThrow(\"aten::add\", \"Tensor\")\n      .typed<Tensor (const Tensor &, const Tensor &, Scalar)>();\n  auto result =c10::Dispatcher::singleton()\n      .redispatch<Tensor, const Tensor &, const Tensor &, Scalar>(op,\n  if (tracer_state) {\n    jit::tracer::setTracingState(std::move(tracer_state));\n    jit::tracer::addOutput(node, result);\n  }\n  return result;\n}\n```\n\nafter: (note the lack of `Dispatcher::` calls)\n```\nTensor add_Tensor(c10::DispatchKeySet ks, const Tensor & self, const Tensor & other, Scalar alpha)\n  torch::jit::Node* node = nullptr;\n  std::shared_ptr<jit::tracer::TracingState> tracer_state;\n  if (jit::tracer::isTracing()) {\n    tracer_state = jit::tracer::getTracingState();\n    at::Symbol op_name;\n    op_name = jit::Symbol::fromQualString(\"aten::add\");\n    node = tracer_state->graph->create(op_name, /*num_outputs=*/0);\n    jit::tracer::recordSourceLocation(node);\n    jit::tracer::addInputs(node, \"self\", self);\n    jit::tracer::addInputs(node, \"other\", other);\n    jit::tracer::addInputs(node, \"alpha\", alpha);\n    tracer_state->graph->insertNode(node);\n\n    jit::tracer::setTracingState(nullptr);\n  }\n  auto result =at::redispatch::add(ks & c10::DispatchKeySet(c10::DispatchKeySet::FULL_AFTER, c10::D\n  if (tracer_state) {\n    jit::tracer::setTracingState(std::move(tracer_state));\n    jit::tracer::addOutput(node, result);\n  }\n  return result;\n}\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D26356078\n\nPulled By: bdhirsh\n\nfbshipit-source-id: bc96ca4c6d90903f1e265859160d4b13a8cc7310", "pr_number": "52009", "files_changed": ["tools/autograd/gen_trace_type.py", "tools/autograd/templates/TraceType.cpp"], "labels": ["Merged", "cla signed"]}, "2eb9c0832e": {"title": "Modernize for-loops in torch misc (#52452)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52452\n\nTest Plan: Sandcastle\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D26520760\n\nfbshipit-source-id: c13161324f24f553ad679308d0dc279ab178e129", "pr_number": "52452", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp", "torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "bfc2645981": {"title": "[BE] force cmake to always generate version.py (#52477)", "body": "Summary:\nFix the issue that `add_custom_command(OUTPUT ...)` will only be called when target output is missing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52477\n\nReviewed By: malfet\n\nDifferential Revision: D26538718\n\nPulled By: walterddr\n\nfbshipit-source-id: 0fef40585a0f888dcbe268deb2e7a7a8d81e6aa1", "pr_number": "52477", "files_changed": ["torch/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "a39b1c42c1": {"title": "MHA: Fix regression and apply bias flag to both in/out proj (#52537)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52257\n\n## Background\nReverts MHA behavior for `bias` flag to that of v1.5: flag enables or disables both in and out projection biases.\n\nUpdates type annotations for both in and out projections biases from `Tensor` to `Optional[Tensor]` for `torch.jit.script` usage.\n\nNote: With this change, `_LinearWithBias` defined in `torch/nn/modules/linear.py` is no longer utilized. Completely removing it would require updates to quantization logic in the following files:\n```\ntest/quantization/test_quantized_module.py\ntorch/nn/quantizable/modules/activation.py\ntorch/nn/quantized/dynamic/modules/linear.py\ntorch/nn/quantized/modules/linear.py\ntorch/quantization/quantization_mappings.py\n```\nThis PR takes a conservative initial approach and leaves these files unchanged.\n\n**Is it safe to fully remove `_LinearWithBias`?**\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52537\n\nTest Plan:\n```\npython test/test_nn.py TestNN.test_multihead_attn_no_bias\n```\n\n## BC-Breaking Note\nIn v1.6, the behavior of `MultiheadAttention`'s `bias` flag was incorrectly changed to affect only the in projection layer. That is, setting `bias=False` would fail to disable the bias for the out projection layer. This regression has been fixed, and the `bias` flag now correctly applies to both the in and out projection layers.\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26583639\n\nPulled By: jbschlosser\n\nfbshipit-source-id: b805f3a052628efb28b89377a41e06f71747ac5b", "pr_number": "52537", "files_changed": ["test/test_nn.py", "torch/nn/functional.py", "torch/nn/modules/activation.py", "torch/nn/quantizable/modules/activation.py"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "973e306c84": {"title": "changed TE 'Allocate' API to take one argument 'Buf' instead of three arguments 'Var', 'dtype', 'dims'. (#50167)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50167\n\nTest Plan:\nImported from OSS\n\n`python test/test_jit_fuser_te.py`\n`python test/test_jit_fuser_legacy.py`\n`python test/test_jit_fuser.py`\n`build/bin/test_tensorexpr`\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D25814342\n\nPulled By: huiguoo\n\nfbshipit-source-id: 44cba7f92365b826c9cb1d385a94858934570dee", "pr_number": "50167", "files_changed": ["test/cpp/tensorexpr/test_cpp_codegen.cpp", "test/cpp/tensorexpr/test_cuda.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.h", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "09fe753a33": {"title": "Enable TCPStore fixed slow test (#52511)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52511\n\nRe-enable a test that was previously fixed but forgot to be re-enabled.\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26586980\n\nPulled By: H-Huang\n\nfbshipit-source-id: 3cfe21de09036d2b87273680dae351e20125e815", "pr_number": "52511", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "fd5792f857": {"title": "docs: add :nosignatures: in torch.jit (#52555)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52554\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52555\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D26573956\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: ce011c66ce771bc7e9357f98db9994d54faa7013", "pr_number": "52555", "files_changed": ["docs/source/jit.rst"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "3309f034aa": {"title": "remove pointless test (#52609)", "body": "Summary:\nFixes T81870118\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52609\n\nReviewed By: mruberry\n\nDifferential Revision: D26584288\n\nPulled By: ngimel\n\nfbshipit-source-id: 7cec37db46cfe5b5b2fd21fe7c3e3fcbb8aba049", "pr_number": "52609", "files_changed": ["test/test_binary_ufuncs.py"], "labels": ["Merged", "cla signed"]}, "108ec77fa7": {"title": "[NNC] Added reductions to NNC python bindings. (#52492)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52492\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26575506\n\nPulled By: Chillee\n\nfbshipit-source-id: 9a070f591a9709dab55dfff849184b1bcffc4fa5", "pr_number": "52492", "files_changed": ["torch/csrc/jit/tensorexpr/tensorexpr_init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "1865499d49": {"title": "[Pytorch Mobile] Improve export_opnames Documentation (#52333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52333\n\nExport_opnames current documentation is a bit misleading. Change it to better clarify what it does.\nghstack-source-id: 121810264\n\nTest Plan: n/a\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26471803\n\nfbshipit-source-id: 496d10b161c9a4076c4e12db8a0affafc4e1e359", "pr_number": "52333", "files_changed": ["torch/csrc/jit/serialization/export.h", "torch/jit/__init__.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7f4dff5496": {"title": "docs: add FractionalMaxPool3d in pooling layers (#52556)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51625\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52556\n\nReviewed By: smessmer\n\nDifferential Revision: D26593666\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 3d81d23fa70efa0f794dde47a34baad0aaa9c751", "pr_number": "52556", "files_changed": ["docs/source/nn.rst"], "labels": ["Merged", "cla signed", "open source"]}, "a0652c8f08": {"title": "[static runtime] Fix up deprecated exact equality in tests (#52617)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52617\n\nswaps `.equals` with `torch::allclose`\n\ntests are broken right now\n\nTest Plan: buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest -- --run-disabled\n\nReviewed By: bertmaher, maratsubkhankulov, yinghai\n\nDifferential Revision: D26585079\n\nfbshipit-source-id: 9bd2a7b87208301415a8925f95c69fe44accf159", "pr_number": "52617", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "a59c4039e0": {"title": "Fix undefined symbol for CUDA 11.1 Windows (#52506)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52467.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52506\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26582788\n\nPulled By: seemethere\n\nfbshipit-source-id: a03489449e0492ed023bf54aa9da194491f0e67f", "pr_number": "52506", "files_changed": ["aten/src/ATen/native/DispatchStub.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "6cfe55dea9": {"title": "Add psutil to requirements.txt (#52285)", "body": "Summary:\npsutil is used in many test scripts under test/\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52285\n\nReviewed By: jbschlosser\n\nDifferential Revision: D26516673\n\nPulled By: malfet\n\nfbshipit-source-id: 09a81d5dba3bf5189e3e5575c2095eb069b93ade", "pr_number": "52285", "files_changed": ["requirements.txt"], "labels": ["Merged", "cla signed", "module: tests", "open source", "triaged"]}, "f111ec48c1": {"title": "docs: add fractional_max_pool in nn.functional (#52557)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51708\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52557\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26591388\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 42643864df92ea014e69a8ec5c29333735e98898", "pr_number": "52557", "files_changed": ["docs/source/nn.functional.rst", "torch/nn/functional.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "fa8568184f": {"title": "[caffe2] Delete some unused fields from TensorProto (#52521)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52521\n\nThe `storage_type` and `external_data` fields were added a few years ago in\nD10246743 (https://github.com/pytorch/pytorch/commit/30aaa075940cd2739f9786ee96c42b0caefd8094) but don't appear to have been used anywhere.  Let's remove them to\nhelp simplify the `TensorProto` message definition.\nghstack-source-id: 122110201\n\nTest Plan: Confirmed the code still builds.\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D26500028\n\nfbshipit-source-id: 1e188f98f59e0b8673ea342ad9aaf7e5ba9b5fac", "pr_number": "52521", "files_changed": ["caffe2/proto/caffe2.proto", "caffe2/proto/caffe2_pb2.pyi"], "labels": ["Merged", "cla signed"]}, "7ecc1b603a": {"title": "[TensorPipe] Update [Cpu|Cuda]Buffer fwd declarations (#52600)", "body": "Summary:\nThey've changed from class to struct in tensorpipe repo, but have not\nbeen updated in the header, which triggers compiler warning if clang is\nused and would have triggered a linker error if the same code is\ncompiled with MSVC\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52600\n\nReviewed By: lw\n\nDifferential Revision: D26579754\n\nPulled By: malfet\n\nfbshipit-source-id: 800c02e7ba839bac01adf216de2d8547b7e9128b", "pr_number": "52600", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "67794b14bb": {"title": "Use `int8_t` instead of `char` in [load|store]_scalar` (#52616)", "body": "Summary:\nSince `char` is not guaranteed to be signed on all platforms (it is unsigned on ARM)\nFixes https://github.com/pytorch/pytorch/issues/52146\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52616\n\nTest Plan: Run ` python3 -c \"import torch;a=torch.tensor([-1], dtype=torch.int8);print(a.tolist())\"` on arm-linux system\n\nReviewed By: walterddr\n\nDifferential Revision: D26586678\n\nPulled By: malfet\n\nfbshipit-source-id: 91972189b54f86add516ffb96d579acb0bc13311", "pr_number": "52616", "files_changed": ["torch/csrc/utils/python_scalars.h"], "labels": ["Merged", "cla signed"]}, "49b59e3472": {"title": "Add OpInfo entries for i0 and logical_not (#51956)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51956\n\nReviewed By: albanD\n\nDifferential Revision: D26404440\n\nPulled By: mruberry\n\nfbshipit-source-id: dd73e63155dd4a200afb38a5e566eb2132e69fde", "pr_number": "51956", "files_changed": ["test/test_ops.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2680ff7759": {"title": "Revert D26598115: [pytorch][PR] Update XNNPACK", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26598115 (https://github.com/pytorch/pytorch/commit/3721962c33a64946bd3be8ce8c032c4e7d895d22)\n\nOriginal commit changeset: d652bacdee10\n\nfbshipit-source-id: 7e0128aa9b7691ecd323687da6f6054363b3174a", "pr_number": null, "files_changed": [".jenkins/pytorch/common_utils.sh", "test/test_mobile_optimizer.py", "third_party/XNNPACK"], "labels": []}, "e658d7c37b": {"title": "Ignore user annotated ignored attributes (#52367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52367\n\nThis fixes https://github.com/pytorch/pytorch/issues/52217\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar, gmagogsfm\n\nDifferential Revision: D26574411\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 7eac097f5b97cfe65854bceca14d41c156cd6e0a", "pr_number": "52367", "files_changed": ["test/test_jit.py", "torch/jit/_recursive.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e3a805b9c5": {"title": "Fake Quantization support for f16 and f32 (#52612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52612\n\nused the float type macro to generalize the fake_quantization per tensor functions to f16 and f64.\n\nTest Plan:\nadded test to show it works in AMP and extended the forward and backward tests below to test float16 and float64 operations. Note: the reference function doesn't work with with these types so I had to convert in and back out of these types to compare.\n\n```test python test/test_quantization.py\nTestFakeQuantize.test_forward_backward_per_tensor_with_amp\n\ntest python test/test_quantization.py TestFakeQuantize.test_forward_per_tensor_cachemask_cpu\n\ntest python test/test_quantization.py TestFakeQuantize.test_backwards_per_tensor_cachemask_cpu\n\ntest python test/test_quantization.py TestFakeQuantize.test_forward_per_tensor_cachemask_cuda\n\ntest python test/test_quantization.py TestFakeQuantize.test_backwards_per_tensor_cachemask_cuda\n\ntest python test/test_quantization.py\n\n```\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D26586416\n\nfbshipit-source-id: 55fe83c5e47f45cd1de8ddd69bd4a5653ab6dc12", "pr_number": "52612", "files_changed": ["aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cuda/fake_quantize_core.cu", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "test/quantization/test_workflow_module.py"], "labels": ["Merged", "cla signed"]}, "1c63cb2c0f": {"title": "Pass child error to parent in distributed tests. (#52632)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52632\n\nDistributed tests run in a multiprocessing environment, where a parent\nprocess drives the tests through several child processes. As a result, when a\nchild process fails the parent only prints the following:\n\n```\nProcess 0 exited with error code 10\n```\n\nThe child process also logs its own exception, but it is cumberson to go\nthrough the logs and track this down.\n\nTo alleviate this, I've added a bunch of pipes for each child process so that\nthe child process writes the error to the pipe before exiting and the parent\nprocess can read the appropriate error from the pipe and display it.\n\nThe new output printed by the parent is as follows:\n\n```\n> RuntimeError: Process 0 exited with error code 10 and exception:\nTraceback (most recent call last):\n  File \"torch/testing/_internal/common_distributed.py\", line 361, in _run\n    getattr(self, test_name)()\n  File \"torch/testing/_internal/common_distributed.py\", line 288, in wrapper\n    fn()\n  File \"test_c10d.py\", line 789, in test_broadcast_checks\n    pg.broadcast([t1], opts)\nValueError: ProcessGroupGloo::broadcast: invalid root rank: -1\n\nProcess 1 exited with error code 10 and exception:\nTraceback (most recent call last):\n  File \"torch/testing/_internal/common_distributed.py\", line 361, in _run\n    getattr(self, test_name)()\n  File \"torch/testing/_internal/common_distributed.py\", line 288, in wrapper\n    fn()\n  File \"test_c10d.py\", line 789, in test_broadcast_checks\n    pg.broadcast([t1], opts)\nValueError: ProcessGroupGloo::broadcast: invalid root rank: -1\n\nProcess 2 exited with error code 10 and exception:\nTraceback (most recent call last):\n  File \"torch/testing/_internal/common_distributed.py\", line 361, in _run\n    getattr(self, test_name)()\n  File \"torch/testing/_internal/common_distributed.py\", line 288, in wrapper\n    fn()\n  File \"test_c10d.py\", line 789, in test_broadcast_checks\n    pg.broadcast([t1], opts)\nValueError: ProcessGroupGloo::broadcast: invalid root rank: -1\n\nProcess 3 exited with error code 10 and exception:\nTraceback (most recent call last):\n  File \"torch/testing/_internal/common_distributed.py\", line 361, in _run\n    getattr(self, test_name)()\n  File \"torch/testing/_internal/common_distributed.py\", line 288, in wrapper\n    fn()\n  File \"test_c10d.py\", line 789, in test_broadcast_checks\n    pg.broadcast([t1], opts)\nValueError: ProcessGroupGloo::broadcast: invalid root rank: -1\n```\nghstack-source-id: 122273793\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26589274\n\nfbshipit-source-id: 7b7a71ec790b216a89db7c157377f426531349a5", "pr_number": "52632", "files_changed": ["torch/testing/_internal/common_distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed"]}, "e2afb269b8": {"title": "[caffe2] add a Python test for SaveOp chunking", "body": "Summary:\nAdd a test in `load_save_test.py` that passes in a chunk_size parameter,\nto ensure that we exercise the logic that passes the chunk size to the C++\nserialization code.\n\nTest Plan:\nRan the tests with the vlog level set to 3 and manually verified the log\nmessages showed that we were serializing in the expected chunks.\nThere are existing C++ tests that confirm chunking behavior works as expected\nin the pure C++ code.\n\nReviewed By: mraway\n\nDifferential Revision: D26502578\n\nfbshipit-source-id: cd0074f2358da81c68b0fed2c2a94818d83a957d", "pr_number": null, "files_changed": ["caffe2/python/operator_test/load_save_test.py"], "labels": []}, "1cddb27f39": {"title": "[FX acc]Store shape and dtype in serialized output node args (#52462)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52462\n\nThis is the step one for supporting multiple outputs in fx nnpi path.\n\nDuring serialization, we store the shape and dtype in output args, so that importer doesn't need to go back and find the nodes.\n\nThe output nodes will looks like\n```\n                {\n                    \"target\": \"output\",\n                    \"op_code\": \"output\",\n                    \"name\": \"output\",\n                    \"args\": [\n                        {\n                            \"is_node\": true,\n                            \"name\": \"add_1\",\n                            \"shape\": \"[1, 1]\",\n                            \"dtype\": \"torch.float32\"\n                        }\n                    ],\n                    \"kwargs\": {}\n                }\n```\n\nTest Plan: Doesn't break existing tests and will test on step two.\n\nReviewed By: jfix71\n\nDifferential Revision: D26500742\n\nfbshipit-source-id: 755d2dec704d9da579af40e754b556d6c01aa796", "pr_number": "52462", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "ecf3ca00d8": {"title": "[fx] Separate globals assignment from code generation (#51974)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51974\n\nRight now, when an FX `Graph` references an external object, we will emit\ncode like:\n\n    import foo\n    def forward(input: foo.bar.baz):\n        ...\n\nThis is problematic in a world with `torch.package`, since then name\n`foo.bar.baz` may reference a name from any number of packages.\n\nThis PR lays the groundwork for FX-package integration by separating the\nresolution of external references from the genration of the function\ncode.\n\nWhen generating a Graph's Python source, we keep track of all external\nreferences and assign them unique names. At the end, we have a\ndictionary mapping names -> actual objects. This becomes the `globals`\nnamespace we pass to `exec` when installing the forward function in a\n`GraphModule`. This is nice because we can always be sure that `exec` is\nseeing the same objects that were referenced from the `Graph`, no import\nstatements needed.\n\nAt serialization time, we use a `ModuleEnv` to resolve the globals dict\nto a set of import statements that can be run to reprodce the `global`\nnamespace. This is only used on serialiation/deserialization, and those\nfunctions are expected to check that the import statements are producing\nthe correct results.\n\nConcretely, the code above will now look like:\n\n    from foo.bar import baz as foo_bar_baz\n    def forward(input: foo_bar_baz):\n        ...\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26340593\n\nPulled By: suo\n\nfbshipit-source-id: fe247f75205d0a03fd067bdd0f95491e8edf1436", "pr_number": "51974", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "test/test_fx.py", "test/test_fx_experimental.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/fx/node.py"], "labels": ["Merged", "cla signed", "fx"]}, "d5ed57569b": {"title": "Move cuda9 and cuda11.2 CI jobs to a scheduled workflow (#52693)", "body": "Summary:\nMoving master only resource-interactive CI jobs to a less regular basis.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52693\n\nReviewed By: malfet, seemethere\n\nDifferential Revision: D26615060\n\nPulled By: janeyx99\n\nfbshipit-source-id: def46a7890ea46c655ef2ee0f7c548171464cb48", "pr_number": "52693", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/generate_config_yml.py", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "cla signed"]}, "df30cb78d2": {"title": "Remove unused variable (#52652)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52652\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D26589146\n\nfbshipit-source-id: 704a93e479e1bf2420dd47589319a5438a2f92f1", "pr_number": "52652", "files_changed": ["aten/src/THCUNN/generic/MultiMarginCriterion.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "420fc42eab": {"title": "add OneDNN pooling backward (#49454)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49454\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D26006888\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 6a4930982db784819fea70ffc9029441d673d90e", "pr_number": "49454", "files_changed": ["aten/src/ATen/native/mkldnn/Pooling.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_mkldnn.py", "tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed", "open source"]}, "cabb1e7a94": {"title": "Fix wrong TORCH_CHECK usages (#52670)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52670\n\nTORCH_CHECK followed by a string literal is a no-op, and from the text of the message its clear that authors intended those instances to be `TORCH_CHECK(false, \"msg\")`\n\nDiscovered while trying to figure out of tensor_offset can be negative in Resize.h\n\ns/TORCH_CHECK\\(\"/TORCH_CHECK(false, \"/\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr, janeyx99, mruberry\n\nDifferential Revision: D26607546\n\nPulled By: malfet\n\nfbshipit-source-id: 661812da84adb1d1af0284da60c93ec4bf5ef08e", "pr_number": "52670", "files_changed": ["aten/src/ATen/native/Resize.h", "test/cpp/jit/test_gpu.cpp", "torch/csrc/jit/passes/onnx/helper.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "b534466f01": {"title": "[DataLoader] TransformsIterDataPipe (#52604)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52604\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26581511\n\nPulled By: ejguan\n\nfbshipit-source-id: c927726b7afba14586f16cde0237f2cef9080079", "pr_number": "52604", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/callable.py"], "labels": ["Merged", "cla signed"]}, "caa377f546": {"title": "replace type().backend() with device() (#52558)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52558\n\nReviewed By: malfet\n\nDifferential Revision: D26616025\n\nPulled By: jbschlosser\n\nfbshipit-source-id: ef9f3f42e830788c21feab533e192ba9c6eb8edb", "pr_number": "52558", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Memory.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/mkldnn/Linear.cpp", "torch/csrc/utils/tensor_apply.cpp", "torch/csrc/utils/tensor_list.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "26419815af": {"title": "Modernize for-loops (#52330)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52330\n\nTest Plan: Sandcastle\n\nReviewed By: mruberry\n\nDifferential Revision: D26001961\n\nfbshipit-source-id: e75cc8f1a8d30917b4d55df9e1a3c7836c271820", "pr_number": "52330", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/passes/quantization/quantization_patterns.h", "torch/csrc/jit/tensorexpr/eval.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed", "oncall: jit"]}, "238b0bbb68": {"title": "Allow Transformer accept output result that is not Proxy (#52473)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52473\n\nUse `map_aggregate` to create output for new graph so that it won't raise error when we have outputs that is not `Proxy`.\n\nTest Plan: `test_transformer_multi_outputs` in `test_fx.py`\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26502277\n\nfbshipit-source-id: 404d9030a9b84db3f66f8505887a75717a28ad30", "pr_number": "52473", "files_changed": ["test/test_fx.py", "torch/fx/interpreter.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "0c455332e8": {"title": "docs: add link to Tensor.share_memory_ in Module.share_memory (#52561)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/48228\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52561\n\nReviewed By: malfet\n\nDifferential Revision: D26626012\n\nfbshipit-source-id: 7aab44c60d1bcbda68012521ec852843864abc7f", "pr_number": "52561", "files_changed": ["torch/nn/modules/module.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "755c60bffc": {"title": "[PyTorch Mobile] Allow loading of all extra files using the extra_file argument (#52635)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52635\n\nCurrently, the method `_load_for_mobile()` accepts an extra files map named `extra_files` which serves as an in-out parameter. i.e. the call fills in the keys of this map with all files under the `extra/` folder that they wish to extract, and the method fills in the `extra_files` map with the contents of those files.\n\nIn a specific case we have encountered, it is desirable to extract all the extra files so that they can be forwarded in an opaque manner into a `save_for_mobile()` call with the same set of extra files as during load.\n\nThis change adds a method `_get_all_archive_file_names()` which returns the names of all files in the `.ptl` archive. The caller can then extract the ones within the `extra/` directory and pass them in to the `extra_files` map argument.\n\nghstack-source-id: 122356928\n\nTest Plan: Added additional test + `buck test //xplat/caffe2:test_lite_interpreter`\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26590027\n\nfbshipit-source-id: 4dc30997929e132f319c32cb9435d8a40fe0db5e", "pr_number": "52635", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "2d75346c25": {"title": "[Gradient Compression] Add a minimum compression rate threshold for PowerSGD communication hook (#52541)", "body": "Summary:\nFixes #{52034}\n- Add a minimum compression rate threshold to `PowerSGDState`\n- Use the threshold to determine whether to compress high-rank tensors or not\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52541\n\nTest Plan:\nNo performance regression using rank-8 compression:\nbaseline: f253000411\nupdated one: f253010955\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26594862\n\nPulled By: SciPioneer\n\nfbshipit-source-id: 2859a91b4ca6bd1862bf6cd6441dc2a89badb2d5", "pr_number": "52541", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "075bbe0d6a": {"title": "Fix for incorrect usage of logging in torch/distributed/distributed_c10d.py (#51739)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51428\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51739\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26599390\n\nfbshipit-source-id: d822658076f7b08ebfde3dc9994159539490fda0", "pr_number": "51739", "files_changed": ["test/distributed/test_c10d.py", "torch/distributed/distributed_c10d.py"], "labels": ["Merged", "Reverted", "cla signed", "oncall: distributed", "open source", "triaged"]}, "958d9a8364": {"title": "[fx/package] make GraphModules packageable (#51976)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51976\n\nFX serializes things by serializing Python code as a string and exec'ing\nit on load. This accomplishes one goal (we don't have to pickle the\ngraph object directly) but breaks the pickle abstraction in ways that\nare not composable with `torch.package`.\n\nIn particular:\n1. `forward` is serialized by saving Python code. On load, it's\ninstalled\nby  `exec`ing that code. This `exec` call needs to have the right\nimporter installed, otherwise it will not import modules from the\n`torch.package` but instead import from the Python environment.\n2. Any types/functions used are emitted as `import` statement in the\ngenerated Python code. These are effectively dynamic dependencies of the\n`GraphModule` being saved, and need to be registered as such so that the\n`PackageImporter` will package them.\n\nTo address these, this PR introduces a new protocol for the\nimporter/exporter: `__reduce_package__`.\n\nA class can implement `__reduce_package__` to customize how it is placed\nin the importer/exproter. It functions very similarly to `__reduce__`,\nexcept:\n- `__reduce_package__` takes one argument, which is the\n`PackageExporter`\ninstance. Users can use this instance to save stuff to the package to\nimplement their serialization. `__reduce__` takes no args.\n- Only the 2-element tuple version of the return value for `__reduce__`\nis supported (this could be extended if necessary).\n- When the reduction function is called on load, an additional argument\nis added to the beginning of the args tuple. This is the\n`PackageImporter`\ninstance doing the loading.\n\nThe `__reduce_package__` protocol is defined using `persistent_id` and\n`persistent_load`, which ensures that we can still use the cpickle\nimplementation of the pickler by default.\n\nPull Request resolved: #51971\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D26340591\n\nPulled By: suo\n\nfbshipit-source-id: 5872a7d22e832056399a7372bae8a57807717882", "pr_number": "51976", "files_changed": ["test/package_a/subpackage.py", "test/package_a/test_module.py", "test/test_fx.py", "test/test_package.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/package/package_exporter.py", "torch/package/package_importer.py"], "labels": ["Merged", "cla signed", "fx"]}, "b56f59ea20": {"title": "Revert D26599390: [pytorch][PR] Fix for incorrect usage of logging in torch/distributed/distributed_c10d.py", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26599390 (https://github.com/pytorch/pytorch/commit/075bbe0d6aa9d5129870f91cc40a9890a5aed3ce)\n\nOriginal commit changeset: d822658076f7\n\nfbshipit-source-id: 6c4421f4de99794ea66780175af549cef9410a20", "pr_number": null, "files_changed": ["test/distributed/test_c10d.py", "torch/distributed/distributed_c10d.py"], "labels": []}, "8af648354f": {"title": "[nnc] Benchmarks for concat (#52592)", "body": "Summary:\nThis PR adds a c++ benchmark for \"concat\" with 3 different versions - 1) aten::cat, 2) NNC implementation with if-then-else, 3) NNC implementation using multiple loops. It also adds a python benchmark for \"concat\" which can now be invoked with and without CPU fusion.\n\nHere are the results of these benchmarks on a `Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz` machine with `OMP_NUM_THREADS=1`\n\n```\n--------------------------------------------------------------------------------------------------------------------------\nBenchmark                                                                   Time           CPU Iterations UserCounters...\n--------------------------------------------------------------------------------------------------------------------------\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/ATen/1/160/1/14/1                                         1211 ns       1211 ns     567896 GB/s=1.14953G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/ATen/1/580/1/174/1                                        1296 ns       1296 ns     537060 GB/s=4.65362G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/ATen/20/160/20/14/1                                       1823 ns       1823 ns     382052 GB/s=15.2677G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/ATen/20/580/20/174/1                                      3347 ns       3347 ns     210036 GB/s=36.0432G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/ATen/8/512/8/512/1                                        2093 ns       2093 ns     324760 GB/s=31.3061G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNC/1/160/1/14/1                                           694 ns        694 ns    1002902 GB/s=2.00692G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNC/1/580/1/174/1                                          852 ns        852 ns     803002 GB/s=7.08127G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNC/20/160/20/14/1                                        1639 ns       1639 ns     419683 GB/s=16.9828G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNC/20/580/20/174/1                                       5956 ns       5956 ns     117833 GB/s=20.2548G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNC/8/512/8/512/1                                         3136 ns       3136 ns     224122 GB/s=20.8958G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNCLoop/1/160/1/14/1                                       581 ns        581 ns    1209873 GB/s=2.39737G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNCLoop/1/580/1/174/1                                      614 ns        614 ns    1132332 GB/s=9.82955G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNCLoop/20/160/20/14/1                                    1091 ns       1091 ns     622952 GB/s=25.5247G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNCLoop/20/580/20/174/1                                   2399 ns       2399 ns     288376 GB/s=50.289G/s\nConcat2D2 (https://github.com/pytorch/pytorch/commit/678fe9f0771a5cd98ead214363d70480ba03000d)Input/NNCLoop/8/512/8/512/1                                     1500 ns       1500 ns     478360 GB/s=43.6968G/s\nConcat2D3 (https://github.com/pytorch/pytorch/commit/e23ddf06e925695df1d7f35c897535e8e9613386)Input/ATen/8/512/8/512/8/512/1                                  2584 ns       2584 ns     266394 GB/s=38.0397G/s\nConcat2D3 (https://github.com/pytorch/pytorch/commit/e23ddf06e925695df1d7f35c897535e8e9613386)Input/NNC/8/512/8/512/8/512/1                                   5056 ns       5056 ns     139768 GB/s=19.4416G/s\nConcat2D3 (https://github.com/pytorch/pytorch/commit/e23ddf06e925695df1d7f35c897535e8e9613386)Input/NNCLoop/8/512/8/512/8/512/1                               1917 ns       1917 ns     369626 GB/s=51.2758G/s\nConcat2D7 (https://github.com/pytorch/pytorch/commit/b5edf329f8d4ca0508ac52f347893694b71f8aba)Input/ATen/8/128/8/256/8/384/8/512/8/512/8/512/8/512/1          3888 ns       3888 ns     178124 GB/s=46.3571G/s\nConcat2D7 (https://github.com/pytorch/pytorch/commit/b5edf329f8d4ca0508ac52f347893694b71f8aba)Input/NNC/8/128/8/256/8/384/8/512/8/512/8/512/8/512/1          24639 ns      24638 ns      28336 GB/s=7.31481G/s\nConcat2D7 (https://github.com/pytorch/pytorch/commit/b5edf329f8d4ca0508ac52f347893694b71f8aba)Input/NNCLoop/8/128/8/256/8/384/8/512/8/512/8/512/8/512/1       3093 ns       3093 ns     226326 GB/s=58.265G/s\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52592\n\nReviewed By: bertmaher\n\nDifferential Revision: D26596701\n\nPulled By: navahgar\n\nfbshipit-source-id: 650fa88febf4423ea49f5a1d3d734edc2294d257", "pr_number": "52592", "files_changed": ["benchmarks/cpp/tensorexpr/CMakeLists.txt", "benchmarks/cpp/tensorexpr/bench_concat.cpp", "benchmarks/tensorexpr/__main__.py", "benchmarks/tensorexpr/concat.py", "benchmarks/tensorexpr/pt_engine.py"], "labels": ["Merged", "cla signed"]}, "3489b4a7b8": {"title": "Fix the ordering of TCPStore's compare_set parameters (#52696)", "body": "Summary:\n- Fixes the ordering of the value parameters of TCPStore's `compare_set()` in the pybind11 interop layer. The C++ API expects (old, new) while we are passing (new, old) in Python.\n- Fixes the implementation of TCPStore's `compareSetHandler()` for cases where the key already exists in the store.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52696\n\nTest Plan: `python test/distributed/test_c10d.py`\n\nReviewed By: malfet, H-Huang\n\nDifferential Revision: D26616976\n\nPulled By: cbalioglu\n\nfbshipit-source-id: e6a70542e837be04697b5850947924edd896dbf6", "pr_number": "52696", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/TCPStore.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "08d7f29601": {"title": "Add discontiguous kwarg to make_tensor (#51985)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51985\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D26375733\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: bb7831dc28c24b90c6f83885681eeccfdbb83438", "pr_number": "51985", "files_changed": ["test/test_testing.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "59ac0ff037": {"title": "Change `maybe_resize_storage_cpu` `new_size` arg to unsigned (#52671)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52671\n\nCode is written with the assumption that new_size is unsigned value,\nand when function is called with negative value it silently returns a nullptr rather than raise an exception.\nFix above-mentioned logic by converting new_size to unsigned type and let cpu_allocator raise exception on negative alloc.\n\nUnroll nested if blocks by returning early if new_size is 0\n\nAdd TestNN.test_adaptive_pooling_size_overflow to indirecty validate the fix.\n\nFixes https://github.com/pytorch/pytorch/issues/50960\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D26607549\n\nPulled By: malfet\n\nfbshipit-source-id: e3d4f7548b098f24fa5aba42d8f4e9288ece1e2e", "pr_number": "52671", "files_changed": ["aten/src/ATen/native/Resize.h", "test/test_nn.py"], "labels": ["Merged", "cla signed"]}, "b4b7db2f3b": {"title": "[FX acc]Add fx_glow support for multi outputs (#52527)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52527\n\nAdd e2e support for multiple outputs.\n\nTest Plan: `buck test glow/fb/fx/fx_glow:test_fx_glow -- test_fx_glow_binding_with_multiple_outputs`\n\nReviewed By: gcatron\n\nDifferential Revision: D26555520\n\nfbshipit-source-id: f3ccd61a0c2429d4a5f511c403fa6e782012e21e", "pr_number": "52527", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "27d04f291e": {"title": "Clarify usage and output of tools/test_history.py (#52640)", "body": "Summary:\nThis PR makes several UX improvements to `tools/test_history.py`:\n- warn if `--all` is unset and no jobs are passed\n- print output even in `multiline` mode if no reports are found for a commit\n  - this makes it easier to tell whether the script is just hanging\n- if there are multiple reports for a commit/job pair, say so\n- distinguish between not finding any reports and just not finding the desired test in the reports found\n- don't require the suite name as a CLI arg, just use the test name\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52640\n\nTest Plan: Example shell session: https://pastebin.com/SSemHqP8\n\nReviewed By: walterddr\n\nDifferential Revision: D26594350\n\nPulled By: samestep\n\nfbshipit-source-id: 9ce2245f91eef289817aafe955a4343d4a068eda", "pr_number": "52640", "files_changed": ["tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "7ae7768617": {"title": "[ZeroRedundancyOptimizer] Remove pseudo futures handling, not needed (#52698)", "body": "Summary:\nThis was mostly needed for ShardedDDP, not used here, dead code removal\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52698\n\nReviewed By: mrshenli\n\nDifferential Revision: D26617893\n\nPulled By: blefaudeux\n\nfbshipit-source-id: 9bcfca5135bf332ebc1240300978c138d2041146", "pr_number": "52698", "files_changed": ["torch/distributed/optim/zero_redundancy_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "98873b9258": {"title": "Update Gloo submodule (#52754)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52754\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D26608421\n\nfbshipit-source-id: 034ee34faa62ec4d4672d0197c59fa48894adae0", "pr_number": "52754", "files_changed": ["third_party/gloo"], "labels": ["Merged", "cla signed", "fb-exported"]}, "0567988e74": {"title": "Kernel launch checks for aten/src/ATen (#52185)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52185\n\nTest Plan: Sandcastle tests\n\nReviewed By: ngimel\n\nDifferential Revision: D26408276\n\nfbshipit-source-id: 554dcfca52304b8e17ffbd0ba0dcf73f99cf28c6", "pr_number": "52185", "files_changed": ["aten/src/ATen/cuda/CUDAApplyUtils.cuh", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/WeightNorm.cu", "aten/src/ATen/native/sparse/cuda/SoftMax.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/ATen/test/cuda_atomic_ops_test.cu", "aten/src/ATen/test/cuda_distributions_test.cu", "aten/src/ATen/test/cuda_integer_divider_test.cu", "aten/src/ATen/test/cuda_packedtensoraccessor_test.cu", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "373a20ad4a": {"title": "Modernize for-loops in caffe2/torch (#52618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52618\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50797\n\nModernize for-loops throughout caffe2/ subdirs to use ranged-loops where possible (all `.cpp` files were examined).\n\n```\nfind caffe2/ -iname \"*.cpp\" > /home/rbarnes/files\nbuck run mode/opt foundation/clangr:clangr_local -- -j 10 --file=/home/rbarnes/files --multi --apply-replacements=true tidy '--checks=-*,modernize-loop-convert'\n```\n\nTest Plan: Sandcastle tests\n\nReviewed By: suo\n\nDifferential Revision: D26585065\n\nfbshipit-source-id: 439b9f9ce7c54fa9b4b80161f6bb27ebe8a35967", "pr_number": "52618", "files_changed": ["torch/csrc/jit/tensorexpr/stmt.h", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp", "torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed", "oncall: jit"]}, "29c4290a8d": {"title": "Use c10::irange for great good (#52153)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52153\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D26407087\n\nfbshipit-source-id: ea8ce1c17299cb9d89621e4a39f31edc2faa9fd6", "pr_number": "52153", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/Linear.cpp", "aten/src/ATen/test/cuda_stream_test.cpp", "c10/cuda/CUDAStream.cpp", "c10/util/irange.h", "test/cpp/jit/test_fuser.cpp", "torch/csrc/jit/codegen/cuda/ir_nodes.cpp", "torch/csrc/jit/codegen/cuda/lower_validation.cpp", "torch/csrc/jit/codegen/cuda/mutator.cpp", "torch/csrc/jit/codegen/cuda/tensor_view.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "c954817696": {"title": "print matrix dims in torch cuda matrix multiply error (#52780)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52780\n\ntrying to improve the error message for torch matrix multiply dimension mismatch\n\nTest Plan: check if code compiles\n\nReviewed By: akyrola\n\nDifferential Revision: D26617036\n\nfbshipit-source-id: de23e551af985a00384fb1cccd04120b9d2728b3", "pr_number": "52780", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "c140a5ec04": {"title": "Use finer-grained mutexes in TensorPipe RPC agent (#52749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52749\n\nTensorPipe has recently changed some implementation details in how it schedules callbacks and this has exposed an issue in the RPC agent. Previously the callbacks of each pipe were executed independently and possibly simultaneously. For safety reasons (especially during shutdown) TensorPipe now synchronizes the pipes and thus invokes one callback at a time. Another characteristic of TensorPipe is that it \"hijacks\" some user threads to run some callbacks inline (e.g., if a low-level event loop completes an operation while a pipe is already busy, this completion is queued up and the user callback could be invoked later by a different thread, including the user's own thread).\n\nThese two effects combined caused a \"reentrancy\" phenomenon, where calling `context->connect` (formerly on line 850) to create a new client-side pipe could cause invoking a read callback on another pipe. Since we were holding `mutex_` when calling `context->connect`, and we were trying to re-acquire `mutex_` inside the read callback, this lead to a deadlock.\n\nOne solution to this problem is using finer-grained mutexes. In particular, introduce a mutex for each outgoing pipe (rather than a global one), which thus becomes the only one we need to acquire inside callbacks. At this point, the old `mutex_` is only guarding the vector of ClientPipes, thus we can rename it and release it earlier.\n\nI also fixed the agent not acquiring any mutex when it set a message to error after a failed write (and also not removing the message from the timeout map).\nghstack-source-id: 122410367\n\nTest Plan: Ran CI in #52677 together with the TensorPipe submodule update.\n\nReviewed By: mrshenli\n\nDifferential Revision: D26636345\n\nfbshipit-source-id: d36da989f2aab51f4acb92d2e81bb15b76088df1", "pr_number": "52749", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a6b7da7dfe": {"title": "Add 64bit indexing support for softmax (#52713)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/52715 https://github.com/pytorch/pytorch/issues/52716\n\nsplit across batch dimension\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52713\n\nReviewed By: ailzhang\n\nDifferential Revision: D26640033\n\nPulled By: ngimel\n\nfbshipit-source-id: f169cb0d6abc1cfbddf658d9775759a7d56f5c12", "pr_number": "52713", "files_changed": ["aten/src/ATen/native/cuda/SoftMax.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed", "module: cuda", "open source"]}, "a649d808e6": {"title": "Added fast path in the case of no hooks (#52576)", "body": "Summary:\nSee the discussion here: https://github.com/pytorch/pytorch/pull/50431\n\n~~Not completely done yet - need to figure out the backwards compatibility stuff as well as `RemovableHandle`.~~\n\n~~Also, this concretely breaks Torchscript (which tries to script the properties), and more generally, probably requires modifying Torchscript hook support: https://github.com/pytorch/pytorch/issues/34329~~\n\nJust kidding, I think all problems are solved :)\n\nAnother thing I could do in this PR is to simply replace all the `len(x) > 0` checks with the faster checks. That's about 1.5-2k more Python instructions and .4 - .5 microseconds slower.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52576\n\nReviewed By: ailzhang\n\nDifferential Revision: D26650352\n\nPulled By: Chillee\n\nfbshipit-source-id: 0fd73e916354b9e306701a8a396c5dc051e69f0d", "pr_number": "52576", "files_changed": ["torch/nn/modules/module.py"], "labels": ["Merged", "cla signed"]}, "2bdf6305a0": {"title": "Drop unused variables (#52643)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52643\n\nTest Plan: Sandcastle\n\nReviewed By: borovsky-d\n\nDifferential Revision: D26588961\n\nfbshipit-source-id: 5e00ec05d006ccad8ff8cd98916a0265e592f9fd", "pr_number": "52643", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseMatMul.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "30cb6ac53c": {"title": "Introduce `mlc` device (ML Compute device) to PyTorch's device list (#50634)", "body": "Summary:\nApple recently announced ML Compute, a new framework available in macOS Big Sur, which enables users to accelerate the training of neural networks on Mac hardware. This PR is the first on a series of PRs that will enable the integration with ML Compute. Most of the integration code will live on a separate subrepo named `mlc`.\nThe integration with `mlc` (ML Compute) will be very similar to that of xla. We rely on registering our ops through:\n\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n m.impl_UNBOXED(<op_schema_name>, &customized_op_kernel)\n ...\n}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50634\n\nReviewed By: malfet\n\nDifferential Revision: D26614213\n\nPulled By: smessmer\n\nfbshipit-source-id: 3b492b346c61cc3950ac880ac01a82fbdddbc07b", "pr_number": "50634", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", "aten/src/ATen/Context.h", "aten/src/ATen/Version.cpp", "aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.cpp", "c10/core/Backend.h", "c10/core/Device.cpp", "c10/core/DeviceType.cpp", "c10/core/DeviceType.h", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/TensorImpl.h", "c10/core/TensorOptions.h", "caffe2/proto/caffe2.proto", "torch/_utils.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/library.h", "torch/overrides.py", "torch/tensor.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "0048d97eda": {"title": "remove index_fill side-effect for scalar tensors (#52209)", "body": "Summary:\n`index_fill` silently promotes zero dim Tensors to 1-dim Tensors. This PR fixes that.\nWas:\n```\nIn [1]: import torch\n\nIn [2]: x = torch.tensor(1)\n\nIn [3]: idx = torch.tensor(0).long()\n\nIn [4]: x.dim()\nOut[4]: 0\n\nIn [5]: x.index_fill(0, idx, -1).dim()\nOut[5]: 1\n\n```\nNow:\n```\nIn [6]: x.index_fill(0, idx, -1).dim()\nOut[6]: 0\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52209\n\nReviewed By: ejguan\n\nDifferential Revision: D26446470\n\nPulled By: ngimel\n\nfbshipit-source-id: 4737e6941a7216b57f3416b59362817834df3a3a", "pr_number": "52209", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "test/test_torch.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "f71d9e28f9": {"title": "Store test filename in test report path (#52791)", "body": "Summary:\nThis way, we can have a mapping from the test files we directly execute (the tests [here](https://github.com/pytorch/pytorch/blob/master/test/run_test.py#L20)) to the test suites that we store data for in XML reports.\n\nThis will come in use later for categorizing the tests we run in CI.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52791\n\nReviewed By: samestep\n\nDifferential Revision: D26655086\n\nPulled By: janeyx99\n\nfbshipit-source-id: 94be32f80d7bc0ea1a7a11d4c4b1d3d8e774c5ea", "pr_number": "52791", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "1ac59d9db3": {"title": "Fix RPC get_worker_info for rank=0 (#52804)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52804\n\n`rpc.get_worker_info` used to only take string in v1.6. We recently\nallow it to accept `int` and `WorkerInfo`, but the previous check\non `worker_name` is no longer correct. This commit adds explicit\n`not None` check.\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26655089\n\nPulled By: mrshenli\n\nfbshipit-source-id: fa1545bd6dd2b33bc1e919de46b94e799ab9719c", "pr_number": "52804", "files_changed": ["torch/distributed/rpc/api.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "914126901e": {"title": "Fix typos in tools/test_history.py helpstring (#52840)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52840\n\nTest Plan:\n```\n$ tools/test_history.py --help\n```\n\nReviewed By: janeyx99\n\nDifferential Revision: D26665121\n\nPulled By: samestep\n\nfbshipit-source-id: 3607a4a598f1b1639ac1752b4e377491bff7188f", "pr_number": "52840", "files_changed": ["tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "fa7575ea05": {"title": "Update backwards compatibility check to ignore reverted op (#52841)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52841\n\nghstack-source-id: 122515522\n\nTest Plan: CircleCI\n\nReviewed By: malfet\n\nDifferential Revision: D26665136\n\nfbshipit-source-id: f2aafa8e05f39e284f66f88685d9ce675bebe1cf", "pr_number": "52841", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["Merged", "cla signed"]}, "99a428ab22": {"title": "Lower ReLu6 to aten (#52723)", "body": "Summary:\n-Lower Relu6 to ATen\n-Change Python and C++ to reflect change\n-adds an entry in native_functions.yaml for that new function\n-this is needed as we would like to intercept ReLU6 at a higher level with an XLA-approach codegen.\n-Should pass functional C++ tests pass. But please let me know if more tests are required.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52723\n\nReviewed By: ailzhang\n\nDifferential Revision: D26641414\n\nPulled By: albanD\n\nfbshipit-source-id: dacfc70a236c4313f95901524f5f021503f6a60f", "pr_number": "52723", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/csrc/api/include/torch/nn/functional/activation.h", "torch/nn/functional.py"], "labels": ["Merged", "cla signed", "open source"]}, "13121598ef": {"title": "[Pytorch, sparsity] Bug fix to update requantization and zp parameters of input (#52797)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52797\n\nAlso sneaking in change to check for realloc failure for packed activation buffer\n\nFB:\nIn dynamic quantization input's quantization scale and zero point can be\ndifferent on every iterations. Thus requantization scale needs to be\nrecomputed.\n\nEarlier bug that calculated those only at op creation time results in wrong\nresults on subsequent runs.\n\nThis diff fixes that.\n\nTest Plan:\nFB:\nbuck test caffe2/torch/fb/model_optimization:sparsity_test\n\nReviewed By: z-a-f, jiatongzhou\n\nDifferential Revision: D26651968\n\nfbshipit-source-id: e5b9acef03fc45f31c43d88a175f3a64f7dbf4bd", "pr_number": "52797", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/operator-run.c"], "labels": ["Merged", "cla signed", "fb-exported"]}, "64b4e37c26": {"title": "ns for fx: allow graph matching of parents of cat (#52368)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52368\n\nBefore this PR, the graph matching logic only handles node arguments of\ntype Node. This PR extends it to allow to handle node arguments of type\nTuple, so that the matcher can properly navigate through the arguments\nof `cat`.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher.test_nodes_before_cat\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26490101\n\nfbshipit-source-id: 2de8d6acc30f237e22bfc3cfa89728b37411aab6", "pr_number": "52368", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "4483c48eb1": {"title": "ns for fx: support linear_relu for weight matching (#52395)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52395\n\nSimple change to add logic to get the weight of a quantized\n`linear_relu` node.\n\nMore flavors of conv and linear will be added in future PRs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_compare_weights_fun\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26497992\n\nfbshipit-source-id: e6d88e92eedd6cdbf9116cbcfc8f6164f8499246", "pr_number": "52395", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "608f44b24b": {"title": "ns for fx: update graph matching to not match nodes with equal types (#52402)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52402\n\nBefore this PR, any pair of subgraphs with base nodes of equal\ntypes matched.\n\nWhile sometimes this is useful, this should be off by default to\nproperly handle user defined modules and functions, for which we do not\nknow how to extract weights or cast to the right input type.\n\nIn a future PR, we can add hooks to turn on matching for nodes\nof equal types, for the situations where it makes sense.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher.test_nodes_with_equal_types_do_not_get_matched\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26499848\n\nfbshipit-source-id: 5818b88eb7fd8ed36390f60aa1a18228bb50507e", "pr_number": "52402", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "1618dc2ac6": {"title": "ns for fx: update graph matching to handle dicts and tuples in node args (#52681)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52681\n\nUpdates the NS graph matching to properly traverse through args of nodes\nif args are lists or tuples.  As a side benefit, refactors the code to\nmake future similar improvements easier.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26611221\n\nfbshipit-source-id: 4ddd9b26338a5a2763b2883967e100f73e207538", "pr_number": "52681", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "316eabe9ba": {"title": "fix(docs): remove redundant hardsigmoid() in docstring to show up `inplace` parameter (#52559)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50016\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52559\n\nReviewed By: ailzhang\n\nDifferential Revision: D26636347\n\nPulled By: vkuzo\n\nfbshipit-source-id: da615d0eb6372637a6441e53698e86252591f6d8", "pr_number": "52559", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "39fa0b5d0a": {"title": "Add scatter_add to amp promote list (#52133)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51730\n\nI've added the `scatter_add` and `scatter_add.dimname` to the promote list as well as test cases for the former op.\nHowever, it seems that `scatter_add` [doesn't support named tensors yet](https://github.com/pytorch/pytorch/blob/8b0cb5ede3eddb96aa0423b2c73c8560ab44788e/aten/src/ATen/native/NamedTensor.cpp#L356-L358) (thanks t-vi for the pointer):\n```python\ndev = 'cuda'\ntorch.scatter_add(torch.zeros(2, 2, 2, dtype=torch.float16, device=dev, names=('N', 'C', 'L')),\n                             'C',\n                             torch.randint(0, 2, (2, 2, 2), device=dev),\n                             torch.randn((2, 2, 2), dtype=torch.float32, device=dev))\n> RuntimeError: scatter_add: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this.\n```\nwhich raised this error after adding this test case.\n\nI'm thus unsure, if I should also remove `scatter_add.dimname` from the promote list or not.\n\nIn any case, once named tensors are supported a potential test could be added as:\n```python\n            (\"scatter_add\", (torch.zeros(2, 2, 2, dtype=torch.float16, device=dev, names=('N', 'C', 'L')),\n                             'C',\n                             torch.randint(0, 2, (2, 2, 2), device=dev),\n                             torch.randn((2, 2, 2), dtype=torch.float32, device=dev))),\n```\n\nCC mcarilli ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52133\n\nReviewed By: ejguan\n\nDifferential Revision: D26440392\n\nPulled By: ngimel\n\nfbshipit-source-id: f4ee2d0b9e1f81afb6f94261c497cf2bf79ec115", "pr_number": "52133", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "torch/testing/_internal/autocast_test_lists.py"], "labels": ["Merged", "cla signed", "module: amp (automated mixed precision)", "open source"]}, "3ff6c9174a": {"title": "Update TensorPipe submodule (#52677)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52677\n\nTest Plan: CircleCI\n\nReviewed By: beauby\n\nDifferential Revision: D26609075\n\nfbshipit-source-id: 7dc2f8a1e6b9d8fe1ff49398379888237c115f2b", "pr_number": "52677", "files_changed": ["third_party/tensorpipe", "third_party/tensorpipe.BUILD"], "labels": ["Merged", "cla signed", "fb-exported"]}, "163a91bed3": {"title": "Fix TensorPipe agent trying to double-set error (#52837)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52837\n\nAfter https://github.com/pytorch/pytorch/pull/52749 we started seeing an increased flakiness of the TensorPipeDistAutogradTestWithSpawn.test_backward_node_failure_python_udf test, with failures like this one:\n\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/277824/workflows/cfcbef5a-544e-43bd-b3b0-ebc7b95134fe/jobs/11145394\n\nhttps://gist.github.com/lw/a0b48900673b5ae0f5d03aca1e72ffff\n\nThe logs are very clear and point to the changes in the error handling code upon a write error. Namely, the bug is triggered when a incoming read fails while there is an outgoing write, in which case the read callback (invoked first) will flush all pending futures, which then causes the write callback (invoked after) to not find the future it's looking for.\n\nIn a sense this bug wasn't introduced by https://github.com/pytorch/pytorch/pull/52749, however that PR introduced a check for whether the outgoing message was found, whereas before we would silence such a condition.\n\nA fix for this could be to just resume silencing the error. However, I'm trying to go a bit further: when an outgoing write fails, we know that all subsequent callbacks will fail too, and thus all pending operations should be flushed. Hence we can do so, instead of just trying to flush a single given operation. This allows us to merge the error-handling code of both the read and write paths.\nghstack-source-id: 122509550\n\nTest Plan: Will export to GitHub, run on CircleCI, and manually SSH into a machine and stress-run that test that was flaky.\n\nReviewed By: mrshenli\n\nDifferential Revision: D26663448\n\nfbshipit-source-id: fbff0f6aff0d98994c08018a27c47c97149b920c", "pr_number": "52837", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "f974cf4688": {"title": "Test for distributed RL with RPC (#52393)", "body": "Summary:\nAddresses one item in https://github.com/pytorch/pytorch/issues/46321\n\n## Background\nThis is a test version of the RL RPC example defined [here](https://github.com/pytorch/examples/blob/master/distributed/rpc/rl/main.py) and [here](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html), with the following differences:\n* It defines and uses a `DummyEnv` to avoid a dependency on `gym`. The `DummyEnv` simply returns random states & rewards for a small number of iterations.\n* It removes the `ArgumentParser` and utilizes `RpcAgentTestFixture` + hard-coded constants for configuration and launching.\n* It changes the worker names to match what the internal Thrift RPC tests expect.\n\nThe code is purposefully kept very similar to the original example code outside of these differences.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52393\n\nTest Plan:\n```\npytest test/distributed/rpc/test_tensorpipe_agent.py -k test_rl_rpc -vs\npytest test/distributed/rpc/test_process_group_agent.py -k test_rl_rpc -vs\n```\n\nReviewed By: glaringlee\n\nDifferential Revision: D26515435\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 548548c4671fe353d83c04108580d807108ca76e", "pr_number": "52393", "files_changed": ["torch/testing/_internal/distributed/rpc/examples/reinforcement_learning_rpc_test.py", "torch/testing/_internal/distributed/rpc_utils.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a11b601100": {"title": "Expose Store's timeout and TCPStore's host and port in Python API (#52784)", "body": "Summary:\nThis PR introduces the `timeout` accessor to `Store` and `host`, `port` accessors to `TCPStore` to help testing and troubleshooting higher level APIs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52784\n\nReviewed By: anjali411\n\nDifferential Revision: D26648202\n\nPulled By: cbalioglu\n\nfbshipit-source-id: 9cf23bf998ed330d648dfec2a93e1bbb50817292", "pr_number": "52784", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/Store.cpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7972036bbb": {"title": "Adding functional way of stacking DataPipes (#52507)", "body": "Summary:\nAllows to use functional API to stack datapipes:\n```python\nnumbers_dp = NumbersDataset(size=10).filter(filter_fn = lambda x: x % 2 == 1).map(fn = lambda x: x * 10)\n```\n\nDataPipes have to be decorated with:\n```python\nfunctional_datapipe('map')\nclass MapIterDataPipe(IterDataPipe[T_co]):\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52507\n\nReviewed By: ailzhang\n\nDifferential Revision: D26644079\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: dcf464637b4fcf9ea1eb8e84c2a0cd4dfd58b43d", "pr_number": "52507", "files_changed": ["torch/utils/data/__init__.py", "torch/utils/data/datapipes/iter/callable.py", "torch/utils/data/datapipes/iter/selecting.py", "torch/utils/data/dataset.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "0e86f14ec0": {"title": "Upgrade onednn to v.1.8.1 (#51184)", "body": "Summary:\nThis PR is upgrade onednn to v1.8.1 to bug fixed.\n\n- https://github.com/pytorch/pytorch/issues/50042 is fixed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51184\n\nReviewed By: ailzhang\n\nDifferential Revision: D26645894\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 5fb3e5f673c819bccc158672e4b648e570bda3a0", "pr_number": "51184", "files_changed": ["third_party/ideep"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c871abecf5": {"title": "Added torch.no_grad() to update_bn (#52654)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52055\n\nThis fixes the **out of memory error** while using update_bn in **SWA**, by not allocating memory for backpropagation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52654\n\nReviewed By: malfet\n\nDifferential Revision: D26620077\n\nPulled By: albanD\n\nfbshipit-source-id: 890b5a78ba9c1a148f3ab7c63472a73d8f6412a4", "pr_number": "52654", "files_changed": ["torch/optim/swa_utils.py"], "labels": ["Merged", "cla signed", "open source"]}, "19a8ada8d5": {"title": "quant: fix conv transpose with qconfig == None (#52844)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52844\n\nFixes a crash in qconfig checking which happened if a model had conv transpose\nwith qconfig set to None.\n\nTest Plan:\n```\npython test/test_quantization.py TestPostTrainingStatic.test_convtranspose_per_channel_qconfig_none\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26666043\n\nfbshipit-source-id: e1b62840b4e3c67acbb4dbdcd32514b374efce1e", "pr_number": "52844", "files_changed": ["test/quantization/test_quantize.py", "torch/quantization/qconfig.py"], "labels": ["Merged", "cla signed"]}, "e94940b169": {"title": "Use touch() in pathlib for better compatibility on Windows (#52729)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/52477 introduced the usage of `touch`, which is not available on plain Windows environment, unless you made all the things come with Git Bash available. This PR fixes the build break on those systems by using the `touch` provided by Python pathlib.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52729\n\nReviewed By: anjali411\n\nDifferential Revision: D26666724\n\nPulled By: walterddr\n\nfbshipit-source-id: aae357eb55c6787631eadf22bee7901ad3c2604e", "pr_number": "52729", "files_changed": ["torch/CMakeLists.txt"], "labels": ["Merged", "cla signed", "merge-this-please", "open source"]}, "da732c76c4": {"title": "Revert D26644079: [pytorch][PR] Adding functional way of stacking DataPipes", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26644079 (https://github.com/pytorch/pytorch/commit/7972036bbb4b157721f9af800b10b50c8cd31bf3)\n\nOriginal commit changeset: dcf464637b4f\n\nfbshipit-source-id: a12a06d7e7fb3821a0990bbc6305d02721ead82c", "pr_number": null, "files_changed": ["torch/utils/data/__init__.py", "torch/utils/data/datapipes/iter/callable.py", "torch/utils/data/datapipes/iter/selecting.py", "torch/utils/data/dataset.py"], "labels": []}, "0b93974075": {"title": "Fix incorrect runtime error in mul_() when the tensor layout is Mkldnn (#51758)", "body": "Summary:\nCalling Mkl-layout's mul_ from C++ API raises a RuntimeError.\nError message is bellow:\n```\nterminate called after throwing an instance of 'c10::Error'\n  what():  unsupported tensor layout: Mkldnn\n```\n\nEnvironment\n\u30fbCPU : Intel(R) Core(TM) i7-8086K CPU @ 4.00GHz\n\u30fbOS : 18.04.1 LTS\n\u30fbcompiler : gcc 7.5.0\n\u30fbbranch : master\n\u30fbcommit ID: 16cfe97\n\u30fbbuild Environment variable: USE_CUDA=0, USE_DISTRIBUTED=0, USE_MKLDNN=1\n\u30fbPython: 3.6.9\n\nCMakeLists.txt\n```\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(mkldnn_test)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(mkldnn_test mkldnn_test.cpp)\ntarget_link_libraries(mkldnn_test \"${TORCH_LIBRARIES}\")\nset_property(TARGET mkldnn_test PROPERTY CXX_STANDARD 14)\n```\n\nmkldnn_test.cpp\n```\n#include <torch/torch.h>\n\nint main() {\n  torch::Tensor a = torch::randn({2, 2});\n  torch::Tensor a_mkl = a.to_mkldnn();\n  a.mul_(0.5)\n  a_mkl.mul_(0.5);\n  std::cout << a << std::endl;\n  std::cout << a_mkl.to_dense() << std::endl;\n  return 0;\n}\n```\n\nExpected Result\n```\n$ ./mkldnn_test\n 0.1344  0.8107\n-0.8157 -0.2610\n[ CPUFloatType{2,2} ]\n 0.1344  0.8107\n-0.8157 -0.2610\n[ CPUFloatType{2,2} ]\n```\n\nExecution Result\n```\n$ ./mkldnn_test\nterminate called after throwing an instance of 'c10::Error'\n  what():  unsupported tensor layout: Mkldnn\nException raised from validate at /home/gtka7311/pytorch_v180/c_api_test/pytorch/aten/src/ATen/TensorIterator.h:128 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f8a1472690b in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libc10.so)\nframe https://github.com/pytorch/pytorch/issues/1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xce (0x7f8a1472316e in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libc10.so)\nframe https://github.com/pytorch/pytorch/issues/2: <unknown function> + 0x965bc3 (0x7f8a0d07dbc3 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/3: at::TensorIteratorBase::populate_operands(at::TensorIteratorConfig&) + 0xf1 (0x7f8a0d079ee1 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/4: at::TensorIteratorBase::build(at::TensorIteratorConfig&) + 0x3b (0x7f8a0d07ad3b in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/5: at::TensorIteratorBase::build_binary_op(at::Tensor const&, at::Tensor const&, at::Tensor const&) + 0x129 (0x7f8a0d07b339 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/6: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&) + 0x38 (0x7f8a0d07b418 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/7: at::native::mul_out(at::Tensor&, at::Tensor const&, at::Tensor const&) + 0x33 (0x7f8a0d217793 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/8: at::native::mul_(at::Tensor&, c10::Scalar) + 0x45 (0x7f8a0d217865 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/9: <unknown function> + 0x1435c21 (0x7f8a0db4dc21 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/10: at::Tensor& c10::Dispatcher::call<at::Tensor&, at::Tensor&, c10::Scalar>(c10::TypedOperatorHandle<at::Tensor& (at::Tensor&, c10::Scalar)> const&, at::Tensor&, c10::Scalar) const + 0x15c (0x7f8a0d9e482c in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/11: <unknown function> + 0x2a86269 (0x7f8a0f19e269 in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/12: at::Tensor& c10::Dispatcher::call<at::Tensor&, at::Tensor&, c10::Scalar>(c10::TypedOperatorHandle<at::Tensor& (at::Tensor&, c10::Scalar)> const&, at::Tensor&, c10::Scalar) const + 0x15c (0x7f8a0d9e482c in /home/gtka7311/pytorch_v180/c_api_test/pytorch/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/13: main + 0xfd (0x5653221cd282 in ./mkldnn_test)\nframe https://github.com/pytorch/pytorch/issues/14: __libc_start_main + 0xe7 (0x7f8a0bba5b97 in /lib/x86_64-linux-gnu/libc.so.6)\nframe https://github.com/pytorch/pytorch/issues/15: _start + 0x2a (0x5653221ccf2a in ./mkldnn_test)\n```\n\nModification policy for the code\nGenerally ``mul_`` is processed by ``TensorIterator`` of ``mul_out``.\nHowever, ``TensorIterator`` does not support ``Mkl-Layout tensor``.\nTherefore, to solve this problem, modified ``aten/src/ATen/native/BinaryOps.cpp`` so that ``mkldnn_mul_out`` would be executed if ``Mkl-Layout tensor`` is inputed in ``mul_out``.\nThe modifications of the code are as follows:\n```\n diff --git a/aten/src/ATen/native/BinaryOps.cpp b/aten/src/ATen/native/BinaryOps.cpp\nindex ee55114285..5c403546f2 100644\n --- a/aten/src/ATen/native/BinaryOps.cpp\n+++ b/aten/src/ATen/native/BinaryOps.cpp\n@@ -270,6 +270,9 @@ Tensor& floor_divide_(Tensor& self, const Tensor& other) {\n }\n\n Tensor& mul_out(Tensor& result, const Tensor& self, const Tensor& other) {\n+  if (self.is_mkldnn()) {\n+    return native::mkldnn_mul_out(result, self, other);\n+  }\n   auto iter = TensorIterator::binary_op(result, self, other);\n   mul_stub(iter.device_type(), iter);\n   return result;\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51758\n\nReviewed By: pbelevich\n\nDifferential Revision: D26655442\n\nPulled By: bdhirsh\n\nfbshipit-source-id: fcc5e74734cae91f725fab525f181b3066eafa28", "pr_number": "51758", "files_changed": ["aten/src/ATen/native/BinaryOps.cpp"], "labels": ["Merged", "cla signed", "module: dispatch", "module: internals", "module: mkldnn", "open source", "triaged"]}, "8ba7c4918a": {"title": "[nnc] Test for direct usage of ramp/broadcast", "body": "Summary:\nI was attempting to experiment with \"manual\" vectorization, and boy\nwas it hard.  I finally came up with this, which I want to write down as a test\ncase.  Eventually the APIs should make this easier...\n\nTest Plan: buck test\n\nReviewed By: navahgar\n\nDifferential Revision: D26631189\n\nfbshipit-source-id: c28794b25d7852890ea843fdbcaf8751648258c0", "pr_number": null, "files_changed": ["test/cpp/tensorexpr/test_llvm.cpp"], "labels": []}, "94da8b9816": {"title": "Fix resource leak bug in TCPStore constructor (#52860)", "body": "Summary:\nThis PR fixes a resource leakage bug in the constructor of `TCPStore` where an exception thrown in `TCPStoreDaemon` or `tcputil::connect()` can leave the server socket dangling. The ideal long-term solution would be to have a RAII wrapper for TCP sockets returned by `tcputil`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52860\n\nReviewed By: osalpekar\n\nDifferential Revision: D26671775\n\nPulled By: cbalioglu\n\nfbshipit-source-id: ccebbd7533ac601a4b80e6e759f2fb4fe01c70fa", "pr_number": "52860", "files_changed": ["torch/lib/c10d/TCPStore.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a52001f923": {"title": "Improve test_reference_numerics (#51604)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50749\nci-all version of https://github.com/pytorch/pytorch/pull/50550\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51604\n\nReviewed By: anjali411\n\nDifferential Revision: D26666951\n\nPulled By: mruberry\n\nfbshipit-source-id: b87db68f1d2a0f6c151edbc5c7809bbceece69b0", "pr_number": "51604", "files_changed": ["test/test_unary_ufuncs.py", "torch/testing/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "0f3a3f22af": {"title": "Add sample validation for LKJCholesky.log_prob (#52763)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52724.\n\nThis fixes the following for the LKJCholesky distribution in master:\n - `log_prob` does sample validation when `validate_args=True`.\n - exposes documentation for the LKJCholesky distribution.\n\ncc. fehiepsi, fritzo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52763\n\nReviewed By: anjali411\n\nDifferential Revision: D26657216\n\nPulled By: neerajprad\n\nfbshipit-source-id: 12e8f8384cf0c3df8a29564c1e1718d2d6a5833f", "pr_number": "52763", "files_changed": ["docs/source/distributions.rst", "test/distributions/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/lkj_cholesky.py"], "labels": ["Merged", "cla signed", "module: distributions"]}, "649760e5f1": {"title": "`maybe_resize_storage_cuda` new_size argument should be unsigned (#52672)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52672\n\nThis allows correct handling on a very large tensor allocations\n\nAlso, replace AT_ERROR with TORCH_CHECK(false)\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D26607547\n\nPulled By: malfet\n\nfbshipit-source-id: 247f7e8c59f76af3b95799afc9bc4ab4cc228739", "pr_number": "52672", "files_changed": ["aten/src/ATen/native/cuda/Resize.cuh"], "labels": ["Merged", "cla signed"]}, "569d4fe3f9": {"title": ".github: Add workflow to build conda packages (#51243)", "body": "Summary:\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51243\n\nReviewed By: walterddr\n\nDifferential Revision: D26669795\n\nPulled By: seemethere\n\nfbshipit-source-id: 1e54aa8cab2b0b5324815fa4f1706e468f9f57dd", "pr_number": "51243", "files_changed": [".github/scripts/generate_binary_build_matrix.py", ".github/workflows/build_linux_binaries.yml", ".github/workflows/build_linux_conda.yml", ".github/workflows/build_linux_wheels.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "51d8543ac7": {"title": "[FX] Use precompiled regex in graph name processing (#52853)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52853\n\nghstack-source-id: 122531132\n\nTest Plan: waitforsadcastle\n\nReviewed By: anjali411\n\nDifferential Revision: D26668527\n\nfbshipit-source-id: bd34d860cd3a71d3b29f2430df97a0501d542f5b", "pr_number": "52853", "files_changed": ["torch/fx/graph.py"], "labels": ["Merged", "cla signed", "fx"]}, "a27aaa49aa": {"title": "quant norm layers: move scale + zp to buffers (#52861)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52861\n\nCurrently, scale and zp in these layers are not buffers, which\nmeans they do not get saved to the state dict.  Movin them\ninto buffers to allow people to properly use state_dict.\n\nNote: this is a redo of https://github.com/pytorch/pytorch/pull/45313,\nwith BN taken out. Not doing this for BN because it has dependencies on existing\nbehavior.  We should clean it up eventually.\n\nNote: not handling BC because it's 100% broken now, so there is\nno practical value in handling BC.\n\nTest Plan:\n```\npython test/test_quantization.py TestPostTrainingStatic.test_normalization\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D26671761\n\nfbshipit-source-id: 7615b1dd0d1ae88eeff8b1d150f3846815dc2bc9", "pr_number": "52861", "files_changed": ["torch/nn/quantized/modules/normalization.py"], "labels": ["Merged", "cla signed"]}, "f40c9db622": {"title": "[FX][EZ] Hoist custom class .so loading into setUp (#52883)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52883\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D26675802\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7a7bcb1d0a6f8c9b1431bc3e09143ada6e5fbf4d", "pr_number": "52883", "files_changed": ["test/test_fx.py"], "labels": ["Merged", "cla signed"]}, "9a03e65456": {"title": "Adding functional way of stacking DataPipes with fixed mypy (#52885)", "body": "Summary:\nReadding reverted PR with MyPY fixed\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52885\n\nReviewed By: ejguan\n\nDifferential Revision: D26676405\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 020216c5522d21a4994cd896ae778c0b77f6444b", "pr_number": "52885", "files_changed": ["torch/utils/data/__init__.py", "torch/utils/data/datapipes/iter/callable.py", "torch/utils/data/datapipes/iter/selecting.py", "torch/utils/data/dataset.py"], "labels": ["Merged", "cla signed"]}, "a0a1bb074b": {"title": "Make NumPy dependency dynamic (#52794)", "body": "Summary:\nMove NumPy initialization from `initModule()` to singleton inside\n`torch::utils::is_numpy_available()` function.\nThis singleton will print a warning, that NumPy integration is not\navailable, rather than fails to import torch altogether.\nThe warning be printed only once, and will look something like the\nfollowing:\n```\nUserWarning: Failed to initialize NumPy: No module named 'numpy.core' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:66.)\n```\n\nThis is helpful if PyTorch was compiled with wrong NumPy version, of\nNumPy is not commonly available on the platform (which is often the case\non AARCH64 or Apple M1)\n\nTest that PyTorch is usable after numpy is uninstalled at the end of\n`_test1` CI config.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52794\n\nReviewed By: seemethere\n\nDifferential Revision: D26650509\n\nPulled By: malfet\n\nfbshipit-source-id: a2d98769ef873862c3704be4afda075d76d3ad06", "pr_number": "52794", "files_changed": [".jenkins/pytorch/fake_numpy/numpy.py", ".jenkins/pytorch/test.sh", "setup.py", "torch/csrc/Module.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_numpy.h"], "labels": ["Merged", "cla signed"]}, "fdd25f82c9": {"title": "Update to replace AT_ERROR with TORCH_CHECK (#52711)", "body": "Summary:\nFixes #{52699}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52711\n\nReviewed By: ailzhang\n\nDifferential Revision: D26654677\n\nPulled By: malfet\n\nfbshipit-source-id: 97079250d144c9b1c69028f35e4a23a34481b2a5", "pr_number": "52711", "files_changed": ["c10/core/Backend.h", "c10/core/Device.cpp", "c10/core/DeviceType.cpp", "c10/core/Layout.h", "c10/core/MemoryFormat.h", "c10/core/ScalarType.h", "c10/core/Storage.h", "c10/core/TensorOptions.h", "c10/core/UndefinedTensorImpl.cpp", "c10/cuda/CUDACachingAllocator.cpp", "c10/util/tempfile.h"], "labels": ["Merged", "cla signed", "open source"]}, "3969391c07": {"title": "[caffe2] move the SaveOp implementation from a header to a .cc file", "body": "Summary:\nMove the `SaveOp` code from `load_save_op.h` to `load_save_op.cc`.\n\nPreviously this implementation was all in the templatized `SaveOp` class, even\nthough most of the logic didn't depend on the template parameters.  Having\nthis code be in the header file slows down the build, and forces more files to\nbe rebuilt than necessary when changing the SaveOp code.  Having this code be\nin a template class can also increase the generated code size be larger than\nneeded, as we don't need separate copies instantiated for each context type.\n\nTest Plan: buck test //caffe2/caffe2/python/operator_test:load_save_test\n\nReviewed By: mraway\n\nDifferential Revision: D26641600\n\nfbshipit-source-id: 84ebe8164ffac1e4a691be41147f0c5d8e890e09", "pr_number": null, "files_changed": ["caffe2/operators/load_save_op.cc", "caffe2/operators/load_save_op.h"], "labels": []}, "b4a8d98247": {"title": "[caffe2] use AddNAlreadyReserved() when serializing blobs", "body": "Summary:\nOptimize the blob serialization code by using `AddNAlreadyReserved()` when\nserializing tensor data, rather than making N separate `Add()` calls.\n`AddNAlreadyReserved()` is a simple addition operation, while each `Add()`\ncall checks to see if it needs to reserve new space, and then updates the\nelement data, which is unnecessary in this case.\n\nTest Plan:\nThis appears to improve raw serialization performance by 30 to 35% for float,\ndouble, and int64_t types which use this function.  This improvement appears\nrelatively consistent across large and small tensor sizes.\n\nDifferential Revision: D26617038\n\nfbshipit-source-id: 97dedbae889d35463628f3016ac56986e685289e", "pr_number": null, "files_changed": ["caffe2/core/blob_serialization.h"], "labels": []}, "cd9ac54ea7": {"title": "[caffe2] update load_save_test.py to also verify the chunking behavior", "body": "Summary:\nAdd some small utility functions to read the blob names back from the minidb\nfile so that we can verify how many chunks were written for each blob.\n\nTest Plan: buck test caffe2/caffe2/python/operator_test:load_save_test\n\nReviewed By: mraway\n\nDifferential Revision: D26641599\n\nfbshipit-source-id: bccb0af157d85e585e95bc7be61c4584fba3cb04", "pr_number": null, "files_changed": ["caffe2/python/operator_test/load_save_test.py"], "labels": []}, "7094d970d1": {"title": "ns for fx: decouple subgraph names from node names (#52771)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52771\n\nBefore this PR, subgraph names were derived from node names\nin model B.  For example, if we had\n\n```\nA: linear0 -> relu0 -> ...\nB: linear_relu0 -> ...\n```\n\nThen the subgraph name would be `linear_relu0`, and the outputs before this\nPR would look like\n\n```\n{\n  'linear_relu0': {\n    'model_a': ...,\n    'model_b': ...,\n  },\n}\n```\n\nThis PR decouples subgraph naming from node names.\nThe outputs after this PR look like:\n\n```\n{\n  # guaranteed to match the right subgraphs across different models\n  # without needing more than one model during the prepare passes\n  'base_op_torch.nn.functional.linear_0': {\n    'model_a': ...,\n    'model_b': ...,\n  },\n}\n```\n\nThere are future requirements for which using node_name as subgraph name does not work well:\na. the need to support N models, without having all of them in memory at the same time\nb. the need to support fusions and match subgraphs with related but non-equal types\n\nThis PR changes the naming of subgraphs to be based on two things:\n1. the name of the underlying set of related ops (i.e. `torch.nn.functional.linear`)\n2. the order in which this subgraph was named (i.e. `foo_0`, `foo_1`, ...)\n\nBasically, we can't use a node name because of (a), since there must be\na reference model which node name other models must use, but that\nreference model is not guaranteed to be available.  Note: we could add\nsome state and require the reference model to go through the APIs first,\nsaving the reference node names, but I'm deliberately not doing that\nto minimize the state used throughout.\n\nTo support (b), we need a way to determine a name of a subgraph which is\nthe same for all related subgraphs (i.e. linear-relu vs quantized_linear\nvs quantized_linear_relu). In this PR, this is done by using the base\naten op's name.  We use a string name so it looks nice in the output\n(I tried `str(underlying_type)`, and it is not easy for humans to read).\n\nNote: after this PR, it's hard to parse the results to see which layer\nis related to which node in the graph. This will be fixed in a future PR\nwhere we will store the node name on the logger, and expose it in the\noutput.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\npython test/test_quantization.py TestFXGraphMatcherModels\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26652641\n\nfbshipit-source-id: ee8dacc2d6e875357c1574cbf426923f9466ea10", "pr_number": "52771", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "fe068157de": {"title": "ns for fx: unify return types of weight and activation APIs (#52779)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52779\n\n1. makes the return type of the weight comparison APIs match the return\ntype of the activation comparison APIs:\n\n```\n# before\n{layer_name: {model_name: weight_tensor}}\n{layer_name: {model_name: [activation_tensor]}}\n\n# after\n{layer_name: {model_name: [weight_tensor]}}\n{layer_name: {model_name: [activation_tensor]}}\n```\n\n2. makes a type alias for the type, so future changes are easier\n\nTest Plan:\n```\nmypy torch/quantization\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26652639\n\nfbshipit-source-id: eb1f04d6913cedf88d628f362468875ae9ced928", "pr_number": "52779", "files_changed": ["torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "d2e88246d8": {"title": "ns for fx: make return type of ns APIs future proof (#52789)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52789\n\nChanges the return type of NS APIs from\n\n```\n{\n  layer_name: {\n    model_name: [torch.Tensor(...), ...],\n  },\n}\n```\n\nto\n\n```\n{\n  layer_name: {\n    model_name: {\n      'type': 'weight',  # or node_output, etc\n      'values': [torch.Tensor(...), ...],\n      // future info can be added here, such as node name, etc\n  },\n}\n```\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26652640\n\nfbshipit-source-id: 4b31164e402754141368d5a04d595f2b643af3bb", "pr_number": "52789", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "1d3172130d": {"title": "ns for fx: add node name and type to results (#52798)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52798\n\nAdds the node name and node target type to Numerical Suite outputs.\nThis is useful to debug which node got matched to which node,\nand what is the type of the operation.\n\n```\n// before\n{\n  layer_name: {\n    model_name: {\n      'type': 'weight',\n      'values': [...],\n    },\n  },\n}\n\n// after\n{\n  layer_name: {\n    model_name: {\n      'type': 'weight',\n      'values': [...],\n      'node_name': '0',\n      'node_target_type': \"<class 'torch.nn.modules.conv.Conv2d'>\",\n    },\n  },\n}\n```\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26652637\n\nfbshipit-source-id: ba75b110cb91234f17a926ccbc5d0ccee2c3faeb", "pr_number": "52798", "files_changed": ["torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "25001a0148": {"title": "ns for fx: remove \".stats\" suffix (#52799)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52799\n\nWe agreed that it's better to not add this, removing.\nWe can make Eager mode NS match this in a future PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26652638\n\nfbshipit-source-id: 5baa51a6bf6de5632946417fe9fd3d0f3e78f7fa", "pr_number": "52799", "files_changed": ["torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "c423733967": {"title": "Add support for builtin sum (#52188)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/18627\nAdds torch.sum support for JIT\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52188\n\nTest Plan:\npython test/test_jit.py -k test_list_sum\npython test/test_jit.py -k test_torch_sum\n\nReviewed By: pbelevich, anjali411\n\nDifferential Revision: D26670022\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: eb58f0a3a64dab4b9fa1f4eb854e9854fa9bda55", "pr_number": "52188", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "69b2d5c7c3": {"title": "Revert D26641599: [caffe2] update load_save_test.py to also verify the chunking behavior", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26641599 (https://github.com/pytorch/pytorch/commit/cd9ac54ea75d4a530b6b9dccaaf1167cabafc789)\n\nOriginal commit changeset: bccb0af157d8\n\nfbshipit-source-id: 9fe35382876d19aefd16496bf8f920e12aa6f169", "pr_number": null, "files_changed": ["caffe2/python/operator_test/load_save_test.py"], "labels": []}, "21c3f6f415": {"title": "Revert D26617038: [caffe2] use AddNAlreadyReserved() when serializing blobs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26617038 (https://github.com/pytorch/pytorch/commit/b4a8d98247c4eac60ae594ae626886408dc4e2c0)\n\nOriginal commit changeset: 97dedbae889d\n\nfbshipit-source-id: 6921d0a64dee26e18f16628773953bbe7280998e", "pr_number": null, "files_changed": ["caffe2/core/blob_serialization.h"], "labels": []}, "af1fb4e4ee": {"title": "Revert D26641600: [caffe2] move the SaveOp implementation from a header to a .cc file", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26641600 (https://github.com/pytorch/pytorch/commit/3969391c0732bd1e6329438664dba3d7f5ee74b7)\n\nOriginal commit changeset: 84ebe8164ffa\n\nfbshipit-source-id: c3a85b7b15b8cdbf019abfabfd740a5b1d5e8775", "pr_number": null, "files_changed": ["caffe2/operators/load_save_op.cc", "caffe2/operators/load_save_op.h"], "labels": []}, "a3cd881890": {"title": "Fix grammar in reducer warning (#52835)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52835\n\nAddresses comment in https://github.com/pytorch/pytorch/pull/52385\nthat was missed before landing the PR\nghstack-source-id: 122543534\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26660764\n\nfbshipit-source-id: 3edfebed56f382c1414ba9eb65a753ced7e34154", "pr_number": "52835", "files_changed": ["torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7cfe140705": {"title": "Add distributed debug mode func to python (#52481)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52481\n\nAdds an API `get_debug_mode` that can be used by distributed package and users to retrieve debug mode. Currently no functionality changes, but wanted to get the bare bones function out and add relevant debug mode logging in follow up diffs.\nghstack-source-id: 122471216\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26508972\n\nfbshipit-source-id: d1153774f8697bc925a05db177d71c0566d25344", "pr_number": "52481", "files_changed": ["test/distributed/test_c10d.py", "torch/distributed/__init__.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7a178a8a52": {"title": "[Static Runtime] Add memoray alloc/dealloc time to benchmark (#52902)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52902\n\nAdd more metrics to track memory_alloc_time, memory_dealloc_time, and output_dealloc_time.\n\nReviewed By: maratsubkhankulov\n\nDifferential Revision: D26660715\n\nfbshipit-source-id: 96c6cfac2d2ec66d4c31c84129721a846c3914f0", "pr_number": "52902", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/init.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "97568d7471": {"title": "Use --delta=0 by default for tools/test_history.py (#52877)", "body": "Summary:\nThis is less surprising than the current default, `--delta=12`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52877\n\nTest Plan: Run the example commands from `tools/test_history --help` and check that their output matches that shown.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D26674258\n\nPulled By: samestep\n\nfbshipit-source-id: 1413e11519854b0a47e14af2f1d20c57f145dacd", "pr_number": "52877", "files_changed": ["tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "155b19ef1a": {"title": "[Pytorch Mobile] Remove useless line from bundled_inputs (#52824)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52824\n\nHow was this not breaking? _bundled_inputs_deflated doesnt exist\nghstack-source-id: 122491970\n\nTest Plan: unit tests\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26658098\n\nfbshipit-source-id: 9ebf961b8764ba8779052c520dd46a8724be042a", "pr_number": "52824", "files_changed": ["torch/utils/bundled_inputs.py"], "labels": ["Merged", "cla signed"]}, "44b9fcfb55": {"title": "Fix local version generation (#52898)", "body": "Summary:\nAdd \"git\" prefix to PyTorch local version, otherwise it might strip leading zeroes from git hashum according to https://www.python.org/dev/peps/pep-0440/#local-version-identifiers:\n> If a segment consists entirely of ASCII digits then that section should be considered an integer for comparison purposes\n\nFixes https://github.com/pytorch/pytorch/issues/52857\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52898\n\nReviewed By: anjali411\n\nDifferential Revision: D26681878\n\nPulled By: malfet\n\nfbshipit-source-id: 0e7baa2716fc06193cfacd7c4e6cdc6f4bbac4a9", "pr_number": "52898", "files_changed": ["tools/generate_torch_version.py"], "labels": ["Merged", "cla signed"]}, "502a85990d": {"title": "[PyTorch] Move Aten level source list to build_variable.bzl (#52792)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52792\n\nMove the aten level source code list from `pt_template_srcs.bzl` to `build_variables.bzl`, such that this source list can be shared by both OSS and internal.\nghstack-source-id: 122458909\n\nTest Plan: CI\n\nReviewed By: dhruvbird, iseeyuan\n\nDifferential Revision: D26647695\n\nfbshipit-source-id: 88469c934d4a73c261418c0c584e46104295a0c2", "pr_number": "52792", "files_changed": ["tools/build_variables.bzl"], "labels": ["Merged", "cla signed"]}, "b2520ab3dc": {"title": "Add a demo backend with compiler (#52603)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52603\n\nThis PR introduced a backend with minimum compilation capability to the to_<backend> flow. The targets are:\n\n- Demonstrate the end-to-end flow with adding a backend -> compilation -> runtime\n- How the backend compilation errors be surfaced to the user, with the original model's source code information. (C++ only in this PR. Python APIs will be demonstrated in a following PR.)\n\nChanges:\n\n- Compilation\n\n1. A backend with minimum compilation features, \"backend_with_compiler_demo\" is added.\n2. The compilation happens AOT in the ```pre_process``` function registered to this backend.\n3. Compiled results are stored in a string blob for each method. They are serialized to the lowered module with ```__get_state__``` function.\n4. Error message with model source code is thrown, for features not handled by the backend compiler.\n\n- Runtime\n\n1. The compiled blob is loaded in ```__set_state__``` method.\n2. The ```compile``` function of the backend pass through the AOT compiled blob. (TODO: parsing the blob to the format that the backend can understand can happen here.)\n3. The ```execute``` function of the backend executes the specified method (handle).\n\nTest Plan:\n- ```BackendTest.TestCompiler```: the C++ end-to-end demonstration on a supported model. After compilation and running, the lowered model produces the same result as the original torchscript model.\n- ```BackendTest.TestCompilerNotSupport```: Demonstrate the error message from the AOT compilation for a feature not supported from the input module. The error message looks like:\n\n```\n\"The node of aten::mul is not supported in this compiler. Source code:   File \"<string>\", line 3\n\n    def forward(self, x, h):\n        return x * h\n               ~~~~~ <--- HERE\n```\n\nReviewed By: raziel\n\nDifferential Revision: D26593968\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 8f264f60a0470e9f07e36fdeccbf17da6c1d7cd7", "pr_number": "52603", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_compiler_lib.cpp", "test/cpp/jit/test_backend_lib.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/test_utils.h", "torch/custom_class.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "b8e6e2971c": {"title": "Run distributed_test with NCCL_ASYNC_ERROR_HANDLING (#52619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52619\n\nRuns this test suite with nccl_async_error_handling enabled. It is the\ndefault to run many distributed training jobs, and can also help catch\nerrors/hangs in tests more easily. We don't expect any changes in the actual\nexisting tests since they shouldn't have any hangs.\n\nAlso removes a commented out line\nghstack-source-id: 122595646\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D26588108\n\nfbshipit-source-id: a57bbe2ae5a0c86731d77be45756b17151618eb6", "pr_number": "52619", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "1d6bd15790": {"title": "[JIT] Add torch._C._jit submodule (#52910)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52910\n\n**Summary**\nPR #52158 tried to move all JIT bindings from `torch._C` to a new\nsubmodule `torch._C._jit`, but that...did not go well. This pull request\nadds the new `torch._C._jit` submodule, but does not migrate the\nexisting bindings. Instead, it adds a unit test that fails if any new\nbindings are added to `torch._C`. A comment in the test instructs\ndevelopers to add their new binding to the allowlist if it really should\nbe in `torch._C`, or to add it to the appropriate submodule (e.g\n`torch._C._jit`, for example). The idea is to prevent the issue\ndescribed in #51691 from getting *worse* if it cannot be fixed.\n\n**Test Plan**\nContinuous integration.\n\n**Fixes**\nThis commit fixes #51691.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26698373\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: ec9f5426051227a513d4fd09512b624420e0100b", "pr_number": "52910", "files_changed": ["test/run_test.py", "test/test_public_bindings.py", "torch/csrc/jit/python/init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "10087337c7": {"title": "Exclude 'test' from codecoverage (#52935)", "body": "Summary:\nAlso, do not generate coverage report on patch level\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52935\n\nReviewed By: walterddr\n\nDifferential Revision: D26696285\n\nPulled By: malfet\n\nfbshipit-source-id: 87518682f883c94409778525524e7c392407efa8", "pr_number": "52935", "files_changed": ["codecov.yml"], "labels": ["Merged", "cla signed"]}, "94e23e51c4": {"title": "[caffe2] EnforceFinite: log blobs finiteness in workspace on error (#52892)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52892\n\nWhen an EnforceFinite check fails this logs all of the tensors in the workspace and whether they are finite or not.\n\nThis is a little bit hacky since it uses the aten APIs. I've `ifdef`ed the implementation so it should compile fine on xplat and mobile. It's also accessing the workspace directly but since this is a logging op it seems fine to bend the rules.\n\nTest Plan:\n$ buck test //caffe2/caffe2/python/operator_test:enforce_finite_op_test\n\n  $ buck-out/gen/caffe2/caffe2/python/operator_test/enforce_finite_op_test#binary.par\n  I0225 16:29:46.166507 311548 enforce_finite_op.h:62] blob X isfinite=false\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D26626336\n\nfbshipit-source-id: f68e219b910a7242f2e72bb4d734c3e84f46eec5", "pr_number": "52892", "files_changed": ["caffe2/operators/enforce_finite_op.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "7f1693d95e": {"title": "Fix type hints of the callable arguments for DataLoader (#52924)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52806\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52924\n\nReviewed By: malfet\n\nDifferential Revision: D26694894\n\nPulled By: ejguan\n\nfbshipit-source-id: 55734ec9684caa90f1e599b65659b7c57047f802", "pr_number": "52924", "files_changed": ["torch/utils/data/dataloader.py"], "labels": ["Merged", "cla signed", "open source"]}, "249c213462": {"title": "[ZeroRedundancyOptimizer] Pytorch compliant state (#52960)", "body": "Summary:\nSame as https://github.com/pytorch/pytorch/issues/52760 which I could not get to land. I just could not live with ghstack/ghimport/randomly broken things, I break enough of them myself, so this is a fresh copy without ghstack shenanigans. I'm hopeful that this can land relatively bug free, and am sorry for the duplications..\n\nWhat this does:\n- call the common_utils test runner instead of unittest, because it seems that it's how it should be done\n- change the returned state from ZeroRedundancyOptimizer to be PyTorch compliant, which has the added benefit of being elastic (world size independent)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52960\n\nReviewed By: mrshenli\n\nDifferential Revision: D26710932\n\nPulled By: blefaudeux\n\nfbshipit-source-id: 1d914bc9221442ba1bb2b48f5df10c313e674ece", "pr_number": "52960", "files_changed": ["test/distributed/optim/test_zero_redundancy_optimizer.py", "torch/distributed/optim/zero_redundancy_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a296fa36ac": {"title": "[Caffe2] Implement BlackBoxPredictor::BenchmarkIndividualOps (#52903)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52903\n\nImplement BlackBoxPredictor::BenchmarkIndividualOps so that we can clean up the output tensors properly after each iteration and get more accurate per operator timing.\n\nAdd four more metrics to track setup_time, memory_alloc_time, memory_dealloc_time, and output_dealloc_time.\n\nReviewed By: ajyu\n\nDifferential Revision: D26657473\n\nfbshipit-source-id: 1cf282192b531513b9ee40b37252087818412f81", "pr_number": "52903", "files_changed": ["caffe2/core/net_simple.cc", "caffe2/core/net_simple.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "729d88119a": {"title": "Fix GradBucket Typing (#52943)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52943\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26699759\n\nPulled By: mrshenli\n\nfbshipit-source-id: 712165a29d114da761ef4f161096ca46a958df03", "pr_number": "52943", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "4d94ee566e": {"title": "Ge v1 (#52136)", "body": "Summary:\nThis is a second attempt to use graph executor to run forward on a gradient. This allows a secondary chance to profile intermediate tensor introduced by autodiff.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52136\n\nReviewed By: pbelevich\n\nDifferential Revision: D26693978\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 91dde8009a210950af8e5173668ada241e16dd52", "pr_number": "52136", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.h", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp", "torch/jit/_fuser.py", "torch/testing/_internal/jit_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source", "triaged"]}, "b9e12a0e82": {"title": "[pytorch] Fix mkldnn heuristic for multithreaded convolution (#52909)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52909\n\nPR #46675 introduced heuristics to use thnn_conv2d for 1x1\nconvolutions, since mkldnn had a bug that was slowing those cases\ndown. Unfortunately, the test plan for that PR only tested single-threaded\nconvolutions; mkldnn is considerably faster on multithreaded convolutions.\n\nAn example from yolov3, on 24 cores of a Xeon Platinum 8175M CPU @ 2.50GHz\n```\ninput:{1, 64, 192, 256}, weight:{32, 64, 1, 1}\nthnn_conv2d: GFLOPS/s=104.574G/s\nmkldnn_convolution: GFLOPS/s=467.357G/s\n```\nghstack-source-id: 122627564\n\nTest Plan: Multithreaded 1x1 convolutions\n\nReviewed By: wconstab, xuzhao9\n\nDifferential Revision: D26685272\n\nfbshipit-source-id: e8e05db89e43856969e26570a170c13b3e73ac74", "pr_number": "52909", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["Merged", "cla signed"]}, "57c7a61237": {"title": "[NNC] Added NNC IR specification (#52912)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52912\n\nReviewed By: bhosmer\n\nDifferential Revision: D26695726\n\nPulled By: Chillee\n\nfbshipit-source-id: c2f1efe0696d7567d4ed85487cc20a2db4e73cd5", "pr_number": "52912", "files_changed": ["torch/csrc/jit/tensorexpr/DesignOverview.md"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e43ea227fe": {"title": "Automated submodule update: tensorpipe (#52930)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/tensorpipe](https://github.com/pytorch/tensorpipe).\n\nNew submodule commit: https://github.com/pytorch/tensorpipe/commit/4b9f7f8abe789bfbecc4610a99c41abb8ec58372\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52930\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: lw\n\nDifferential Revision: D26694739\n\nfbshipit-source-id: d8c835f6e74fec6e2c9a3a6e6713926ccf7dcedd", "pr_number": "52930", "files_changed": ["third_party/tensorpipe"], "labels": ["Merged", "cla signed", "open source"]}, "f5617b0932": {"title": "[testing] Add Opinfo for torch.frac and minor fixes (#52660)", "body": "Summary:\nReference : https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52660\n\nReviewed By: ailzhang\n\nDifferential Revision: D26618151\n\nPulled By: mruberry\n\nfbshipit-source-id: cf0df38e46f44d3afff6e0015af5a840c661aa0e", "pr_number": "52660", "files_changed": ["test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "a06cf5d8a4": {"title": "[numpy] torch.{rad2deg, deg2rad}: promote integer inputs to float (#51853)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nDepends on https://github.com/pytorch/pytorch/issues/51283\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51853\n\nReviewed By: albanD\n\nDifferential Revision: D26399743\n\nPulled By: mruberry\n\nfbshipit-source-id: a6f0e12723e1451c6479d818752fe5d41788715d", "pr_number": "51853", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "0569f638fe": {"title": "Update CODEOWNERS for torch.nn (#52942)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52942\n\nReviewed By: H-Huang\n\nDifferential Revision: D26699633\n\nPulled By: albanD\n\nfbshipit-source-id: cf8b213e9bb69fa4980dba380bd42deee40faf85", "pr_number": "52942", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed"]}, "907ee5b290": {"title": "ns for fx: docblock fixes (#52925)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52925\n\nCleans up some incorrect comments and docblocks in\n`numeric_suite_core_apis.py`.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26693472\n\nfbshipit-source-id: 17f3ff464c6ea01374bcc6ac5899da7034627152", "pr_number": "52925", "files_changed": ["torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "5b93cdace1": {"title": "ns for fx: remove model_name from get_matching_activations API (#52926)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52926\n\nModel name is already stored in the Loggers in the prepare call.\nRemoving the need to specify it again in the extract activations\nfunctions, to simplify things.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26693473\n\nfbshipit-source-id: 52511cacc16f79fa09c78ccde78e7f439f4b315c", "pr_number": "52926", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "87be8c1d7c": {"title": "ns for fx: clean up duplicate code in get_matching_activations_a_shadows_b (#52927)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52927\n\nRefactor to use an existing util instead of duplicating code, no logic\nchange.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26693474\n\nfbshipit-source-id: 06b7047eb9a762557b7f679347e424c0dd009aad", "pr_number": "52927", "files_changed": ["torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "0d46926c63": {"title": "ns for fx: remove subgraphs from user facing API (#52928)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52928\n\nChanges the user facing API of `prepare_single_model_output` to\nrequire a list of nodes instead of a list of subgraphs. This ensures\nthat how we define a subgraph is an implementation detail and is\nnot exposed to the user, keeping the eng cost of updating this\nimplementation later low.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26693471\n\nfbshipit-source-id: 67c2feb844556225e36f8d6d4023246939bcb445", "pr_number": "52928", "files_changed": ["torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "66f07c0c12": {"title": "Optimized bilinear interpolation using TensorIterator (#51653)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/10482\n\nDescription:\n\n- Optimized bilinear interpolation for 1d, 2d, 3d cases using TensorIterator\n\n<details>\n<summary>\nInterpolation 2d - 6 thread(s)\n</summary>\n\nIn | Out | Is contiguous | Channels last | master | this PR | speed-up\n---|---|---|---|---|---|---\n[1, 3, 320, 320] | [256, 256] | True | False | 0.3938 | 0.0782 | 5.0339\n[1, 3, 320, 320] | [512, 512] | True | False | 1.5585 | 0.4105 | 3.7965\n[1, 3, 320, 320] | [256, 256] | False | False | 0.3481 | 0.0760 | 4.5780\n[1, 3, 320, 320] | [512, 512] | False | False | 1.5848 | 0.4091 | 3.8734\n[1, 3, 320, 320] | [256, 256] | False | True | 1.2058 | 1.2034 | 1.0020\n[1, 3, 320, 320] | [512, 512] | False | True | 4.8691 | 4.8537 | 1.0032\n[32, 128, 64, 64] | [32, 32] | False | True | 6.3915 | 6.4041 | 0.9980\n[32, 128, 64, 64] | [128, 128] | False | True | 166.1769 | 164.5621 | 1.0098\n[32, 128, 64, 64] | [32, 32] | True | False | 3.7194 | 2.4720 | 1.5046\n[32, 128, 64, 64] | [128, 128] | True | False | 86.6704 | 52.3754 | 1.6548\n[1, 3, 500, 500] | [256, 256] | True | False | 0.3270 | 0.0792 | 4.1307\n[1, 3, 500, 500] | [800, 800] | True | False | 3.3116 | 0.5567 | 5.9482\n[1, 3, 500, 500] | [256, 256] | False | False | 0.3763 | 0.0773 | 4.8700\n[1, 3, 500, 500] | [800, 800] | False | False | 3.2577 | 0.5590 | 5.8279\n\n</details>\n\n<details>\n<summary>\nInterpolation 1d - 6 thread(s)\n</summary>\n\nIn | Out | Is contiguous | Channels last | master | this PR | speed-up\n---|---|---|---|---|---|---\n[4, 512, 320] | 256 | True | False | 0.2795 | 0.1032 | 2.7089\n[4, 512, 320] | 512 | True | False | 0.5533 | 0.1888 | 2.9303\n\n</details>\n\n<details>\n<summary>\nInterpolation 3d - 6 thread(s)\n</summary>\n\nIn | Out | Is contiguous | Channels last | master | this PR | speed-up\n---|---|---|---|---|---|---\n[1, 3, 16, 320, 320] | [8, 256, 256] | True | False | 4.4105 | 2.1236 | 2.0769\n[1, 3, 16, 320, 320] | [32, 512, 512] | True | False | 83.9426 | 42.6641 | 1.9675\n[1, 3, 16, 320, 320] | [8, 256, 256] | False | True | 15.5736 | 15.5758 | 0.9999\n[1, 3, 16, 320, 320] | [32, 512, 512] | False | True | 272.4795 | 273.2745 | 0.9971\n\n</details>\n\n<details>\n<summary>\nInterpolation 2d - 1 thread(s)\n</summary>\n\nIn | Out | Is contiguous | Channels last | master | this PR | speed-up\n---|---|---|---|---|---|---\n[1, 3, 320, 320] | [256, 256] | True | False | 1.0240 | 0.4145 | 2.4705\n[1, 3, 320, 320] | [512, 512] | True | False | 4.0771 | 1.3836 | 2.9467\n[1, 3, 320, 320] | [256, 256] | False | False | 0.9771 | 0.3270 | 2.9878\n[1, 3, 320, 320] | [512, 512] | False | False | 4.1732 | 1.2209 | 3.4180\n[1, 3, 320, 320] | [256, 256] | False | True | 1.5466 | 1.5363 | 1.0067\n[1, 3, 320, 320] | [512, 512] | False | True | 6.1555 | 6.1199 | 1.0058\n[32, 128, 64, 64] | [32, 32] | False | True | 27.6362 | 27.5901 | 1.0017\n[32, 128, 64, 64] | [128, 128] | False | True | 468.6442 | 465.5163 | 1.0067\n[32, 128, 64, 64] | [32, 32] | True | False | 20.1495 | 10.0694 | 2.0011\n[32, 128, 64, 64] | [128, 128] | True | False | 400.0401 | 204.0662 | 1.9603\n[1, 3, 500, 500] | [256, 256] | True | False | 0.8956 | 0.3366 | 2.6606\n[1, 3, 500, 500] | [800, 800] | True | False | 8.6554 | 2.9530 | 2.9310\n[1, 3, 500, 500] | [256, 256] | False | False | 1.0921 | 0.3385 | 3.2263\n[1, 3, 500, 500] | [800, 800] | False | False | 8.9594 | 2.9627 | 3.0241\n\n</details>\n\n<details>\n<summary>\nInterpolation 1d - 1 thread(s)\n</summary>\n\nIn | Out | Is contiguous | Channels last | master | this PR | speed-up\n---|---|---|---|---|---|---\n[4, 512, 320] | 256 | True | False | 1.5233 | 0.5027 | 3.0301\n[4, 512, 320] | 512 | True | False | 3.0302 | 0.9735 | 3.1128\n\n</details>\n\n<details>\n<summary>\nInterpolation 3d - 1 thread(s)\n</summary>\n\nIn | Out | Is contiguous | Channels last | master | this PR | speed-up\n---|---|---|---|---|---|---\n[1, 3, 16, 320, 320] | [8, 256, 256] | True | False | 12.0477 | 11.3196 | 1.0643\n[1, 3, 16, 320, 320] | [32, 512, 512] | True | False | 222.8618 | 209.9955 | 1.0613\n[1, 3, 16, 320, 320] | [8, 256, 256] | False | True | 17.9883 | 17.9937 | 0.9997\n[1, 3, 16, 320, 320] | [32, 512, 512] | False | True | 380.7244 | 380.1916 | 1.0014\n\n</details>\n\n<details>\n<summary>\nVersions and build configs\n</summary>\n\nPyTorch master: 1.9.0.dev20210223\nPyTorch master build setting:\n```\nBLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON,\n```\n\nPR : 1.9.0a0+74b172b\nPR build setting:\n```\nBUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/g++-7, CXX_FLAGS=-O3 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON,\n```\n</details>\n\nThis description is based on the benchmarks and the code from [here](https://github.com/vfdev-5/interpolate-tensoriterator/tree/master/step_six).\n\nTL;DR\n- Linear upsampling generic implementation using TensorIterator for Nd case (single loop function for 1d, 2d and 3d cases)\n  - can be generalized to nearest, bicubic interpolation modes.\n- works for channels first and last cases.\n\nJoint work with Francisco Massa (fmassa).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51653\n\nReviewed By: malfet\n\nDifferential Revision: D26619437\n\nPulled By: fmassa\n\nfbshipit-source-id: 7d435e23881c5b40a18bf0dbcab4906d5462025f", "pr_number": "51653", "files_changed": ["aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "6d29aa5486": {"title": "Make lambda supported by Map DataPipe (#52856)", "body": "Summary:\nPickle lambda function with `dill` module. Tests are in `torchdata`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52856\n\nReviewed By: anjali411\n\nDifferential Revision: D26673337\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: c2a1b41b7c4cd824945a016d3c1637eb489da700", "pr_number": "52856", "files_changed": ["mypy.ini", "torch/utils/data/datapipes/iter/callable.py"], "labels": ["Merged", "cla signed"]}, "3403babd94": {"title": "[doc] Fix documentations of torch functions (#52982)", "body": "Summary:\nThis PR includes multiple small fixes of docstrings.\n\n* Fix documentation for [`torch.atleast_2d`](https://pytorch.org/docs/master/generated/torch.atleast_2d.html) and [`torch.atleast_3d`](https://pytorch.org/docs/master/generated/torch.atleast_3d.html) by adding a new line before `Args::`.\n* Fix indentation for [`torch.isfinite`](https://pytorch.org/docs/master/generated/torch.isfinite.html) and [`torch.isinf`](https://pytorch.org/docs/master/generated/torch.isinf.html). The \"Arguments\", \"Parameters\" and \"Examples\" sections need to be at the same level as the first description.\n* Insert a new line after `Example::` where it is missing. This makes difference in the way the documentations are rendered: see [this](https://pytorch.org/docs/master/generated/torch.gt.html) (with a new line) and [this](https://pytorch.org/docs/master/generated/torch.triu_indices.html) (without). As the majority of the docs seems to follow the former style, this PR amends the latter cases.\n* Fix the \"Returns\" section of [`torch.block_diag`](https://pytorch.org/docs/master/generated/torch.block_diag.html) and [`torch.cartesian_prod`](https://pytorch.org/docs/master/generated/torch.cartesian_prod.html). The second and the subsequent lines shouldn't be indented, as can be seen in the docstring of [`torch.vander`](https://pytorch.org/docs/master/generated/torch.vander.html).\n* Fix variable names in the example of `torch.fft.(i)fftn`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52982\n\nReviewed By: mruberry\n\nDifferential Revision: D26724408\n\nPulled By: H-Huang\n\nfbshipit-source-id: c65aa0621f7858b05fd16f497caacf6ea8eb33c9", "pr_number": "52982", "files_changed": ["torch/_torch_docs.py", "torch/fft/__init__.py", "torch/functional.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b22b082cc8": {"title": "Fixed the error of generator in the RandomSampler. (#52956)", "body": "Summary:\nIn  `__iter__` of the `RandomSampler`, when `self.replacement` is `False` in the original code, `self.generator` is always used in the `torch.randperm` instead of the generator we set.\n\nFixes https://github.com/pytorch/pytorch/issues/52568\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52956\n\nReviewed By: mruberry\n\nDifferential Revision: D26724303\n\nPulled By: H-Huang\n\nfbshipit-source-id: 86f2795c76f3548e31181fb077af046078a173cb", "pr_number": "52956", "files_changed": ["torch/utils/data/sampler.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d4527b4e16": {"title": "add a full pipeline test for a TypeCheck (#52933)", "body": "Summary:\nThis tests a simple failure mode for a TypeCheck when a shape changes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52933\n\nReviewed By: H-Huang\n\nDifferential Revision: D26727583\n\nPulled By: Krovatkin\n\nfbshipit-source-id: b277218af9572cd6f89f2ece044f7d84d4c10283", "pr_number": "52933", "files_changed": ["test/test_jit_fuser_te.py"], "labels": ["Merged", "cla signed"]}, "8870c391e9": {"title": "Update mkl to 2020.2.254 (#52964)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52907\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52964\n\nReviewed By: H-Huang\n\nDifferential Revision: D26726464\n\nPulled By: seemethere\n\nfbshipit-source-id: 8f3067292e6416e299b4b040c8fb73510134f02e", "pr_number": "52964", "files_changed": [".jenkins/pytorch/win-test-helpers/installation-helpers/install_mkl.bat", "caffe2/CMakeLists.txt", "cmake/Modules/FindOpenMP.cmake", "docs/source/notes/windows.rst"], "labels": ["Merged", "cla signed", "open source"]}, "e36576d153": {"title": "Probable fix for out of place BinaryOpScalar bad values and/or IMAs on 11.2 (ci-all edition) (#52634)", "body": "Summary:\nShould close https://github.com/pytorch/pytorch/issues/51992.\n\nci-all resubmit of https://github.com/pytorch/pytorch/pull/52591. The plot also thickened considerably since then. Every foreach functor, it turns out, has bad `r_args` accesses for certain code paths and instantiations.\n\nAlso, I noticed the [`n % kILP == 0`](https://github.com/pytorch/pytorch/blob/2680ff7759d8a441eada383ba7aa0fa42c7d35ed/aten/src/ATen/native/cuda/ForeachFunctors.cuh#L87) condition for vectorization in all functors is way too restrictive: it'll refuse to vectorize anything on any tensor whose overall numel is not a multiple of ILP. That's out of scope though.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52634\n\nReviewed By: H-Huang\n\nDifferential Revision: D26725991\n\nPulled By: izdeby\n\nfbshipit-source-id: 4bade0ac186bf85527baddc1c44b2c2b8e3c9777", "pr_number": "52634", "files_changed": ["aten/src/ATen/native/cuda/ForeachFunctors.cuh", "test/test_foreach.py", "test/test_optim.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "4daa81e267": {"title": "Automated submodule update: FBGEMM (#52992)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/a431ee37cb28e81ed2c245b1f0aca7d43f40aac5\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52992\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: dskhudia\n\nDifferential Revision: D26718007\n\nfbshipit-source-id: 7b35ab2012b8b6300a6e78c8425f9e08864a9f68", "pr_number": "52992", "files_changed": ["third_party/fbgemm"], "labels": ["Merged", "cla signed", "open source"]}, "0a70ec45d1": {"title": "[ROCm] Enable test cases in autocast_test_lists.py for ROCm (#52737)", "body": "Summary:\nEnabling test cases in autocast_test_lists.py for ROCm because they are passing.\n\nSigned-off-by: Kyle Chen <kylechen@amd.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52737\n\nReviewed By: H-Huang\n\nDifferential Revision: D26706346\n\nPulled By: ngimel\n\nfbshipit-source-id: c1b3b3d8c0ef2a5b1f7e2bd061a749afbae16590", "pr_number": "52737", "files_changed": ["torch/testing/_internal/autocast_test_lists.py"], "labels": ["Merged", "cla signed", "module: rocm", "module: testing", "open source", "triaged"]}, "f2657d2e4f": {"title": "[ROCm] Enable test cases in test_cuda.py for ROCm (#52739)", "body": "Summary:\nEnabling four test cases in test_cuda.py for ROCm because they are passing.\n\nSigned-off-by: Kyle Chen <kylechen@amd.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52739\n\nReviewed By: H-Huang\n\nDifferential Revision: D26706321\n\nPulled By: ngimel\n\nfbshipit-source-id: 6907c548c4ac4e387f0eb7c646e8a01f0d036c8a", "pr_number": "52739", "files_changed": ["test/test_cuda.py"], "labels": ["Merged", "cla signed", "module: cuda", "module: rocm", "module: testing", "open source", "triaged"]}, "89b1053413": {"title": "[DataLoader] Move BufferedShuffle from Dataset to DataPipe (#52141)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52141\n\nRemove BufferShuffleDataSet, as it's not being used anywhere within PyTorch (no usage on Github based on a search) and it's not included in the release of PyTorch 1.7.1.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D26710940\n\nPulled By: ejguan\n\nfbshipit-source-id: 90023b4bfb105d6aa392753082100f9181ecebd0", "pr_number": "52141", "files_changed": ["docs/source/data.rst", "test/test_dataloader.py", "test/test_datapipe.py", "torch/utils/data/__init__.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/combinatorics.py", "torch/utils/data/dataset.py"], "labels": ["Merged", "cla signed"]}, "a9f7ae5357": {"title": "[ROCm] Enable test cases in test/test_dataloader.py for ROCm (#52766)", "body": "Summary:\nEnabling test cases in test_dataloader.py for ROCm because they are passing now.\n\nSigned-off-by: Kyle Chen <kylechen@amd.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52766\n\nReviewed By: H-Huang\n\nDifferential Revision: D26706402\n\nPulled By: ngimel\n\nfbshipit-source-id: 63d4ea6d9b16f6244eb0f0f8f7a957bac8469111", "pr_number": "52766", "files_changed": ["test/test_dataloader.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "084839faa6": {"title": "Clang-format test_c10d.py (#52978)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52978\n\nghstack-source-id: 122701029\n\nTest Plan: N/A\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26713240\n\nfbshipit-source-id: 25301f794a68bee3d6a73d15986a96edab498310", "pr_number": "52978", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e10d2f477b": {"title": "Clang-format c10d/init.cpp (#53008)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53008\n\nghstack-source-id: 122722409\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26720507\n\nfbshipit-source-id: e3ddbcd9e430c8261cc5364795e4b55320e05c5c", "pr_number": "53008", "files_changed": ["torch/csrc/distributed/c10d/init.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "812339ca3d": {"title": "[ZeroRedundancyOptimizer] Buckets as tensor view + minimize public interface (#52987)", "body": "Summary:\nUpdated version following  https://github.com/pytorch/pytorch/issues/52764 (including comments from Shen), but this one I expect to be able to land.\nZeroRedundancyOptimizer:\n- bucket as tensor views, optional\n- make a lot of attributes private\n- minor unit test refactor\n- adding coverage in the unit test for with and without bucket views\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52987\n\nReviewed By: mrshenli\n\nDifferential Revision: D26728851\n\nPulled By: blefaudeux\n\nfbshipit-source-id: f8c745966719c9076c20a554ef56198fb838856c", "pr_number": "52987", "files_changed": ["test/distributed/optim/test_zero_redundancy_optimizer.py", "torch/distributed/optim/zero_redundancy_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "b039dd15ce": {"title": "Delete defunct LegacyTHFunctions templates (#53016)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53016\n\nWe just checked in the generated files directly.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26724876\n\nPulled By: ezyang\n\nfbshipit-source-id: 887d781cac47b7cf16ba2cd6079c63b8f186fe44", "pr_number": "53016", "files_changed": ["aten/src/ATen/templates/LegacyTHFunctions.cpp", "aten/src/ATen/templates/LegacyTHFunctions.h"], "labels": ["Merged", "cla signed"]}, "ec42c2d89c": {"title": "[pyper] fuse clip_ranges+gather_ranges (#52461)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52461\n\nTODO: add tests\n\nTest Plan:\nBefore:\n7.10623 ms/iter\n0.0849279 ms.    1.21267%. fb::clip_ranges (212 nodes)\n0.254071 ms.    3.62783%. fb::gather_ranges (214 nodes)\n\nAfter:\n7.0654 ms/iter\n0.300174 ms.     4.2739%. fb::clip_ranges_gather (264 nodes)\n\nReviewed By: hlu1\n\nDifferential Revision: D26523903\n\nfbshipit-source-id: 9b2420c522232659b198cbe250d4454bbcd9297b", "pr_number": "52461", "files_changed": ["torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "b3bf08e67f": {"title": "Log nccl debug level in ProcessGroupNCCL (#52803)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52803\n\nThis is useful for double checking we have the expected nccl_debug\nlevel when debugging problematic jobs.\n\nNew logs:\n\nWhen default is warn:\n```\nNCCL_ASYNC_ERROR_HANDLING: 0\nNCCL_BLOCKING_WAIT: 0\nTIMEOUT(ms): 60000\nUSE_HIGH_PRIORITY_STREAM: 0\nNCCL_DEBUG: WARN\n```\n\noff:\n\n```\nNCCL_ASYNC_ERROR_HANDLING: 0\nNCCL_BLOCKING_WAIT: 0\nTIMEOUT(ms): 1800000\nUSE_HIGH_PRIORITY_STREAM: 0\nNCCL_DEBUG: N/A\n```\nghstack-source-id: 122751110\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D26653699\n\nfbshipit-source-id: 845cc1236f3838f4763c6dcf2a30d059b3d44f02", "pr_number": "52803", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3993fb2bf9": {"title": "fix(docs): indent in docstring of key_averages (#53006)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52742\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53006\n\nReviewed By: H-Huang\n\nDifferential Revision: D26725101\n\nPulled By: albanD\n\nfbshipit-source-id: 867be12b0ee363a3c0ddcaf8cb4f6354dd4aa901", "pr_number": "53006", "files_changed": ["torch/autograd/profiler.py"], "labels": ["Merged", "cla signed", "open source"]}, "e2462745ba": {"title": "Update kineto submodule (#53039)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53039\n\nReviewed By: gdankel\n\nDifferential Revision: D26732608\n\nPulled By: malfet\n\nfbshipit-source-id: 5c7f30d237f238fc69a6d2a18a0aee41a68f6f09", "pr_number": "53039", "files_changed": ["third_party/kineto"], "labels": ["Merged", "cla signed"]}, "fd4722949d": {"title": "Fix the repeated entry in the Tensor Attributes doc (#52995)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52995\n\nReviewed By: H-Huang\n\nDifferential Revision: D26732911\n\nPulled By: iramazanli\n\nfbshipit-source-id: 86ab93f7f3540cf16dde02670e05cb56999b4929", "pr_number": "52995", "files_changed": ["docs/source/tensor_attributes.rst"], "labels": ["Merged", "cla signed", "module: docs"]}, "07ae4e9309": {"title": "scripts: Add script to prep wheels for pypi (#53056)", "body": "Summary:\nAdds a script so that we can take wheels directly from\ndownload.pytorch.org and publish them to pypi\n\nThis is currently mainly used to prep windows binaries for publication to PyPI\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53056\n\nReviewed By: H-Huang\n\nDifferential Revision: D26738642\n\nPulled By: seemethere\n\nfbshipit-source-id: 96777ed6c3f3454bddb4bc13121f727074312816", "pr_number": "53056", "files_changed": ["scripts/release/promote/prep_binary_for_pypi.sh"], "labels": ["Merged", "cla signed", "releng"]}, "c7c03dd388": {"title": "[PyTorch] Fix TORCH_CHECK_INDEX(false, ...) in IndexKernel (#53028)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53028\n\nTORCH_CHECK (and variants) wrap the condition in C10_UNLIKELY, so this code is both prettier and better.\nghstack-source-id: 122755165\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D26522821\n\nfbshipit-source-id: 70aa11f1859f979657a1f376f7039b5015c69321", "pr_number": "53028", "files_changed": ["aten/src/ATen/native/cpu/IndexKernel.cpp"], "labels": ["Merged", "cla signed"]}, "cfa41cea7e": {"title": "[numpy] torch.logit: promote integer inputs to float (#52028)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52028\n\nReviewed By: ngimel\n\nDifferential Revision: D26400552\n\nPulled By: mruberry\n\nfbshipit-source-id: 5aec9c9755a7ae283aa52294517ea28f4b0fd3e7", "pr_number": "52028", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "8467e5cad3": {"title": "Remove ci-all and release branches running scheduled tests (#53069)", "body": "Summary:\nThe previous code allowed these tests to run every four hours on certain ci-all branches...which is really bad and resource intensive. This code removes that, but then disallows the 11.2 and 9.2 tests to be run on ci-all branches.\n\nTo debug CUDA 11.2 or 9.2 tests, one must now manually change the config to allow for them. (Look at https://github.com/pytorch/pytorch/issues/51888 and https://github.com/pytorch/pytorch/issues/51598 for examples of how to do that.)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53069\n\nReviewed By: H-Huang\n\nDifferential Revision: D26739738\n\nPulled By: janeyx99\n\nfbshipit-source-id: 7577b9b2e876bac0e4e868ce2a1f3ffdb6aca597", "pr_number": "53069", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "b5ae8e69a7": {"title": "[Lite Interpreter] Support features from to_backend (#52870)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52870\n\nAdd the missing parts to support to_backend modules by lite interpreter.\n1. Add ISINSTANCE instruction support, which is used in to_backend for output type check.\n2. Bypass lite interpreter's type parser by checking the qualified name. If it starts with \"torch.jit\", use the same type resolver as nn module (starting with \"__torch__\").\n\nTests\nMobile module is serialized and loaded in ```BackendTest.TestCompiler```. The results are compared to those from original torchscript module.\n\nTest Plan: Imported from OSS\n\nReviewed By: raziel\n\nDifferential Revision: D26715351\n\nPulled By: iseeyuan\n\nfbshipit-source-id: ad9d74ee81c6aa692ab9e5dd7a9003bae5d4f01f", "pr_number": "52870", "files_changed": ["test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_compiler_lib.cpp", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/runtime/instruction.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "272dfc7bb9": {"title": "Add MANIFEST.in (#52908)", "body": "Summary:\nDo not build PyTorch if `setup.py` is called with  'sdist' option\nRegenerate bundled license while sdist package is being built\nRefactor `check_submodules` out of `build_deps` and check that submodules project are present during source package build stage.\n\nTest that sdist package is configurable during `asan-build` step\n\nFixes https://github.com/pytorch/pytorch/issues/52843\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52908\n\nReviewed By: walterddr\n\nDifferential Revision: D26685176\n\nPulled By: malfet\n\nfbshipit-source-id: 972a40ae36e194c0b4e0fc31c5e1af1e7a815185", "pr_number": "52908", "files_changed": [".gitignore", ".jenkins/pytorch/build-asan.sh", "MANIFEST.in", "setup.py"], "labels": ["Merged", "cla signed"]}, "ec128eadea": {"title": "[package] _custom_import_pickler -> _package_pickler (#53048)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53048\n\nI am planning the custom pickler and unpicklers that we use as\nsemi-public interfaces for `torch.rpc` to consume. Some prefatory\nmovements here.\n\nTest Plan: Imported from OSS\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26734594\n\nPulled By: suo\n\nfbshipit-source-id: 105ae1161d90f24efc7070a8d80c6ac3d2111bea", "pr_number": "53048", "files_changed": ["torch/_deploy.py", "torch/package/_custom_import_pickler.py", "torch/package/_package_pickler.py", "torch/package/package_exporter.py"], "labels": ["Merged", "cla signed"]}, "83a93ee145": {"title": "[package] Pull out _UnpicklerWrapper into PackageUnpickler (#53049)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53049\n\nThis makes our API symmetric--now we have an `Importer` aware Pickler\nand Unpickler implementation that have similar interfaces.\n\nTest Plan: Imported from OSS\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26734593\n\nPulled By: suo\n\nfbshipit-source-id: 3479437cf6b98e0d6a8aa4907c75f0c61d5495d4", "pr_number": "53049", "files_changed": ["torch/_deploy.py", "torch/package/_package_pickler.py", "torch/package/_package_unpickler.py", "torch/package/package_importer.py"], "labels": ["Merged", "cla signed"]}, "3bd779cec6": {"title": "[rpc] make pickler/unpickler pluggable in RPC (#53050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53050\n\nAs title. We would like to use alternative pickler/unpickler\nimplementations without changing the entire RPCPickler, to make it\npossible to send objects over the wire that are coming from a\ntorch.package\n\nTest Plan: Imported from OSS\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26734592\n\nPulled By: suo\n\nfbshipit-source-id: d9d9fa62ee15bfcb00e09192030541b61df8c682", "pr_number": "53050", "files_changed": ["torch/distributed/rpc/internal.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a586c02962": {"title": "Update and expose ZeroRedundancyOptimizer docs (#52937)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52937\n\nTest Plan: Imported from OSS\n\nReviewed By: blefaudeux\n\nDifferential Revision: D26696938\n\nPulled By: mrshenli\n\nfbshipit-source-id: dafb00e5c9f0c0c602f471fdcb6416bde74f806b", "pr_number": "52937", "files_changed": ["docs/source/distributed.optim.rst", "docs/source/index.rst", "test/distributed/optim/test_zero_redundancy_optimizer.py", "torch/distributed/optim/zero_redundancy_optimizer.py"], "labels": ["Merged", "Reverted", "cla signed", "oncall: distributed"]}, "7a60b7dc3e": {"title": "Add support to compare devices (#53045)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53045\n\nTest Plan:\n=====\npython test/test_jit.py -k test_device_not_equal\n\nReviewed By: pbelevich\n\nDifferential Revision: D26737964\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 2205aa1f214a86282602168c364dca1363d2f7dd", "pr_number": "53045", "files_changed": ["test/test_jit.py", "torch/csrc/jit/runtime/register_prim_ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "bfae3789ba": {"title": "Move conv to mkldnn (#51483)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51483\n\nThis PR moves the conv weights of a frozen model to MKLDNN, and AOT reorders the weights. When the weights are already in MKLDNN, just computing a single conv by converting the input and output from/to mkldnn provides large speedups. I benchmark'd the results of the top 200 shapes in predictor [here](https://www.internalfb.com/phabricator/paste/view/P171537938), as well as verified that it sped up popular models in torchvision.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696703\n\nPulled By: eellison\n\nfbshipit-source-id: 0b4441bee4f6e0890a4540fbca3bb5e58b8c5adf", "pr_number": "51483", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_freezing.py", "test/linear.py", "tools/build_variables.bzl", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/node_hashing.cpp", "torch/csrc/jit/passes/frozen_conv_folding.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.h", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/runtime/operator.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4b40141d2c": {"title": "Add support for linear in mkldnn fusion (#51484)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51484\n\nThis PR moves the linear weights of a frozen model to MKLDNN. When the weights are already in MKLDNN, just computing a single linear by converting the input and output from/to mkldnn provides large speedups. I benchmark'd the results of the top 200 shapes in predictor [here](https://www.internalfb.com/phabricator/paste/view/P171537854) (taken from aten::matmul), as well as verified that it sped up popular models. .\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696698\n\nPulled By: eellison\n\nfbshipit-source-id: 53d03b9e6956e11b700ee58214e2266e2aa4106a", "pr_number": "51484", "files_changed": ["aten/src/ATen/native/mkldnn/Linear.cpp", "test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "43f56e19a6": {"title": "[NNC] Make NNC sanitize input names (#52786)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52786\n\nPreviously, NNC did not sanitize input names. I ran into this in the next PR when making subgraph creation preserve debug names caused a number of NNC cuda failures. I also previously ran into this with some masked_fill failures internally, which led me to disable the operator.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696699\n\nPulled By: eellison\n\nfbshipit-source-id: 7c3af4d559d58762fb8332666784a4d5cd6a4167", "pr_number": "52786", "files_changed": ["test/cpp/tensorexpr/test_kernel.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a2f7e929ef": {"title": "Add MKLDNN fuser (#51600)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51600\n\nLooking for notes on implementation first, will post more notes on benchmarks and overall thoughts/implementation and solicit more input soon.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696702\n\nPulled By: eellison\n\nfbshipit-source-id: cd612f093fe3859e42fb0b77560ebd1b44fccff7", "pr_number": "51600", "files_changed": ["aten/src/ATen/core/Formatting.cpp", "aten/src/ATen/core/interned_strings.h", "test/jit/test_freezing.py", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/node_hashing.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.cpp", "torch/csrc/jit/runtime/operator.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "32fed3f375": {"title": "Handle mkldnn broadcasting in mkldnn fuser (#51736)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51736\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696694\n\nPulled By: eellison\n\nfbshipit-source-id: 473cc64c8d9f775e9d06340437aff2eb6c0619b9", "pr_number": "51736", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_freezing.py", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/runtime/operator.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "42bfda36e1": {"title": "Add 0-dim support for binary mkldnn ops (#51921)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51921\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696696\n\nPulled By: eellison\n\nfbshipit-source-id: 96ca79c0d6b5ed7c32c14dc4e7c383f2522a85cb", "pr_number": "51921", "files_changed": ["aten/src/ATen/native/mkldnn/BinaryOps.cpp", "test/test_mkldnn.py"], "labels": ["Merged", "cla signed"]}, "f41c80c267": {"title": "Dont error on 0-dim in convolution (#51922)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51922\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696701\n\nPulled By: eellison\n\nfbshipit-source-id: f8b2c19e134931971fac00246920c1584dd43581", "pr_number": "51922", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "test/test_mkldnn.py"], "labels": ["Merged", "cla signed"]}, "9a990dafd9": {"title": "Add a filter to remove mutation (#51923)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51923\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696700\n\nPulled By: eellison\n\nfbshipit-source-id: 9665e9b786f55b6e5b98420eae19de262d46bb96", "pr_number": "51923", "files_changed": ["test/cpp/jit/test_misc.cpp", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/passes/remove_mutation.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "b1284cfbfb": {"title": "Only functionalize ops which we want to include in mkldnn group (#51924)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51924\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696705\n\nPulled By: eellison\n\nfbshipit-source-id: df2a780f6316d66f0d6ae99bbb54d044947195e5", "pr_number": "51924", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6149a26adb": {"title": "Extend subgraph utils to cover merging a node following a subgraph (#52513)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52513\n\nSubgraph Utils previously only worked with merging a node into a subgraph if the node was before the subgraph; extend the logic for the case where the subgraph is first.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696697\n\nPulled By: eellison\n\nfbshipit-source-id: b0595b7d400161b0972321c55718b67103c7bbcd", "pr_number": "52513", "files_changed": ["test/cpp/jit/test_subgraph_utils.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "506f756a0a": {"title": "Include max pool in fusion groups (#52613)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52613\n\nIncluding MaxPool as part of the MKLDNN fusion group sped up resnet18 by ~20%, and was a win on other models I tested as well. I will post more complete benchmarks.\n\nAs mentioned in the diff, in some cases MaxPool can be slower than aten - ideally we'd only include maxpool if it decreased the number of layout transformations that occur. That hasnt actually matttered for all of the torchvision models, I don't think its necessary for this PR.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696704\n\nPulled By: eellison\n\nfbshipit-source-id: 61a025dbf5e7591c0a0f75def3beb439a138a21e", "pr_number": "52613", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ac122a5a6d": {"title": "[package] catch exceptions from calling reduce function. (#53061)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53061\n\nWe only care about evaluating the string return version. If `reduce()`\nthrows an error, we should just continue on with pickling.\n\nTest Plan: Imported from OSS\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26737652\n\nPulled By: suo\n\nfbshipit-source-id: 0b6fbbe345ad0b6a33330b2efa39d7bab703193d", "pr_number": "53061", "files_changed": ["torch/package/importer.py"], "labels": ["Merged", "cla signed"]}, "87b6702833": {"title": "[distributed] make the pickler in distributed_c10d pluggable (#53060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53060\n\nAs title. We would like to use alternative pickler/unpickler\nimplementations, to make it possible to send objects over the wire that\nare coming from a torch.package\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D26737317\n\nPulled By: suo\n\nfbshipit-source-id: 6bdef9824e48ef657dcad72cc5a9114e6612ea4a", "pr_number": "53060", "files_changed": ["torch/distributed/distributed_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "aae188c529": {"title": "[NNC] Handle non literal constant bounds in Unroll. (#53029)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53000\n\nAlso added test to confirm this case works in FlattenLoop as well.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53029\n\nReviewed By: bertmaher\n\nDifferential Revision: D26742705\n\nPulled By: navahgar\n\nfbshipit-source-id: d87a0f9698411026b5b6e55eee7c2b9fb123d06b", "pr_number": "53029", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "f448c59a57": {"title": "Fix jit.trace mis-handling of InterfaceType (#53052)", "body": "Summary:\n`jit.trace` recursively gathers all named attributes in module at beginning of\ntracing. This is fine in a pure-tracing environment, but breaks when a\nscripted module that contains an InterfaceType'd submodule is involved.\nBecause InterfaceType, by design, is not allowed to have any attribute,\nthus some of the gathered attributes will turn into fatal errors in\nfollowing some graph rewrite passes.\n\nThis PR fixes this bug by distinguishing InterfaceType'd submodules from\nnormal ClassType'd submodules.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53052\n\nReviewed By: wanchaol\n\nDifferential Revision: D26735566\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: a14aee6f1fe8000f80c2dc60bdf19acee6225090", "pr_number": "53052", "files_changed": ["test/jit/test_tracer.py", "torch/csrc/jit/frontend/tracer.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "d382693263": {"title": "[NNC] Build aggregate stmt for kernel before LoopNest. (#53024)", "body": "Summary:\nThis PR builds an aggregate stmt for all the tensors in the kernel before constructing LoopNest. This migrates to using the LoopNest constructor that takes in a stmt and output buffers. This is one more step closer to eliminating the dependency of LoopNest on Tensor.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53024\n\nReviewed By: H-Huang\n\nDifferential Revision: D26729221\n\nPulled By: navahgar\n\nfbshipit-source-id: 43e972585351f6902c14b383b137aaaee3aaa3e1", "pr_number": "53024", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "09ce9b5877": {"title": "Store test file in S3 as well for every TestSuite (#52869)", "body": "Summary:\nWe want to store the file names that triggers each test suite so that we can use this data for categorizing those test files.\n\n~~After considering several solutions, this one is the most backwards compatible, and the current test cases in test_testing.py for print test stats don't break.~~\n\nThe previous plan did not work, as there are multiple Python test jobs that spawn the same suites. Instead, the new S3 format will store test files (e.g., `test_nn` and `distributed/test_distributed_fork`) which will contain the suites they spawn, which will contain the test cases run within the suite. (Currently, there is no top layer of test files.)\n\nBecause of this major structural change, a lot of changes have now been made (thank you samestep!) to test_history.py and print_test_stats.py to make this new format backwards compatible.\n\nOld test plan:\nMake sure that the data is as expected in S3 after https://github.com/pytorch/pytorch/pull/52873 finishes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52869\n\nTest Plan: Added tests to test_testing.py which pass, and CI.\n\nReviewed By: samestep\n\nDifferential Revision: D26672561\n\nPulled By: janeyx99\n\nfbshipit-source-id: f46b91e16c1d9de5e0cb9bfa648b6448d979257e", "pr_number": "52869", "files_changed": [".jenkins/pytorch/test.sh", "test/test_testing.py", "test/test_utils.py", "tools/test_history.py", "torch/testing/_internal/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "28f87bb734": {"title": "Don't run cpp tests a second time in the sharded ort_test2 job (#53067)", "body": "Summary:\nCurrently, the same C++ tests are run in CI twice in the onnx_ort_test1 job as well as the onnx_ort_test2 job. This PR runs it once on our test1 job only.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53067\n\nReviewed By: walterddr\n\nDifferential Revision: D26739857\n\nPulled By: janeyx99\n\nfbshipit-source-id: 8960ad5c70181b8154a230914167286f1d9b64f6", "pr_number": "53067", "files_changed": [".jenkins/caffe2/test.sh"], "labels": ["Merged", "cla signed"]}, "048e3917f9": {"title": "Add duplicate scheduled-ci to allow for debugging (#53109)", "body": "Summary:\nThis should trigger the 11.2 and 9.2 tests on ci-all and release branch pushes so that debugging can happen.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53109\n\nReviewed By: yns88\n\nDifferential Revision: D26752151\n\nPulled By: janeyx99\n\nfbshipit-source-id: 3272038cc97560896ee3e9f5bc461212806c71e2", "pr_number": "53109", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "cla signed"]}, "2d67b76fa6": {"title": "[static runtime] Add Alias analysis to Memory Management/Planning (#50060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50060\n\nAliasing is currently mishandled in SR.\n\nThis diff fixes that issue entirely and allows us to avoid hard coded \"view\" registration.  I'll remove the macro in a follow up diff.\n\nHowever, this diff introduces a subtle assumption when memory optimization is turned on: operators cannot \"sometimes alias.\"  Some care will need to be taken to actually make sure this is enforced going forward.\n\nThis diff\n```\n$ batch=20 ./run.sh --pt_optimize_memory=false |& grep \"finished\"\nC2 run finished. Milliseconds per iter: 0.512114. Iters per second: 1952.69\nPyTorch run finished. Milliseconds per iter: 0.51176. Iters per second: 1954.04\n\n$ batch=20 ./run.sh --pt_optimize_memory=true |& grep \"finished\"\nC2 run finished. Milliseconds per iter: 0.511402. Iters per second: 1955.41\nPyTorch run finished. Milliseconds per iter: 0.506493. Iters per second: 1974.36\n\n$ batch=1 iters=100000 ./run.sh --pt_optimize_memory=false |& grep \"finished\"\nC2 run finished. Milliseconds per iter: 0.0562877. Iters per second: 17765.9\nPyTorch run finished. Milliseconds per iter: 0.0667712. Iters per second: 14976.5\n\n$ batch=1 iters=100000 ./run.sh --pt_optimize_memory=true |& grep \"finished\"\nC2 run finished. Milliseconds per iter: 0.0561829. Iters per second: 17799\nPyTorch run finished. Milliseconds per iter: 0.0665069. Iters per second: 15036\n```\n\nTest Plan:\nbuck test //caffe2/test:static_runtime\nbuck test caffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nReviewed By: eellison\n\nDifferential Revision: D25581156\n\nfbshipit-source-id: 41e68119d53e687a9c32d966ed420b270aea4b5b", "pr_number": "50060", "files_changed": ["benchmarks/static_runtime/deep_wide_pt_bench.cc", "benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/passes.cpp", "torch/csrc/jit/runtime/static/passes.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "d4e64dad15": {"title": "[static runtime] Register both TupleConstruct and ListConstruct as out variants (#52684)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52684\n\nWith alias analysis we get much more powerful registration and we can start removing \"native\" and fallback interpreted implementations.  `inputsOutOfPlace` is an artifact of the hardcoded \"native\" and lax fallback implementations.  Ideally every node will run out of place every time.  Afaik, there's never a reason to disable it and we may want to remove that functionality.\n\nThis diff does introduce a \"leak\" in the memory management - containers are not cleaned up.  This only happens when out variants are enabled\n\nTest Plan: buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest -- --run-disabled\n\nReviewed By: maratsubkhankulov, hlu1\n\nDifferential Revision: D26515801\n\nfbshipit-source-id: 7391d66b9d36e15fc2955a5c34a04d027d18fe78", "pr_number": "52684", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "4fb82a8808": {"title": "Skip dispatch for `is_floating_point` (#52998)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52998\n\nReviewed By: H-Huang\n\nDifferential Revision: D26733731\n\nPulled By: iramazanli\n\nfbshipit-source-id: 87398d3b7583632ca18e906fc997e939c73a57e3", "pr_number": "52998", "files_changed": ["aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h"], "labels": ["Merged", "Reverted", "cla signed"]}, "aa603cb2ce": {"title": "add OpInfo entry for signbit (#52198)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52198\n\nReviewed By: H-Huang\n\nDifferential Revision: D26727598\n\nPulled By: mruberry\n\nfbshipit-source-id: 282350febbd0b1af73320f0e912bf553d386d4b0", "pr_number": "52198", "files_changed": ["test/test_ops.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "fbf9745c85": {"title": "add submodules to sys.modules so their attributes can be pickled (#53107)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/38137\n\nAs mentioned in the issue, this is a workaround for [python issue 43367](https://bugs.python.org/issue43367). There are a number of other places where `sys.modules` is modified, if something changes in python perhaps those should be reviewed as well.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53107\n\nReviewed By: zou3519\n\nDifferential Revision: D26753571\n\nPulled By: ezyang\n\nfbshipit-source-id: 2bda03bab39ff9ca58ce4bc13befe021da91b9c4", "pr_number": "53107", "files_changed": ["test/test_nn.py", "torch/__init__.py"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "2bf079d060": {"title": "Remove useless test_reference_numerics skip infos (#52890)", "body": "Summary:\nThese are no longer useful. Let's wait for a few days before merging this, just in case somebody finds failures in them.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52890\n\nReviewed By: H-Huang\n\nDifferential Revision: D26725500\n\nPulled By: mruberry\n\nfbshipit-source-id: 3ebc18ee11ebef34451e60861414521730742288", "pr_number": "52890", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d8ef3a4793": {"title": "[ROCm] Enable test cases in test_nn.py for ROCm (#52836)", "body": "Summary:\nEnabling tests in test_nn.py for ROCm because they are passing.\n\nSigned-off-by: Kyle Chen <kylechen@amd.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52836\n\nReviewed By: H-Huang\n\nDifferential Revision: D26725891\n\nPulled By: mruberry\n\nfbshipit-source-id: 59655a2515ddce92ffc4c55dcf6f28257c05e3c9", "pr_number": "52836", "files_changed": ["test/test_nn.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "e5e54ada61": {"title": "fix logcumsumexp functor to properly handle infs and nans (#52947)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52213\nNans were previously inconsistently propagated due to std::min always returning first argument if one of the args in nan\nwhen reduction functor was called on 2 `-inf` arguments, `std::min(x,y) - std::max(x,y)` resulted in `-inf - (-inf)` = nan, even though logcumsumexp is well defined for `-inf, -inf` pair.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52947\n\nReviewed By: H-Huang\n\nDifferential Revision: D26718456\n\nPulled By: ngimel\n\nfbshipit-source-id: a44433889da352cc959786dd15b6361a68fcfed7", "pr_number": "52947", "files_changed": ["aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ScanKernels.cu", "test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "43f810fa96": {"title": "Add streams boundary check to `torch::cuda::scatter`` (#53057)", "body": "Summary:\nAccessing elements of `std::vector` outside of its boundaries can lead to crashes/memory corruptions\n\nFixes https://github.com/pytorch/pytorch/issues/52526\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53057\n\nReviewed By: janeyx99\n\nDifferential Revision: D26736829\n\nPulled By: malfet\n\nfbshipit-source-id: 7aa13c53c8d062adfef082153809a7a724a74ee5", "pr_number": "53057", "files_changed": ["torch/csrc/cuda/comm.cpp"], "labels": ["Merged", "cla signed"]}, "73a57246d9": {"title": "disable dill extension behavior (#53118)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53118\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26754878\n\nPulled By: suo\n\nfbshipit-source-id: e088d1dc841633bfc0902e3d19f151892ac5c38c", "pr_number": "53118", "files_changed": ["torch/utils/data/datapipes/iter/callable.py"], "labels": ["Merged", "cla signed"]}, "cb1596a193": {"title": "[operator_benchmark] Added channels last 3d option to interpolate test (#53117)", "body": "Summary:\nDescription:\n\n- Added channels last 3d option to interpolate test\n  - split config non-4d into two : 3d and 5d\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53117\n\nReviewed By: NicolasHug\n\nDifferential Revision: D26754243\n\nPulled By: fmassa\n\nfbshipit-source-id: 49bbab3bb47de27790e39537d0fbeca0f01782c4", "pr_number": "53117", "files_changed": ["benchmarks/operator_benchmark/pt/interpolate_test.py"], "labels": ["Merged", "cla signed", "open source"]}, "890e051047": {"title": "Clang-format quantization_hooks.py (#53100)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53100\n\nghstack-source-id: 122723751\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26721146\n\nfbshipit-source-id: 985057fc02c997124b676854eb0a55e569971a3f", "pr_number": "53100", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c5a67f1675": {"title": "Fix minor inaccuracy in translate error reporting (#53032)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53032\n\nPreviously, you could get this error message:\n\n```\nFailed to synthesize the expression \"Tensor & out\".\nWhen I failed, the following bindings were available in the context:\n\n  const Tensor & self;\n  const Tensor & other;\n  Scalar alpha;\n  const Tensor & op.outputs_[0];\n```\n\nThere's a problem with this error message: it doesn't seem like there\nis any 'out' argument available, but actually there is: the last\nbinding in the context is it.  We printed the *expression*, not\nthe *ctype name*.\n\nAfter this patch, the context now prints as:\n\n```\n  const Tensor & self; // self\n  const Tensor & other; // other\n  Scalar alpha; // alpha\n  const Tensor & out; // op.outputs_[0]\n```\n\nNow it becomes clear that it's a const mismatch.  Maybe we could also\nbeef up the error message so it points out near misses, but I'll leave\nthat to future work.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53\n\nDifferential Revision: D26729768\n\nPulled By: ezyang\n\nfbshipit-source-id: adb363551a7145eac788943c20969c86b1f8a81b", "pr_number": "53032", "files_changed": ["tools/codegen/api/translate.py"], "labels": ["Merged", "cla signed"]}, "37bf6c134b": {"title": "Register DefaultBackend implementations for functional/inplace structured operators (#53037)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53037\n\nAs remarked in #52277 it is easy to give an (inefficient, due to extra\nredispatches) DefaultBackend implementation of foo and foo_ in terms of\nfoo_out.  This patch enables code generation for DefaultBackend in these\ncases by default for all structured kernels.  You can see the payoff\nin MSNPU extension: it only has to register a kernel for add.out, and it\ngets add and add_ kernels automatically.\n\nThe actual code changes are very modest:\n- When DefaultBackend, call the dispatched (not direct native::)\n  functions to allocate tensors, change device guard, etc\n- Don't call impl() for DefaultBackend (as it doesn't exist); instead,\n  directly generate a call to at::foo_out to do the actual work.\n- Do NOT generate DefaultBackend implementation for foo_out.  Actually,\n  there is a case to be made for this being a good idea with more infra;\n  see comments inside.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26731225\n\nPulled By: ezyang\n\nfbshipit-source-id: 939da7cb69f694722ec293e5e42e74a755dd0985", "pr_number": "53037", "files_changed": ["test/cpp_extensions/msnpu_extension.cpp", "tools/codegen/api/types.py", "tools/codegen/dest/register_dispatch_key.py"], "labels": ["Merged", "cla signed"]}, "66b20bb738": {"title": "[CUDA graphs] [JIT] improves readability and nvfuser convenience for graph-safe cuda RNG (#51580)", "body": "Summary:\nI'm trying to make jitted RNG graph-safe in csarofeen 's nvfuser branch. Doing so requires diffs in files outside torch/csrc/jit, and we'd like these to go upstream through the present simple separate PR (instead of needing to be reviewed as part of Christian's branch's eventual merge, which will be massive).\n\nFrom the perspective of eager mode consumers, diffs here are purely cosmetic. I moved raw definitions of `PhiloxCudaState` and `at::cuda::philox::unpack` to standalone headers the codegen can easily copy from.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51580\n\nReviewed By: malfet\n\nDifferential Revision: D26626972\n\nPulled By: ngimel\n\nfbshipit-source-id: 7f04d6c5ffe0af7a8a66d3ae6ed36191d12f7d67", "pr_number": "51580", "files_changed": ["aten/src/ATen/CUDAGeneratorImpl.h", "aten/src/ATen/cuda/CUDAGraphsUtils.cuh", "aten/src/ATen/cuda/detail/PhiloxCudaStateRaw.cuh", "aten/src/ATen/cuda/detail/UnpackRaw.cuh", "caffe2/CMakeLists.txt", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "module: cuda graphs", "oncall: jit", "open source", "triaged"]}, "29034b9487": {"title": "[Reland] Update and expose ZeroRedundancyOptimizer docs (#53112)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53112\n\nTest Plan: Imported from OSS\n\nReviewed By: blefaudeux\n\nDifferential Revision: D26752289\n\nPulled By: mrshenli\n\nfbshipit-source-id: 897257417b530e6e18788cb40c44e5cb7ac688d5", "pr_number": "53112", "files_changed": ["docs/source/distributed.optim.rst", "docs/source/index.rst", "test/distributed/optim/test_zero_redundancy_optimizer.py", "torch/distributed/optim/zero_redundancy_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d697090260": {"title": "Add a note in DDP doc to point to ZeroRedundancyOptimizer (#53113)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53113\n\nTest Plan: Imported from OSS\n\nReviewed By: blefaudeux\n\nDifferential Revision: D26752339\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7a082f1007bc550eabb82b559d020bbe717fa497", "pr_number": "53113", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "41765d4681": {"title": "Store coverage files as artifacts for better debugging (#53126)", "body": "Summary:\nHelps with https://github.com/pytorch/pytorch/issues/44120 by storing coverage as artifacts to be investigated\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53126\n\nReviewed By: walterddr\n\nDifferential Revision: D26757702\n\nPulled By: janeyx99\n\nfbshipit-source-id: f7db2b3f51b9ee1a95178bdbd4b1c453078d2ba7", "pr_number": "53126", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed"]}, "2444b4d122": {"title": "Add wait_for_worker param to TCPStore and fix port in use flaky test failures (#52888)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52888\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26678707\n\nPulled By: H-Huang\n\nfbshipit-source-id: 5662e60c4d06d88d2e57834f496b52fb7600de29", "pr_number": "52888", "files_changed": ["test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ab7f6f3f5b": {"title": "Add default arguments to cuda stream and events (#53025)", "body": "Summary:\n* **https://github.com/pytorch/pytorch/issues/53025 Add default args for CUDA stream and events**\n\nTests:\n=====\npython test/test_jit.py -v TestCUDA\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53025\n\nReviewed By: H-Huang\n\nDifferential Revision: D26734499\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 5311623a501e2e6fb3fc70e39522e3970e401feb", "pr_number": "53025", "files_changed": ["test/cpp/jit/tests_setup.py", "test/jit/test_cuda.py", "torch/csrc/jit/cuda/cuda.h", "torch/jit/cuda.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ecb5ac90ed": {"title": "[Gradient Compression] Add get_per_parameter_tensors method to GradBucket class (#53009)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53009\n\nIt can be a common operation to apply layer-wise operations over per-parameter tensors in a DDP communication hook.\n\nCreate a util method in GradBucket class before publishing GradBucket APIs.\nghstack-source-id: 122833594\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/distributed:c10d -- test_powerSGD_ddp_comm_hook_nccl\n\nf254364097\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26717893\n\nfbshipit-source-id: 916db319de8b85dd22bc4e35db5671bf4e34740f", "pr_number": "53009", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py", "torch/lib/c10d/comm.cpp", "torch/lib/c10d/comm.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "4997c38a15": {"title": "[Gradient Compression] Don't provide default values in GradBucket constructor (#53102)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53102\n\nIn `GradBucket` constructor, `offsets`, `lengths`, and `sizes_vec` are optional arguments and could possibly be empty. It will be safe to remove the default values.\nghstack-source-id: 122833603\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26748199\n\nfbshipit-source-id: 2e3bcd1b732851919a64bbbd20fe85e77a616fe3", "pr_number": "53102", "files_changed": ["torch/lib/c10d/comm.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "b05dd931ee": {"title": "[Gradient Compression] Add is_the_last_bucket_to_allreduce method to GradBucket class (#53010)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53010\n\nTo determine the boundary between different iterations in a DDP communication hook, currently the user code needs `bucket.get_index() == 0`, which involves internal bucketization implementation details and undermines the usability of DDP communication hook.\n\nCreate an API to hide the details and improve the usability before publishing GradBucket APIs.\nghstack-source-id: 122723081\n\nTest Plan: buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_powerSGD_ddp_comm_hook_nccl\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26720813\n\nfbshipit-source-id: f4a3147382c1f970534d7f0dee0cd599156c8b8c", "pr_number": "53010", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py", "torch/lib/c10d/comm.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "521e1e83ea": {"title": "[Gradient Compression] Remove some low-level methods of GradBucket class (#53098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53098\n\nRemove some low-level methods that are no longer needed since `get_per_parameter_tensors` method is added to `GradBucket` class.\n\nAvoid unnecessary exposure to the internals before publishing GradBucket APIs.\nghstack-source-id: 122723683\n\nTest Plan: buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_powerSGD_ddp_comm_hook_nccl\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26720919\n\nfbshipit-source-id: 46fb6423008792e72d7a1dd68930a31e0724c92c", "pr_number": "53098", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/comm.hpp"], "labels": ["Merged", "Reverted", "cla signed", "oncall: distributed"]}, "a3a2150409": {"title": "Codegen python bindings to access attributes of grad_fn (#52451)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/9922\n\nAdds python bindings to *selected* fields that grad_fn saves - we did not add python bindings to certain types such as 'TypeAndSize' and 'TensorGeometry'. All field names are prefixed with `_saved_` so they are easy to discern. User code should not depend on particular saved fields to exist as what grad_fn saves for the backward pass is considered an implementation detail and thus prone to change.\n\nWarning: Not all parameters that are passed in are necessarily stored to be used for the backward pass. What you put in is not necessarily what you get out either. Here we pass `kernel_size=3`, but `b.grad_fn._saved_kernel_size` returns `(3, 3)` instead of 3. It seems to vary case-by-case.\n\nFor example:\n```\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1, dilation=1)\n\na = torch.ones(1, 3, 32, 32, requires_grad=True)\nb = model(a)\n\nprint(\"kernel_size: \", b.grad_fn._saved_kernel_size)\nprint(\"stride: \", b.grad_fn._saved_stride) # returns tuple: (3, 3)\n# print(\"dilation: \", b.grad_fn._saved_dilation) # dilation is not stored for backward pass\nprint(\"padding: \", b.grad_fn._saved_padding)\nprint(\"weight: \", b.grad_fn._saved_weight)\n```\n\nSample of generated code:\n```\nPyObject* THPThnnConv2DBackward_self_getter(THPCppFunction *self, void *_unused) {\n  const auto& prop = static_cast<ThnnConv2DBackward*>(self->cdata.get())->self_;\n  return THPVariable_Wrap(prop.unpack());\n}\n\nPyObject* THPThnnConv2DBackward_weight_getter(THPCppFunction *self, void *_unused) {\n  const auto& prop = static_cast<ThnnConv2DBackward*>(self->cdata.get())->weight_;\n  return THPVariable_Wrap(prop.unpack());\n}\n\nPyObject* THPThnnConv2DBackward_kernel_size_getter(THPCppFunction *self, void *_unused) {\n  auto prop = static_cast<ThnnConv2DBackward*>(self->cdata.get())->kernel_size;\n  PyObject* tup = PyTuple_New((Py_ssize_t) prop.size());\n  for (int i = 0; i < prop.size(); i++) {\n    PyTuple_SetItem(tup, (Py_ssize_t) i, PyLong_FromUnsignedLong((uint64_t) prop[i]));\n  }\n  return tup;\n}\n\nPyObject* THPThnnConv2DBackward_stride_getter(THPCppFunction *self, void *_unused) {\n  auto prop = static_cast<ThnnConv2DBackward*>(self->cdata.get())->stride;\n  PyObject* tup = PyTuple_New((Py_ssize_t) prop.size());\n  for (int i = 0; i < prop.size(); i++) {\n    PyTuple_SetItem(tup, (Py_ssize_t) i, PyLong_FromUnsignedLong((uint64_t) prop[i]));\n  }\n  return tup;\n}\n\nPyObject* THPThnnConv2DBackward_padding_getter(THPCppFunction *self, void *_unused) {\n  auto prop = static_cast<ThnnConv2DBackward*>(self->cdata.get())->padding;\n  PyObject* tup = PyTuple_New((Py_ssize_t) prop.size());\n  for (int i = 0; i < prop.size(); i++) {\n    PyTuple_SetItem(tup, (Py_ssize_t) i, PyLong_FromUnsignedLong((uint64_t) prop[i]));\n  }\n  return tup;\n}\n\nPyObject* THPThnnConv2DBackward_finput_getter(THPCppFunction *self, void *_unused) {\n  const auto& prop = static_cast<ThnnConv2DBackward*>(self->cdata.get())->finput_;\n  return THPVariable_Wrap(prop.unpack());\n}\n\nPyObject* THPThnnConv2DBackward_fgrad_input_getter(THPCppFunction *self, void *_unused) {\n  const auto& prop = static_cast<ThnnConv2DBackward*>(self->cdata.get())->fgrad_input_;\n  return THPVariable_Wrap(prop.unpack());\n}\n\nstatic struct PyGetSetDef ThnnConv2DBackward_properties[] = {\n  THP_FUNCTION_DEFAULT_PROPERTIES,\n  {(char*)\"_saved_self\", (getter)THPThnnConv2DBackward_self_getter, nullptr, nullptr, nullptr},\n  {(char*)\"_saved_weight\", (getter)THPThnnConv2DBackward_weight_getter, nullptr, nullptr, nullptr},\n  {(char*)\"_saved_kernel_size\", (getter)THPThnnConv2DBackward_kernel_size_getter, nullptr, nullptr, nullptr},\n  {(char*)\"_saved_stride\", (getter)THPThnnConv2DBackward_stride_getter, nullptr, nullptr, nullptr},\n  {(char*)\"_saved_padding\", (getter)THPThnnConv2DBackward_padding_getter, nullptr, nullptr, nullptr},\n  {(char*)\"_saved_finput\", (getter)THPThnnConv2DBackward_finput_getter, nullptr, nullptr, nullptr},\n  {(char*)\"_saved_fgrad_input\", (getter)THPThnnConv2DBackward_fgrad_input_getter, nullptr, nullptr, nullptr},\n  {nullptr} /* sentinel */\n};\n\n...\n\nvoid initialize_autogenerated_functions() {\n...\n  static PyTypeObject ThnnConv2DBackwardClass;\n  addClass<ThnnConv2DBackward>(ThnnConv2DBackwardClass, \"ThnnConv2DBackward\", ThnnConv2DBackward_properties);\n...\n}\n```\n\nBefore:\n```\nvoid initialize_autogenerated_functions() {\n...\n  static PyTypeObject ThnnConv2DBackwardClass;\n  addClass<ThnnConv2DBackward>(ThnnConv2DBackwardClass, \"ThnnConv2DBackward\");\n...\n}\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52451\n\nReviewed By: H-Huang\n\nDifferential Revision: D26692633\n\nPulled By: soulitzer\n\nfbshipit-source-id: a09b5b8138e4641093aff68c7e9dffdbb96911b8", "pr_number": "52451", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_autograd_functions.py", "tools/autograd/templates/python_functions.cpp"], "labels": ["Merged", "cla signed"]}, "6ab3a8b6f2": {"title": "Update torch.nn.quantizable.MultiHeadAttention docstring (#53106)", "body": "Summary:\nApply the same fix as PR https://github.com/pytorch/pytorch/pull/49950\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53106\n\nReviewed By: zou3519\n\nDifferential Revision: D26752234\n\nPulled By: albanD\n\nfbshipit-source-id: 5c924319b8365da4d3d2ba2206e2586e23e718f0", "pr_number": "53106", "files_changed": ["torch/nn/quantizable/modules/activation.py"], "labels": ["Merged", "cla signed", "open source"]}, "c4c20a5d2d": {"title": "Suppress unsigned comparison warning (#52653)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52653\n\nFixes:\n```\ncaffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu(105): warning: pointless comparison of unsigned integer with zero\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D26588918\n\nfbshipit-source-id: b1a72cebbb7dcb516f63c7c8e2526840ed7c85d1", "pr_number": "52653", "files_changed": ["aten/src/ATen/native/cuda/BinaryMulDivKernel.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "e2ecfb60a6": {"title": "FIX Validates target in cosine_embedding (#53110)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53030\n\nThis PR validates the target for `cosine_embedding_loss`. This is consistent with how `cross_entropy` handles non 1d targets:\n\n```py\nimport torch\nimport torch.nn.functional as F\n\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randint(5, (3, 1))\n\n# Raises RuntimeError\nloss = F.cross_entropy(input, target)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53110\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26766579\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 73ad559ff9376543b6528a36af094e82eb6f9735", "pr_number": "53110", "files_changed": ["aten/src/ATen/native/Loss.cpp", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2c8f9aec64": {"title": "avoid TLS in has_names (#53003)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53003\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26719724\n\nPulled By: bhosmer\n\nfbshipit-source-id: b575e2cec6509e287ed216d9926bbf1108eb7636", "pr_number": "53003", "files_changed": ["aten/src/ATen/core/NamedTensor.cpp", "c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "e86476f736": {"title": "Huber loss (#50553)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/48595.\n\n## Background\n\nThis PR implements HuberLoss, which differs from SmoothL1Loss by a factor of beta. The current implementation does not share logic between the two. Feedback is welcome for the optimal way to minimize code duplication while remaining performant.\n\nI've done some early [benchmarking](https://pytorch.org/tutorials/recipes/recipes/benchmark.html#collecting-instruction-counts-with-callgrind) with Huber calling in to the Smooth L1 kernel and scaling afterwards; for the simple test case I used, instruction counts are as follows:\n```\nHuber loss calls dedicated Huber kernel: 2,795,300\nHuber loss calls Smooth L1 kernel and scales afterwards: 4,523,612\n```\nWith these numbers, instruction counts are ~62% higher when using the pre-existing Smooth L1 kernel.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50553\n\nTest Plan:\n```\npython test/test_nn.py TestNN.test_HuberLoss\npython test/test_nn.py TestNN.test_HuberLoss_delta\npython test/test_nn.py TestNN.test_huber_loss_invalid_delta\npython test/test_nn.py TestNNDeviceTypeCPU.test_smooth_l1_loss_vs_huber_loss_cpu\npython test/test_nn.py TestNNDeviceTypeCUDA.test_smooth_l1_loss_vs_huber_loss_cuda\npython test/test_nn.py TestNNDeviceTypeCPU.test_invalid_reduction_strings_cpu\npython test/test_nn.py TestNNDeviceTypeCUDA.test_invalid_reduction_strings_cuda\npython test/test_nn.py TestNN.test_loss_equal_input_target_shape\npython test/test_nn.py TestNN.test_pointwise_loss_broadcast\npython test/test_overrides.py\npython test/test_jit.py TestJitGeneratedFunctional.test_nn_huber_loss\npython test/test_type_hints.py\npython test/test_cpp_api_parity.py\nbuild/bin/test_api\n```\n\n## Documentation\n<img width=\"677\" alt=\"Screen Shot 2021-01-14 at 4 25 08 PM\" src=\"https://user-images.githubusercontent.com/75754324/104651224-5a445980-5685-11eb-884b-14ea517958c2.png\">\n<img width=\"677\" alt=\"Screen Shot 2021-01-14 at 4 24 35 PM\" src=\"https://user-images.githubusercontent.com/75754324/104651190-4e589780-5685-11eb-974d-8c63a89c050e.png\">\n<img width=\"661\" alt=\"Screen Shot 2021-01-14 at 4 24 45 PM\" src=\"https://user-images.githubusercontent.com/75754324/104651198-50225b00-5685-11eb-958e-136b36f6f8a8.png\">\n<img width=\"869\" alt=\"Screen Shot 2021-01-14 at 4 25 27 PM\" src=\"https://user-images.githubusercontent.com/75754324/104651208-53b5e200-5685-11eb-9fe4-5ff433aa13c5.png\">\n<img width=\"862\" alt=\"Screen Shot 2021-01-14 at 4 25 48 PM\" src=\"https://user-images.githubusercontent.com/75754324/104651209-53b5e200-5685-11eb-8051-b0cfddcb07d3.png\">\n\nReviewed By: H-Huang\n\nDifferential Revision: D26734071\n\nPulled By: jbschlosser\n\nfbshipit-source-id: c98c1b5f32a16f7a2a4e04bdce678080eceed5d5", "pr_number": "50553", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/PointwiseOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/PointwiseOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/nn.functional.rst", "docs/source/nn.rst", "test/cpp/api/functional.cpp", "test/cpp/api/modules.cpp", "test/cpp_api_parity/parity-tracker.md", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/csrc/api/include/torch/nn/modules/loss.h", "torch/csrc/api/include/torch/nn/options/loss.h", "torch/csrc/api/src/nn/modules/loss.cpp", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/nn/functional.py", "torch/nn/functional.pyi.in", "torch/nn/modules/__init__.py", "torch/nn/modules/loss.py", "torch/overrides.py", "torch/testing/_internal/common_nn.py", "torch/testing/_internal/jit_metaprogramming_utils.py"], "labels": ["Merged", "cla signed"]}, "fd582af06c": {"title": "enable coverage test for dataloader on Windows (#52550)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50661\nFor coverage,\nThe class qualified name is `'SimpleCustomBatch': <class '__mp_main__.SimpleCustomBatch'>`\n\nFor pytest\nThe class qualified name is `'SimpleCustomBatch': <class 'test_dataloader.SimpleCustomBatch'>`\n\nSo move the class to one separate file\n\n![image](https://user-images.githubusercontent.com/16190118/108611869-d6b51f80-741d-11eb-908e-be7a64da916d.png)\n\nAs malfet suggestion, use __import__ to avoid adding new file.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52550\n\nReviewed By: walterddr\n\nDifferential Revision: D26754023\n\nPulled By: malfet\n\nfbshipit-source-id: 34b0fbe7336b9303cedc28ec6116ab752a2d3630", "pr_number": "52550", "files_changed": ["test/run_test.py", "test/test_dataloader.py"], "labels": ["Merged", "ci/all", "cla signed", "open source", "triaged"]}, "c0b31a5ba7": {"title": "[StaticRuntime] Clean up (#53096)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53096\n\n- auto[&] -> const auto[&]\n- clean up size() calls\n\nTest Plan:\n```\nbuck test //caffe2/torch/fb/sparsenn:test\nbuck test //caffe2/benchmarks/static_runtime:static_runtime_cpptest\n```\n\nReviewed By: ajyu\n\nDifferential Revision: D26747001\n\nfbshipit-source-id: 6ec81310747d86f7c5d2d17202eef7e299ef610c", "pr_number": "53096", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "0f7f600e01": {"title": "Fix constexpr __host__ warning (#52702)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52702\n\nFixes:\n```\nstderr: caffe2/c10/util/MathConstants.h(22): warning: calling a constexpr __host__ function(\"from_bits\") from a __host__ __device__ function(\"pi\") is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D26589533\n\nfbshipit-source-id: 42c4b36b0ba1e08cbdc9a122fedf35610483c764", "pr_number": "52702", "files_changed": ["c10/util/Half.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "565d8235e5": {"title": "[nnc] Test cases for uneven split + reorder (#53091)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53091\n\nSplit with tail followed by reorder causes a segfault in NNC\nSplit with mask followed by reorder generates invalid code that writes out of\nbounds\nghstack-source-id: 122870733\n\nTest Plan: LoopNest.ColReduceSplit*\n\nReviewed By: navahgar\n\nDifferential Revision: D26746254\n\nfbshipit-source-id: f8a0de18531b34d2bf06ccaa35d9c98b81b5c600", "pr_number": "53091", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp"], "labels": ["Merged", "cla signed"]}, "0dac7d86ca": {"title": "blas copy and axpy to aten (#52345)", "body": "Summary:\nFixes #{issue number}\n\nFollow-up PR: https://github.com/pytorch/pytorch/pull/50984\n\n`copy` and `axpy` functions are ported to ATen. `THBlas_axpy` and `THBlas_copy` are removed.\n\nLooking forward your comments cc ngimel, mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52345\n\nReviewed By: zou3519\n\nDifferential Revision: D26756533\n\nPulled By: ngimel\n\nfbshipit-source-id: 97649485eeb6b361d6434c4701539b5abba4a17d", "pr_number": "52345", "files_changed": ["aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/CPUBlas.h", "aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/cpu/BlasKernel.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/TH/THBlasUtils.h", "aten/src/TH/generic/THBlas.cpp", "aten/src/TH/generic/THBlas.h"], "labels": ["Merged", "cla signed", "module: complex", "module: porting", "open source", "triaged"]}, "5c1c8cb93b": {"title": "[caffe2] Fix shape inference for pruning ops (#53082)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53082\n\nReviewed By: yinghai, khabinov\n\nDifferential Revision: D26742532\n\nfbshipit-source-id: 6cdfb293541b601f7916a95e08bf573876c9ca74", "pr_number": "53082", "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "e1e19a71ce": {"title": "[shape inference] fix pruning", "body": "Summary: Use the dim type of the first input for output.\n\nTest Plan:\nunit test\nflow test: f254777437\nhttps://fburl.com/n933wc3a\nshapes {\n  shape {\n    dims: 19102004\n    dims: 68\n    data_type: UINT8\n    name: \"sparse_nn_2/sparse_arch_2/grouped_embedding_10/grouped_generic_embedding_10/GSF_IDLIST_IG_BUSINESS_AUTHOR_PPR_ORGANIC_ENGAGEMENT_UNIFORM_RIDS/w_EmbeddingFusedUint4Quantization\"\n  }\n  dim_type: CONSTANT\n  dim_type: CONSTANT\n  name: \"sparse_nn_2/sparse_arch_2/grouped_embedding_10/grouped_generic_embedding_10/GSF_IDLIST_IG_BUSINESS_AUTHOR_PPR_ORGANIC_ENGAGEMENT_UNIFORM_RIDS/w_EmbeddingFusedUint4Quantization\"\n  shape_is_final: true\n}\n\nReviewed By: yinghai, khabinov\n\nDifferential Revision: D26763978\n\nfbshipit-source-id: b9c0d6ca4a2b0e4d50d34e08f724e99ad705196b", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": []}, "0819d5f9e9": {"title": "[FX] Added docstring for concrete_args (#53151)", "body": "Summary:\nAn oversight.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53151\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26766450\n\nPulled By: Chillee\n\nfbshipit-source-id: 26e6e44386bbff4bc06b41c39dff9e02cadfcc73", "pr_number": "53151", "files_changed": ["torch/fx/symbolic_trace.py"], "labels": ["Merged", "cla signed", "fx"]}, "e29d8477a6": {"title": "Added CUDA support for torch.orgqr (#51348)", "body": "Summary:\nThis PR adds support for CUDA inputs for `torch.orgqr`.\n\nCUDA implementation is based on both [cuSOLVER](https://docs.nvidia.com/cuda/cusolver/index.html#cuSolverDN-lt-t-gt-orgqr) and MAGMA. cuSOLVER doesn't have a specialized routine for the batched case. While MAGMA doesn't have a specialized GPU native (without CPU sync) `orgqr`. But MAGMA has implemented (and not documented) the batched GPU native version of `larft` function (for small inputs of size <= 32), which together with `larfb` operation form `orgqr` (see the call graph [here at the end of the page](http://www.netlib.org/lapack/explore-html/da/dba/group__double_o_t_h_e_rcomputational_ga14b45f7374dc8654073aa06879c1c459.html)).\n\nSo now there are two main codepaths for CUDA inputs (if both MAGMA and cuSOLVER are available):\n* if `batchsize > 1` and `tau.shape[-1] <= 32` then MAGMA based function is called\n* else [cuSOLVER's `orgqr`](https://docs.nvidia.com/cuda/cusolver/index.html#cuSolverDN-lt-t-gt-orgqr) is used.\n\nIf MAGMA is not available then only cuSOLVER is used and vice versa.\n\nDocumentation updates and possibly a new name for this function will be in a follow-up PR.\n\nRef. https://github.com/pytorch/pytorch/issues/50104\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51348\n\nReviewed By: ngimel\n\nDifferential Revision: D26727918\n\nPulled By: mruberry\n\nfbshipit-source-id: 1c4d15fa76ba624e341a69a32337a9a16cc01013", "pr_number": "51348", "files_changed": ["aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/CUDASolver.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h", "aten/src/ATen/native/native_functions.yaml", "test/test_linalg.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source", "triaged"]}, "bd7ac755d8": {"title": "Fix loop type (#50484)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50484\n\nI currently see the compilation warning:\n```\nJan 13 16:46:21 [3644/5223] Building CXX object caffe2/CMakeFiles/torch_cpu.dir/__/aten/src/ATen/core/ivalue.cpp.o\nJan 13 16:46:21 ../aten/src/ATen/core/ivalue.cpp:855:22: warning: comparison of integers of different signs: 'int' and 'std::__1::vector<c10::IValue, std::__1::allocator<c10::IValue> >::size_type' (aka 'unsigned long') [-Wsign-compare]\nJan 13 16:46:21   for (auto i = 0; i < slots_.size(); ++i) {\n```\nThis diff fixes that\n\nTest Plan: Sandcastle tests\n\nReviewed By: ngimel\n\nDifferential Revision: D25901674\n\nfbshipit-source-id: 0a09570866f23b5878bf06f46f918d71a733974f", "pr_number": "50484", "files_changed": ["aten/src/ATen/core/ivalue.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "816646bd6f": {"title": "Add OpInfo for `bitwise_not` and make ROCM and CUDA OpInfo tests consistent (#51944)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nThis PR also enables the OpInfo tests on ROCM to check the same dtypes that of CUDA.\n\nFew tests have to be skipped (due to failure).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51944\n\nReviewed By: H-Huang\n\nDifferential Revision: D26727660\n\nPulled By: mruberry\n\nfbshipit-source-id: 3aea236cf0002f46c2737afbda2ed3efccfe14f5", "pr_number": "51944", "files_changed": ["test/test_ops.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "70d0aab7bd": {"title": "De-prioritise Dimname and DimnameList in python overload resolution (#51350)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51350\n\n`None` being a valid `Dimname` is awkward for optional `dim` arguments, as found\non NumPy's reduction functions like `std` and `var`. In these cases `dim=None`\nshould mean an all-reduction, but instead you get an error\n\"Please look up dimensions by name\".\n\nI've also had to fix `FunctionParameter::check` to actually check the first\nelement of `INT_LIST` arguments and reject non-int types. Otherwise, the dim\nnames end up calling the `int[]` overload and fail.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D26756208\n\nPulled By: mruberry\n\nfbshipit-source-id: 44221ca0f4822ec2c1f62b092466fd4f779eb45a", "pr_number": "51350", "files_changed": ["test/test_native_functions.py", "tools/autograd/gen_python_functions.py", "torch/csrc/utils/python_arg_parser.cpp"], "labels": ["Merged", "cla signed", "module: codegen", "open source"]}, "a2a88990cd": {"title": "[PyTorch] Remove extra RNN.cpp file (#53169)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53169\n\nAs title, there are two `aten/src/ATen/native/RNN.cpp` in `aten_native_source_list`\nghstack-source-id: 122936706\n\nTest Plan: CI\n\nReviewed By: dhruvbird, iseeyuan\n\nDifferential Revision: D26715640\n\nfbshipit-source-id: 54717ded9b293e022a47ab7891dfd04afae48ce5", "pr_number": "53169", "files_changed": ["tools/build_variables.bzl"], "labels": ["Merged", "cla signed"]}, "30dd15e778": {"title": "[PyTorch] Add doc string for lite interpreter related api in Android (#53136)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53136\n\nAs title, doc string in ios, c++ and python is ready.\n\nAs a reference, the doc string for other lite interpreter related apis\n[_load_for_mobile](https://www.internalfb.com/intern/diffusion/FBS/browsefile/master/fbcode/caffe2/torch/csrc/jit/mobile/import.h?commit=c95d12f9d67ee198aa4b5aafec980e9048de1702&lines=16-43)\n[_save_for_lite_interpreter](https://www.internalfb.com/intern/diffusion/FBS/browsefile/master/fbcode/caffe2/torch/jit/_script.py?commit=b1d7f0ba6001beed6ba3b0a69a225abab4ed3866&lines=496-509)\nghstack-source-id: 122936777\n\nTest Plan: CI\n\nReviewed By: IvanKobzarev, iseeyuan\n\nDifferential Revision: D26742092\n\nfbshipit-source-id: 76464b5e4ceafe71348b58ba2af98c3debdaae63", "pr_number": "53136", "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/LiteModuleLoader.java", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java"], "labels": ["Merged", "cla signed"]}, "d90d7245f4": {"title": "[PyPer] Optimize sigrid_hash (#53065)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53065\n\nReviewed By: ajyu\n\nDifferential Revision: D26563512\n\nfbshipit-source-id: a1a76f92ba500605ab2e3370737bd3965d81deb1", "pr_number": "53065", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/passes.cpp", "torch/csrc/jit/runtime/static/passes.h"], "labels": ["cla signed", "oncall: jit"]}, "0aa9f22f1a": {"title": "Move groupbykey to grouping (#53122)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53122\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26756641\n\nPulled By: ejguan\n\nfbshipit-source-id: c4bc5864d841ce20c49446a03cfd195245b2be6e", "pr_number": "53122", "files_changed": ["torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/groupbykey.py", "torch/utils/data/datapipes/iter/grouping.py"], "labels": ["Merged", "cla signed"]}, "c957e2ab42": {"title": "Add more datapipe to functional API (#53123)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53123\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26756638\n\nPulled By: ejguan\n\nfbshipit-source-id: 6ff0eb6c7ee702056ff19eeb723949e4642f2784", "pr_number": "53123", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/callable.py", "torch/utils/data/datapipes/iter/combinatorics.py", "torch/utils/data/datapipes/iter/grouping.py"], "labels": ["Merged", "cla signed"]}, "b3c4ac6319": {"title": "Fix OpenBLAS discovery (#53168)", "body": "Summary:\nFix accidental regression introduced by https://github.com/pytorch/pytorch/issues/47940\n\n`FIND_PACKAGE(OpenBLAS)` does not validate that discovered library can actually be used, while `check_fortran_libraries` does that\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53168\n\nTest Plan: Build PyTorch with static OpenBLAS and check that `torch.svd(torch.ones(3, 3)).S` do not raise an exception\n\nReviewed By: walterddr\n\nDifferential Revision: D26772345\n\nPulled By: malfet\n\nfbshipit-source-id: 3e4675c176b30dfe4f0490d7d3dfe4f9a4037134", "pr_number": "53168", "files_changed": ["cmake/Modules/FindBLAS.cmake"], "labels": ["Merged", "cla signed"]}, "85e5fdb919": {"title": "disable TCPStore multi_worker tests for windows (#53156)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53156\n\nWill SSH into windows machine to validate that these tests are skipped.\n\nTest Plan: Imported from OSS\n\nReviewed By: osalpekar\n\nDifferential Revision: D26769791\n\nPulled By: H-Huang\n\nfbshipit-source-id: e4427ba2d6cfe5a1de26e335cd27c1e8875174d3", "pr_number": "53156", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "1559fa6a5c": {"title": "[operator benchmarks] Added more modes to interpolation tests (#53186)", "body": "Summary:\nDescription:\n- Added more modes: bicubic and nearest to interpolation tests\n- Added a test case for downsampling a small image\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53186\n\nReviewed By: albanD\n\nDifferential Revision: D26780116\n\nPulled By: fmassa\n\nfbshipit-source-id: f4f498e6e1da1ec131e6d9d9f42dc482135ae9e2", "pr_number": "53186", "files_changed": ["benchmarks/operator_benchmark/pt/interpolate_test.py"], "labels": ["Merged", "cla signed", "open source"]}, "a1d204807a": {"title": "Add shape inference for SparseLengthsSumSparseLookup", "body": "Summary: Just copy whatever corresponding input shape info. Or we will miss the shape info of output of SparseLengthsSumSparseLookup, which will be infered as the input of downstream SparseLengthsSum op, whose int64/int32 mode is undetermined.\n\nTest Plan:\n```\nbuck test caffe2/caffe2/opt:bound_shape_inference_test\n```\n\nReviewed By: khabinov, ChunliF\n\nDifferential Revision: D26769226\n\nfbshipit-source-id: 4032bc4643a125095a48fa8c23ca4ebcf26dc29c", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h"], "labels": []}, "c8cc2e2133": {"title": "Update CODEOWNERS for test_public_bindings (#53158)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53158\n\nReviewed By: glaringlee\n\nDifferential Revision: D26779568\n\nPulled By: albanD\n\nfbshipit-source-id: f7d56a30dff95dc3f24608ff01367c134cc08bbf", "pr_number": "53158", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed"]}, "fbf60b5aaf": {"title": "Store only coverage info as artifacts (#53150)", "body": "Summary:\nI noticed https://github.com/pytorch/pytorch/issues/53126 stored everything in the test folder as an artifact, which isn't exactly what we want. Here, I try to store just the relevant info, coverage files.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53150\n\nReviewed By: albanD\n\nDifferential Revision: D26767185\n\nPulled By: janeyx99\n\nfbshipit-source-id: 286d341ccdfa97d138a2048bb4ee01c7ae2579a1", "pr_number": "53150", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed"]}, "5c15a5bb46": {"title": "Deduplicate shared params before constructing Reducer in DDP (#51929)", "body": "Summary:\nCurrently, `torch.nn.parallel.DistributedDataParallel(model...)` doesn't deduplicate params shared across `model`'s child Modules before calling Reducer with the param list. This can cause Reducer to register more than one hook on the shared param(s), at which point who knows what happens.\n\nWe ran into this in mlperf BERT, which has at least one param shared across submodules (an embedding weight iirc, not 100% sure). Running with `gradient_as_bucket_view = False` produced different numerics from running with `gradient_as_bucket_view = True` (which i guess is one potential consequence of multiple DDP hooks on a given param, not sure why, i'd have to dig further).\n\nThis PR changes DDP to deduplicate shared params (a small diff), and adds some tests (right now just `test_ddp_weight_sharing`, but I'll add more). `test_ddp_weight_sharing` fails with bad numerics on current master (proving the shared param issue is real) and passes with the deduplication diff.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51929\n\nReviewed By: zou3519\n\nDifferential Revision: D26625807\n\nPulled By: zhaojuanmao\n\nfbshipit-source-id: f5f5959fef90dfe2c55812d79fa88b877f22ecc3", "pr_number": "51929", "files_changed": ["test/distributed/test_c10d.py", "torch/lib/c10d/reducer.cpp", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "43906f9b8b": {"title": "[ZeroRedundancyOptimizer] Minor stub fix (#53165)", "body": "Summary:\nNot sure how important that is\nTied to https://github.com/pytorch/pytorch/issues/53108\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53165\n\nReviewed By: albanD\n\nDifferential Revision: D26781956\n\nPulled By: blefaudeux\n\nfbshipit-source-id: b7daca0ea95be190a5ffeae12123e301204ed4eb", "pr_number": "53165", "files_changed": ["torch/distributed/optim/zero_redundancy_optimizer.pyi"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "f5e725527d": {"title": "[PyTorch] Save a single add instruction in the dispatcher (#52543)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52543\n\nThis saves one (1) add instruction. New code comments should\nexplain exactly why. In short, we store a direct pointer in\n`OperatorHandle` in addition to the `std::list<OperatorDef>::iterator`\nbecause converting the latter to the former requires an add instruction.\n\nIt is not clear to me whether this is a particularly great tradeoff,\nbut I spent (more) time on it (than I expected), so here it is for\nreview.\nghstack-source-id: 122147199\n\nTest Plan:\nInspect assembly for at::empty in benchmark code -- see add\ninstruction disappeared.\n\nCompare empty benchmark performance to baseline with perf stat.\n\nBaseline:\n          5,077.43 msec task-clock                #    1.000 CPUs utilized            ( +-  0.25% )\n               405      context-switches          #    0.080 K/sec                    ( +-  1.37% )\n                 3      cpu-migrations            #    0.001 K/sec                    ( +- 18.22% )\n            12,259      page-faults               #    0.002 M/sec                    ( +-  0.10% )\n    10,089,754,343      cycles                    #    1.987 GHz                      ( +-  0.25% )  (50.04%)\n    29,516,000,227      instructions              #    2.93  insn per cycle           ( +-  0.04% )  (50.08%)\n     5,662,629,032      branches                  # 1115.256 M/sec                    ( +-  0.02% )  (50.08%)\n         1,955,729      branch-misses             #    0.03% of all branches          ( +-  0.88% )  (50.04%)\n\n            5.0796 +- 0.0128 seconds time elapsed  ( +-  0.25% )\n\nAfter:\n```\n          5,017.77 msec task-clock                #    1.001 CPUs utilized            ( +-  0.19% )\n               400      context-switches          #    0.080 K/sec                    ( +-  3.09% )\n                 4      cpu-migrations            #    0.001 K/sec                    ( +- 46.91% )\n            12,240      page-faults               #    0.002 M/sec                    ( +-  0.37% )\n     9,960,189,535      cycles                    #    1.985 GHz                      ( +-  0.19% )  (50.02%)\n    29,467,149,773      instructions              #    2.96  insn per cycle           ( +-  0.11% )  (50.03%)\n     5,661,074,219      branches                  # 1128.206 M/sec                    ( +-  0.02% )  (50.07%)\n         2,032,712      branch-misses             #    0.04% of all branches          ( +-  1.35% )  (50.07%)\n\n            5.0151 +- 0.0101 seconds time elapsed  ( +-  0.20% )\n```\n\n1.2% cycles win, outside the noise\n0.16% instruction count win, barely outside noise\n\nI am surprised at the size of the cycles win.\n\nReviewed By: bhosmer\n\nDifferential Revision: D26564192\n\nfbshipit-source-id: 71f731ba54ec1cb407673db691eaf77a257de4a9", "pr_number": "52543", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["Merged", "cla signed"]}, "99098c1d70": {"title": "Delete dead Backend toSparse (#53116)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53116\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26753226\n\nPulled By: ezyang\n\nfbshipit-source-id: 2941876d546c39ee3913c2ffffdb0a0ea7360f0c", "pr_number": "53116", "files_changed": ["c10/core/Backend.h"], "labels": ["Merged", "cla signed"]}, "fd3004d3ee": {"title": "Add NoOpDeviceGuardImpl (#53142)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53142\n\nIt turns out to make Meta a device I need to substantively reuse\nthe CPUGuardImpl implementation.  It's pretty parametrizable so\njust move this over to DeviceGuardImplInterface templated over\nthe DeviceType.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411, samestep\n\nDifferential Revision: D26763553\n\nPulled By: ezyang\n\nfbshipit-source-id: 464fb3e3a72ba7c55a12adffe01c18171ce3e857", "pr_number": "53142", "files_changed": ["aten/src/ATen/detail/CPUGuardImpl.cpp", "aten/src/ATen/detail/CPUGuardImpl.h", "c10/core/impl/DeviceGuardImplInterface.h"], "labels": ["Merged", "cla signed"]}, "0f81a69a96": {"title": "Make meta a device (getting rid of empty_meta) (#53143)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53143\n\nMeta is now an honest to goodness device type, like cpu, so you can use\ndevice='meta' to trigger allocation of meta tensors.  This way better\nthan empty_meta since we now have working API for most factory functions\n(they don't necessarily work yet, though, because need to register Meta\nversions of those functions.)\n\nSome subtleties:\n- I decided to drop the concept of CPU versus CUDA meta tensors; meta\n  tensors are device agnostic.  It's hard to say exactly what the\n  correct level of abstraction here is, but in this particular case\n  implementation considerations trump semantic considerations: it\n  is way easier to have just a meta device, than to have a meta device\n  AND a cpu device AND a cuda device.  This may limit the applicability\n  of meta tensors for tracing models that do explicit cpu()/cuda()\n  conversions (unless, perhaps, we make those operations no-ops on meta\n  tensors).\n- I noticed that the DeviceType uppercase strings are kind of weird.\n  Are they really supposed to be all caps?  That's weird.\n- I moved the Meta dispatch key to live with the rest of the \"device\"\n  dispatch keys.\n- I intentionally did NOT add a Backend for Meta.  For now, I'm going to\n  hope meta tensors never exercise any of the Backend conversion code;\n  even if it does, better to fix the code to just stop converting to and\n  from Backend.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D26763552\n\nPulled By: ezyang\n\nfbshipit-source-id: 14633b6ca738e60b921db66a763155d01795480d", "pr_number": "53143", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/detail/MetaGuardImpl.cpp", "aten/src/ATen/native/MetaTensor.cpp", "aten/src/ATen/native/native_functions.yaml", "c10/core/Device.cpp", "c10/core/DeviceType.cpp", "c10/core/DeviceType.h", "c10/core/DispatchKey.h", "c10/core/TensorOptions.h", "test/backward_compatibility/check_backward_compatibility.py", "test/test_torch.py", "tools/codegen/dest/register_dispatch_key.py", "torch/csrc/autograd/init.cpp", "torch/library.h", "torch/overrides.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ecd8e4c1d5": {"title": "Add guard to run on current thread (#52361)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52361\n\nTest Plan:\nbuck build //xplat/caffe2:aten_test_test_thread_pool_guard\n./aten_test_test_thread_pool_guard\n\nReviewed By: kimishpatel\n\nDifferential Revision: D26429540\n\nfbshipit-source-id: 16e4a56d4bf9b73b1ea1ff88d7dc6730e0b1e029", "pr_number": "52361", "files_changed": ["BUILD.bazel", "aten/src/ATen/test/test_thread_pool_guard.cpp", "caffe2/utils/CMakeLists.txt", "caffe2/utils/threadpool/pthreadpool-cpp.cc", "caffe2/utils/threadpool/thread_pool_guard.cpp", "caffe2/utils/threadpool/thread_pool_guard.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "f8238d7917": {"title": "[optim] bugfix when all parameters have no grad (#52944)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52944\n\nThis fix the bug introduced during refactoring optimizers https://github.com/pytorch/pytorch/pull/50411. When all parameters have no grads, we should still allows `beta` like hyper params to be defined.\n\nReviewed By: ngimel\n\nDifferential Revision: D26699827\n\nfbshipit-source-id: 8a7074127704c7a4a1fbc17d48a81e23a649f280", "pr_number": "52944", "files_changed": ["test/test_optim.py", "torch/optim/adadelta.py", "torch/optim/adam.py", "torch/optim/adamw.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "510c03d922": {"title": "[Gradient Compression] Remove some low-level methods of GradBucket class (#53098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53098\n\nRemove some low-level methods that are no longer needed since `get_per_parameter_tensors` method is added to `GradBucket` class.\n\nAvoid unnecessary exposure to the internals before publishing GradBucket APIs.\nghstack-source-id: 122979064\n\nTest Plan: buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_powerSGD_ddp_comm_hook_nccl\n\nReviewed By: osalpekar\n\nDifferential Revision: D26784249\n\nfbshipit-source-id: d1b27bb026989c25a5b65be4767cb752afd6f19b", "pr_number": "53098", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/comm.hpp"], "labels": ["Merged", "Reverted", "cla signed", "oncall: distributed"]}, "8b5b7fa83d": {"title": "[WIP][FX] Optionally record stack traces when symtracing (#53081)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53081\n\nTest Plan: Imported from OSS\n\nReviewed By: ansley\n\nDifferential Revision: D26742402\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 7987f9ddf061f6de3b4a638d98e0fae6d68d90c6", "pr_number": "53081", "files_changed": ["test/test_fx.py", "torch/fx/node.py", "torch/fx/proxy.py"], "labels": ["Merged", "cla signed", "fx"]}, "59c0c19be2": {"title": "Add RemoteModule to master RPC docs. (#53084)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53084\n\nAdding RemoteModule to master RPC docs since it is a prototype\nfeature.\nghstack-source-id: 122816689\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26743372\n\nfbshipit-source-id: 00ce9526291dfb68494e07be3e67d7d9c2686f1b", "pr_number": "53084", "files_changed": ["docs/source/rpc.rst", "torch/distributed/nn/api/remote_module.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "4008df3507": {"title": "Add property binding in torchbind (#50670)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50670\n\nThis PR adds property support to Torchbind. There are two cases that it needs to work:\n\n**Torchscript**\nInside Torchscript, we don't go through pybind so there is no issue with accessing properties through ClassType.\n\n**Eager Mode**\nIn Eager Mode, Torchbind creates ScriptObject which we cannot dynamically add (aka access) properties after initializing it. (https://stackoverflow.com/questions/1325673/how-to-add-property-to-a-class-dynamically\n) Therefore we created a Python wrapper (ScriptObjectWrapper) around ScriptObject where we can use property method to set properties.  By doing so, we can look up wrapped object's property through __getattr__ method of the ScriptObjectWrapper. This logic is inspired from https://github.com/pytorch/pytorch/pull/44324\n\nTest Plan:\ntest cases in test_torchbind.py\n\nImported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D26632781\n\nfbshipit-source-id: dd690887cfda0c48ff0d104aa240ce0ab09055bc", "pr_number": "50670", "files_changed": ["aten/src/ATen/core/jit_type.h", "test/cpp/jit/test_custom_class_registrations.cpp", "test/jit/test_torchbind.py", "torch/csrc/jit/api/object.h", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/custom_class.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "f7d65c5cd2": {"title": "Use .gv instead of .dot for Graphviz in fast_nvcc (#53208)", "body": "Summary:\nSee this page for context: https://marc.info/?l=graphviz-devel&m=129418103126092\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53208\n\nTest Plan:\n```\ntools/fast_nvcc/fast_nvcc.py --help\n```\n\nReviewed By: janeyx99\n\nDifferential Revision: D26791398\n\nPulled By: samestep\n\nfbshipit-source-id: 6a0363a4664e79b80ddf2ae799ec05ee7d028357", "pr_number": "53208", "files_changed": ["tools/fast_nvcc/fast_nvcc.py"], "labels": ["Merged", "cla signed"]}, "096c66a99f": {"title": "[sparsity][refactor] Rename row/col to out/in features", "body": "Summary: Names such as `row_block_size` and `col_block_size` might be ambiguous, especially if different engines use different tensor layouts (i.e. rows=output features, etc.). Having names such as `out_features_block_size` and `in_features_block_size` makes more sense\n\nTest Plan:\n`buck test mode/opt //caffe2/torch/fb/model_optimization:sparsity_test`\n\n```\nBuilding with Remote Execution [RE]. Used 36:09 minutes of total time.\n[RE] Waiting on 0 remote actions. Completed 264 actions remotely.\nBuilding: finished in 02:34.4 min (100%) 18884/18884 jobs, 420 updated\n  Total time: 02:34.8 min\nMore details at https://www.internalfb.com/intern/buck/build/b34b5c52-eba6-4e17-92f9-1f5ce620f8f0\nTpx test run coordinator for Facebook. See https://fburl.com/tpx for details.\nRunning with tpx session id: 8fe8fa95-c1f8-4b4f-9cbf-88b3b1b28eaf\nTrace available for this run at /tmp/tpx-20210302-000019.503678/trace.log\nStarted reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/4785074650825194\n    \u2713 ListingSuccess: caffe2/torch/fb/model_optimization:sparsity_test - main (4.094)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseKernels) (1.896)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (1.907)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear_serdes (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (2.035)\nSummary\n  Pass: 3\n  ListingSuccess: 1\nFinished test run: https://www.internalfb.com/intern/testinfra/testrun/4785074650825194\n```\n\nReviewed By: dskhudia\n\nDifferential Revision: D26747065\n\nfbshipit-source-id: 685fe864062ed532de284b22db757a921806d4ab", "pr_number": null, "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/include/pack_block_sparse.h"], "labels": []}, "a812175173": {"title": "Update Kineto revision (#53199)", "body": "Summary:\nUpdate Kineto revision\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53199\n\nReviewed By: gdankel\n\nDifferential Revision: D26784476\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 7e908f63ee2790ddb5348c580ad5a4d5ad94b921", "pr_number": "53199", "files_changed": ["third_party/kineto"], "labels": ["Merged", "cla signed"]}, "d30f4d1dfd": {"title": "Migrate apex.parallel.SyncBatchNorm channels_last to pytorch (#46906)", "body": "Summary:\nper title\n\nThis PR did\n- Migrate `apex.parallel.SyncBatchNorm` channels_last to pytorch `torch.nn.SyncBatchNorm`\n- Fix a TODO here by fusing `sum`, `div` kernels into backward elementwise kernel\nhttps://github.com/pytorch/pytorch/blob/b167402e2e66a663cd9913885552929b4c045ffa/torch/nn/modules/_functions.py#L76-L95\n\nTodo\n- [x] Discuss a regression introduced in https://github.com/pytorch/pytorch/pull/37133#discussion_r512530389, which is the synchronized copy here\nhttps://github.com/pytorch/pytorch/blob/b167402e2e66a663cd9913885552929b4c045ffa/torch/nn/modules/_functions.py#L32-L34\n\n**Comment**: This PR uses apex version for the size check. Test passed and I haven't seen anything wrong so far.\n\n- [x] The restriction to use channels_last kernel will be like this\n```\ninline bool batch_norm_use_channels_last_kernels(const at::Tensor& self) {\n  return self.is_contiguous(at::MemoryFormat::ChannelsLast) || self.ndimension() == 2;\n}\n```\nI think we can relax that for channels_last_3d as well?\n\n**Comment**: we don't have benchmark for this now, will check this and add functionality later when needed.\n- [x] Add test\n- [x] Add benchmark\n\nDetailed benchmark is at https://github.com/xwang233/code-snippet/tree/master/syncbn-channels-last\n\nClose https://github.com/pytorch/pytorch/issues/50781\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46906\n\nReviewed By: albanD\n\nDifferential Revision: D26771437\n\nPulled By: malfet\n\nfbshipit-source-id: d00387044e9d43ac7e6c0e32a2db22c63d1504de", "pr_number": "46906", "files_changed": ["aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "torch/nn/modules/_functions.py", "torch/nn/modules/batchnorm.py", "torch/overrides.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ba36e32406": {"title": "[Gradient Compression] Correct the usage of min_compression_rate (#52979)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52979\n\nCompression rate = uncompressed size / compressed size, so the compression rate is usually greater than 1.\n\nPreviously the compression rate was perceived as compressed size / uncompressed size, which can be very confusing.\nghstack-source-id: 122996272\n\nTest Plan: unit tests\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26713349\n\nfbshipit-source-id: 83b7f8908c101954cf01f56a22161047fbfeaa53", "pr_number": "52979", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "79944f7ad9": {"title": "[fx] simple doc fix", "body": "Reviewed By: houseroad\n\nDifferential Revision: D26739803\n\nfbshipit-source-id: e680ce961a9ed1a5042d675aca9f5cf118c8ff85", "pr_number": null, "files_changed": ["docs/source/fx.rst", "torch/fx/symbolic_trace.py"], "labels": []}, "14a2ef0932": {"title": "Deduplicate test cases in suites by taking the longer test case (#53154)", "body": "Summary:\nAlso removes unneeded filename field in S3.\n\nTested locally:\nI locally installed\n```\nconda install -c anaconda boto3\nconda install -c conda-forge unittest-xml-reporting\n```\nI ran `python test/test_type_hints.py --save-xml=/tmp/reports/test_type_hints` twice to generate two reports of the same test cases.\nThen, I edited the print_test_stats.py file to print the report instead of upload to S3, and then ran `CIRCLE_SHA1=\"$(git rev-parse HEAD)\" CIRCLE_JOB=foo python torch/testing/_internal/print_test_stats.py --upload-to-s3 /tmp/reports/test_type_hints`. I verified the report object looked correct:\n```\n{\n   'build_pr': '',\n   'build_tag': '',\n   'build_sha1': '67cecd7f6cf2956bda1178ae2369cd74ba946f78',\n   'build_branch': '',\n   'build_job': 'foo',\n   'build_workflow_id': '',\n   'total_seconds': 67.316,\n   'format_version': 2,\n   'files': {\n       'test/test_type_hints': {\n             'total_seconds': 67.316,\n             'suites': {\n                    'TestTypeHints': {\n                           'total_seconds': 67.316,\n                           'cases': {\n                                 'test_doc_examples': {\n                                        'seconds': 8.821,\n                                        'status': None\n                                  },\n                                 'test_run_mypy': {\n                                       'seconds': 58.495,\n                                       'status': None\n                                  }\n                           }\n                    }\n             }\n       }\n   }\n}\n```\nIt did take the longer of the two test cases for both test cases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53154\n\nReviewed By: samestep\n\nDifferential Revision: D26793522\n\nPulled By: janeyx99\n\nfbshipit-source-id: 5644c1bd38acb8bca0d69851cf1d549a03334b7a", "pr_number": "53154", "files_changed": ["torch/testing/_internal/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "12d63cc2f5": {"title": "Add assert_async (#53086)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53086\n\nFixes #36853\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26744062\n\nPulled By: ezyang\n\nfbshipit-source-id: be6d2653afe584adf67a05b5d43185b40764650d", "pr_number": "53086", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_cuda.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "9b7396e7e2": {"title": "[pyper] casted_batch_one_hot_lengths with 4-arg to (#53215)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53215\n\nThe current 5-arg version doesn't fuse the inline_cvr model instances\n\nTest Plan:\n```\nMKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --c2_weights=/data/users/ansha/tmp/adfinder/models/c2_local_weight_data.pb --c2_inputs=/data/users/ansha/tmp/adfinder/models/c2_local_input_data.pb --pred_net=/data/users/ansha/tmp/adfinder/models/c2_local_net.pb --c2_sigrid_transforms_opt=1 --c2_apply_nomnigraph_passes=1 --c2_use_memonger=1 --scripted_model=/data/users/ansha/tmp/adfinder/models_dianshi/210494966_0.predictor.disagg.local.pt --pt_inputs=/data/users/ansha/tmp/adfinder/models/local_wrapped_input_data.pt --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --compare_results=1 --iters=2000 --warmup_iters=2000 --num_threads=1 --do_profile=1 --do_benchmark --benchmark_c2_predictor=1\n```\n\n```\nTime per node type:\n        3.82029 ms.    71.8523%. aten::addmm (9 nodes)\n       0.926298 ms.    17.4219%. fb::sigrid_transforms (1 nodes)\n       0.122496 ms.    2.30391%. fb::clip_ranges_gather (210 nodes)\n        0.11985 ms.    2.25416%. fb::clip_ranges_gather_sigrid_hash_precompute_v3 (54 nodes)\n      0.0973721 ms.    1.83138%. aten::sigmoid (3 nodes)\n      0.0352937 ms.   0.663807%. fb::batch_box_cox (1 nodes)\n       0.034759 ms.    0.65375%. prim::TupleConstruct (1 nodes)\n      0.0222235 ms.   0.417981%. aten::index (4 nodes)\n      0.0215314 ms.   0.404964%. fb::casted_batch_one_hot_lengths (1 nodes)\n      0.0199659 ms.   0.375521%. fb::concat_add_mul_replacenan_clip (1 nodes)\n      0.0192885 ms.   0.362779%. aten::cat (2 nodes)\n      0.0181285 ms.   0.340963%. aten::mul (2 nodes)\n      0.0109381 ms.   0.205725%. aten::pow (1 nodes)\n      0.0091476 ms.   0.172049%. prim::ListConstruct (8 nodes)\n     0.00794012 ms.   0.149338%. aten::relu (2 nodes)\n     0.00668873 ms.   0.125802%. prim::ListUnpack (1 nodes)\n     0.00569745 ms.   0.107158%. aten::to (4 nodes)\n     0.00527507 ms.   0.099214%. aten::narrow_copy (4 nodes)\n     0.00483189 ms.  0.0908785%. fb::lengths_range (4 nodes)\n     0.00399056 ms.  0.0750548%. aten::logit (1 nodes)\n     0.00324574 ms.  0.0610462%. fb::gather_ranges (4 nodes)\n     0.00161166 ms.  0.0303122%. fb::clip_ranges (2 nodes)\n        5.31686 ms. in Total\nStaticRuntime setup time: 0.016461 ms\nMemory allocation time: 0.00220284 ms\nMemory deallocation time: 0.118134 ms\nOutputs deallocation time: 0.0674883 ms\nTotal memory managed: 716352 bytes\nTotal number of reused tensors: 22\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D26789260\n\nfbshipit-source-id: 52adadddaae29a946de8a58bd592c06e6d4ce8c8", "pr_number": "53215", "files_changed": ["torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "248e8b42fa": {"title": "[Static Runtime] Use native version of at::empty (#53216)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53216\n\n- at::native::empty_cpu calls at::detail::empty_cpu without any changes to the arguments. So we could call at::detail::empty_cpu directly.\n- There is no need to create a TensorOptions object first since we can get all the relevant information from the tensor directly.\n\nReviewed By: bertmaher, swolchok\n\nDifferential Revision: D26792255\n\nfbshipit-source-id: 7a4e368a19cea79e136e34dab854cb1d37dbeb58", "pr_number": "53216", "files_changed": ["torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "b59075eced": {"title": "[Gradient Compression] Refactor tensor grouping in PowerSGD (#52981)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52981\n\nNo need to create a hard boundary between rank-1 tensors and high-rank tensors, since some high-rank tensors will not be compressed if the compression cannot save enough bandwidth, according to `_should_compress` function.\n\nTherefore, refactor and simplify the tensor grouping logic, which addresses the comment in https://github.com/pytorch/pytorch/pull/52541#discussion_r580867311\nghstack-source-id: 122997032\n\nTest Plan:\nwaitforbuildbot\n\nAlready LGTMed by PowerSGD paper author.\n\nAds1x (completed):\nhttps://www.internalfb.com/intern/tupperware/details/job/?handle=priv3_global%2Fmast_hpc%2Ftsm_hpc-wayi_ads_10x_POWER_SGD_gpu8_2021-02-28_15-29.trainer&tatwTabs=tasks&task_id=0&task_tab=TASK_LOGS\n\nDetectron2:\n1) Before refactoring:\nf254353864\nAccuracy: 39.972\nOverall training speed: 67498 iterations in 6:15:42 (0.3340 s / it)\n\n2) After refactoring:\nf254353380\nAccuracy: 39.944\nOverall training speed: 67498 iterations in 6:09:41 (0.3286 s / it)\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26713689\n\nfbshipit-source-id: 12cfcb65feaa2a2d94e3c7793073031f13828305", "pr_number": "52981", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "68b62493b8": {"title": "[Gradient Compression] Make GradBucket class public (#53099)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53099\n\nPublish GradBucket APIs for publishing DDP communication hooks.\n\ns/_GradBucket/GradBucket\nghstack-source-id: 123030921\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26721121\n\nfbshipit-source-id: ee5f68e33095b9965b51937b86cdeb331fd2419a", "pr_number": "53099", "files_changed": ["test/distributed/test_c10d.py", "torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/__init__.py", "torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py", "torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py", "torch/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "38a34887ac": {"title": "[PyTorch] Fix missing move in {List,Tuple}Construct (#53206)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53206\n\nCopying the List in ListConstruct is 1 extra refcount bump. Copying the vector in TupleConstruct is 1 extra bump per tuple element.\nghstack-source-id: 123001815\n\nTest Plan: Don't have a precise measurement but it's very roughly 0.5% off total time for AdIndexer inline_cvr based on wall time, and more like 1.2% based on change in perf profile.\n\nReviewed By: hlu1\n\nDifferential Revision: D26790670\n\nfbshipit-source-id: 697ef82fe72a85719bf8ce28f2bb87fe56bbd8ad", "pr_number": "53206", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "fc7171badc": {"title": "inline TensorIteratorConfig setters (#52661)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52661\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26602944\n\nPulled By: bhosmer\n\nfbshipit-source-id: 54ab402a33cb35927ca5de0106884223475f7528", "pr_number": "52661", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed"]}, "d5507aa5b5": {"title": "fix output dtype test in compute_types (#52731)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52731\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D26630251\n\nPulled By: bhosmer\n\nfbshipit-source-id: 5f61967c7e94882a3cc3c1b6beaa2b69d68b9656", "pr_number": "52731", "files_changed": ["aten/src/ATen/TensorIterator.cpp"], "labels": ["Merged", "cla signed"]}, "457b9f672c": {"title": "[CI]Shard cuda11_1 tests (#53235)", "body": "Summary:\nAs single pytorch_linux_xenial_cuda11_1_cudnn8_py3_gcc7_test hits the timeout\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53235\n\nReviewed By: glaringlee\n\nDifferential Revision: D26802806\n\nPulled By: malfet\n\nfbshipit-source-id: 8dbd30defa978e806d685b0d851145dc7a9049b4", "pr_number": "53235", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml"], "labels": ["Merged", "ci/all", "cla signed"]}, "68810c1836": {"title": "Delete test_rand_quantization (#53234)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53234\n\nTest has been permanently skipped since Nov 2019, see https://github.com/pytorch/pytorch/pull/29463\n\nTest Plan: CI\n\nReviewed By: mruberry\n\nDifferential Revision: D26802660\n\nfbshipit-source-id: ea66be1afd4d7cfbe692594df5d9dd8c29bc5d23", "pr_number": "53234", "files_changed": ["caffe2/python/operator_test/rand_quantization_op_test.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "d98839e53e": {"title": "[static runtime] register pow out variant (#52454)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52454\n\nTest Plan:\nadfinder local net\nBefore:\n7.13307 ms/iter\n0.0222672 ms.   0.311136%. aten::pow (1 nodes)\nAfter:\n7.10623 ms/iter\n0.0174462 ms.   0.242774%. aten::pow (1 nodes)\n\nReviewed By: malfet, hlu1\n\nDifferential Revision: D26521717\n\nfbshipit-source-id: 8d9279b59d37c8786a9eeccd0f54bd84c400c128", "pr_number": "52454", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "c4c77e2001": {"title": "[special] add `torch.special` namespace (#52296)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/50345\n\n * Add `torch.special` namespace\n* Add `torch.special.gammaln` (alias to `torch.lgamma`)\n\nTODO:\n* Add proper entries for docs.\n   * [x] Add .rst file entry\n   * [x] Add documentation\n   * [x] Update `lgamma` OpInfo entry for alias to `special.gammaln`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52296\n\nReviewed By: ngimel\n\nDifferential Revision: D26754890\n\nPulled By: mruberry\n\nfbshipit-source-id: 73479f68989d6443ad07b7b02763fa98973c15f6", "pr_number": "52296", "files_changed": ["BUILD.bazel", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/README.md", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "caffe2/CMakeLists.txt", "docs/source/index.rst", "docs/source/special.rst", "test/cpp/api/CMakeLists.txt", "test/cpp/api/special.cpp", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_special_functions.cpp", "tools/build_variables.bzl", "torch/__init__.py", "torch/_torch_docs.py", "torch/csrc/Module.cpp", "torch/csrc/api/include/torch/all.h", "torch/csrc/api/include/torch/special.h", "torch/csrc/autograd/python_special_functions.h", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/jit/_builtins.py", "torch/special/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "5095332ab9": {"title": "Minor cleanup of interpolate microbenchmark", "body": "Summary: Minor cleanup, addresses comments from https://www.internalfb.com/diff/D26780116 (https://github.com/pytorch/pytorch/commit/1559fa6a5c10705d5b1380ad782f3f602e263acc)\n\nTest Plan:\n```\n\u279c  vision buck run //caffe2/benchmarks/operator_benchmark/pt:interpolate_test -- --tag_filter short\nParsing buck files: finished in 0.6 sec\nBuilding: finished in 6.2 sec (100%) 10951/10951 jobs, 0 updated\n  Total time: 6.9 sec\n/data/users/nicolashug/fbsource/fbcode/buck-out/dev/gen/caffe2/benchmarks/operator_benchmark/pt/interpolate_test#link-tree/torch/utils/cpp_extension.py:3: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  import imp\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modenearest\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest\nForward Execution Time (us) : 1346.156\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modelinear\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: linear\nForward Execution Time (us) : 1283.784\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modebicubic\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: bicubic\nForward Execution Time (us) : 4769.578\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modenearest\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest\nForward Execution Time (us) : 982.910\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modelinear\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: linear\nForward Execution Time (us) : 1182.191\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modebicubic\n# Input: input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: bicubic\nForward Execution Time (us) : 3545.873\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modenearest\n# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest\nForward Execution Time (us) : 34373.955\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modelinear\n# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: linear\nForward Execution Time (us) : 42248.109\n\n# Benchmarking PyTorch: interpolate\n# Mode: Eager\n# Name: interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modebicubic\n# Input: input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: bicubic\nForward Execution Time (us) : 405944.286\n...\n```\n\nReviewed By: fmassa\n\nDifferential Revision: D26782757\n\nfbshipit-source-id: 2039e1e6b4fea2b56bb4bcf2a017476f928e4928", "pr_number": null, "files_changed": ["benchmarks/operator_benchmark/pt/interpolate_test.py"], "labels": []}, "e698a634cc": {"title": "Enabled amin & amax for float16 & bfloat16 (#52579)", "body": "Summary:\n1. Enabled `amax` & `amin` for `float16` & `bfloat16` dtypes for both CPU & CUDA.\n2. Added `OpInfo`s for `amax` & `amin`.\n3. Enabled `test_min_with_inf` & `test_max_with_inf` for both `float16` & `bfloat16`, as they also use `torch.amin` & `torch.amax` respectively.\n4. Enabled `test_amax` & `test_amin` for `float16` but not for `bfloat16`, as comparison is done with `numpy`, which doesn't support `bfloat16`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52579\n\nReviewed By: pbelevich\n\nDifferential Revision: D26784194\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 1050de3e155b83f282fb30b0db6658eead89936c", "pr_number": "52579", "files_changed": ["aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "test/test_reductions.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "6dce0cd0d4": {"title": "Optimize module path finding (#52990)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52990\n\nThis PR changes module path finding from O(N^2) to O(1)\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D26779399\n\nPulled By: ansley\n\nfbshipit-source-id: ff49d8e10bb4f82583ab4757926198ed46507c29", "pr_number": "52990", "files_changed": ["torch/fx/symbolic_trace.py"], "labels": ["Merged", "cla signed", "fx"]}, "6557ea0509": {"title": "Context manager for hiding source ranges (#53188)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52456\n\n## Background\n\nProvides a context manager `_hide_source_ranges()` that disables printing graph source ranges by default. It can be overridden on a per-graph basis if desired.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53188\n\nTest Plan:\n```\npython test/test_jit.py TestJit.test_hide_source_ranges_context_manager\n```\n\n```python\nimport torch\n\ntorch.jit.script\ndef foo(x):\n    return torch.add(x, x)\n\nprint(foo.graph)\nwith torch.jit._hide_source_ranges():\n    print(foo.graph)\n\n    # Override context manager\n    print(foo.graph.str(print_source_ranges=True))\n\nprint(foo.graph)\n```\n\n```\ngraph(%x.1 : Tensor):\n  %3 : int = prim::Constant[value=1]()\n  %4 : Tensor = aten::add(%x.1, %x.1, %3) # /Users/jbschlosser/misc/example.py:5:11\n  return (%4)\n\ngraph(%x.1 : Tensor):\n  %3 : int = prim::Constant[value=1]()\n  %4 : Tensor = aten::add(%x.1, %x.1, %3)\n  return (%4)\n\ngraph(%x.1 : Tensor):\n  %3 : int = prim::Constant[value=1]()\n  %4 : Tensor = aten::add(%x.1, %x.1, %3) # /Users/jbschlosser/misc/example.py:5:11\n  return (%4)\n\ngraph(%x.1 : Tensor):\n  %3 : int = prim::Constant[value=1]()\n  %4 : Tensor = aten::add(%x.1, %x.1, %3) # /Users/jbschlosser/misc/example.py:5:11\n  return (%4)\n```\n\nReviewed By: walterddr, zhangguanheng66\n\nDifferential Revision: D26817070\n\nPulled By: jbschlosser\n\nfbshipit-source-id: e9d123452c616b0a9dda9e134ef6c2886f229d9b", "pr_number": "53188", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/python_ir.cpp", "torch/jit/__init__.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "9e5e5a7d96": {"title": "Revert D26815021: Revert D26744062: Add assert_async", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26815021\n\nOriginal commit changeset: 972eaafcdf14\n\nfbshipit-source-id: e528260e1aa91df1873c73af00aa57addd671607", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_cuda.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": []}, "8c54cd7f37": {"title": "Declare NamedTuple at top level (#53273)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53273\n\nThis prevents a mypy bug.  Fixes #53272\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26819428\n\nPulled By: ezyang\n\nfbshipit-source-id: e71575ed13321665a976cc5ef8b2993c00626b7d", "pr_number": "53273", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3826a07a63": {"title": "[PyTorch] Don't inline Dispatcher::call on mobile (#53197)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53197\n\nThis probably causes a code size blowup and we care more about the size savings than the incremental perf on mobile.\nghstack-source-id: 122977713\n\nTest Plan: buildsizebot some mobile apps\n\nReviewed By: dhruvbird\n\nDifferential Revision: D26731181\n\nfbshipit-source-id: 78a926278a85028af09bfa0731d4d59a55ee3746", "pr_number": "53197", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["Merged", "cla signed"]}, "7aeee2849b": {"title": "Parametrization Functionality (#33344)", "body": "Summary:\nProvides the implementation for feature request issue https://github.com/pytorch/pytorch/issues/28937.\n\nAdds the `Parametrization` functionality and implements `Pruning` on top of it.\nIt adds the `auto` mode, on which the parametrization is just computed once per forwards pass. The previous implementation computed the pruning on every forward, which is not optimal when pruning RNNs for example.\n\nIt implements a caching mechanism for parameters. This is implemented through the mechanism proposed at the end of the discussion https://github.com/pytorch/pytorch/issues/7313. In particular, it assumes that the user will not manually change the updated parameters between the call to `backwards()` and the `optimizer.step()`. If they do so, they would need to manually call the `.invalidate()` function provided in the implementation. This could be made into a function that gets a model and invalidates all the parameters in it. It might be the case that this function has to be called in the `.cuda()` and `.to` and related functions.\n\nAs described in https://github.com/pytorch/pytorch/issues/7313, this could be used, to implement in a cleaner way the `weight_norm` and `spectral_norm` functions. It also allows, as described in https://github.com/pytorch/pytorch/issues/28937, for the implementation of constrained optimization on manifolds (i.e. orthogonal constraints, positive definite matrices, invertible matrices, weights on the sphere or the hyperbolic space...)\n\nTODO (when implementation is validated):\n- More thorough test\n- Documentation\n\nResolves  https://github.com/pytorch/pytorch/issues/28937\n\nalbanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33344\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26816708\n\nPulled By: albanD\n\nfbshipit-source-id: 07c8f0da661f74e919767eae31335a9c60d9e8fe", "pr_number": "33344", "files_changed": ["docs/source/nn.rst", "test/test_nn.py", "torch/nn/utils/parametrize.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b4395b046a": {"title": "Edit SiLU documentation (#53239)", "body": "Summary:\nI edited the documentation for `nn.SiLU` and `F.silu` to:\n- Explain that SiLU is also known as swish and that it stands for \"Sigmoid Linear Unit.\"\n- Ensure that \"SiLU\" is correctly capitalized.\n\nI believe these changes will help users find the function they're looking for by adding relevant keywords to the docs.\n\nFixes: N/A\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53239\n\nReviewed By: jbschlosser\n\nDifferential Revision: D26816998\n\nPulled By: albanD\n\nfbshipit-source-id: b4e9976e6b7e88686e3fa7061c0e9b693bd6d198", "pr_number": "53239", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/activation.py"], "labels": ["Merged", "cla signed", "open source"]}, "b0aa03b703": {"title": "fix tensorpipe_agent linked even when USE_TENSORPIPE is turned off (#53281)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53281\n\nReviewed By: xuzhao9\n\nDifferential Revision: D26822375\n\nPulled By: walterddr\n\nfbshipit-source-id: d4e2b7ed1b38782a9e7f6c5b96b7bb0e31c4bdae", "pr_number": "53281", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/rpc/CMakeLists.txt"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "51718c2f3c": {"title": "Update CODEOWNERS to be tagged as reviewer (#53277)", "body": "Summary:\nFixes #FOMOOCR (fear of missing out on code review)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53277\n\nReviewed By: mrshenli\n\nDifferential Revision: D26820361\n\nPulled By: H-Huang\n\nfbshipit-source-id: 9e985a6a7e6dbda5e454f54fa95cc7d7050245b2", "pr_number": "53277", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed", "oncall: distributed", "triaged"]}, "72ec718373": {"title": "Leak autograd threads after wait limit (#53170)", "body": "Summary:\nLeak autograd threads if TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT is reached\n(default to 10 seconds)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53170\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26821983\n\nPulled By: malfet\n\nfbshipit-source-id: 310960564da7cd8c9f475432a8efbee32cfe6009", "pr_number": "53170", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["Merged", "cla signed"]}, "85109ce427": {"title": "Support submodule manipulation in GraphModule (#52358)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52358\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26759260\n\nPulled By: ansley\n\nfbshipit-source-id: 25d2b9124a7d957704f1700a45dca143aaed391d", "pr_number": "52358", "files_changed": ["test/fx/test_subgraph_rewriter.py", "test/test_fx.py", "test/test_fx_experimental.py", "torch/fx/experimental/merge_matmul.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/nn/modules/module.py"], "labels": ["Merged", "cla signed", "fx"]}, "4739d15a67": {"title": "Skip some nodes during discovery using sequence number (#52180)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/12635\n\nThis change will help us speed up autograd's discovery algorithm in cases where we use `.grad` and we try to \"unroll\" the training loop. For example the example in the issue and also https://github.com/pytorch/pytorch/pull/52180#issuecomment-783400832 observe an unbounded multiple of speed-up.\n\nWe do this by adding a new sequence_nr-type numbering: for each node, we maintain the length of the longest path from it to any leaf node. How does this help us speed up discovery (dfs)? Previously the bottleneck was that the dfs that computes which nodes need to be executed always explored every node. With this change, before we run dfs, we first compute the mininum seq_nr among all the nodes passed as the `inputs`. If let this be some number N, intuitively this means that dfs should stay at least N units away from any leaf node. So, if we find ourselves too close to any leaf node, we should stop our search early.\n\nEdit:\nAfter some discussion offline, the plan is:\n - make old sequence_nr a construct of the profiler. This means we can avoid accessing thread local state in cases where the profiler is disabled. Note that we cannot replace sequence_nr as-is because profiler's use-case requires that thread-id + sequence_nr can uniquely identify a given node in order for downstream users/programs to correlate nodes from backward and forward passes. This means we must maintain two sequence_nr's and that we have an extra field in Node.\n - In a future PR, we can potentially remove sequence_nr entirely from the profiler as well, but we avoid doing it now because we haven't measured, and its a larger effort because we'd have to mess around with the dispatcher and profiler\n\nTesting with this [code](https://gist.github.com/kyunghyuncho/5fb9991ce1233f909051854a84b7148e), we see that runtime no longer increases as we iterate.\n\nBefore:\n```\n100: Time taken: 0.47s, loss: 1.1e+06\n200: Time taken: 0.064s, loss: 6.5e+05\n300: Time taken: 0.088s, loss: 4.4e+05\n400: Time taken: 0.1s, loss: 3.2e+05\n500: Time taken: 0.12s, loss: 2.5e+05\n600: Time taken: 0.15s, loss: 2e+05\n700: Time taken: 0.18s, loss: 1.7e+05\n800: Time taken: 0.2s, loss: 1.4e+05\n900: Time taken: 0.22s, loss: 1.2e+05\n1000: Time taken: 0.24s, loss: 1.1e+05\n1100: Time taken: 0.27s, loss: 9.3e+04\n1200: Time taken: 0.3s, loss: 8.3e+04\n1300: Time taken: 0.34s, loss: 7.4e+04\n1400: Time taken: 0.36s, loss: 6.7e+04\n1500: Time taken: 0.38s, loss: 6.1e+04\n1600: Time taken: 0.4s, loss: 5.6e+04\n1700: Time taken: 0.42s, loss: 5.1e+04\n1800: Time taken: 0.44s, loss: 4.7e+04\n1900: Time taken: 0.47s, loss: 4.4e+04\n2000: Time taken: 0.5s, loss: 4.1e+04\n```\nAfter:\n```\n100: Time taken: 0.49s, loss: 1.2e+06\n200: Time taken: 0.031s, loss: 6.9e+05\n300: Time taken: 0.031s, loss: 4.6e+05\n400: Time taken: 0.031s, loss: 3.3e+05\n500: Time taken: 0.031s, loss: 2.6e+05\n600: Time taken: 0.031s, loss: 2.1e+05\n700: Time taken: 0.031s, loss: 1.7e+05\n800: Time taken: 0.031s, loss: 1.4e+05\n900: Time taken: 0.031s, loss: 1.2e+05\n1000: Time taken: 0.031s, loss: 1.1e+05\n1100: Time taken: 0.031s, loss: 9.6e+04\n1200: Time taken: 0.031s, loss: 8.6e+04\n1300: Time taken: 0.031s, loss: 7.7e+04\n1400: Time taken: 0.031s, loss: 7e+04\n1500: Time taken: 0.031s, loss: 6.3e+04\n1600: Time taken: 0.031s, loss: 5.8e+04\n1700: Time taken: 0.031s, loss: 5.3e+04\n1800: Time taken: 0.031s, loss: 4.9e+04\n1900: Time taken: 0.031s, loss: 4.5e+04\n2000: Time taken: 0.032s, loss: 4.2e+04\n\n```\nTesting w/ small graph to check for regression:\n```\nimport torch\nfrom torch.utils.benchmark import Timer\n\nsetup=\"\"\"\na = torch.rand((2, 2), requires_grad=True)\nb = torch.rand((2, 2), requires_grad=True)\ngradient = torch.ones(2, 2)\n\"\"\"\n\nstmt=\"\"\"\ntorch.autograd.grad(a*b, [a, b], gradient)\n\"\"\"\n\ntimer = Timer(stmt, setup)\n\nprint(timer.timeit(10000))\nprint(timer.collect_callgrind(100))\n```\nResult: there doesn't seem to be any significant regression\n```\nTime before: 12.74 us\nTime after: 13.12 us\nInstruction count before:\n                           All          Noisy symbols removed\n    Instructions:      8078960                    8000882\n    Baseline:             4226                       3838\nInstruction count after:\n                           All          Noisy symbols removed\n    Instructions:      8091846                    8017940\n    Baseline:             4336                       3838\n100 runs per measurement, 1 thread\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52180\n\nReviewed By: gchanan, zhangguanheng66\n\nDifferential Revision: D26794387\n\nPulled By: soulitzer\n\nfbshipit-source-id: c00d387a29f151109c33dc6f1b56a8f275cdec58", "pr_number": "52180", "files_changed": ["torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/function.h", "torch/csrc/autograd/functions/accumulate_grad.cpp", "torch/csrc/autograd/functions/basic_ops.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp"], "labels": ["Merged", "cla signed"]}, "6db2f012a5": {"title": "[PyTorch] Reduce size of register_symbols.cpp (#53278)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53278\n\nWe can avoid duplicating the string data for the namespaces\nby assembling qualified names ourselves as needed.\nghstack-source-id: 123111718\n\nTest Plan:\nCI\n\nbuildsizebot some iOS apps\n\nReviewed By: dhruvbird, walterddr, ot\n\nDifferential Revision: D26820648\n\nfbshipit-source-id: e2560874c54f46210181ddfee354967644bd41e1", "pr_number": "53278", "files_changed": ["aten/src/ATen/core/register_symbols.cpp"], "labels": ["Merged", "cla signed"]}, "a3c3141dd2": {"title": "Fix gradfn attr bindings when saved variable is of an output (#53205)", "body": "Summary:\nWhen saved variable is of an output, its grad_fn is not saved in SavedVariable, so it must be passed in during `unpack`.\nHere, we can always pass in grad_fn (whether or not saved variable is an output) because it is ignored if the saved variable is not an output.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53205\n\nReviewed By: gchanan, zhangguanheng66\n\nDifferential Revision: D26794365\n\nPulled By: soulitzer\n\nfbshipit-source-id: e039baba20c364c4ab42ff99d0b242dd95c67fb3", "pr_number": "53205", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_autograd_functions.py"], "labels": ["Merged", "cla signed"]}, "18277137ff": {"title": "make torch.load() aware of import path changes: torch.tensor -> torch._tensor (#53139)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53139\n\nghstack-source-id: 123090847\n\nTest Plan:\nSandcastle\n\nAlso explicitly tests that this test passes after incorporating the changes from D26656767, and adding a `torch.tensor` -> `torch._tensor` mapping to the `load_module_mapping` dict: `buck test mode/dev //pandora/utils/tests:manifold_utils_tests -- --exact 'pandora/utils/tests:manifold_utils_tests - test_load_dataset_valid_dir (pandora.utils.tests.manifold_utils_tests.TestManifoldUtils)'`\n\nWith just D26656767, that test fails. With D26656767 + the changes in this diff, that test passes.\n\nReviewed By: ezyang\n\nDifferential Revision: D26760600\n\nfbshipit-source-id: cb16493b858a358acf468d755740aa272ae9d363", "pr_number": "53139", "files_changed": ["torch/serialization.py"], "labels": ["Merged", "cla signed"]}, "36180c1322": {"title": "[static runtime] aten::to copy out variant (#52343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52343\n\naten::to returns self when the TensorOptions match and copy is set to false. For static runtime, always copy. There isn't a separate op for aten::to copy, but instead the same function\nwith different arguments.\n\nTest Plan:\nOn AdFinder local_ro:\n\nBefore:\n0.896742\n0.00824827 ms.    0.92773%. aten::to (5 nodes)\n\nAfter:\n0.88233\n0.0056607 ms.   0.644675%. aten::to (5 nodes)\n\nbuck test mode/opt caffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nReviewed By: hlu1\n\nDifferential Revision: D26477980\n\nfbshipit-source-id: 8e8448092adff38c141af1ce27a10acd39c07dd1", "pr_number": "52343", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "ac668c55e5": {"title": "[Static Runtime] Remove dead code in MemoryPlanner and rename unmanaged_value_set to unmanaged_ivalue_set", "body": "Test Plan:\n```\nbuck test mode/opt //caffe2/caffe2/fb/predictor:ptvsc2_predictor_bench_test -- --run-disabled\nbuck test //caffe2/benchmarks/static_runtime:static_runtime_cpptest\n```\n\nReviewed By: bwasti\n\nDifferential Revision: D26827700\n\nfbshipit-source-id: a8696af3e1d2b504fa5754f823b389d45b48af38", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp"], "labels": []}, "97d4ed3d2d": {"title": "[torch.futures] Add note about error handling for non-chained futures. (#53212)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53212\n\nRan into a strange issue with error handling in future callbacks, more\ndetails in https://github.com/pytorch/pytorch/issues/52132, but essentially,\nafter a callback throws all additional processing stops, and other futures can\nnever be completed, resulting in a hang. Add a note to warn about this.\nghstack-source-id: 123122890\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26793310\n\nfbshipit-source-id: b1ae73a81163d7b37ba07b0685e8de4228f01da6", "pr_number": "53212", "files_changed": ["torch/futures/__init__.py"], "labels": ["Merged", "cla signed"]}, "47dbdfcfe9": {"title": "[Static Runtime] remove redundant gather_ranges when fusing (#53323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53323\n\nWhilst optimizing inline cvr local ro, found a pattern where gather_ranges is used redundantly. Fuse this pattern to remove unnecessary gather_ranges.\n\nReviewed By: hlu1\n\nDifferential Revision: D26659824\n\nfbshipit-source-id: 6420afa3a2c3272c57706b70c2e9834014d6c32d", "pr_number": "53323", "files_changed": ["torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "1accffe450": {"title": "Revert D26819810: Revert D26815021: Revert D26744062: Add assert_async", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26819810\n\nOriginal commit changeset: e528260e1aa9\n\nfbshipit-source-id: 21567cab5c0ff5f5e60a699d4d4678773a567c30", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_cuda.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": []}, "17495e0318": {"title": "[PyTorch Mobile] Fix case when error messages are stripped, and stack value isn't popped off in lite-interpreter (#53201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53201\n\nThis resulted in [S22350](https://www.internalfb.com/intern/sevmanager/view/s/223540), which caused truoble on Android.\n\n1. The Python has a call to `warnings.warn()`, which resulted in code generated to emit the `WARN` instruction on lite-interpreter.\n2. The code for handling that instruction/op-code popped off the value in a call to the `TORCH_WARN()` *macro*.\n3. This macro conditionally compiled out evaluation of the arguments if `STRIP_ERROR_MESSAGES` was defined, which resulted in the stack not getting popped, and the lite-interpreter returning the last pushed value on to the stack.\n\nI've attempted to re-produce it using this python code: {P243842428}\nghstack-source-id: 122990001\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\nCreated a new unit test to re-produce the failure in the test. Was able to do so locally using the following command:\n\n```\nbuck test -c pt.strip_error_messages=1 //xplat/caffe2:test_s223540\n```\n\nHowever, since `pt.strip_error_messages=0` for dev and continuous builds, I have had to check in a separate contbuild config to try and trigger this failure on contbuild.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26765662\n\nfbshipit-source-id: 63c3c96d84ce6a9e5471f13d80165aa3718be9a2", "pr_number": "53201", "files_changed": ["torch/csrc/jit/mobile/interpreter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "795ed5ca3f": {"title": "Enable Kineto in CPU builds (#53174)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53174\n\nEnable Kineto also in the CPU builds (non-mobile, non-Windows(atm))\n\nTest Plan: CI\n\nReviewed By: gdankel\n\nDifferential Revision: D26776112\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 8733f65c2993105136c853f2a7b6e497d0fa53bf", "pr_number": "53174", "files_changed": ["cmake/Dependencies.cmake", "test/test_profiler.py"], "labels": ["Merged", "cla signed"]}, "fdd074e806": {"title": "[caffe2] Fix shape inference for Softmax (#53132)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53132\n\nInput and output should have the same shape for Softmax https://caffe2.ai/docs/operators-catalogue.html#softmax.\n\nReviewed By: walterddr, yinghai, ChunliF\n\nDifferential Revision: D26536592\n\nfbshipit-source-id: 8b50794803aeadcb75d8f370c77f4fef98a1f2ad", "pr_number": "53132", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "842ba90739": {"title": "[iOS] Bump up the Cocoapods version (#53335)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53335\n\nghstack-source-id: 123166245\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: xta0\n\nDifferential Revision: D26838693\n\nfbshipit-source-id: 0007eba40b3145c8ba77b3211759f0609e17f561", "pr_number": "53335", "files_changed": ["ios/LibTorch.podspec"], "labels": ["Merged", "cla signed"]}, "f1eedfa2c8": {"title": "[package] Add `allow_empty` flag to mock and extern (#53232)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53232\n\n**Summary**\nThis commit adds an optional `allow_empty` argument to\n`PackageExporter.mock` and `PackageExporter.extern` that allows certain\npatterns for mocked modules and extern modules to be marked ones that\n*must* be matched during the packaging process. If a mock or extern\nmodule with `allow_empty=False` is not matched while packaging, an error\nis thrown.\n\n**Test Plan**\nThis commit adds two new test cases to `PackagingTest`,\n`test_extern_glob_allow_empty` and `test_mock_glob_allow_empty` that\ntest this new flag. Existing tests already tests `allow_empty=True`.\n\n**Fixes**\nThis commit fixes #53217.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D26834011\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 9cf4ea56079ae210d6cfa8604218849eb5cde5f4", "pr_number": "53232", "files_changed": ["test/test_package.py", "torch/package/__init__.py", "torch/package/package_exporter.py"], "labels": ["Merged", "cla signed"]}, "51592a9e0a": {"title": "[package] Add `deny` method to `PackageExporter` (#53233)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53233\n\n**Summary**\nThis commit adds a `deny` method to `PackageExporter` that allows\nmodules to be prohibited during the packaging process. A dependency on a\nmodule matching the names or globs that `deny` was called with will\ncause an exception to be raised.\n\n**Test Plan**\nThis commit adds unit tests to `PackagingTest` for this new method:\n`test_deny` and `test_deny_glob`.\n\n**Fixes**\nThis commit fixes #53217.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D26834010\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 469b5c6741bcc6dab77e352f41db38fa1e0dae12", "pr_number": "53233", "files_changed": ["test/test_package.py", "torch/package/__init__.py", "torch/package/package_exporter.py"], "labels": ["Merged", "cla signed"]}, "cfd9360d09": {"title": "Revert D26837780: Revert D26819810: Revert D26815021: Revert D26744062: Add assert_async", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD26837780\n\nOriginal commit changeset: 21567cab5c0f\n\nfbshipit-source-id: 8ea735e5fdc97e32ae3fafd40297a1b8a7cd34b0", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_cuda.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": []}, "dfd5331e9c": {"title": "Skip tests on ROCm (#53339)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53339\n\nSkip tests on ROCm\n\nTest Plan: CI\n\nReviewed By: gdankel, ZolotukhinM\n\nDifferential Revision: D26838813\n\nfbshipit-source-id: e26286a61a192710e393c19d3eb2316b6c76a42e", "pr_number": "53339", "files_changed": ["test/test_profiler.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "35364c3641": {"title": "[static runtime] Enable ClipRangesGatherRangesX2SigridHash fusion for SigridHashPrecompute (#53324)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53324\n\nReviewed By: maratsubkhankulov\n\nDifferential Revision: D26833478\n\nfbshipit-source-id: 55ab63faf5b535f2acd2ec5dc5721f5b692832d7", "pr_number": "53324", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "69bb0e0285": {"title": "[caffe2] Avoid some double (and triple) lookups in workspace (#53319)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53319\n\nNoticed these in profiles.\n\nAlso switch to `unordered_map`.\n\nTest Plan: Unit tests.\n\nReviewed By: swolchok\n\nDifferential Revision: D26504408\n\nfbshipit-source-id: 9e14d55909a4af019058b8c27c67ee2348cd02a9", "pr_number": "53319", "files_changed": ["caffe2/core/workspace.cc", "caffe2/core/workspace.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "63e0e88ccc": {"title": "[PyPer] More at::empty -> at::detail::empty_cpu (#53333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53333\n\n- Add more variants to `create_empty_from` to take more args, like dtype/layout/device.\n- Clean up stray at::empty uses, mostly in the out variants.\n\nReviewed By: ajyu\n\nDifferential Revision: D26799900\n\nfbshipit-source-id: 6676d8043fead63208913ef3a28cabbae76e46bb", "pr_number": "53333", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "42e0983230": {"title": "[NNC] Added some APIs for dealing directly with Bufs (instead of Tensors) (#53011)", "body": "Summary:\n(also includes some python binding stuff :P)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53011\n\nReviewed By: gchanan, robieta\n\nDifferential Revision: D26801120\n\nPulled By: Chillee\n\nfbshipit-source-id: 42a1efb6cbc9ddc0b72b780f3d6b712b3ae62b09", "pr_number": "53011", "files_changed": ["torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/tensorexpr/tensor.h", "torch/csrc/jit/tensorexpr/tensorexpr_init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "5ebfabb310": {"title": "MAGMA: Initialize ipiv data to avoid internal memory access violation (#53064)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51930\n\nRunning the reproducer under `cuda-gdb`, I see access violations in either [`zswap_kernel_batched`](https://bitbucket.org/icl/magma/src/4fd4634f35020e315b05689c277899dbd09749ed/magmablas/zgetf2_kernels.cu?at=master#lines-276) (part of the LU factorization) and other times in [`zlaswp_columnserial_kernel`](https://bitbucket.org/icl/magma/src/4fd4634f35020e315b05689c277899dbd09749ed/magmablas/zlaswp_batched.cu?at=master#lines-335) (part of the inverse).\n\nThe common factor between both of these is they use `ipiv` to index into the matrix. My best guess is the `ipiv` indices aren't written when the factorization fails, hence garbage data is used as matrix indices and we get an access violation. Initializing `ipiv` to a known-good value before the  factorization fixes the issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53064\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26829053\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 842854a6ee182f20b2acad0d76d32d27cb51b061", "pr_number": "53064", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "test/test_linalg.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "56f8379802": {"title": "[static runtime] Move all heavy constructor logic into InferenceModule (renamed to StaticModule) (#51564)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51564\n\nConstructor logic was spread throughout InferenceModule and StaticRuntime.  This diff unifies the two.  After a lot of discussion on this diff D25961626 it became apparent that `clone` is uglier than a cheap StaticRuntime.\n\nThis means StaticRuntime is effectively StaticModule and the only code in the new StaticRuntime is the `run` functions.\n\n```\ngraph, schema = PrepareForStaticModule(torchscript_module)\nsm = StaticModule(graph, schema, options)\nsm(inputs)\n// or create many cheap runtimes with the module\nsr = StaticRuntime(sm)\nsr(inputs)\n```\n\nChangelist:\n- Rename InferenceModule StaticModule\n- Move all logic for construction into StaticModule\n- Create a new StaticRuntime that only has a unique memory planner (everything else is in StaticModule)\n- Update comments with explanation\n- Propagate all changes to predictor integration\n- Propagate all changes to python integration\n- Change semantics to be a bit more PyTorch-standard (no \"run\" calls, no \"get_\" getters).\n\nTest Plan:\nbuck test //caffe2/test:static_runtime\nbuck test caffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nReviewed By: hlu1\n\nDifferential Revision: D25592967\n\nfbshipit-source-id: 8233bed03137ce129137af2d44bce0095033ef0f", "pr_number": "51564", "files_changed": ["benchmarks/static_runtime/deep_wide_pt_bench.cc", "benchmarks/static_runtime/test_static_runtime.cc", "test/test_static_runtime.py", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/init.cpp", "torch/csrc/jit/runtime/static/init.h", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "1b35b1a0c4": {"title": "Properly skip distributed tests when distributed module is not built (#52945)", "body": "Summary:\nCurrently there is some code that intends to skip distributed tests if\nthe distributed module is not built. However, they are missing in some\ntest files; and in some other test files they are checked after\ndistributed module is imported, which leads to failure.  This is\ngenerating a lot of headaches when testing minimal builds locally.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52945\n\nReviewed By: anjali411\n\nDifferential Revision: D26848241\n\nPulled By: ezyang\n\nfbshipit-source-id: 983a848844add40869a86f3c9413503a3659b115", "pr_number": "52945", "files_changed": ["test/distributed/algorithms/ddp_comm_hooks/test_ddp_hooks.py", "test/distributed/nn/jit/test_instantiator.py", "test/distributed/optim/test_zero_redundancy_optimizer.py", "test/distributed/rpc/test_faulty_agent.py", "test/distributed/rpc/test_process_group_agent.py", "test/distributed/rpc/test_tensorpipe_agent.py", "test/distributed/test_c10d.py", "test/distributed/test_distributed_fork.py", "test/distributed/test_distributed_spawn.py", "test/distributed/test_jit_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit", "open source"]}, "68134374cb": {"title": "Refactor/fix DDP model check during init (#52887)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52887\n\nThis diff changes the way to do model consistency check (i.e. `_verify_replicas_across_processes`) in DDP.\n\nThere were a few things that could be improved with the way we verify model across processes in DDP initialization:\n\n1. We should do this check before syncing module states in DDP init, otherwise with Gloo backend this will throw but we would like to throw the error corresponding to different models on different ranks. To do this, we move the methods to be standalone C++ functions (not part of reducer) and move this check to before synchronizing parameters.\n2. Refactor DDP init in the following ways:\n- Run model consistency check before creating reducer, 2\n- add helper functions to build params to pass into reducer\n- add helper function to call `_verify_model_across_ranks`\n- move `def parameters` to a helper function `_get_parameters` to be used more broadly within DDP\n\nIn follow up changes we will add the ability to detect which rank had inconsistent model (https://github.com/pytorch/pytorch/issues/52876 would be useful for this to determine which ranks(s) had errors).\nghstack-source-id: 123171877\n\nTest Plan:\nCI/unittest\nbuck test mode/dev-nosan //caffe2/test/distributed:c10d\nBACKEND=\"nccl\" WORLD_SIZE=\"2\" ~/fbcode/buck-out/dev/gen/caffe2/test/distributed/distributed_nccl_fork#binary.par -r test_ddp_model_diff_across_ranks\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26565290\n\nfbshipit-source-id: f0e1709585b53730e86915e768448f5b8817a608", "pr_number": "52887", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/__init__.py", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "bdbfc2582d": {"title": "[Dist Debugality] Log key DDP metrics to stderr under debug mode. (#52957)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52957\n\nThis diff:\n1. Under TORCH_DISTRIBUTED_DEBUG=INFO or DETAIL, logs DDP information during init time (all stats in ddp_logging_data_)\n2. Under TORCH_DISTRIBUTED_DEBUG=DETAIL, logs runtime stats when they are collected (first 10 iterations and then once every 100 iterations). Avoiding logging every iteration to not spam logs.\n\nVerified by inspecting logs:\n\n```\nI0226 19:12:47.109243 2818475 logger.cpp:140] [Rank 1]: DDP Initialized with:\nworld_size: 2 module_name: Linear device_ids: 1 output_device: 1 backend_name: nccl parameter_dtype: float total\n_parameter_size_in_bytes: 40 num_parameter_tensors: 2 bucket_sizes: 40 CUDA_VISIBLE_DEVICES: N/Abroadcast_buffer\ns: 1 bucket_cap_mb: 25 find_unused_parameters: 0 gradient_as_bucket_view: 0\n Backend Info: nccl_socket_ifname: N/A nccl_blocking_wait: N/A nccl_debug: WARN nccl_nthreads: N/A nccl_ib_timeo\nut: N/A\nI0226 19:12:47.109252 2818473 logger.cpp:140] [Rank 0]: DDP Initialized with:\nworld_size: 2 module_name: Linear device_ids: 0 output_device: 0 backend_name: nccl parameter_dtype: float total\n_parameter_size_in_bytes: 40 num_parameter_tensors: 2 bucket_sizes: 40 CUDA_VISIBLE_DEVICES: N/Abroadcast_buffer\ns: 1 bucket_cap_mb: 25 find_unused_parameters: 0 gradient_as_bucket_view: 0\n Backend Info: nccl_socket_ifname: N/A nccl_blocking_wait: N/A nccl_debug: WARN nccl_nthreads: N/A nccl_ib_timeo\nut: N/A\n```\n\n```\nI0226 19:12:48.117936 2818473 logger.cpp:286] [Rank 0 / 2] Training Linear unused_parameter_size=0\n Avg forward compute time: 568944\n Avg backward compute time: 885504\nAvg backward comm. time: 692496\n Avg backward comm/comp overlap time: 113536\nI0226 19:12:48.118517 2818475 logger.cpp:286] [Rank 1 / 2] Training Linear unused_parameter_size=0\n Avg forward compute time: 565584\n Avg backward compute time: 876992\nAvg backward comm. time: 201872\n Avg backward comm/comp overlap time: 128624\n```\nghstack-source-id: 123171875\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26708184\n\nfbshipit-source-id: 16defd5610d28bc4cf3fc2a0cc564e84efcfa791", "pr_number": "52957", "files_changed": ["c10/util/Logging.h", "test/distributed/test_c10d.py", "torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/__init__.py", "torch/lib/c10d/Utils.cpp", "torch/lib/c10d/Utils.hpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5d9b7bee1a": {"title": "[DDP Logging] Log nccl_async_error_handling (#52965)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52965\n\nLogs nccl async error handling in ddp logger\nghstack-source-id: 123171876\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26709030\n\nfbshipit-source-id: 530456a5005b8e4956d7fb023986e9b948ebe1a8", "pr_number": "52965", "files_changed": ["c10/util/Logging.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "14fa47631b": {"title": "[DDP Logging] Log comm. hook in ddp logging (#52966)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52966\n\nLogs registerd comm hook if there is one, else logs\n\"builtin_allreduce\"\nghstack-source-id: 123174803\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26709388\n\nfbshipit-source-id: 484fdbbd6643ec261b3797bd8d9824b2b6a1a490", "pr_number": "52966", "files_changed": ["c10/util/Logging.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ba75cedfc5": {"title": "[1/n][torch/elastic][upstream] Move torchelastic/rendezvous to torch/distributed/rendezvous (#53172)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53172\n\nPull Request resolved: https://github.com/pytorch/elastic/pull/141\n\nUpstreams two modules to torch:\n\n1. `torchelastic.rendezvous`\n2. `torchelastic.utils`\n\nThese modules were chosen as `[1/n]` since they are the leaf modules in torchelastic.\n\n==== NOTES: ====\n1. I'm disabling etcd_rendezvous and etcd_server tests in CIRCLECI for the moment since I need to edit the test dockers to contain the etcd server binary (there's 4-5 test dockers - one for each platform so this is going to take some time for me to set up the environments and test) - T85992919.\n\n2. I've fixed all lint errors on python files but there are ones on the cpp files on the ZeusRendezvous. I took a look at them, and I don't want to fix the linter errors right now for 2 major reasons:\n     1. Some of them are more than formatting changes (e.g. std::move vs pass by value) and I don't want to introduce bundled changes with the move\n     1. The old rendezvous code (the one we forked from in caffe2/fb) has the same problems and I think its better for us to deal with this when we deprecate caffe2/fb/rendezvous in favor of the one in torchelastic -T86012579.\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/torch/distributed/elastic/utils/test/...\nbuck test mode/dev-nosan //caffe2/torch/distributed/elastic/utils/data/test/...\nbuck test mode/dev-nosan //caffe2/torch/distributed/elastic/rendezvous/test/...\nbuck test mode/dev-nosan //caffe2/torch/distributed/elastic/rendezvous/fb/...\nbuck test mode/dev-nosan //pytorch/elastic/torchelastic/...\n```\n\\+ Sandcastle\n\nReviewed By: H-Huang\n\nDifferential Revision: D26718746\n\nfbshipit-source-id: 67cc0350c3d847221cb3c3038f98f47915362f51", "pr_number": "53172", "files_changed": ["test/distributed/elastic/rendezvous/__init__.py", "test/distributed/elastic/rendezvous/api_test.py", "test/distributed/elastic/rendezvous/etcd_rendezvous_test.py", "test/distributed/elastic/rendezvous/etcd_server_test.py", "test/distributed/elastic/utils/__init__.py", "test/distributed/elastic/utils/data/__init__.py", "test/distributed/elastic/utils/data/cycling_iterator_test.py", "test/distributed/elastic/utils/distributed_test.py", "test/distributed/elastic/utils/logging_test.py", "test/distributed/elastic/utils/util_test.py", "torch/distributed/elastic/__init__.py", "torch/distributed/elastic/rendezvous/__init__.py", "torch/distributed/elastic/rendezvous/api.py", "torch/distributed/elastic/rendezvous/etcd_rendezvous.py", "torch/distributed/elastic/rendezvous/etcd_server.py", "torch/distributed/elastic/rendezvous/registry.py", "torch/distributed/elastic/rendezvous/utils.py", "torch/distributed/elastic/utils/__init__.py", "torch/distributed/elastic/utils/api.py", "torch/distributed/elastic/utils/data/__init__.py", "torch/distributed/elastic/utils/data/cycling_iterator.py", "torch/distributed/elastic/utils/data/elastic_distributed_sampler.py", "torch/distributed/elastic/utils/distributed.py", "torch/distributed/elastic/utils/logging.py", "torch/distributed/elastic/utils/store.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "efebc6524d": {"title": "Call nvidia-smi.exe before running tests Windows (#53334)", "body": "Summary:\nTo display the basic information about the GPUs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53334\n\nReviewed By: anjali411\n\nDifferential Revision: D26849826\n\nPulled By: ngimel\n\nfbshipit-source-id: 14f0d9dfe41a35fa45fdf6aa7bf2a41704887c0c", "pr_number": "53334", "files_changed": [".jenkins/pytorch/win-test.sh"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "00bd0e9862": {"title": "[caffe2] Fix shape inference for LpNorm (#53332)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53332\n\nThis is to make sure we don't get `BATCH` dim type for the output.\n\nReviewed By: ChunliF\n\nDifferential Revision: D26836902\n\nfbshipit-source-id: bedbd12330c608406e3466b240015235a28d2c4a", "pr_number": "53332", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "c0adabe172": {"title": "automate sharding using S3 test time stats (#53269)", "body": "Summary:\nUses nightly commit stats to automatically shard tests based on execution time.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53269\n\nTest Plan:\nset CIRCLE_JOB to an existing job, like `pytorch_linux_bionic_py3_6_clang9_test`\nThen you can run something like: `python test/run_test.py --shard 1 10`\n\nReviewed By: malfet\n\nDifferential Revision: D26819440\n\nPulled By: janeyx99\n\nfbshipit-source-id: 6bc73d6aa3d52d9850817536be15d7b54a72780e", "pr_number": "53269", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed"]}, "387d9a6bab": {"title": "Simplify async execution for script calls. (#53204)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53204\n\nAsync execution for script calls in request_callback_impl.cpp had two\nsimilar if-else blocks that were hard to read. This PR simplifies some of the\nlogic by breaking the logic into resuable components.\nghstack-source-id: 122996440\n\nTest Plan: waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D26788459\n\nfbshipit-source-id: f2818c6251a465936ed75b7bd356b616f0580094", "pr_number": "53204", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "f58f7b786c": {"title": "add distributed backend options in setup.py (#53214)", "body": "Summary:\nCurrently there's only one indicator for build_ext regarding distributed backend `USE_DISTRIBUTED`.\n\nHowever one can build with selective backends. adding the 3 distributed backend option in setup.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53214\n\nTest Plan: Set the 3 options in environment and locally ran `python setup.py build_ext`\n\nReviewed By: janeyx99\n\nDifferential Revision: D26818259\n\nPulled By: walterddr\n\nfbshipit-source-id: 688e8f83383d10ce23ee1f019be33557ce5cce07", "pr_number": "53214", "files_changed": ["setup.py"], "labels": ["Merged", "cla signed"]}, "f595ba1bae": {"title": "[caffe2] move the SaveOp implementation from a header to a .cc file (#53298)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53298\n\nThis is a re-land of D26641600 (https://github.com/pytorch/pytorch/commit/3969391c0732bd1e6329438664dba3d7f5ee74b7), but with the `SaveOpImpl` class marked as\n`TORCH_API` to ensure that its symbols get exported properly in shared library\nbuilds.\n\nThis moves the `SaveOp` code from `load_save_op.h` to `load_save_op.cc`.\n\nPreviously this implementation was all in the templatized `SaveOp` class, even\nthough most of the logic didn't depend on the template parameters.  Having\nthis code be in the header file slows down the build, and forces more files to\nbe rebuilt than necessary when changing the SaveOp code.  Having this code be\nin a template class can also increase the generated code size be larger than\nneeded, as we don't need separate copies instantiated for each context type.\nghstack-source-id: 123146018\n\nTest Plan:\nbuck test //caffe2/caffe2/python/operator_test:load_save_test\n\nAlso tested performing the CMake-based build using shared libraries with CUDA\nenabled, and confirmed that the build succeeded.\n\nReviewed By: mraway\n\nDifferential Revision: D26802576\n\nfbshipit-source-id: fc2dbdc1cd20680b082c887366a6305d86688138", "pr_number": "53298", "files_changed": ["caffe2/operators/load_save_op.cc", "caffe2/operators/load_save_op.h"], "labels": ["Merged", "cla signed"]}, "6cbbef2fea": {"title": "Modify assert order to correct the error message when nan appears in multinomial on cuda (#53288)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53288\n\nModify assert order to correct the error message when nan appears in multinomial on cuda\n\nTest Plan: unittest\n\nReviewed By: ngimel\n\nDifferential Revision: D26824353\n\nfbshipit-source-id: af6195e7c36fd51b3fc90df558ad6fac41288142", "pr_number": "53288", "files_changed": ["aten/src/ATen/native/cuda/MultinomialKernel.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "7bfa9dc7de": {"title": "Simplify async execution for script remote calls. (#53207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53207\n\nSimplifying some of the async execution logic in request_callback_impl\nas part of https://github.com/pytorch/pytorch/issues/39351.\nghstack-source-id: 123004020\n\nTest Plan: waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D26791325\n\nfbshipit-source-id: 790ad413dad410dbcd07787583674cb5af1d1c92", "pr_number": "53207", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "1974969842": {"title": "Cleanup async execution for python RPC calls. (#53230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53230\n\nAs part of https://github.com/pytorch/pytorch/issues/39351, cleaning\nup the python call async execution.\nghstack-source-id: 123120119\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26800758\n\nfbshipit-source-id: 50fe94c684bf53b907762e8bf196a6f6b97e4cf0", "pr_number": "53230", "files_changed": ["torch/csrc/distributed/rpc/request_callback_impl.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8c798e0622": {"title": "Forbid trailing whitespace (#53406)", "body": "Summary:\nContext: https://github.com/pytorch/pytorch/pull/53299#discussion_r587882857\n\nThese are the only hand-written parts of this diff:\n- the addition to `.github/workflows/lint.yml`\n- the file endings changed in these four files (to appease FB-internal land-blocking lints):\n  - `GLOSSARY.md`\n  - `aten/src/ATen/core/op_registration/README.md`\n  - `scripts/README.md`\n  - `torch/csrc/jit/codegen/fuser/README.md`\n\nThe rest was generated by running this command (on macOS):\n```\ngit grep -I -l ' $' -- . ':(exclude)**/contrib/**' ':(exclude)third_party' | xargs gsed -i 's/ *$//'\n```\n\nI looked over the auto-generated changes and didn't see anything that looked problematic.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53406\n\nTest Plan:\nThis run (after adding the lint but before removing existing trailing spaces) failed:\n- https://github.com/pytorch/pytorch/runs/2043032377\n\nThis run (on the tip of this PR) succeeded:\n- https://github.com/pytorch/pytorch/runs/2043296348\n\nReviewed By: walterddr, seemethere\n\nDifferential Revision: D26856620\n\nPulled By: samestep\n\nfbshipit-source-id: 3f0de7f7c2e4b0f1c089eac9b5085a58dd7e0d97", "pr_number": "53406", "files_changed": [".circleci/scripts/binary_ios_test.sh", ".github/workflows/lint.yml", ".jenkins/caffe2/bench.sh", "CONTRIBUTING.md", "GLOSSARY.md", "aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/CPUGeneratorImpl.cpp", "aten/src/ATen/SparseTensorUtils.h", "aten/src/ATen/Version.cpp", "aten/src/ATen/VmapTransforms.h", "aten/src/ATen/core/Generator.h", "aten/src/ATen/core/PhiloxRNGEngine.h", "aten/src/ATen/core/op_registration/README.md", "aten/src/ATen/core/type.cpp", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vsx/vec256_complex_double_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_complex_float_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_double_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_qint32_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_qint8_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_quint8_vsx.h", "aten/src/ATen/cuda/CUDAGeneratorImpl.cpp", "aten/src/ATen/native/AffineGridGenerator.cpp", "aten/src/ATen/native/AveragePool2d.cpp", "aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/ComplexHelper.h", "aten/src/ATen/native/Distributions.h", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/GatedLinearUnit.cpp", "aten/src/ATen/native/GridSampler.cpp", "aten/src/ATen/native/Im2Col.cpp", "aten/src/ATen/native/LossMulti.h", "aten/src/ATen/native/LossMultiLabelMargin.cpp", "aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/native/LossNLL2d.cpp", "aten/src/ATen/native/Pool.h", "aten/src/ATen/native/Pow.h", "aten/src/ATen/native/ReplicationPadding.cpp", "aten/src/ATen/native/StridedRandomAccessor.h", "aten/src/ATen/native/TensorTransformations.h", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cpu/SoftMaxKernel.cpp", "aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp", "aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp", "aten/src/ATen/native/cpu/batch_norm_kernel.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/CuFFTPlanCache.h", "aten/src/ATen/native/cuda/ForeachBinaryOpList.cu", "aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu", "aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu", "aten/src/ATen/native/cuda/ForeachPointwiseOp.cu", "aten/src/ATen/native/cuda/ForeachUnaryOp.cu", "aten/src/ATen/native/cuda/GridSampler.cuh", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/Loss.cu", "aten/src/ATen/native/cuda/MiscUtils.h", "aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/cuda/PowKernel.cu", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8conv/4x8-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8conv/8x8-aarch64-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/4x8-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/4x8-dq-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm/8x8-aarch64-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/4x4-packA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/4x8c1x4-dq-packedA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/4x8c8x1-dq-packedA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x4-packA-aarch32-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x4-packA-aarch64-neon.S", "aten/src/ATen/native/quantized/cpu/qnnpack/src/q8gemm_sparse/8x8c1x4-dq-packedA-aarch64-neon.S", "aten/src/ATen/native/sparse/SoftMax.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu", "aten/src/ATen/nnapi/codegen.py", "aten/src/ATen/nnapi/nnapi_bind.cpp", "aten/src/ATen/test/cpu_generator_test.cpp", "aten/src/ATen/test/cuda_generator_test.cu", "aten/src/TH/vector/VSX.cpp", "aten/src/TH/vector/simd.h", "aten/src/THC/THCAtomics.cuh", "benchmarks/cpp/tensorexpr/bench_reduce.cpp", "benchmarks/distributed/rpc/rl/agent.py", "benchmarks/distributed/rpc/rl/coordinator.py", "benchmarks/distributed/rpc/rl/launcher.py", "benchmarks/fastrnns/README.md", "benchmarks/operator_benchmark/c2/add_test.py", "benchmarks/operator_benchmark/c2/matmul_test.py", "benchmarks/operator_benchmark/common/tests/add_ops_list_test.py", "benchmarks/operator_benchmark/common/tests/c2_cpu_gpu_forward_backward_test.py", "benchmarks/operator_benchmark/common/tests/jit_forward_test.py", "benchmarks/operator_benchmark/common/tests/pt_backward_test.py", "benchmarks/operator_benchmark/common/tests/pt_configs_list_test.py", "benchmarks/operator_benchmark/common/tests/pt_cpu_gpu_forward_backward_test.py", "benchmarks/operator_benchmark/common/tests/random_sample_test.py", "benchmarks/sparse/matmul_dlmc_bench.py", "benchmarks/sparse/test.sh", "benchmarks/tensorexpr/benchmark.py", "binaries/lite_interpreter_model_load.cc", "binaries/make_mnist_db.cc", "c10/core/GeneratorImpl.cpp", "c10/test/util/complex_test_common.h", "c10/util/Bitset.h", "caffe2/core/common_test.cc", "caffe2/ideep/operators/sigmoid_op.cc", "caffe2/operators/accuracy_op.cu", "caffe2/operators/affine_channel_op.cu", "caffe2/operators/ceil_op.cu", "caffe2/operators/channel_shuffle_op.cu", "caffe2/operators/clip_op.cu", "caffe2/operators/cosine_embedding_criterion_op.cu", "caffe2/operators/elementwise_linear_op.cu", "caffe2/operators/hip/conv_op_miopen.hip", "caffe2/operators/hip/pool_op_miopen.hip", "caffe2/operators/multi_class_accuracy_op.cu", "caffe2/operators/top_k.cc", "caffe2/opt/optimizer.cc", "caffe2/python/onnx/tests/onnx_backend_test.py", "caffe2/python/operator_test/adam_test.py", "caffe2/python/serialized_test/README.md", "caffe2/release-notes.md", "caffe2/serialize/crc.cc", "caffe2/serialize/crc_alt.h", "caffe2/video/CMakeLists.txt", "cmake/Caffe2ConfigVersion.cmake.in", "cmake/External/nnpack.cmake", "cmake/GoogleTestPatch.cmake", "cmake/Modules/FindMAGMA.cmake", "cmake/Modules/FindVSX.cmake", "cmake/Modules/FindZMQ.cmake", "cmake/Modules/FindvecLib.cmake", "cmake/TorchConfig.cmake.in", "cmake/iOS.cmake", "codecov.yml", "docker/caffe2/jenkins/common/install_rocm.sh", "docker/caffe2/readme.md", "docs/caffe2/README.md", "docs/cpp/source/notes/tensor_cuda_stream.rst", "docs/source/backends.rst", "docs/source/cudnn_persistent_rnn.rst", "docs/source/cudnn_rnn_determinism.rst", "docs/source/data.rst", "docs/source/multiprocessing.rst", "docs/source/named_tensor.rst", "docs/source/notes/ddp.rst", "docs/source/notes/randomness.rst", "docs/source/notes/serialization.rst", "docs/source/notes/windows.rst", "docs/source/onnx.rst", "docs/source/optim.rst", "docs/source/pipeline.rst", "docs/source/rpc.rst", "docs/source/rpc/rref.rst", "docs/source/torch.nn.quantized.dynamic.rst", "docs/source/torch.quantization.rst", "ios/README.md", "ios/TestApp/bootstrap.sh", "scripts/README.md", "scripts/build_raspbian.sh", "scripts/build_tegra_x1.sh", "scripts/diagnose_protobuf.py", "scripts/xcode_build.rb", "test/cpp/api/dataloader.cpp", "test/cpp/api/functional.cpp", "test/cpp/api/nn_utils.cpp", "test/cpp/dist_autograd/test_dist_autograd.cpp", "test/distributed/test_c10d_spawn.py", "test/jit/test_isinstance.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_utility_funs.py", "test/run_test.py", "test/test_foreach.py", "test/test_ops.py", "test/test_sparse.py", "test/test_tensor_creation_ops.py", "tools/docker/Dockerfile_runtime", "torch/csrc/Generator.cpp", "torch/csrc/Module.cpp", "torch/csrc/api/include/torch/nn/functional/upsampling.h", "torch/csrc/api/include/torch/nn/modules/_functions.h", "torch/csrc/api/include/torch/nn/modules/activation.h", "torch/csrc/api/include/torch/nn/modules/adaptive.h", "torch/csrc/api/include/torch/nn/modules/container/moduledict.h", "torch/csrc/api/include/torch/nn/modules/container/parameterdict.h", "torch/csrc/api/include/torch/nn/modules/container/parameterlist.h", "torch/csrc/api/include/torch/nn/modules/linear.h", "torch/csrc/api/include/torch/nn/utils/rnn.h", "torch/csrc/api/src/data/samplers/distributed.cpp", "torch/csrc/api/src/nn/modules/_functions.cpp", "torch/csrc/api/src/nn/modules/adaptive.cpp", "torch/csrc/api/src/nn/modules/batchnorm.cpp", "torch/csrc/api/src/nn/modules/pooling.cpp", "torch/csrc/autograd/python_function.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/jit/codegen/fuser/README.md", "torch/csrc/utils/cuda_lazy_init.cpp", "torch/distributed/nn/functional.py", "torch/distributed/optim/optimizer.py", "torch/distributions/mixture_same_family.py", "torch/fx/graph.py", "torch/fx/graph_module.py", "torch/jit/_freeze.py", "torch/linalg/__init__.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/container.py", "torch/nn/modules/conv.py", "torch/nn/modules/dropout.py", "torch/nn/modules/flatten.py", "torch/nn/modules/lazy.py", "torch/nn/modules/module.py", "torch/nn/modules/normalization.py", "torch/nn/modules/padding.py", "torch/nn/modules/transformer.py", "torch/nn/parallel/scatter_gather.pyi", "torch/nn/parameter.pyi", "torch/onnx/__init__.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset13.py", "torch/optim/_multi_tensor/adadelta.py", "torch/optim/_multi_tensor/adam.py", "torch/optim/_multi_tensor/adamw.py", "torch/optim/_multi_tensor/rprop.py", "torch/optim/_multi_tensor/sgd.py", "torch/optim/sgd.py", "torch/optim/swa_utils.py", "torch/optim/swa_utils.pyi", "torch/package/_importlib.py", "torch/sparse/__init__.py", "torch/testing/_internal/common_cuda.py", "torch/testing/_internal/common_jit.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/jit_metaprogramming_utils.py", "torch/testing/_internal/print_test_stats.py", "torch/utils/benchmark/utils/sparse_fuzzer.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed"]}, "758fb94fcb": {"title": "Prefix assert_async with underscore, fix some bugs in assert_async CUDA testing (#53276)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53276\n\n- One of the tests had a syntax error (but the test\n  wasn't fine grained enough to catch this; any error\n  was a pass)\n- Doesn't work on ROCm\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: D26820048\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nPulled By: ezyang\n\nfbshipit-source-id: b02c4252d10191c3b1b78f141d008084dc860c45", "pr_number": "53276", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_cuda.py", "test/test_torch.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["Merged", "cla signed"]}, "369601355f": {"title": "[caffe2] Use extended versions of cuDNN calls for SpatialBN", "body": "Summary: Using `cudnnBatchNormalizationForwardTrainingEx` and `cudnnBatchNormalizationBackwardEx` if cuDNN version is greater than 8.0.0.\n\nReviewed By: xw285cornell\n\nDifferential Revision: D26794173\n\nfbshipit-source-id: dc4994375350f303a3fa0aee03255e8f8be1c605", "pr_number": null, "files_changed": ["caffe2/operators/spatial_batch_norm_op_cudnn.cu"], "labels": []}, "a0d1e701db": {"title": "Replace `internal::GRAIN_SIZE` by `grain_size` (parameter). (#53177)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53013\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53177\n\nReviewed By: SplitInfinity, nikithamalgifb\n\nDifferential Revision: D26860248\n\nPulled By: ngimel\n\nfbshipit-source-id: 56917f8421f7b45c461945fd3d1ff107ce8535b2", "pr_number": "53177", "files_changed": ["aten/src/ATen/TensorIterator.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "1fe6a6507e": {"title": "[WIP][FX] Fix tracing support for torchbind (#52884)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52884\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D26675801\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 8e5100bcea17589a53163abf6ab991658e11fa3a", "pr_number": "52884", "files_changed": ["test/cpp/jit/test_custom_class_registrations.cpp", "test/test_fx.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/python/script_init.cpp", "torch/fx/proxy.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "1ba80264f4": {"title": "[DataLoader] ConcatDataPipe (#53301)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53301\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D26829322\n\nPulled By: ejguan\n\nfbshipit-source-id: eeea42fd9ab267d10f39ad7debc279eaded23570", "pr_number": "53301", "files_changed": ["test/test_datapipe.py", "torch/utils/data/datapipes/iter/__init__.py", "torch/utils/data/datapipes/iter/combining.py"], "labels": ["Merged", "cla signed"]}, "dbbe0a2105": {"title": "[DataLoader] Introduce deterministic context (#53271)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53271\n\n- [x] Add `set_determinism` context manager\n- [x] Add `non_deterministic` decorator for `DataPipe`\n  - Raise error at the construction time for non-deterministic DataPipe when `determinism` is set to `True`\n - [ ] Support `non_deterministic` with option\n   - When `GreedyJoin` only contains one datapipe, it should still be deterministic.\n\nNote: Test is in the [PR](https://github.com/facebookexternal/torchdata/pull/15). As the main repo doesn't have non-deterministic DataPipe yet.\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D26823023\n\nPulled By: ejguan\n\nfbshipit-source-id: 51bb92fc3d18d1fc9536c1229363c536ad120876", "pr_number": "53271", "files_changed": ["torch/utils/data/__init__.py", "torch/utils/data/dataset.py", "torch/utils/data/decorator.py"], "labels": ["Merged", "cla signed"]}, "3236efa4de": {"title": "[Static Runtime] Call native resize_/resize_as_ as much as possible (#53425)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53425\n\nt.resize_ goes through the dispatcher. Replace with direct native calls\n- t.resize_/resize_as_ -> at::native::resize_/resize_as_\n- t.resize_({0}) -> fastResizeToZero(t)\n\nReviewed By: ajyu, edvgha\n\nDifferential Revision: D26836278\n\nfbshipit-source-id: d1a95240099a35f5ece0de2eea50620ba8054ee5", "pr_number": "53425", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "b26c0bb2b9": {"title": "[PyTorch Mobile] Allow skipping operator exists check when bytecode model is loaded (#52814)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52814\n\nCurrently, there is no way to load a model on a devvm (CPU) if that model has operators that the runtime doesn't support. This ends up happening (currently) for Metal GPU models, and potentially in the future for other backends that have backend-specific operators that don't have a registered implementation (even a dummy one) on CPU.\n\nThere are at least a couple reasons for why this is needed:\n\n1. We want to extract operator list directly from the bytecode (instead of looking it up from `mobile_info.json).\n2. We want to be able to trace the quantized operators that are invoked when loading the compressed weights for a model that has prepacked weights. xta0 root-caused this after husthyc discovered that there are untraced operators showing up when loading a Metal GPU model.\n\nIf we want to scale out to support different types of models, we absolutely need the ability to load a model on a devvm irrespective of what backend (device/etc...) it is targeted at.\n\nghstack-source-id: 123284366\n\nTest Plan: The next diff in this stack is using the newly introduced methods.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26656266\n\nfbshipit-source-id: eed9af2f7b55979e9c18b986b8c3b9a767153297", "pr_number": "52814", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "affdcce833": {"title": "Extract TensorPipeAgent's collectNames to be a standalone utility function (#53202)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53202\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D26791525\n\nPulled By: mrshenli\n\nfbshipit-source-id: 8234c4d0350a5cd61926dce4ecc9e918960d30d2", "pr_number": "53202", "files_changed": ["caffe2/CMakeLists.txt", "torch/csrc/distributed/rpc/agent_utils.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c7b1979b6b": {"title": "Use Store collect and verify names in all RPC agents (#53209)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53209\n\ncloses #40048\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D26791524\n\nPulled By: mrshenli\n\nfbshipit-source-id: fc75589f9707014334fcfae6f05af3c04217783b", "pr_number": "53209", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/rpc/test_e2e_process_group.cpp", "tools/build_variables.bzl", "torch/_C/_distributed_rpc.pyi", "torch/_C/_distributed_rpc_testing.pyi", "torch/csrc/distributed/rpc/agent_utils.cpp", "torch/csrc/distributed/rpc/agent_utils.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.cpp", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.h", "torch/csrc/distributed/rpc/testing/init.cpp", "torch/distributed/rpc/_testing/faulty_agent_backend_registry.py", "torch/distributed/rpc/backend_registry.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d3cde6c23c": {"title": "[NNC] Implementation for aten::cat without conditionals. (#53128)", "body": "Summary:\nThis PR adds an implementation for `aten::cat` in NNC without any conditionals. This version is not enabled by default.\n\nHere is the performance of some micro benchmarks with and without conditionals. There is up to 50% improvement in performance without conditionals for some of the shapes.\n\naten::cat implementation in NNC **with** conditionals\n```\n$ python -m benchmarks.tensorexpr --device cpu --mode fwd --jit_mode trace --cpu_fusion concat\npt: concat2d2input_fwd_cpu_1_160_1_14_1: 5.44 us, SOL 0.26 GB/s, algorithmic 0.51 GB/s\npt: concat2d2input_fwd_cpu_1_580_1_174_1: 5.75 us, SOL 1.05 GB/s, algorithmic 2.10 GB/s\npt: concat2d2input_fwd_cpu_20_160_20_14_1: 6.87 us, SOL 4.05 GB/s, algorithmic 8.11 GB/s\npt: concat2d2input_fwd_cpu_20_580_20_174_1: 14.52 us, SOL 8.31 GB/s, algorithmic 16.62 GB/s\npt: concat2d2input_fwd_cpu_8_512_8_512_1: 9.58 us, SOL 6.84 GB/s, algorithmic 13.68 GB/s\n```\naten::cat implementation in NNC **without** conditionals\n```\n$ python -m benchmarks.tensorexpr --device cpu --mode fwd --jit_mode trace --cpu_fusion --cat_wo_conditionals concat\npt: concat2d2input_fwd_cpu_1_160_1_14_1: 4.67 us, SOL 0.30 GB/s, algorithmic 0.60 GB/s\npt: concat2d2input_fwd_cpu_1_580_1_174_1: 5.65 us, SOL 1.07 GB/s, algorithmic 2.14 GB/s\npt: concat2d2input_fwd_cpu_20_160_20_14_1: 6.10 us, SOL 4.56 GB/s, algorithmic 9.12 GB/s\npt: concat2d2input_fwd_cpu_20_580_20_174_1: 7.44 us, SOL 16.22 GB/s, algorithmic 32.44 GB/s\npt: concat2d2input_fwd_cpu_8_512_8_512_1: 6.46 us, SOL 10.14 GB/s, algorithmic 20.29 GB/s\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53128\n\nReviewed By: bertmaher\n\nDifferential Revision: D26758613\n\nPulled By: navahgar\n\nfbshipit-source-id: 00f56b7da630b42bc6e7ddd4444bae0cf3a5780a", "pr_number": "53128", "files_changed": ["benchmarks/tensorexpr/__main__.py", "test/cpp/tensorexpr/test_kernel.cpp", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "54a2498919": {"title": "Modify tests to use assertWarnsOnceRegex instead of maybeWarnsRegex (#52387)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/50006\n\nFollow on for https://github.com/pytorch/pytorch/issues/48560 to ensure TORCH_WARN_ONCE warnings are caught. Most of this is straight-forward find-and-replace, but I did find one place where the TORCH_WARN_ONCE warning was not wrapped into a python warning.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52387\n\nReviewed By: albanD\n\nDifferential Revision: D26773387\n\nPulled By: mruberry\n\nfbshipit-source-id: 5be7efbc8ab4a32ec8437c9c45f3b6c3c328f5dd", "pr_number": "52387", "files_changed": ["test/test_binary_ufuncs.py", "test/test_jit.py", "test/test_linalg.py", "test/test_sparse.py", "test/test_spectral_ops.py", "test/test_tensor_creation_ops.py", "test/test_torch.py", "torch/csrc/Module.cpp", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "module: tests", "open source", "triaged"]}, "a5ada2127d": {"title": "[reland] Add OpInfo for `bitwise_not` and make ROCM and CUDA OpInfo tests consistent (#53181)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nThis PR also enables the OpInfo tests on ROCM to check the same dtypes that of CUDA.\n\nNote: Reland https://github.com/pytorch/pytorch/issues/51944\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53181\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26811466\n\nPulled By: mruberry\n\nfbshipit-source-id: 8434a7515d83ed859db1b2f916fad81a9deaeb9b", "pr_number": "53181", "files_changed": ["test/test_ops.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "open source"]}, "d54be1a946": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D26879724\n\nfbshipit-source-id: 0e2dd4c5f7ba96e97e7cbc078184aed2a034ad2c", "pr_number": null, "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp"], "labels": []}, "0ca029b22d": {"title": "[caffe2] Fix DBFileReader (#53498)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53498\n\nThis code depended on `Blobs()` being returned in sorted order:\n\nhttps://www.internalfb.com/intern/diffusion/FBS/browsefile/master/fbcode/caffe2/caffe2/python/db_file_reader.py?commit=472774e7f507e124392491800d9654e01269cbaf&lines=89-91\n\nBut D26504408 (https://github.com/pytorch/pytorch/commit/69bb0e028596d481cda38a0f49bcb5965fab4293) changed the underlying storage to a hashmap, so now the blobs are returned in arbitrary order (Note that `Blobs()` returns also non-local blobs, and for those there was already no guarantee of ordering).\n\nSo we need to explicitly sort the result.\n\nTest Plan:\n```\n$ buck test dper3/dper3/toolkit/tests:lime_test\n$ buck test //dper3/dper3/toolkit/tests:model_insight_test\n```\nPass after this diff.\n\nDifferential Revision: D26879502\n\nfbshipit-source-id: d76113f8780544af1d97ec0a818fb21cc767f2bf", "pr_number": "53498", "files_changed": ["caffe2/python/db_file_reader.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "c07a62b854": {"title": "[FX] change dynamic control flow example to a *more* dynamic version (#53250)", "body": "Summary:\nThis is a more fundamental example, as we may support some amount of shape specialization in the future.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53250\n\nReviewed By: navahgar\n\nDifferential Revision: D26841272\n\nPulled By: Chillee\n\nfbshipit-source-id: 027c719afafc03828a657e40859cbfbf135e05c9", "pr_number": "53250", "files_changed": ["docs/source/fx.rst"], "labels": ["Merged", "cla signed"]}, "656930df26": {"title": "[FX] Fix default to align with documentation in `fuser.py` (#53457)", "body": "Summary:\nCurrently it says it does a deepcopy by default, but that's not true.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53457\n\nReviewed By: navahgar\n\nDifferential Revision: D26876781\n\nPulled By: Chillee\n\nfbshipit-source-id: 26bcf76a0c7052d3577f217e79545480c9118a4e", "pr_number": "53457", "files_changed": ["torch/fx/experimental/fuser.py"], "labels": ["Merged", "cla signed", "fx"]}, "93f1b10f72": {"title": "Add missing attr in LazyModuleMixin doc (#53363)", "body": "Summary:\nTo fix some rendering issues.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53363\n\nReviewed By: izdeby\n\nDifferential Revision: D26884560\n\nPulled By: albanD\n\nfbshipit-source-id: fedc9c9972a6c68f311c6aafcbb33a3a881bbcd2", "pr_number": "53363", "files_changed": ["torch/nn/modules/lazy.py"], "labels": ["Merged", "cla signed"]}, "1e306b9a71": {"title": "Disable failing distributed test (#53527)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/53526. We're disabling the test temporarily until we can figure out what's going on (since it's unclear what needs to be reverted).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53527\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26888037\n\nPulled By: samestep\n\nfbshipit-source-id: f21a2d665c13181ed3c8815e352770b2f26cdb84", "pr_number": "53527", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "36dc5d3b3a": {"title": "[iOS GPU][BE][1/n] - Remove unused headers + improve error message (#53428)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53428\n\nStart to do some code clean up work.\nghstack-source-id: 123038070\n\nTest Plan:\n- CircleCI\n- Sandcastle CI\n- AIBench\n\nReviewed By: SS-JIA, AshkanAliabadi\n\nDifferential Revision: D26681115\n\nfbshipit-source-id: b1b7cfc6543b73928f517cd52e94a2664ee0bd21", "pr_number": "53428", "files_changed": ["aten/src/ATen/native/metal/MetalAten.mm", "aten/src/ATen/native/metal/MetalCommandBuffer.h", "aten/src/ATen/native/metal/MetalCommandBuffer.mm", "aten/src/ATen/native/metal/MetalConvolution.h", "aten/src/ATen/native/metal/MetalConvolution.mm", "aten/src/ATen/native/metal/MetalShaders.h", "aten/src/ATen/native/metal/MetalTensor.h", "aten/src/ATen/native/metal/MetalTensorImpl.h", "aten/src/ATen/native/metal/mpscnn/MPSCNN.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNContext.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNNeuronOp.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNOp.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/MPSImage+Tensor.h", "aten/src/ATen/native/metal/mpscnn/MPSImageWrapper.h", "aten/src/ATen/native/metal/mpscnn/MPSImageWrapper.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": ["Merged", "cla signed"]}, "1588df6b99": {"title": "Fix typo in tools/test_history.py (#53514)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53514\n\nTest Plan:\n```\ntools/test_history.py columns --ref=0ca029b22d17d236d34bcecad44b94b35b1af4bb test_common_errors pytorch_linux_xenial_cuda11_1_cudnn8_py3_gcc7_test1\n```\n\nReviewed By: janeyx99\n\nDifferential Revision: D26886385\n\nPulled By: samestep\n\nfbshipit-source-id: d3d79282e535707616d992ab8cf6216dfb777639", "pr_number": "53514", "files_changed": ["tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "1fc8831322": {"title": "Add missing tensor header (#53489)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53489\n\nIt appears that D26675801 (https://github.com/pytorch/pytorch/commit/1fe6a6507e6a38534f0dc6265fd44cd26c80e5b0) broke Glow builds (and probably other instals) with the inclusion of the python_arg_parser include. That dep lives in a directory of its own and was not included in the setup.py.\n\nTest Plan: OSS tests should catch this.\n\nReviewed By: ngimel\n\nDifferential Revision: D26878180\n\nfbshipit-source-id: 70981340226a9681bb9d5420db56abba75e7f0a5", "pr_number": "53489", "files_changed": ["setup.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "25a9f45a5a": {"title": "fix broken quantization_test in operator_benchmark (#53153)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53153\n\nThis diff is a fix for quantization_test in operator_benchmark, which is broken because of removing the py_module for learnable fake_quantization.\nghstack-source-id: 123103477\n\nTest Plan: `buck run mode/opt //caffe2/benchmarks/operator_benchmark/pt:quantization_test`\n\nReviewed By: z-a-f\n\nDifferential Revision: D26764881\n\nfbshipit-source-id: 8d40c6eb5e7090ca65f48982c837f7dc87d14378", "pr_number": "53153", "files_changed": ["benchmarks/operator_benchmark/pt/quantization_test.py"], "labels": ["Merged", "cla signed"]}, "98943bb863": {"title": "[PyTorch] Enable explicit ATen level sources for lite interpreter (#52769)", "body": "Summary:\nEnable partial explicit Aten level sources list for lite interpreter. More aten level source list will be added.\n\nx86:\n`SELECTED_OP_LIST=/Users/chenlai/Documents/pytorch/experiemnt/deeplabv3_scripted.yaml BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh x86`\n\nlibpytorch_jni_lite.so -- 3.8 MB\n\narmeabi-v7a\n`SELECTED_OP_LIST=/Users/chenlai/Documents/pytorch/experiemnt/deeplabv3_scripted.yaml BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh armeabi-v7a`\nlibpytorch_jni_lite.so -- 2.8 MB\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52769\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D26717268\n\nPulled By: cccclai\n\nfbshipit-source-id: 208300f198071bd6751f76ff4bc24c7c9312d337", "pr_number": "52769", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/TH/CMakeLists.txt", "caffe2/CMakeLists.txt", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed"]}, "b64acfa9ac": {"title": "[PyTorch] Move non-template part of TensorImpl::Resize to cpp (#53388)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53388\n\nMost of this method did not depend on the template parameter. No need to include it in the .h file or duplicate it in the generated code.\nghstack-source-id: 123211590\n\nTest Plan: Existing CI should cover this\n\nReviewed By: smessmer\n\nDifferential Revision: D26851985\n\nfbshipit-source-id: 115e00fa3fde547c4c0009f2679d4b1e9bdda5df", "pr_number": "53388", "files_changed": ["c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "b2758cdc77": {"title": "[PyTorch] Don't copy vector arguments to caffe2::Tensor::Resize (#53389)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53389\n\nResize was written to take arguments by value, which was\ntotally fine if they were ArrayRef or a series of integers, but not so\nfine if they're std::vector.\nghstack-source-id: 123212128\n\nTest Plan:\nExisting CI should make sure it builds\n\nInspected assembly for ios_caffe.cc and saw no more vector copy before\ncalling Resize\n\nReviewed By: smessmer\n\nDifferential Revision: D26852105\n\nfbshipit-source-id: 9c3b9549d50d32923b532bbc60d0246e2c2b5fc7", "pr_number": "53389", "files_changed": ["c10/core/TensorImpl.h", "caffe2/core/tensor.h"], "labels": ["Merged", "cla signed"]}, "7b7775bec2": {"title": "feature_segmented_histogram_binning_calibration", "body": "Summary: We implement a hierarchical fine grained binning structure, with the top level corresponding to different feature segments and bottom level corresponding to different range of ECTR. The model is designed to be general enough to perform segmented calibration on any useful feature\n\nTest Plan:\nbuck test dper3/dper3/modules/calibration/tests:calibration_test -- test_histogram_binning_calibration_by_feature\n\nbuck test dper3/dper3_models/ads_ranking/model_impl/mtml/tests:mtml_lib_test -- test_multi_label_dependent_task_with_histogram_binning_calibration_by_feature\n\ne2e test:\nbuck test dper3/dper3_models/ads_ranking/tests:model_paradigm_e2e_tests -- test_sparse_nn_histogram_binning_calibration_by_feature\n\nbuck test dper3/dper3_models/ads_ranking/tests:model_paradigm_e2e_tests -- test_mtml_with_dependent_task_histogram_binning_calibration_by_feature\n\nAll tests passed\n\nCanary packages:\nBackend -> aml.dper2.canary:e0cd05ac9b9e4797a94e930426d76d18\nFrontend -> ads_dper3.canary:55819413dd0f4aa1a47362e7869f6b1f\n\nTest FBL jobs:\n**SparseNN**\nctr mbl feed\nf255676727\n\ninline cvr\nf255677216\n\n**MTML regular task**\noffsite cvr\nf255676719\n\n**MTML dependent task**\nmobile cvr\nf255677551\n\n**DSNN for AI models**\nai oc\nf255730905\n\n**MIMO for both AI DSNN part and AF SNN part**\nmimo ig\nf255683062\n\nReviewed By: zhongyx12\n\nDifferential Revision: D25043060\n\nfbshipit-source-id: 8237cad41db66a09412beb301bc45231e1444d6b", "pr_number": null, "files_changed": ["caffe2/operators/batch_sparse_to_dense_op.cc"], "labels": []}, "64255294ba": {"title": "[PyTorch][CI] Enable building test_lite_interpreter_runtime unittest in CI (macos) (#52566)", "body": "Summary:\n## Summary\n\n1. Enable building libtorch (lite) in CI (macos)\n2. Run `test_lite_interpreter_runtime` unittest in CI (macos)\n\n![image](https://user-images.githubusercontent.com/16430979/110189039-b2b8ed00-7dd2-11eb-8fa1-be2d9e23792a.png)\n\n{F467163464}\n\n![image](https://user-images.githubusercontent.com/16430979/110189119-e3008b80-7dd2-11eb-9e80-7c2ae6862468.png)\n\n{F467164144}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52566\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D26601585\n\nPulled By: cccclai\n\nfbshipit-source-id: da7f47c906317ab3a4ef38fe2dbf2e89bc5bdb24", "pr_number": "52566", "files_changed": [".circleci/cimodel/data/simple/macos_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".jenkins/pytorch/macos-lite-interpreter-build-test.sh", "test/cpp/lite_interpreter_runtime/main.cpp"], "labels": ["Merged", "cla signed"]}, "e90e773445": {"title": "Fix to empty_like example (#53088)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52375\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53088\n\nReviewed By: zou3519\n\nDifferential Revision: D26752772\n\nPulled By: iramazanli\n\nfbshipit-source-id: 21e395c6bbfd8f2cc808ddc12aefb2a426bb50d0", "pr_number": "53088", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: docs"]}, "48ec939d39": {"title": "[iOS GPU][BE][2/n] - Use dispatcher in MPSCNNTests.mm (#53429)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53429\n\nCall the testing ops through dispatcher instead of calling them through `at::native`. Some metal ops can't be called through dispatcher yet. For example, `at::t` will call `at::as_strided` which hasn't been implemented on metal yet. For those ops, we'll skip and call `mpscnn::`directly. We'll convert those ops once we have implemented the missing ops.\nghstack-source-id: 123038068\n\nTest Plan:\n- Sandcastle CI\n- Circle CI\n- AIBench/Mobilelab\n\nReviewed By: SS-JIA, AshkanAliabadi\n\nDifferential Revision: D26683366\n\nfbshipit-source-id: bf130b191046f5d9ac9b544d512bc6cb94f08c09", "pr_number": "53429", "files_changed": ["aten/src/ATen/native/metal/MetalAten.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": ["Merged", "cla signed"]}, "c72473fe2c": {"title": "Adding print_test_stats.py job to Windows CI (#53387)", "body": "Summary:\nThis way, we can get S3 test time stats for windows tests as well.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53387\n\nReviewed By: samestep\n\nDifferential Revision: D26893613\n\nPulled By: janeyx99\n\nfbshipit-source-id: ac59e4406e472c9004eea0aae8a87a23242e3b34", "pr_number": "53387", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/pytorch/win-test-helpers/test_libtorch.bat"], "labels": ["Merged", "cla signed"]}, "067ad31210": {"title": "[NNC] Added some more external function bindings (#53420)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53420\n\nReviewed By: navahgar\n\nDifferential Revision: D26876784\n\nPulled By: Chillee\n\nfbshipit-source-id: 05e7c782a72de5159879f88a104f1a273e0345eb", "pr_number": "53420", "files_changed": ["test/cpp/tensorexpr/test_external_calls.cpp", "torch/csrc/jit/tensorexpr/external_functions.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7c0a4e78ca": {"title": "[static runtime] convert to->to_copy (#53524)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53524\n\nAdd to->to_copy in the ReplaceWithCopy pass for playing well with\nAliasDb\n\nTest Plan:\nRun bench with CastedBatchOneHot fusion off\n(https://www.internalfb.com/intern/diff/view-version/123230476/),\non adindexer and adfinder models\n\nReviewed By: hlu1\n\nDifferential Revision: D26887050\n\nfbshipit-source-id: 3f2fb9e27783bcdeb91c8b4181575f059317aff1", "pr_number": "53524", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "d0b32156f0": {"title": "move test to CUDA only (#53561)", "body": "Summary:\nHelps make master green by removing this hefty memory allocating from CPU test.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53561\n\nReviewed By: malfet, albanD\n\nDifferential Revision: D26897941\n\nPulled By: janeyx99\n\nfbshipit-source-id: 9f6c2d55f4eea1ab48665f7819fc113f21991036", "pr_number": "53561", "files_changed": ["test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "b0984f7925": {"title": "[pytorch] use correct warning type for tracer warnings (#53460)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53460\n\nWe have code to ignore this category of warnings and found this one is incorrect.\n\nUse `stacklevel=2`, otherwise the warning is always filtered by TracerWarning.ignore_lib_warnings()\n\nTest Plan: sandcastle\n\nReviewed By: wanchaol\n\nDifferential Revision: D26867290\n\nfbshipit-source-id: cda1bc74a28d5965d52387d5ea2c4dcd1a2b1e86", "pr_number": "53460", "files_changed": ["test/jit/test_tracer.py", "torch/tensor.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "53c77e7d5d": {"title": "Add mock.patch() to clear environment for test (#53537)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53537\n\nfixes #53526\n\nThis fixes the issue of the one of the environment variables being tested is somehow set by a previous test. For example:\n`WORLD_SIZE=1 python test/distributed/test_c10d.py RendezvousEnvTest.test_common_errors` would have previously failed but now passes\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D26891207\n\nPulled By: H-Huang\n\nfbshipit-source-id: 1c23f6fba60ca01085a634afbafbb31ad693d3ce", "pr_number": "53537", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a947bfaa26": {"title": "[Pytorch] Remove assumption forward exists in freeze_module (#52918)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52918\n\nFreeze_module seems to operate under the assumption that forward always exists. This isnt true, so the change first checks for existence then retrieves the function.\nghstack-source-id: 123215242\n\nTest Plan: Try freezing something with and without forward.\n\nReviewed By: dhruvbird\n\nDifferential Revision: D26671815\n\nfbshipit-source-id: d4140dad3c59d3d20012143175f9b9268bf23050", "pr_number": "52918", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "97460d3545": {"title": "[static runtime] Minimum fusion group size (#50217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50217\n\nIf we fuse small groups, things are slow\n\nTest Plan: buck test //caffe2/test:static_runtime\n\nReviewed By: bertmaher\n\nDifferential Revision: D25643460\n\nfbshipit-source-id: d2f39a4d612df3e1e29362abb23c2d997202f6ea", "pr_number": "50217", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/fusion.h", "torch/csrc/jit/runtime/static/init.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "17d00319bc": {"title": "Install GCC-9 into ROCm builders (#53459)", "body": "Summary:\nShould prevent intermittent internal compiler errors reported in https://bugs.launchpad.net/ubuntu/+source/gcc-7/+bug/1917830\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53459\n\nReviewed By: izdeby\n\nDifferential Revision: D26870602\n\nPulled By: malfet\n\nfbshipit-source-id: 1e90bb0d33736d01a696f80fc981aedcf7e3b639", "pr_number": "53459", "files_changed": [".circleci/docker/build.sh", ".circleci/docker/ubuntu-rocm/Dockerfile"], "labels": ["Merged", "cla signed"]}, "5b52ff6c8e": {"title": "[fx] Add DCE pass (#52658)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52658\n\nDCE will reverse iterate over the graph looking for nodes without users and delete them. It will skip over unused placeholders (since this affects the signature of the method) and outputs (which never have users but we want to keep them :) )\n\nTest Plan: Added unit tests\n\nReviewed By: jamesr66a, khabinov, chenccfb\n\nDifferential Revision: D26602212\n\nfbshipit-source-id: f4f196973e40546076636090bb0008c24f33795e", "pr_number": "52658", "files_changed": ["test/fx/test_dce_pass.py", "test/test_fx.py", "torch/fx/graph.py", "torch/fx/node.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "d521fd799d": {"title": "[FX Acc] Add support for multi partitions in fx-glow (#53280)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53280\n\nAdd supports for handling multiple partitions in fx_glow e2e flow.\n\nTest Plan: `buck test glow/fb/fx/fx_glow:test_fx_glow`\n\nReviewed By: gcatron\n\nDifferential Revision: D26819886\n\nfbshipit-source-id: b31aa4612aab3aee694bb155571ba6f5e75c27ba", "pr_number": "53280", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "dc29604fd1": {"title": "[iOS GPU][BE][3/n] - Rename MetalTensor to MetalTensorImplStorage (#53430)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53430\n\nThe definition of Metal tensor is confusing, as we're using it to initialize the MetalTensorImpl. It acts more like a TensorImplStorage.\nghstack-source-id: 123038073\n\nTest Plan:\n1. Sandcastle CI\n2. Circle CI\n3. AIBench/Mobilelab\n\nReviewed By: SS-JIA\n\nDifferential Revision: D26685439\n\nfbshipit-source-id: e0487d0884e4efc3044d627ed0e4af454eca9d67", "pr_number": "53430", "files_changed": ["aten/src/ATen/native/metal/MetalAten.mm", "aten/src/ATen/native/metal/MetalTensor.h", "aten/src/ATen/native/metal/MetalTensor.mm", "aten/src/ATen/native/metal/MetalTensorImpl.h", "aten/src/ATen/native/metal/MetalTensorImplStorage.h", "aten/src/ATen/native/metal/MetalTensorImplStorage.mm", "aten/src/ATen/native/metal/MetalUtils.h", "aten/src/ATen/native/metal/MetalUtils.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/MPSImage+Tensor.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": ["Merged", "cla signed"]}, "2dffb4e38e": {"title": "[Static Runtime] Back out D26659824 (#53570)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53570\n\nReviewed By: allwu\n\nDifferential Revision: D26899099\n\nfbshipit-source-id: 87c6d74a91c102e6b0487f9e6f49394755792a94", "pr_number": "53570", "files_changed": ["torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "aa687bb6f4": {"title": "[iOS GPU][BE][4/n] - Convert Objective-C class methods to C functions (#53431)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53431\n\nObjective-C\u2019s dynamism comes at the cost of code size, perf and safety. In Facebook, we tend to not use Objective-C primitives or keep it to a minimum unless you need them.\nghstack-source-id: 123063340\n\nTest Plan:\n1. CircleCI\n2. SandCastleCI\n3. Mobilelab\n\nReviewed By: SS-JIA\n\nDifferential Revision: D26800753\n\nfbshipit-source-id: b5a752a700d72ca3654f6826537aa3af47e87ecd", "pr_number": "53431", "files_changed": ["aten/src/ATen/native/metal/MetalUtils.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/MPSImage+Tensor.h", "aten/src/ATen/native/metal/mpscnn/MPSImage+Tensor.mm", "aten/src/ATen/native/metal/mpscnn/MPSImageUtils.h", "aten/src/ATen/native/metal/mpscnn/MPSImageUtils.mm", "aten/src/ATen/native/metal/mpscnn/MPSImageWrapper.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm"], "labels": ["Merged", "cla signed"]}, "cb36e503d8": {"title": "[iOS GPU][BE][5/n] Remove indirection calls from MPSCNNOps.mm and MetalAten.mm (#53432)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53432\n\n1. Creating individual .mm files for each op under the ops/ folder, and each op just has it's own function. The op is registered at the end of the file.\n2. Remove the indirection calls from MetalAten.mm to MPSCNNOps.mm\n3. Delete MPSCNNOps.mm\nghstack-source-id: 123205443\n\nTest Plan:\n1. Sandcastle\n2. CircleCI\n3. Mobilelab\n\nReviewed By: SS-JIA\n\nDifferential Revision: D26840953\n\nfbshipit-source-id: e1664c8d7445fdbd3b016c4dd51de0a6294af3a5", "pr_number": "53432", "files_changed": ["aten/src/ATen/native/metal/MetalAten.mm", "aten/src/ATen/native/metal/MetalConvParams.h", "aten/src/ATen/native/metal/MetalConvParams.mm", "aten/src/ATen/native/metal/MetalConvolution.h", "aten/src/ATen/native/metal/MetalConvolution.mm", "aten/src/ATen/native/metal/MetalNeuronType.h", "aten/src/ATen/native/metal/MetalPrepackOpRegister.cpp", "aten/src/ATen/native/metal/mpscnn/MPSCNN.h", "aten/src/ATen/native/metal/mpscnn/MPSCNN.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNClampOp.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNConvOp.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNConvOp.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNNeuronOp.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNNeuronOp.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNOps.mm", "aten/src/ATen/native/metal/mpscnn/MPSCNNUtils.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNUtils.mm", "aten/src/ATen/native/metal/mpscnn/MPSImageUtils.mm", "aten/src/ATen/native/metal/mpscnn/MPSImageWrapper.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm", "aten/src/ATen/native/metal/ops/MetalAddmm.mm", "aten/src/ATen/native/metal/ops/MetalBinaryElementwise.mm", "aten/src/ATen/native/metal/ops/MetalClamp.mm", "aten/src/ATen/native/metal/ops/MetalConcat.mm", "aten/src/ATen/native/metal/ops/MetalConvolution.h", "aten/src/ATen/native/metal/ops/MetalConvolution.mm", "aten/src/ATen/native/metal/ops/MetalCopy.h", "aten/src/ATen/native/metal/ops/MetalCopy.mm", "aten/src/ATen/native/metal/ops/MetalHardswish.mm", "aten/src/ATen/native/metal/ops/MetalNeurons.mm", "aten/src/ATen/native/metal/ops/MetalPooling.mm", "aten/src/ATen/native/metal/ops/MetalReshape.mm", "aten/src/ATen/native/metal/ops/MetalSoftmax.mm", "aten/src/ATen/native/metal/ops/MetalTranspose.h", "aten/src/ATen/native/metal/ops/MetalTranspose.mm", "aten/src/ATen/native/metal/ops/MetalUpsamplingNearest.mm"], "labels": ["Merged", "cla signed"]}, "c2ccb3578e": {"title": "Fix inport -> import typo in documentation (#53589)", "body": "Summary:\nFixes a small documentation typo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53589\n\nReviewed By: ngimel\n\nDifferential Revision: D26907045\n\nPulled By: Chillee\n\nfbshipit-source-id: 15c35bec8d75dd897fe8886d0e0e1b889df65b24", "pr_number": "53589", "files_changed": ["docs/source/notes/randomness.rst"], "labels": ["Merged", "cla signed", "open source"]}, "34d9278c19": {"title": "Remove notion of \"level\" from `Module::dump_to_str`. (#52539)", "body": "Summary:\nThe code uses `torch::jit::jit_log_prefix` for handling recursive\nindenting in most places in this function. There was one place that was\nusing \"level\", but it was buggy -- it would result in a compounding\nsuperlinear indent. Note that changing it to \"level+1\" doesn't fix the\nbug.\n\nBefore/after:\nhttps://gist.github.com/silvasean/8ee3ef115a48de6c9c54fbc40838d8d7\n\nThe new code establishes a recursive invariant for\n`Module::dump_to_str`: the function returns the module printed at the\nbase indent level (i.e. no indent). `torch::jit:log_prefix` is used\nto prefix recursive calls. The code was already nearly there, except for\nthis spurious use of \"level\".\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52539\n\nReviewed By: navahgar\n\nDifferential Revision: D26773657\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: ab476f0738bf07de9f40d168dd038dbf62a9a79e", "pr_number": "52539", "files_changed": ["torch/csrc/jit/api/module.cpp", "torch/csrc/jit/api/module.h", "torch/csrc/jit/python/script_init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source", "triaged"]}, "0a97712326": {"title": "[caffe2] don't use static for template declarations in headers (#53602)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53602\n\nUsing static in headers causes code bloat. Remove the unnecessary `static` qualifiers.\n\nTest Plan: sandcastle\n\nReviewed By: asp2insp\n\nDifferential Revision: D26886180\n\nfbshipit-source-id: 6008bce0d47f06d3146ce998234574a607c99311", "pr_number": "53602", "files_changed": ["aten/src/ATen/core/boxing/impl/boxing.h", "aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "f4b344ad5c": {"title": "Definition infrastructure for instruction count ubenchmarks (#53293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53293\n\nInstruction count benchmarks need some includes for IValues, but this is also just generally useful. (Unlike Python where you can just drop imports anywhere, C++ will get very upset if you `#include` in a function body...)\n\nTest Plan: Imported from OSS\n\nReviewed By: Chillee\n\nDifferential Revision: D26906684\n\nPulled By: robieta\n\nfbshipit-source-id: cbdfd79d3b8383100ff2e6857b6f309c387cbe2a", "pr_number": "53293", "files_changed": ["test/benchmark_utils/test_benchmark_utils.py", "torch/utils/benchmark/utils/_stubs.py", "torch/utils/benchmark/utils/common.py", "torch/utils/benchmark/utils/cpp_jit.py", "torch/utils/benchmark/utils/timeit_template.cpp", "torch/utils/benchmark/utils/timer.py", "torch/utils/benchmark/utils/valgrind_wrapper/timer_callgrind_template.cpp", "torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py"], "labels": ["Merged", "cla signed"]}, "9df1b98bab": {"title": "Quality of life improvements to Timer (#53294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53294\n\nJust a bunch of little things, none of which are big enough to need a full PR.\n\n1) C++ wall time should release the GIL\n2) Add option to retain `callgrind.out` contents. This will allow processing with kCachegrind for more detailed analysis.\n3) Stop subtracting the baseline instruction counts. (People just found it confusing when they saw negative instruction counts.) There is a finesse in #53295 that drops the baseline to ~800 instructions for `number=100`, and at that level it's not worth correcting.\n4) Add a `__mul__` overload to function counts. e.g. suppose `c0` was run with `number=100`, and `c1` was run with `number=200`, then `c0 * 2 - c1` is needed to properly diff them. (Obviously there are correctness concerns, but I think it's fine as a caveat emptor convenience method.)\n5) Tweak the `callgrind_annotate` call, since by default it filters very small counts.\n6) Move some args to kwargs only since types could be ambiguous otherwise.\n7) Don't omit rows from slices. It was annoying to print something like `stats[:25]` and have `__repr__` hide the lines in the middle.\n\nTest Plan: Imported from OSS\n\nReviewed By: Chillee\n\nDifferential Revision: D26906715\n\nPulled By: robieta\n\nfbshipit-source-id: 53d5cd92cd17212ec013f89d48ac8678ba6e6228", "pr_number": "53294", "files_changed": ["test/benchmark_utils/test_benchmark_utils.py", "torch/utils/benchmark/utils/timeit_template.cpp", "torch/utils/benchmark/utils/timer.py", "torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py"], "labels": ["Merged", "cla signed"]}, "2d36b30a8c": {"title": "Expands OpInfo out= testing (#53259)", "body": "Summary:\nAddresses several of the challenges described in https://github.com/pytorch/pytorch/issues/49468.\n\nThis PR builds on https://github.com/pytorch/pytorch/pull/50741 and https://github.com/pytorch/pytorch/issues/53105 to extend OpInfo out= testing. It covers the following cases for ops that produce a single tensor:\n\n- out= values don't affect computation\n- out= noncontiguous produces the correct output and preserves strides\n- out= with the wrong shape throws a warning\n- out= with an empty tensor throws no warning\n- out= with the wrong device throws an error\n- out= with a dtype the computation's result can't be \"safely\" cast to throws an error\n\nIt works with operations that produce a single tensor and operations that produce an iterable of tensors (the latter is tested with operations like torch.svd).\n\nIn addition to the new out= test, the OpInfos have been updated. \"supports_tensor_out\" is replaced with the more general and straightforward \"supports_out\" metadata, and many operations which previously had to skip out= testing with an explicit SkipInfo no longer need to. A couple redundant tests in test_unary_ufuncs.py have been removed, too.\n\nOne other perk of these tests is that once all operations have OpInfos this will allow us to validate that we've universally deprecated incorrectly sized tensors passed to out=, and give us the option to actually disable the behavior.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53259\n\nReviewed By: mrshenli\n\nDifferential Revision: D26894723\n\nPulled By: mruberry\n\nfbshipit-source-id: 2b536e9baf126f36386a35f2f806dd88c58690b3", "pr_number": "53259", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "28d6e01511": {"title": "Add TORCH_CHECK_NOT_IMPLEMENTED/c10::NotImplementedError; make dispatch use it (#53377)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53377\n\nMy underlying goal is I want to make the test suite ignore\nNotImplementedError without failing when bringing up a backend (meta)\nthat doesn't have very many functions implemented.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D26850766\n\nPulled By: ezyang\n\nfbshipit-source-id: ffbdecd22b06b5ac23e1997723a6e2a71dfcd14a", "pr_number": "53377", "files_changed": ["aten/src/ATen/core/dispatch/OperatorEntry.cpp", "c10/util/Exception.h", "torch/csrc/Exceptions.h"], "labels": ["Merged", "cla signed"]}, "707fc354eb": {"title": "Add debug only layout assert for empty_cpu (#53396)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53396\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26891806\n\nPulled By: ezyang\n\nfbshipit-source-id: 4789ab5587d1a11d50e9a60bbfa1c21c1222823e", "pr_number": "53396", "files_changed": ["aten/src/ATen/Utils.cpp"], "labels": ["Merged", "cla signed"]}, "02c0c7a32b": {"title": "Add Meta support for empty_strided (#53397)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53397\n\nIt turns out once you remove all the indirection from the\nempty_cpu_strided implementation, this implementation is pretty\nsimple.  We should see if we can simplify empty_cpu this way too.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D26891870\n\nPulled By: ezyang\n\nfbshipit-source-id: 9bddd332d32d8bf32fa3175e3bb0ac3a8954ac91", "pr_number": "53397", "files_changed": ["aten/src/ATen/native/MetaTensor.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "2f91cda37e": {"title": "Modify error message (#53525)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53518\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53525\n\nReviewed By: mthrok\n\nDifferential Revision: D26900045\n\nPulled By: carolineechen\n\nfbshipit-source-id: 387301381603d37d24cc829c8fed38123f268c0b", "pr_number": "53525", "files_changed": ["c10/core/CPUAllocator.cpp"], "labels": ["Merged", "cla signed"]}, "7e6a84d238": {"title": "Add logic to auto-fetch submodules (#53461)", "body": "Summary:\nIn setup.py add logic to:\n - Get list of submodules from .gitmodules file\n - Auto-fetch submodules if none of them has been fetched\n\nIn CI:\n - Test this on non-docker capable OSes (Windows and Mac)\n - Use shallow submodule checkouts whenever possible\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53461\n\nReviewed By: ezyang\n\nDifferential Revision: D26871119\n\nPulled By: malfet\n\nfbshipit-source-id: 8b23d6a4fcf04446eac11446e0113819476ef6ea", "pr_number": "53461", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/pytorch/macos-build.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", "setup.py"], "labels": ["Merged", "cla signed"]}, "9f2aea7b88": {"title": "[Pytorch] Fix embedding bag bug accessing unaligned memory (#53300)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53300\n\nFloat scale and bias are packed as per row parameters at the end of each row.\nThis takes 8 bytes. However if the number of elements in row are such that end\nof row address is unaligned for float, not multiply of 4 bytes, we will get\nunaglined memory access.\n\nCurrent solution is inefficient, so this should really be fixed at weight\npacking time.\nIt seems that longer term there will be prepack function that packs weights. So\nthis fallback path should eventually match that and not store scale and bias\ninline.\n\nTest Plan: python test/test_quantization.py\n\nReviewed By: pengtxiafb\n\nDifferential Revision: D26828077\n\nfbshipit-source-id: 8512cd95f3ac3ca53e1048139a9f6e19aa8af298", "pr_number": "53300", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "ef3765b992": {"title": "Fix a cuda max_pool3d issue, do multiplication in int64 (#52828)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/52822\n\n- [x] benchmark\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52828\n\nReviewed By: mrshenli\n\nDifferential Revision: D26866674\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: bd8276dd70316a767dc6e1991c1259f1f0b390b2", "pr_number": "52828", "files_changed": ["aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b0afe945a7": {"title": "Fix pylint error torch.tensor is not callable (#53424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53424\n\nFixes https://github.com/pytorch/pytorch/issues/24807 and supersedes the stale https://github.com/pytorch/pytorch/issues/25093 (Cc Microsheep). If you now run the reproduction\n\n```python\nimport torch\n\nif __name__ == \"__main__\":\n    t = torch.tensor([1, 2, 3], dtype=torch.float64)\n```\n\nwith `pylint==2.6.0`, you get the following output\n\n```\ntest_pylint.py:1:0: C0114: Missing module docstring (missing-module-docstring)\ntest_pylint.py:4:8: E1101: Module 'torch' has no 'tensor' member; maybe 'Tensor'? (no-\nmember)\ntest_pylint.py:4:38: E1101: Module 'torch' has no 'float64' member (no-member)\n```\n\nNow `pylint` doesn't recognize `torch.tensor` at all, but it is promoted in the stub. Given that it also doesn't recognize `torch.float64`, I think fixing this is out of scope of this PR.\n\n ---\n\n## TL;DR\n\nThis BC-breaking only for users that rely on unintended behavior. Since `torch/__init__.py` loaded `torch/tensor.py` it was populated in `sys.modules`. `torch/__init__.py` then overwrote `torch.tensor` with the actual function. With this `import torch.tensor as tensor` does not fail, but returns the function rather than the module. Users that rely on this import need to change it to `from torch import tensor`.\n\nReviewed By: zou3519\n\nDifferential Revision: D26223815\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 125b9ff3d276e84a645cd7521e8d6160b1ca1c21", "pr_number": "53424", "files_changed": ["test/test_jit.py", "torch/__init__.py", "torch/_tensor.py", "torch/autograd/anomaly_mode.py", "torch/csrc/autograd/init.cpp", "torch/functional.py", "torch/serialization.py", "torch/tensor.py"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "bcd94e220d": {"title": "[PyTorch] Fix typo in QNNPACK", "body": "Summary: Build failed when `PYTORCH_QNNPACK_RUNTIME_QUANTIZATION` is unset. According to D21339044 (https://github.com/pytorch/pytorch/commit/622f5b68f08eda6dd7eb6210494ea164d04ab399) it seems like a typo.\n\nTest Plan: buck build //xplat/caffe2/aten/src/ATen/native/quantized/cpu/qnnpack:pytorch_qnnpackWindows xplat/mode/windows-msvc-15.9\n\nReviewed By: kimishpatel\n\nDifferential Revision: D26907439\n\nfbshipit-source-id: ac52eeef4ee70726f2a97b22ae65921b39aa0c0b", "pr_number": null, "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/fully-connected.c"], "labels": []}, "e87a686d21": {"title": ".circleci: Remove hardcoded tag for rocm (#53636)", "body": "Summary:\nWe shouldn't need the hardcoding anymore\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53636\n\nReviewed By: malfet\n\nDifferential Revision: D26921067\n\nPulled By: seemethere\n\nfbshipit-source-id: 1e3ba4bbef4c5c6c6a6bcc2f137fef017cec3bb7", "pr_number": "53636", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed", "module: ci", "module: docker", "module: rocm"]}, "efb1895f81": {"title": "[caffe2] use snprintf() instead of sprintf() in the Checkpoint operator (#53434)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53434\n\nUse `snprintf()` to avoid buffer overflows.\nAlso only throw an exception on error, instead of crashing the entire\napplication.  A failure can occur if the caller supplies an invalid format\nstring.\nghstack-source-id: 123401582\n\nTest Plan:\nRan the checkpoint tests:\n\n  buck test caffe2/caffe2/python/operator_test:checkpoint_test\n\nVerified that the checkpoint file names logged in the output are the same\nbefore and after this change.\n\nI also tested manually changed the initial buffer size to 1 to confirm that\nthe code works when the initial buffer size is too small.  I considered\nupdating the checkpoint_test.py code to test using long db names that would\nexceed this limit, but I figured that long filenames was likely to cause\nother problems on some platforms (Windows has a maximum path length of 260\ncharacters up until pretty recent releases).\n\nDifferential Revision: D26863355\n\nfbshipit-source-id: 8fc24faa2a8dd145471067718d323fdc8ce055d6", "pr_number": "53434", "files_changed": ["caffe2/operators/load_save_op.h"], "labels": ["Merged", "cla signed"]}, "c0c5f80f36": {"title": "Lazy Modules Documentation Clarifications (#53495)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53366\n\ngchanan albanD\nThanks for the feedback. Did a first pass trying to address the concerns in the original issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53495\n\nReviewed By: mrshenli\n\nDifferential Revision: D26914768\n\nPulled By: albanD\n\nfbshipit-source-id: fa049f1952ef05598f0da2abead9a5a5d3602f75", "pr_number": "53495", "files_changed": ["torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/lazy.py", "torch/nn/modules/linear.py"], "labels": ["Merged", "cla signed", "open source"]}, "a9e4bb56e5": {"title": "Add more kernel launch checks (#53286)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53286\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D26818164\n\nfbshipit-source-id: 01ba50dc7e4a863e26c289d746bc5b95aa76d3cc", "pr_number": "53286", "files_changed": ["aten/src/ATen/native/cuda/DepthwiseConv3d.cu", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/THC/generic/THCTensorTopK.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "a8ecf306da": {"title": "[Static Runtime] Remove dead code (#53588)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53588\n\nRemove `SRViewOperatorRegistry` and related code now that it's no longer needed.\n\nReviewed By: swolchok\n\nDifferential Revision: D26901367\n\nfbshipit-source-id: fa73501cd785d4b89466cda81481aea892f8241f", "pr_number": "53588", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "f8e7d8bb0d": {"title": "[FX][docs] Render inherited methods in fx.Tracer API reference (#53630)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53630\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D26918962\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 2c84e308889d4ba3176018c7bd44a841e715e6c8", "pr_number": "53630", "files_changed": ["docs/source/fx.rst"], "labels": ["Merged", "cla signed"]}, "8acb74c405": {"title": "[PyTorch] Make IValue::toTensor() inlineable (#53213)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53213\n\nThe failure path for toTensor() is fairly long because it has to stringify tagKind() and construct a std::string. Forcibly outlining it should allow inlining the happy path.\nghstack-source-id: 123012703\n\nTest Plan:\n1) Compare perf profile on AdIndexer benchmark before/after --\ntoTensor frames no longer show up, demonstrating inlining\n2) Compare perf stat results on AdIndexer benchmark before/after:\n\nBefore:\n```\n         17,104.66 msec task-clock                #    0.999 CPUs utilized            ( +-  0.26% )\n             3,666      context-switches          #    0.214 K/sec                    ( +- 18.53% )\n                 3      cpu-migrations            #    0.000 K/sec                    ( +-  6.25% )\n           102,745      page-faults               #    0.006 M/sec                    ( +-  0.47% )\n    33,860,604,938      cycles                    #    1.980 GHz                      ( +-  0.25% )  (50.02%)\n    69,514,752,652      instructions              #    2.05  insn per cycle           ( +-  0.06% )  (50.01%)\n    11,280,877,966      branches                  #  659.521 M/sec                    ( +-  0.11% )  (50.01%)\n        75,739,099      branch-misses             #    0.67% of all branches          ( +-  0.98% )  (50.03%)\n\n           # Table of individual measurements:\n           17.2467 (+0.1172) #\n           17.0014 (-0.1280) #\n           17.2134 (+0.0840) #\n           17.0951 (-0.0343) #\n           17.0905 (-0.0389) #\n\n           # Final result:\n           17.1294 +- 0.0447 seconds time elapsed  ( +-  0.26% )\n```\nAfter:\n```\n         16,910.66 msec task-clock                #    0.999 CPUs utilized            ( +-  0.27% )\n             3,495      context-switches          #    0.207 K/sec                    ( +- 18.34% )\n                 3      cpu-migrations            #    0.000 K/sec                    ( +-  6.25% )\n           101,769      page-faults               #    0.006 M/sec                    ( +-  0.45% )\n    33,460,776,952      cycles                    #    1.979 GHz                      ( +-  0.28% )  (50.03%)\n    69,243,346,925      instructions              #    2.07  insn per cycle           ( +-  0.17% )  (50.02%)\n    11,229,930,860      branches                  #  664.074 M/sec                    ( +-  0.14% )  (50.03%)\n        72,273,324      branch-misses             #    0.64% of all branches          ( +-  0.55% )  (50.03%)\n\n           # Table of individual measurements:\n           16.9530 (+0.0246) #\n           17.0898 (+0.1614) #\n           16.8493 (-0.0791) #\n           16.8282 (-0.1002) #\n           16.9217 (-0.0067) #\n\n           # Final result:\n           16.9284 +- 0.0464 seconds time elapsed  ( +-  0.27% )\n```\n\n1.1% cycles win, 0.38% instructions win, both apparently outside noise level\n\nReviewed By: smessmer\n\nDifferential Revision: D26793481\n\nfbshipit-source-id: b035b3ad20f9e22ae738d91163641031b1130ce6", "pr_number": "53213", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "0606057af3": {"title": "[PyTorch] Add c10::MaybeOwned and Tensor::expect_contiguous (#53317)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53317\n\nThis seems like it might help in cases where we have to call\n`Tensor::contiguous`, but we expect that the tensor in question will\nbe contiguous a good portion of the time.\nghstack-source-id: 123203771\n\nTest Plan:\nProfiled AdIndexer on inline_cvr; time spent in\nclip_ranges_gather_sigrid_hash_each_feature<int> was cut in half from\n1.37% to 0.66%\n\nReviewed By: smessmer\n\nDifferential Revision: D26738036\n\nfbshipit-source-id: b5db10783ccd103dae0ab3e79338a83b5e507ebb", "pr_number": "53317", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "c10/test/util/MaybeOwned_test.cpp", "c10/util/MaybeOwned.h"], "labels": ["Merged", "cla signed"]}, "a3465214ba": {"title": "move rnn cell size check to cpp (#51964)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/32193.\n\nPossible further improvements:\n- do the same for quantized cells\n- reuse newly written functions in https://github.com/pytorch/pytorch/blob/56034636b9f37b54dacfab8794e93ba6b10bdfc7/torch/csrc/api/src/nn/modules/rnn.cpp#L699-L715\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51964\n\nReviewed By: albanD\n\nDifferential Revision: D26757050\n\nPulled By: ngimel\n\nfbshipit-source-id: 9c917d9124de2b914ad9915c79af675ae561295a", "pr_number": "51964", "files_changed": ["aten/src/ATen/native/RNN.cpp", "test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": ["Merged", "cla signed", "module: nn", "open source", "triaged"]}, "233b9490c2": {"title": "fix channels_last bug in upsample kernels (#53535)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53535\n\nDuring the port to structured kernels for upsample kernels, I missed that a subset of them explicitly pass `memory_format` information from the input to the output tensors.\n\nNote 1:\nI added the logic into the `meta` function of each op, which feels morally correct since this logic affects the output shape/metadata. One consequence is that all backend implementations will get the logic. I synced with fmassa that this seems reasonable.\n\nNote 2:\nThis logic used to happen in the following operators, which this PR fixes:\n- upsample_nearest3d\n- upsample_trilinear3d\n- upsample_nearest2d\n- upsample_bilinear2d\n\nI explicitly didn't patch the other upsample kernels, which look like they never forwarded memory_format information:\n- `upsample_bicubic2d` (maybe this should though? `UpSampleBicubic2d.cpp` isn't currently written to do anything different for `channels_last` tensors)\n- All of the `upsample_{mode}1d` operators. Probably because, afaik, channels_last isn't supported for 3d tensors\n- The corresponding backwards operator for every upsample op.\n\nNote 3:\nI'm also wondering why memory_format isn't just directly a part of the `tensor::options()` method, which would cause all ops to universally forward memory_format information from input to output tensors, rather than just the upsample ops. My guess is:\n- BC-breakage. I'm not sure whether this would really *break* people, but it's an API change\n- performance. `tensor::options()` is called everywhere, and adding a call to `suggest_memory_format()` would probably noticeably hit microbenchmarks. We could probably deal with that by making `memory_format` a precomputed field on the tensor?\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D26891540\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b3845f4dd5646b88bf738b9e41fe829be6b0e5cf", "pr_number": "53535", "files_changed": ["aten/src/ATen/native/UpSampleBilinear2d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "b9c3edd583": {"title": "Remove hacky wrapper from a lot of unary operators. (#52276)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52276\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr, nikithamalgifb\n\nDifferential Revision: D26732399\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 4189594e938c9908a4ea98a0b29d75a494d0dc35", "pr_number": "52276", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "5dca8ff6de": {"title": "[FX] Make TracerBase._find_user_frame private (#53654)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53654\n\nTest Plan: Imported from OSS\n\nReviewed By: suo, Chillee\n\nDifferential Revision: D26924950\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 23e641bbcabff148c18db0edeff0a12c10b8c42d", "pr_number": "53654", "files_changed": ["torch/fx/proxy.py"], "labels": ["Merged", "cla signed", "fx"]}, "409a76f72c": {"title": "[Static Runtime] Fix bug in static_runtime::to_copy (#53634)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53634\n\nMake the op signature of `static_runtime::to_copy` consistent with that of native_functions.yaml so it works with 2-5 args:\n```\n- func: to.dtype(Tensor self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -> Tensor\n  variants: method\n  device_guard: False\n```\n\n(Note: this ignores all push blocking failures!)\n\nReviewed By: ajyu\n\nDifferential Revision: D26906726\n\nfbshipit-source-id: b9203eb23619aba42b1bfed1a077401f9fe2ddf0", "pr_number": "53634", "files_changed": ["torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "0257eddc16": {"title": "Editing pass on native/README.md updates (#53638)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53638\n\nMostly slight edits, and deleting some outdated sections.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D26920600\n\nPulled By: ezyang\n\nfbshipit-source-id: e3bda80ecb622a1fcfde64e4752ba89a71056340", "pr_number": "53638", "files_changed": ["aten/src/ATen/native/README.md"], "labels": ["Merged", "cla signed"]}, "c3f8d57c70": {"title": "use DimVector for sizes and strides (#53001)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53001\n\nTest Plan: Imported from OSS\n\nReviewed By: swolchok\n\nDifferential Revision: D26719508\n\nPulled By: bhosmer\n\nfbshipit-source-id: 4053d632e11b2de1576c59c5a6b881a195d6206b", "pr_number": "53001", "files_changed": ["aten/src/ATen/InferSize.h", "aten/src/ATen/TensorUtils.cpp", "aten/src/ATen/TensorUtils.h", "aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed"]}, "3b0e4a6ed4": {"title": "[GraphModule] Improve buffer registration during init (#53444)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53444\n\nGraphModule construction has two options when constructing the base nn.Module: a dict of names to attrs to assign to the GraphModule, or another nn.Module to copy attrs from.\n\n- For the dict case, add logic to explicitly register `nn.Tensors` that are not `nn.Parameter` as buffers on the GraphModule, else fall back to `__setattr__`.\n- For the other `nn.Module` case, update so that it checks in the other module whether the attr to copy in is a buffer, and register it as such, else fall back to `__setattr__`.\n\nTest Plan: Added tests for fetching params and buffers from a GraphModule using both dict and module `__init__`s\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26860055\n\nfbshipit-source-id: 8d9999f91fef20aaa10969558006fc356247591f", "pr_number": "53444", "files_changed": ["test/test_fx.py", "test/test_fx_experimental.py", "torch/fx/graph_module.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "215950e2be": {"title": "Convert type annotations in nn/functional.py to py3 syntax (#53656)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53656\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D26926018\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 2381583cf93c9c9d0c9eeaa6e41eddce3729942d", "pr_number": "53656", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed"]}, "c5cd993add": {"title": "Adds a bool is_available() method to the backend contract (#53068)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53068\n\nAdds a ```bool is_available()``` method to the backend contract: it returns ```true``` if ```compile()``` and ```execute()``` can be called; ```false``` otherwise.\n\nIt is used to implement the following changes in the ```LoweredModule```:\n* ```compile()``` in ```__setstate__``` will run if ```is_available()```, else ```__setstate__``` throws an exception (\u201cBackend not available.\u201d).\n* ```compile()``` at ```LoweredModule``` creation will run if ```is_available()```, else a WARNING will be thrown.\n* ```execute()``` will only be executed if ```is_available()``` returns true; else throws an exception (\u201cBackend not available.\u201d).\n\nThe goal of these changes is to ensure we have a well defined behaviour for the different combinations of backend availability on-host and on-target.\n\nMore specifically, backends may have different capabilities to compile and/or execute the Module, depending whether this happens on-host (i.e. where the program is being written) or on-target (where the program is being executed).\n\nFirst of all, we know that \"preprocess\" always takes place, and that only happens on-host at creation time. So, we can assume that any compilation is needed/possible on-host then all of it could be pushed here.\n\nOverall, we want to ensure the following:\n\n**On host**\n\n| compile | execute | Outcome |\n| -- | -- | -- |\n| No | No | On module creation, LoweredModule is generated, with a warning  (since compilation and execution can still take place on-target). On module load, throws an exception (since execution is not possible). |\n| No | Yes | This configuration should not be possible. This assumes the full compiler is not available, even if some work was done in preprocess the program cannot be finalized for execution. |\n| Yes | No | In this case, the expectation would be for is_available() to return false, and compilation logic to move into preprocess. |\n| Yes | Yes | All good. This is the only case that is_available() should return true. |\n\n**On target**\n\n| compile | execute | Outcome |\n| -- | -- | -- |\n| No | No | Loading the LoweredModule throws an exception. Since execution is not possible. |\n| No | Yes | Basically this is another instance of Yes/Yes: compilation per se may not be possible on device, which means compile() can be called without issue but it is a no-op, and thus is_available should return true. Consequently, loading the LoweredModule: Succeeds, if the preprocessed module is ready for execution. Fails with exception otherwise. |\n| Yes | No | This configuration should not be possible. Just putting here for completeness. |\n| Yes | Yes | All good. This, along with No/Yes case (because compilation is assumed to have happened on-host, so it's just another instance of Yes/Yes), are the cases where is_available() should return true. |\n\n**Refactoring existing code**\nThis change also updates other backends (Glow) code, to implement the is_available() method to have the same behaviour as before this change (i.e. always available).\n\nThis should not cause backward incompatibilities with already saved models since we're adding a new method to the PyTorchBackendInterface.\nModels saved with the old interface that didn't have is_available() will still find the other 2 methods in the bound object (i.e. compile and execute), and the saved LoweredModule logic will be the old one.\n\n**Future**\nWe plan to use is_available() to implement support for fallback to the PyTorch interpreter.\nghstack-source-id: 123498571\n\nTest Plan: Added C++ (test_backend.cpp) and Python (test_backends.py) tests to validate the exceptions.\n\nReviewed By: jackm321, spaugh, iseeyuan\n\nDifferential Revision: D26615833\n\nfbshipit-source-id: 562e8b11db25784348b5f86bbc4179aedf15e0d3", "pr_number": "53068", "files_changed": ["test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_compiler_lib.cpp", "test/cpp/jit/test_backend_lib.cpp", "test/custom_backend/custom_backend.h", "test/jit/test_backends.py", "torch/csrc/jit/backends/backend.h", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_interface.h", "torch/csrc/jit/backends/backend_resolver.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6aa5148df2": {"title": "Filter 0's returned by exponential distribution (#53480)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/48841 for half datatype (it was fixed for other datatypes before).\nThe reason for https://github.com/pytorch/pytorch/issues/48841 happening for half was that `exponential_` for half was producing 0s.\nExponential distribution implementation on cuda is here https://github.com/pytorch/pytorch/blob/e08aae261397b8da3e71024bbeddfe0487185d1d/aten/src/ATen/native/cuda/DistributionTemplates.h#L535-L545\nwith `transformation::exponential` defined here\nhttps://github.com/pytorch/pytorch/blob/e08aae261397b8da3e71024bbeddfe0487185d1d/aten/src/ATen/core/TransformationHelper.h#L113-L123\nIt takes a uniformly distributed random number and takes `log` of it. If necessary, the result is then converted to low precision datatype (half). To avoid 0's, before applying `log`,  ones are replaced with std::nextafter(1,0). This seems fine, because log(1-eps) is still representable in half precision (`torch.tensor([1.], device=\"cuda\").nextafter(torch.tensor([0.], device=\"cuda\")).log().half()` produces 5.96e-8) , so casting to `scalar_t` should work. However, since fast log approximation is used (`__logf`), the log result is ~3e-9 instead of more accurate 5.96e-8, and underflows when casting to half. Using `::log` instead of fast approximation fixes it, however, it comes with ~20% perf penalty on exponential kernel for fp32 datatype, probably more for half.\n\nEdit: alternative approach used now is to filter all small values returned by transformation. The result is equivalent to squashing of 1's to 1-eps that was used before, and computing correct log of 1-eps (which is -eps, exactly equal even for doubles). This doesn't incur noticeable performance hit.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53480\n\nReviewed By: mruberry\n\nDifferential Revision: D26924622\n\nPulled By: ngimel\n\nfbshipit-source-id: dc1329e4773bf91f26af23c8afa0ae845cfb0937", "pr_number": "53480", "files_changed": ["aten/src/ATen/core/TransformationHelper.h", "aten/src/ATen/native/cuda/DistributionTemplates.h", "test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "039402b945": {"title": "If distributed module isn't available, don't run distributed/pipeline tests (#53547)", "body": "Summary:\nFollowing up https://github.com/pytorch/pytorch/issues/52945\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53547\n\nReviewed By: mrshenli\n\nDifferential Revision: D26946364\n\nPulled By: ezyang\n\nfbshipit-source-id: 9f93e76e2420d19b46d4eb3429eac5f263fd5c23", "pr_number": "53547", "files_changed": ["test/distributed/pipeline/sync/conftest.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "e787872a47": {"title": "[RELAND] Deduplicate shared params before constructing Reducer in DDP (#53279)", "body": "Summary:\nOriginal PR https://github.com/pytorch/pytorch/pull/51929 seemed to trigger failures in `pytorch_linux_xenial_py3_clang5_asan_test2`. Resubmitting to figure out why, and hopefully reland.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53279\n\nReviewed By: mrshenli\n\nDifferential Revision: D26916701\n\nPulled By: zhaojuanmao\n\nfbshipit-source-id: 75c74c8ad8ad24154eb59eddb2b222da0a09897e", "pr_number": "53279", "files_changed": ["test/distributed/test_c10d.py", "torch/lib/c10d/reducer.cpp", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "4dbd0b639d": {"title": "Convert a few more checks to raise NotImplementedError (#53610)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53610\n\nI noticed these because I was running the test suite under\nmeta device and triggered these error checks without getting\na NotImplementedError.  Well, now they raise.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D26918376\n\nPulled By: ezyang\n\nfbshipit-source-id: 20d57417aa64875d43460fce58af11dd33eb4a23", "pr_number": "53610", "files_changed": ["c10/core/TensorOptions.h", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed"]}, "6e020a4844": {"title": "Fix inaccurate dispatch table for fill_ (#53611)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53611\n\nfill_ now uses DispatchStub which means it only works for\nCPU/CUDA.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D26918374\n\nPulled By: ezyang\n\nfbshipit-source-id: fc899c28f02121e7719b596235cc47a0f3da3aea", "pr_number": "53611", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "70733f2e67": {"title": "Marginally improve pytest collection for top-level test files (#53617)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53617\n\nI'm trying to make `pytest test/*.py` work--right now, it fails during\ntest collection.  This removes a few of the easier to fix pytest\ncollection problems one way or another.  I have two remaining problems\nwhich is that the default dtype is trashed on entry to test_torch.py and\ntest_cuda.py, I'll try to fix those in a follow up.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D26918377\n\nPulled By: ezyang\n\nfbshipit-source-id: 42069786882657e1e3ee974acb3ec48115f16210", "pr_number": "53617", "files_changed": ["test/test_cpp_extensions_aot.py", "test/test_openmp.py", "test/test_overrides.py"], "labels": ["Merged", "cla signed"]}, "1c9fc38eb2": {"title": "Remove reference to 9.2 as it's been removed from nightlies (#53716)", "body": "Summary:\nRemoving a tiny bit of unneeded reference to cuda92 for windows binary. Note that the config.yml did not change.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53716\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26947029\n\nPulled By: janeyx99\n\nfbshipit-source-id: 3bbf1faa513756eda182d2d80033257f0c629309", "pr_number": "53716", "files_changed": [".circleci/cimodel/data/binary_build_data.py"], "labels": ["Merged", "cla signed"]}, "bcbe07200c": {"title": "Improve logic for S3 stats gathering. Uses automatic SLOW_TESTS. (#53549)", "body": "Summary:\nThis PR:\n1. refactors the logic for S3 stats gathering.\n2. Renames SLOW_TESTS to TARGET_DET_LIST to disambiguate and remove confusion with slowTest\n2. detects slow tests (tests with time > 5min) to add to the TARGET_DET_LIST based on results in S3 from the previous nightly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53549\n\nTest Plan:\nSet CIRCLE_JOB to your favorite CI job (like `pytorch_linux_bionic_py3_8_gcc9_coverage_test1`).\nRun `python test/run_test.py --determine-from=<your fave pytorch files>`\ne.g., `python test/run_test.py --determine-from=test/run_test.py`\n\nReviewed By: mrshenli\n\nDifferential Revision: D26904478\n\nPulled By: janeyx99\n\nfbshipit-source-id: 9576b34f4fee09291d60e36ff2631753a3925094", "pr_number": "53549", "files_changed": ["test/run_test.py", "test/test_determination.py"], "labels": ["Merged", "cla signed"]}, "e937db5dba": {"title": "Added CUDA support for torch.orgqr (#51348)", "body": "Summary:\n**Update:** MAGMA support was dropped from this PR. Only the cuSOLVER path is implemented and it's used exclusively.\n\n**Original PR message:**\n\nThis PR adds support for CUDA inputs for `torch.orgqr`.\n\nCUDA implementation is based on both [cuSOLVER](https://docs.nvidia.com/cuda/cusolver/index.html#cuSolverDN-lt-t-gt-orgqr) and MAGMA. cuSOLVER doesn't have a specialized routine for the batched case. While MAGMA doesn't have a specialized GPU native (without CPU sync) `orgqr`. But MAGMA has implemented (and not documented) the batched GPU native version of `larft` function (for small inputs of size <= 32), which together with `larfb` operation form `orgqr` (see the call graph [here at the end of the page](http://www.netlib.org/lapack/explore-html/da/dba/group__double_o_t_h_e_rcomputational_ga14b45f7374dc8654073aa06879c1c459.html)).\n\nSo now there are two main codepaths for CUDA inputs (if both MAGMA and cuSOLVER are available):\n* if `batchsize > 1` and `tau.shape[-1] <= 32` then MAGMA based function is called\n* else [cuSOLVER's `orgqr`](https://docs.nvidia.com/cuda/cusolver/index.html#cuSolverDN-lt-t-gt-orgqr) is used.\n\nIf MAGMA is not available then only cuSOLVER is used and vice versa.\n\nDocumentation updates and possibly a new name for this function will be in a follow-up PR.\n\nRef. https://github.com/pytorch/pytorch/issues/50104\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51348\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D26882415\n\nPulled By: mruberry\n\nfbshipit-source-id: 9f91ff962921932777ff108bedc133b55fe22842", "pr_number": "51348", "files_changed": ["aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/CUDASolver.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h", "aten/src/ATen/native/native_functions.yaml", "test/test_linalg.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source", "triaged"]}, "e13ef777a7": {"title": "Use native ctc loss for target length 256 (#53557)", "body": "Summary:\nApparently cudnn (8.1) does not like 256-long targets.\n\nThank you raotnameh for reporting.\n\nFixes https://github.com/pytorch/pytorch/issues/53505\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53557\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26947262\n\nPulled By: albanD\n\nfbshipit-source-id: df6da7db8fd8e35050b4303ff1658646ebc60141", "pr_number": "53557", "files_changed": ["aten/src/ATen/native/cudnn/LossCTC.cpp", "test/test_nn.py"], "labels": ["Merged", "cla signed", "module: cudnn", "module: nn", "open source", "triaged"]}, "05e0ea9661": {"title": "[android] bump gradle version to 6.8.3 (#53567)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53567\n\nUpdating gradle to version 6.8.3\nProper zip was uploaded to aws.\n\nSuccessful CI check: https://github.com/pytorch/pytorch/pull/53619\n\nTest Plan: Imported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D26928885\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: b1081052967d9080cd6934fd48c4dbe933630e49", "pr_number": "53567", "files_changed": [".circleci/docker/android/build.gradle", ".circleci/docker/build.sh", ".circleci/docker/common/install_android.sh", ".circleci/scripts/build_android_gradle.sh", ".circleci/scripts/publish_android_snapshot.sh", "android/build.gradle", "android/gradle/wrapper/gradle-wrapper.properties", "android/pytorch_android/build.gradle"], "labels": ["Merged", "cla signed"]}, "5658ab5f77": {"title": "[andoid] publishing to maven central (#53568)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53568\n\nBintray, JCenter are going to be unavailable on May 1st\n\nhttps://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/\n\nMigrating publishing to Maven Central the same way as other fb oss projects, reference PR https://github.com/pytorch/pytorch/pull/53568/files\n\nto publish\n```\n./android/gradlew -p android publish\n```\n\n<img width=\"697\" alt=\"Screen Shot 2021-03-09 at 3 14 08 PM\" src=\"https://user-images.githubusercontent.com/6638825/110551387-3e3fc000-80ea-11eb-9604-4e69d6e6d085.png\">\n\nTest Plan: Imported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D26928884\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 8754c93a2542405870e2621be5b3f14e3d0081b9", "pr_number": "53568", "files_changed": [".circleci/docker/android/build.gradle", ".circleci/scripts/publish_android_snapshot.sh", "android/build.gradle", "android/gradle.properties", "android/gradle/android_maven_install.gradle", "android/gradle/android_tasks.gradle", "android/gradle/bintray.gradle", "android/gradle/gradle_maven_push.gradle", "android/gradle/release.gradle", "android/gradle/release_bintray.gradle"], "labels": ["Merged", "cla signed"]}, "b99b6065f8": {"title": "Removes trailing whitespace (#53728)", "body": "Summary:\nFixes\n\n```\nRun (! git grep -I -l ' $' -- . ':(exclude)**/contrib/**' ':(exclude)third_party' || (echo \"The above files have trailing spaces; please remove them\"; false))\naten/src/ATen/native/cuda/BatchLinearAlgebra.cu\nThe above files have trailing spaces; please remove them\nError: Process completed with exit code 1.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53728\n\nReviewed By: ngimel\n\nDifferential Revision: D26953099\n\nPulled By: mruberry\n\nfbshipit-source-id: 5f1ed2cd767de49447fcbd8e03cb3af7841cbcaf", "pr_number": "53728", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu"], "labels": ["Merged", "cla signed"]}, "8f15a2f052": {"title": "eig_backward: faster and with complex support (#52875)", "body": "Summary:\nAs per title. Compared to the previous version, it is lighter on the usage of `at::solve` and `at::matmul` methods.\n\nFixes https://github.com/pytorch/pytorch/issues/51621\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52875\n\nReviewed By: mrshenli\n\nDifferential Revision: D26768653\n\nPulled By: anjali411\n\nfbshipit-source-id: aab141968d02587440128003203fed4b94c4c655", "pr_number": "52875", "files_changed": ["test/test_autograd.py", "test/test_linalg.py", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "complex_autograd", "module: complex", "open source", "triaged"]}, "a08fc1a7fc": {"title": "allow users to set sample rate and add per iteration latency breakdowns (#53145)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53145\n\nadd a new API to allow users to set sample rate for runtime stats, also add per iteration latency breakdowns to DDPLoggingData struct. e.g.\nif users set sample rate to be 1, they can analyze per iteration latency change over time (not avged)\nghstack-source-id: 123443369\n\nTest Plan: unit test\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26763957\n\nfbshipit-source-id: baff6a09c2a590e6eb91362ca6f47ae8fa6ddb0e", "pr_number": "53145", "files_changed": ["c10/util/Logging.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7d4b229d61": {"title": "add is_multi_device_module logging field (#53149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53149\n\nadd is_multi_device_module logging field\nghstack-source-id: 123444621\n\nTest Plan: unit test\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26765355\n\nfbshipit-source-id: d4d9c5981b18b1744299aebe8af37eb4e2e35c61", "pr_number": "53149", "files_changed": ["c10/util/Logging.h", "test/distributed/test_c10d.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d032287ec3": {"title": "fix data type logging (#53162)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53162\n\nit is possible there are multiple data types in mixed precision training, so log data types as a list of data type names.\nghstack-source-id: 123452626\n\nTest Plan: unit test\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26769256\n\nfbshipit-source-id: 8f7d73821e89864fedbbce723f301fe8fbad5685", "pr_number": "53162", "files_changed": ["c10/util/Logging.h", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a76b4736db": {"title": "clang format reducer and logger files (#53148)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53148\n\nclang format reducer and logger files\nghstack-source-id: 123453983\n\nTest Plan: unit test\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26764509\n\nfbshipit-source-id: 711efcfd77420f912861cfd20c69e3af5086f4b9", "pr_number": "53148", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "70a43425e0": {"title": "Simplify init._calculate_fan_in_and_fan_out (#53522)", "body": "Summary:\nThis uses the shape of the tensor instead of directly indexing it. This is useful when extending PyTorch's tensor class, e.g. for lazy access. Since the `init` sub-module doesn't check for `torch_function`, it is not possibly to override its functions. Explicitly indexing the tensor will force a call to tensor() and reconstruct the full tensor/explicitly access the elements. Simply using the shape allows to avoid that.\n\nFixes https://github.com/pytorch/pytorch/issues/53540\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53522\n\nReviewed By: anjali411\n\nDifferential Revision: D26947794\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 80cd65efed16383f21363cee2eb404c9bc05971c", "pr_number": "53522", "files_changed": ["torch/nn/init.py"], "labels": ["Merged", "cla signed", "module: nn", "open source", "triaged"]}, "bfc80b3566": {"title": "Give line numbers in git-grep-based lints (#53733)", "body": "Summary:\nMeant to make tasks like https://github.com/pytorch/pytorch/issues/53728 easier. The `-n` flag enables line numbers, and the `-o` flag reduces noise by only showing the part of the line that matched (which in this case is just the trailing whitespace).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53733\n\nTest Plan:\n```\n$ git checkout e937db5dbaeaeae1134b02b3b78c43db3f6a91cd\n```\n\nBefore:\n```\n$ (! git grep -I -l ' $' -- . ':(exclude)**/contrib/**' ':(exclude)third_party' || (echo \"The above files have trailing spaces; please remove them\"; false))\naten/src/ATen/native/cuda/BatchLinearAlgebra.cu\nThe above files have trailing spaces; please remove them\n```\n\nAfter:\n\n```\n$ (! git grep -I -no ' $' -- . ':(exclude)**/contrib/**' ':(exclude)third_party' || (echo \"The above files have trailing spaces; please remove them\"; false))\naten/src/ATen/native/cuda/BatchLinearAlgebra.cu:1972:\nThe above files have trailing spaces; please remove them\n```\n\nReviewed By: mruberry\n\nDifferential Revision: D26953538\n\nPulled By: samestep\n\nfbshipit-source-id: 5f7d48b79f1a02e5e5a09fe00316ec350cfc340e", "pr_number": "53733", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "b4d8f4af82": {"title": "[package] implement `get_resource_reader` API (#51674)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51674\n\nSee\nhttps://docs.python.org/3/library/importlib.html#importlib.abc.ResourceReader\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D26237034\n\nPulled By: suo\n\nfbshipit-source-id: 4c19f6172d16b710737528d3de48372873b9368d", "pr_number": "51674", "files_changed": ["test/test_package.py", "torch/csrc/jit/python/init.cpp", "torch/package/importer.py", "torch/package/package_importer.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "17bc70e6f7": {"title": "[package] make importer a little more obscure (#51676)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51676\n\nWe offer the ability to access the importer from within packaged modules by doing\n`import resources`. This behavior is nice (and more powerful than the\nimportlib resources API), but I think `resources` is too common a name\n(pip has a package for it)\n\nChange to `import torch_package_importer` but open to bikeshedding\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26620314\n\nPulled By: suo\n\nfbshipit-source-id: 0942c99f02c0f55f5f3a1b2566961018b796bdd4", "pr_number": "51676", "files_changed": ["test/test_package.py", "torch/package/importer.py", "torch/package/package_importer.py"], "labels": ["Merged", "cla signed"]}, "cb68039363": {"title": "Port NumPy typing testing style to PyTorch (#52408)", "body": "Summary:\nref: https://github.com/pytorch/pytorch/issues/16574\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52408\n\nReviewed By: anjali411\n\nDifferential Revision: D26654687\n\nPulled By: malfet\n\nfbshipit-source-id: 6feb603d8fb03c2ba2a01468bfde1a9901e889fd", "pr_number": "52408", "files_changed": ["test/run_test.py", "test/test_typing.py", "test/typing/reveal/tensor_constructors.py"], "labels": ["Merged", "cla signed", "module: typing", "open source"]}, "b03c92a9c5": {"title": "[2/n][torch/elastic][upstream] Move torchelastic/timer torchelastic/multiprocessing to torch/distributed/elastic (#53574)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53574\n\nUpstreams `torchelastic/timer|multiprocessing` to `torch/distributed/elastic/timer|multiprocessing`\n\nTest Plan:\n```\nbuck test mode/dev-nosan //caffe2/torch/distributed/elastic/...\nbuck test mode/dev-nosan //caffe2/test/distributed/elastic/...\nbuck test mode/dev-nosan //pytorch/elastic/torchelastic/...\nbuck test mode/dev-nosan //hpc/...\nbuck test mode/dev-nosan //caffe2/torch/fb/training_toolkit/...\n```\n\nReviewed By: borovsky-d, wilson100hong\n\nDifferential Revision: D26899809\n\nfbshipit-source-id: e6dbc2a78282eac296c262b3206a979e3ef1ff53", "pr_number": "53574", "files_changed": ["test/distributed/elastic/multiprocessing/api_test.py", "test/distributed/elastic/multiprocessing/bin/echo1.py", "test/distributed/elastic/multiprocessing/bin/echo2.py", "test/distributed/elastic/multiprocessing/bin/echo3.py", "test/distributed/elastic/multiprocessing/bin/test_script.py", "test/distributed/elastic/multiprocessing/errors/api_test.py", "test/distributed/elastic/multiprocessing/errors/error_handler_test.py", "test/distributed/elastic/multiprocessing/redirects_test.py", "test/distributed/elastic/multiprocessing/tail_log_test.py", "test/distributed/elastic/timer/__init__.py", "test/distributed/elastic/timer/api_test.py", "test/distributed/elastic/timer/local_timer_example.py", "test/distributed/elastic/timer/local_timer_test.py", "torch/distributed/elastic/multiprocessing/__init__.py", "torch/distributed/elastic/multiprocessing/api.py", "torch/distributed/elastic/multiprocessing/errors/__init__.py", "torch/distributed/elastic/multiprocessing/errors/error_handler.py", "torch/distributed/elastic/multiprocessing/errors/handlers.py", "torch/distributed/elastic/multiprocessing/redirects.py", "torch/distributed/elastic/multiprocessing/tail_log.py", "torch/distributed/elastic/timer/__init__.py", "torch/distributed/elastic/timer/api.py", "torch/distributed/elastic/timer/local_timer.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "76b58dd9ae": {"title": "Fix distributions which don't properly honor validate_args=False (#53600)", "body": "Summary:\nA number of derived distributions use base distributions in their\nimplementation.\n\nWe add what we hope is a comprehensive test whether all distributions\nactually honor skipping validation of arguments in log_prob and then\nfix the bugs we found. These bugs are particularly cumbersome in\nPyTorch 1.8 and master when validate_args is turned on by default\nIn addition one might argue that validate_args is not performing\nas well as it should when the default is not to validate but the\nvalidation is turned on in instantiation.\n\nArguably, there is another set of bugs or at least inconsistencies\nwhen validation of inputs does not prevent invalid indices in\nsample validation (when with validation an IndexError is raised\nin the test). We would encourage the implementors to be more\nambitious when validation is turned on and amend sample validation\nto throw a ValueError for consistency.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53600\n\nReviewed By: mrshenli\n\nDifferential Revision: D26928088\n\nPulled By: neerajprad\n\nfbshipit-source-id: 52784a754da2faee1a922976e2142957c6c02e28", "pr_number": "53600", "files_changed": ["test/distributions/test_distributions.py", "torch/distributions/beta.py", "torch/distributions/kumaraswamy.py", "torch/distributions/log_normal.py", "torch/distributions/logistic_normal.py", "torch/distributions/mixture_same_family.py", "torch/distributions/pareto.py", "torch/distributions/relaxed_categorical.py", "torch/distributions/transformed_distribution.py", "torch/distributions/weibull.py"], "labels": ["Merged", "cla signed", "open source"]}, "669fcf3093": {"title": "Replace supports_tensor_out with supports_out (#53745)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/52875 introduced this bug, as `supports_tensor_out` was replaced with `supports_out` in https://github.com/pytorch/pytorch/issues/53259, so CI checks are failing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53745\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D26958151\n\nPulled By: malfet\n\nfbshipit-source-id: 7cfe5d1c1a33f06cb8be94281ca98c635df76838", "pr_number": "53745", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "c68cc24cee": {"title": "update upsample tests in test_nn.py to test for memory_format (#53665)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53665\n\nngimel pointed out to me where we already test the behavior of the `Upsample` ops in `test_nn.py`. This PR deleting my bespoke tests in `test_torch.py` and updates those in `test_nn.py` to test memory format properly.\n\nThere were two reasons the original test didn't pick up on a memory format regression:\n- They didn't test the memory format of the output tensor explicitly, i.e. `output.is_contiguous(memory_format=...)`\n- Even with that change, the test tensors were to simple to fail the tests. From some trial and error, it looks like one of the first two dimensions in the inputs needs to be > 1 in order for the `channels_last` memory format to actually re-order the strides.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D26929683\n\nPulled By: bdhirsh\n\nfbshipit-source-id: d17bc660ff031e9b3e2c93c60a9e9308e56ea612", "pr_number": "53665", "files_changed": ["test/test_nn.py", "test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "56f3cb7a99": {"title": "Add AST rewriter to acc_tracer (#53644)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53644\n\nReviewed By: gcatron\n\nDifferential Revision: D26506841\n\nfbshipit-source-id: 64367d7e9f6619d014a01c147476b50467efa5c8", "pr_number": "53644", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "2cf90982e9": {"title": "[TestZeroRedundancyOptimizer] Add multi gpu checker (#53564)", "body": "Summary:\nThe test test_collect_shards fails on single GPU setup.\nEnabling the multi gpu checker.\n\nSigned-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53564\n\nReviewed By: H-Huang\n\nDifferential Revision: D26952325\n\nPulled By: rohan-varma\n\nfbshipit-source-id: e8956f9277c7320024bece129767e83fbdf02b2c", "pr_number": "53564", "files_changed": ["test/distributed/optim/test_zero_redundancy_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "99d7c8ff94": {"title": "[caffe2] use AddNAlreadyReserved() when serializing blobs (#53400)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53400\n\nThis is a reland of D26617038 (https://github.com/pytorch/pytorch/commit/b4a8d98247c4eac60ae594ae626886408dc4e2c0) after rebasing onto D26802576 (https://github.com/pytorch/pytorch/commit/f595ba1bae9a75c6403e4624e7294b5635acd40c).\n\nOptimize the blob serialization code by using `AddNAlreadyReserved()` when\nserializing tensor data, rather than making N separate `Add()` calls.\n`AddNAlreadyReserved()` is a simple addition operation, while each `Add()`\ncall checks to see if it needs to reserve new space, and then updates the\nelement data, which is unnecessary in this case.\nghstack-source-id: 123567030\n\nTest Plan:\nThis appears to improve raw serialization performance by 30 to 35% for float,\ndouble, and int64_t types which use this function.  This improvement appears\nrelatively consistent across large and small tensor sizes.\n\nReviewed By: mraway\n\nDifferential Revision: D26853941\n\nfbshipit-source-id: 4ccaa5bc1dd7f7864068d71a0cde210c699cbdba", "pr_number": "53400", "files_changed": ["caffe2/core/blob_serialization.h"], "labels": ["Merged", "cla signed"]}, "023948e6d7": {"title": "[caffe2] update load_save_test.py to also verify the chunking behavior (#53401)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53401\n\nThis is a reland of D26641599 (https://github.com/pytorch/pytorch/commit/cd9ac54ea75d4a530b6b9dccaaf1167cabafc789) after rebasing onto D26802576 (https://github.com/pytorch/pytorch/commit/f595ba1bae9a75c6403e4624e7294b5635acd40c).\n\nAdd some small utility functions to read the blob names back from the minidb\nfile so that we can verify how many chunks were written for each blob.\nghstack-source-id: 123567033\n\nTest Plan: buck test caffe2/caffe2/python/operator_test:load_save_test\n\nReviewed By: mraway\n\nDifferential Revision: D26853942\n\nfbshipit-source-id: 0b45078fdd279f547752c8fdb771e296374a00da", "pr_number": "53401", "files_changed": ["caffe2/python/operator_test/load_save_test.py"], "labels": ["Merged", "cla signed"]}, "fee263595c": {"title": "Remove trailing whitespace introduced in #52175 (#53762)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53762\n\nTest Plan: CI.\n\nReviewed By: seemethere\n\nDifferential Revision: D26961133\n\nPulled By: samestep\n\nfbshipit-source-id: 972ea480baa3f34b65327abdf7e8bfdf30788572", "pr_number": "53762", "files_changed": ["test/jit/test_remove_mutation.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4351d09683": {"title": "Fix error message in setStorage (#53198)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53198\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26830030\n\nPulled By: zdevito\n\nfbshipit-source-id: 34d383c4561bba88dee6d570cbd22bd58a3fe856", "pr_number": "53198", "files_changed": ["aten/src/ATen/native/Resize.h"], "labels": ["Merged", "cla signed"]}, "c988b78be2": {"title": "Add a description of GradBucket Python class (#53596)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53596\n\nThis description will be used in ddp_comm_hook docstrings.\nghstack-source-id: 123590360\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26908160\n\nfbshipit-source-id: 824dea9203ca583676bddf0161c9edca52c9d20e", "pr_number": "53596", "files_changed": ["torch/csrc/distributed/c10d/init.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "fe0810e2f8": {"title": "Add a section to introduce GradBucket class in ddp_comm_hooks.rst (#53253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53253\n\nSince GradBucket class becomes public, mention this class in ddp_comm_hooks.rst.\n\nScreenshot:\n{F478201008}\n\nghstack-source-id: 123596842\n\nTest Plan: viewed generated html file\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26812210\n\nfbshipit-source-id: 65b70a45096b39f7d41a195e65b365b722645000", "pr_number": "53253", "files_changed": ["docs/source/ddp_comm_hooks.rst"], "labels": ["Merged", "cla signed"]}, "895735c69f": {"title": "TensorIterator: Avoid nesting two levels of function_ref in for_each (#53613)", "body": "Summary:\nWhen calling `TensorIterator::for_each` with a 1d loop, it creates a `function_ref` for the 1D iteration, then wraps it with `LOOP_WRAPPER` to transform it into a 2d loop. That 2d loop then gets wrapped in another `function_ref`. This can result in significant overhead if the 1d inner loop is over a small number of elements.\n\nInstead, this wraps the 1d loop before type-erasure so only one level of `function_ref` is introduced. A simple benchmark demonstrates this is a win:\n```python\nimport torch\na = torch.rand((10000, 2))[::2]\n%timeit a + a\n```\n\nNote the 2D tensor cannot be coalesced into 1D and both `cpu_kernel` and `cpu_kernel_vec` use 1D for_each. On master, this takes 42 us but with this change it's down to 32us.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53613\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26947143\n\nPulled By: ezyang\n\nfbshipit-source-id: 5189ada0d82bbf74170fb446763753f02478abf6", "pr_number": "53613", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed", "module: TensorIterator", "open source", "triaged"]}, "f9185973d1": {"title": "[quantization] Add some support for 3d operations (#50003)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50002\n\nThe last commit adds tests for 3d conv with the `SubModelFusion` and `SubModelWithoutFusion` classes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50003\n\nReviewed By: mrshenli\n\nDifferential Revision: D26325953\n\nPulled By: jerryzh168\n\nfbshipit-source-id: 7406dd2721c0c4df477044d1b54a6c5e128a9034", "pr_number": "50003", "files_changed": ["aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/library.cpp", "docs/source/quantization-support.rst", "docs/source/torch.nn.intrinsic.qat.rst", "docs/source/torch.nn.intrinsic.rst", "test/quantization/test_quantize_fx.py", "test/quantization/test_quantize_jit.py", "torch/nn/intrinsic/qat/modules/__init__.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/intrinsic/quantized/modules/conv_relu.py", "torch/nn/modules/conv.py", "torch/nn/qat/modules/__init__.py", "torch/nn/qat/modules/conv.py", "torch/nn/quantized/modules/conv.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py", "torch/quantization/quantization_mappings.py"], "labels": ["Merged", "cla signed", "fx", "oncall: quantization", "open source", "triaged"]}, "21f9a6da7d": {"title": "Avoid of creating a copy of statusString every inference time (#53756)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53756\n\nas title\n\nReviewed By: yinghai\n\nDifferential Revision: D26949450\n\nfbshipit-source-id: a737ce1ed25cf53faef8cdc94912542769a1008f", "pr_number": "53756", "files_changed": ["caffe2/opt/onnxifi_op.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "49a5f99440": {"title": "skip dispatch in resize_ (#53575)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53575\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26902348\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b322f233d934278f03e56cd1e35acc0665389398", "pr_number": "53575", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/Resize.h", "tools/codegen/dest/register_dispatch_key.py"], "labels": ["Merged", "cla signed"]}, "1f01899e4a": {"title": "Simplify index expressions constructed in loop flattening - #51173 (#52882)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52882\n\nTest Plan:\nImported from OSS\n\nbuild/bin/test_tensorexpr\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D26676150\n\nPulled By: huiguoo\n\nfbshipit-source-id: e202e0c8610eb107558a3add8a6560a0cb97704a", "pr_number": "52882", "files_changed": ["test/cpp/tensorexpr/test_simplify.cpp", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp"], "labels": ["Merged", "Reverted", "cla signed", "oncall: jit"]}, "14acf92b2b": {"title": "[PyTorch] Speed up Tensor::data_ptr (#53723)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53723\n\nWe know the size of the data item at compile time. Let's take\nadvantage of that instead of doing a runtime multiplication by the\ndata type size. (Presumably, constant propagating through\n`data_type.itemsize()` to optimize the `imul` away was just a bridge\ntoo far for clang -- I checked assembly and we went from a\nload-and-`imul` to a `lea` that multiplied by constant 4 for\n`data_ptr<float>()`.)\nghstack-source-id: 123559924\n\nTest Plan:\nCompared `perf stat` output for Mergenet AdIndexer\nbenchmark before/after this change:\n\nBefore:\n```\n         16,943.46 msec task-clock                #    0.999 CPUs utilized            ( +-  0.16% )\n             3,771      context-switches          #    0.223 K/sec                    ( +- 15.86% )\n                 3      cpu-migrations            #    0.000 K/sec\n           101,660      page-faults               #    0.006 M/sec                    ( +-  1.00% )\n    33,519,516,740      cycles                    #    1.978 GHz                      ( +-  0.20% )  (49.99%)\n    68,556,471,199      instructions              #    2.05  insn per cycle           ( +-  0.08% )  (49.98%)\n    11,100,415,689      branches                  #  655.145 M/sec                    ( +-  0.12% )  (50.02%)\n        73,082,369      branch-misses             #    0.66% of all branches          ( +-  0.45% )  (50.01%)\n```\n\nAfter:\n```\n         16,779.99 msec task-clock                #    0.999 CPUs utilized            ( +-  0.40% )\n             2,815      context-switches          #    0.168 K/sec                    ( +-  7.89% )\n                 3      cpu-migrations            #    0.000 K/sec                    ( +-  6.25% )\n           100,124      page-faults               #    0.006 M/sec                    ( +-  0.40% )\n    33,213,000,715      cycles                    #    1.979 GHz                      ( +-  0.39% )  (49.99%)\n    68,359,270,731      instructions              #    2.06  insn per cycle           ( +-  0.08% )  (50.00%)\n    11,058,104,630      branches                  #  659.005 M/sec                    ( +-  0.11% )  (50.00%)\n        72,840,016      branch-misses             #    0.66% of all branches          ( +-  0.51% )  (49.99%)\n```\n\n0.9% cycles win and 0.29% instruction count win, both of which seem to\nbe outside the noise.\n\nReviewed By: bhosmer\n\nDifferential Revision: D26919110\n\nfbshipit-source-id: 23fab7adcfcf6ec9c87ebfb5d5304b6f155f577f", "pr_number": "53723", "files_changed": ["aten/src/ATen/templates/TensorMethods.cpp", "c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "ced91bb713": {"title": "[deploy] namespace and rename (#53670)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53670\n\nThis puts deploy into the torch::deploy namespace. It also renames some\nobjects to better match their behavior:\n\nPythonObject -> Obj, in the future it will refer to either a python object or a handle to a script obj, so rename it torch::deploy::Obj to be generic\nMovableObject -> ReplicatedObj, to prevent confusion with \"std::move\" which is unrelated, and to note that we are replicating this object across interpreters.\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab\n\nDifferential Revision: D26932131\n\nPulled By: zdevito\n\nfbshipit-source-id: 8041d6c5b2041a7c3192c1a17d2edb38112a89f3", "pr_number": "53670", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/example/benchmark.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.h", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["Merged", "cla signed"]}, "d49c5c74f5": {"title": "[docs] Add starter content for new TorchScript language reference (#52494)", "body": "Summary:\n**Summary**\nThis commit adds a new .rst file to use for updating the language specification and prepopulates it with the updated content for the expressions section.\n\n**Test Plan**\nhttps://user-images.githubusercontent.com/4392003/110441235-638ee880-806e-11eb-83ae-3b908bf00d5b.mov\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52494\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26965463\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 246c76a56d911a8061e720abd200a44d7dfa1f35", "pr_number": "52494", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "Reverted", "cla signed"]}, "ebfa9276d8": {"title": "Move prim::layout for lite jit (#53781)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53781\n\nneeded for running noise suppression model in lite interpreter\n\nTest Plan: run model\n\nReviewed By: linbinyu\n\nDifferential Revision: D26967227\n\nfbshipit-source-id: 19677fc796f1fb4423ebb11b5ffd9df5870a39cf", "pr_number": "53781", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "37ab711822": {"title": "Adding learning rate schedulers to C++ API (#52268)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50577\n\nLearning rate schedulers had not yet been implemented for the C++ API.\n\nThis pull request introduces the learning rate scheduler base class and the StepLR subclass. Furthermore, it modifies the existing OptimizerOptions such that the learning rate scheduler can modify the learning rate.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52268\n\nReviewed By: mrshenli\n\nDifferential Revision: D26818387\n\nPulled By: glaringlee\n\nfbshipit-source-id: 2b28024a8ea7081947c77374d6d643fdaa7174c1", "pr_number": "52268", "files_changed": ["caffe2/CMakeLists.txt", "setup.py", "test/cpp/api/optim.cpp", "tools/build_variables.bzl", "torch/csrc/api/include/torch/optim.h", "torch/csrc/api/include/torch/optim/adagrad.h", "torch/csrc/api/include/torch/optim/adam.h", "torch/csrc/api/include/torch/optim/adamw.h", "torch/csrc/api/include/torch/optim/lbfgs.h", "torch/csrc/api/include/torch/optim/optimizer.h", "torch/csrc/api/include/torch/optim/rmsprop.h", "torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h", "torch/csrc/api/include/torch/optim/schedulers/step_lr.h", "torch/csrc/api/include/torch/optim/sgd.h", "torch/csrc/api/src/optim/adagrad.cpp", "torch/csrc/api/src/optim/adam.cpp", "torch/csrc/api/src/optim/adamw.cpp", "torch/csrc/api/src/optim/lbfgs.cpp", "torch/csrc/api/src/optim/optimizer.cpp", "torch/csrc/api/src/optim/rmsprop.cpp", "torch/csrc/api/src/optim/schedulers/lr_scheduler.cpp", "torch/csrc/api/src/optim/schedulers/step_lr.cpp", "torch/csrc/api/src/optim/sgd.cpp"], "labels": ["Merged", "cla signed", "module: cpp", "open source", "triaged"]}, "1053c96693": {"title": "[GraphModule] Back out changes to module root version of __init__ (#53791)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53791\n\nReviewed By: houseroad\n\nDifferential Revision: D26970869\n\nfbshipit-source-id: 80684516f57fd2d1aca794f17fe488b2fe2b2f64", "pr_number": "53791", "files_changed": ["test/test_fx_experimental.py", "torch/fx/graph_module.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "95d2318510": {"title": "Adding parallel support for the LLVM backend. (#53243)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53243\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher, Chillee\n\nDifferential Revision: D26906509\n\nPulled By: zheng-xq\n\nfbshipit-source-id: 12c17f2f21af11e73fa4c5b5199043a7a15ecdec", "pr_number": "53243", "files_changed": ["benchmarks/cpp/tensorexpr/CMakeLists.txt", "benchmarks/cpp/tensorexpr/bench_parallel.cpp", "benchmarks/cpp/tensorexpr/bench_reduce.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.h", "torch/csrc/jit/tensorexpr/llvm_jit.cpp", "torch/csrc/jit/tensorexpr/llvm_jit.h", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["Merged", "Reverted", "cla signed", "oncall: jit"]}, "f364e492df": {"title": "Autograd functional API should enable_grad (#47543)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44640\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47543\n\nReviewed By: albanD\n\nDifferential Revision: D26965136\n\nPulled By: iramazanli\n\nfbshipit-source-id: 1dd46b9402bb670c0e165db684712e26c1a2036f", "pr_number": "47543", "files_changed": ["test/test_autograd.py", "torch/autograd/functional.py"], "labels": ["Merged", "cla signed"]}, "05f137c765": {"title": "Remove GHA \"Checkout PR tip\" step (#53719)", "body": "Summary:\nThis PR replaces our current \"Checkout PR tip\" step (which is duplicated across many places) using a [scenario](https://github.com/actions/checkout#checkout-pull-request-head-commit-instead-of-merge-commit) from the `actions/checkout` README. We previously tried something similar in https://github.com/pytorch/pytorch/issues/49578, but using `github.head_ref` didn't work.\n\nThe reason this PR works is because, for events besides `pull_request`, the value of `github.event.pull_request.head.sha` defaults to the empty string, so it's as if we didn't set the `ref` option for `actions/checkout` at all, so it just uses its default behavior (e.g. for `push` events).\n\nIncidentally, this PR also upgrades our use of `actions/checkout` from `v1` to `v2`, which introduces shallow clones by default. A couple of our jobs require deep clones, so we use `fetch-depth: 0` in those cases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53719\n\nTest Plan: CI.\n\nReviewed By: albanD\n\nDifferential Revision: D26949121\n\nPulled By: samestep\n\nfbshipit-source-id: e06f8066682ae0557fb5a055a10ea33b6bd320db", "pr_number": "53719", "files_changed": [".github/workflows/clang_format.yml", ".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "0a549f9412": {"title": "[ROCm] Disable flaky tests on ROCm (#53192)", "body": "Summary:\nThe disabled tests are tracked by\nhttps://github.com/pytorch/pytorch/issues/53190\n\nSigned-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53192\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D26782204\n\nPulled By: mrshenli\n\nfbshipit-source-id: bc90b182c236249961da1f0d4894d29f6b44fa27", "pr_number": "53192", "files_changed": ["test/test_cuda.py", "test/test_nn.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "5842d34fac": {"title": "Call nvidia-smi.exe before running tests on Windows (#53422)", "body": "Summary:\nFollow up for https://github.com/pytorch/pytorch/issues/53334\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53422\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26954202\n\nPulled By: malfet\n\nfbshipit-source-id: fe16a2413618e07d6380824e967d87e29a09b178", "pr_number": "53422", "files_changed": [".jenkins/pytorch/win-test.sh"], "labels": ["Merged", "cla signed", "module: tests", "module: windows", "open source", "triaged"]}, "4c1af249fb": {"title": "[ROCM] load hipfft separately from rocfft (#53408)", "body": "Summary:\nThis PR makes changes to how hipfft is loaded in pytorch. hipfft is packaged in a separate library to rocfft following rocm 4.1.\n\nWe check the rocm version and if it is past rocm 4.1 we load hipfft in addition to rocfft.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53408\n\nReviewed By: albanD\n\nDifferential Revision: D26952702\n\nPulled By: malfet\n\nfbshipit-source-id: f42be304b587c060816e39d36f5c1a2cdc37bfab", "pr_number": "53408", "files_changed": ["cmake/Dependencies.cmake", "cmake/public/LoadHIP.cmake"], "labels": ["Merged", "cla signed", "module: build", "module: rocm", "open source", "triaged"]}, "d7b5a6faaa": {"title": "Revert \"Revert D26733731: [pytorch][PR] Skip dispatch for `is_floatin\u2026 (#53242)", "body": "Summary:\n\u2026g_point`\"\n\nThis reverts commit fbf2883d350f62d17292b71a58f404b5e3e58b7b.\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53242\n\nReviewed By: mrshenli\n\nDifferential Revision: D26896105\n\nPulled By: iramazanli\n\nfbshipit-source-id: 279a6f6d4fbb7949a7ed65df848db71a9b8d44e2", "pr_number": "53242", "files_changed": ["aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h", "test/test_vmap.py"], "labels": ["Merged", "cla signed"]}, "ec713c0eb5": {"title": "[Pytorch] Improve scale and zero point extraction for per channel quantized (#53726)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53726\n\nIn quantized linear layers, during deserialization we create scales and zero\npoints which are later used for qnnpack kernels.\nScales and zero pointer extraction for per channel quantized tensors is slow.\nThis is due to the fact that we index directly into zero point and scales\ntensor and this indexing creates a tensor slice of 1 element which is then cast\nto int32 or float.\nThis is super slow and increases model loading time.\nThis diff fixes that.\n\nTest Plan: CI\n\nReviewed By: raziel\n\nDifferential Revision: D26922138\n\nfbshipit-source-id: b78e8548f736e8fa2f6636324ab1a2239b94a27c", "pr_number": "53726", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack_utils.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "5648fe6093": {"title": "Make storage access throw for meta tensors (#53681)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53681\n\nWithout throwing, we can easily segfault trying to access nullptr\nstorage.\n\nTo do this I made set_storage_access_should_throw public so that you\ndon't have to subclass TensorImpl to do it.  An alternative is\nto just bite the bullet and add a MetaTensorImpl subclass.  Let\nme know what is preferred.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D26955540\n\nPulled By: ezyang\n\nfbshipit-source-id: 8ce22dd07ef1beb042f1d91de981954d59c2f84a", "pr_number": "53681", "files_changed": ["aten/src/ATen/native/MetaTensor.cpp", "c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "5cf4527c88": {"title": "Update repo name for add-annotations-github-action (#53826)", "body": "Summary:\nIt looks like https://github.com/suo/add-annotations-github-action redirects to https://github.com/pytorch/add-annotations-github-action, so this is a bit less confusing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53826\n\nTest Plan: The clang-tidy CI job should pass on this PR.\n\nReviewed By: malfet\n\nDifferential Revision: D26981832\n\nPulled By: samestep\n\nfbshipit-source-id: 273c18535d0d27b14942b02ae552020ffc60623b", "pr_number": "53826", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "bbce574ccf": {"title": "Pass commit_sha to add-annotations-github-action again (#53834)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53833.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53834\n\nTest Plan: The CI logs for flake8-py3 and clang-tidy on this PR should show `commit_sha` being set to the PR tip in their respective \"Add annotations\" steps.\n\nReviewed By: malfet\n\nDifferential Revision: D26983201\n\nPulled By: samestep\n\nfbshipit-source-id: e5d1fbbaf2a2611fec583b430c6353e778bc77a6", "pr_number": "53834", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "ec484981c6": {"title": "[3/n][torch/elastic][upstream] Move torchelastic/events to torch/distributed/events (#53760)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53760\n\nPull Request resolved: https://github.com/pytorch/elastic/pull/143\n\nThe diff upsteams torchelastic/events to the torch.\n\nTest Plan:\nbuck test mode/dev-nosan //pytorch/elastic/torchelastic/agent/...\n    buck test mode/dev-nosan //caffe2/test/distributed/elastic/events/fb/...\n\nReviewed By: kiukchung\n\nDifferential Revision: D26932830\n\nfbshipit-source-id: 23fc10d2ead5af7f7ed510ae0d2581cc2421cf76", "pr_number": "53760", "files_changed": ["test/distributed/elastic/events/lib_test.py", "test/run_test.py", "torch/distributed/elastic/events/__init__.py", "torch/distributed/elastic/events/api.py", "torch/distributed/elastic/events/handlers.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "1acced4eba": {"title": "Implemented getCodeText(string attr) in llvm/cuda codegen and added python bindings for it - #52974 (#53664)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53664\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D26929204\n\nPulled By: huiguoo\n\nfbshipit-source-id: 281fe6c25f4664636b29d51dba396056a222a9e7", "pr_number": "53664", "files_changed": ["torch/csrc/jit/tensorexpr/block_codegen.h", "torch/csrc/jit/tensorexpr/codegen.h", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/kernel.h", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.h", "torch/csrc/jit/tensorexpr/tensorexpr_init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "fa980bb22a": {"title": "[wip][Dist Profiling] Enable dist profiling for MPI backend (#52949)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52949\n\nEnables distributed profiling which we have for gloo and nccl for the MPI backend\nghstack-source-id: 123610105\n\nTest Plan: CI\n\nReviewed By: wanchaol\n\nDifferential Revision: D26591590\n\nfbshipit-source-id: a20ec9d104faa26bc62c727dd01319c3ea230f5d", "pr_number": "52949", "files_changed": ["torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupMPI.hpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "14d02517e1": {"title": "replace data with data_ptr (#53097)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53097\n\nReviewed By: albanD\n\nDifferential Revision: D26972445\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 04798a3fd55dd297638377513cfc57ff86c8916d", "pr_number": "53097", "files_changed": ["torch/lib/c10d/test/ProcessGroupMPITest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "8016d28c0b": {"title": "[Gradient Compression] Update the comment on fp16_compress_hook (#53780)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53780\n\nUpdate the comment, because the input data type of `fp16_compress_hook` does not have to be FP32. For example, the input dtype can also be FP64, as long as it can be casted into FP16.\nghstack-source-id: 123680621\n\nTest Plan: N/A\n\nReviewed By: iseessel\n\nDifferential Revision: D26967224\n\nfbshipit-source-id: 26d79a3629a597e6335b6f59c97d25a764a8ed80", "pr_number": "53780", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "cdac61ecd4": {"title": "Prevent VS from emitting ambiguous symbol errors (third time) (#53490)", "body": "Summary:\nFixes: https://github.com/pytorch/pytorch/issues/53409\n\nFirst: https://github.com/pytorch/pytorch/issues/15697\nSecond: https://github.com/pytorch/pytorch/issues/17863\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53490\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26946687\n\nPulled By: mrshenli\n\nfbshipit-source-id: 27f85abecbb75456354cc0373529c8cadc8133bd", "pr_number": "53490", "files_changed": ["c10/util/C++17.h"], "labels": ["Merged", "cla signed", "open source"]}, "ec6a7cace3": {"title": "[ROCm] Fix the flaky test test_stream_event_nogil (#53850)", "body": "Summary:\nFix the flaky test in https://github.com/pytorch/pytorch/issues/53192 properly.\n\nSigned-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53850\n\nReviewed By: albanD\n\nDifferential Revision: D26993582\n\nPulled By: malfet\n\nfbshipit-source-id: b0aefb188a236a5e94ee31a30ede7e8175443ff5", "pr_number": "53850", "files_changed": ["test/test_cuda.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "6da0b94dd8": {"title": "Add note on forwarding arguments in the dispatcher (#53641)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53641\n\nghstack-source-id: 123466764\n\nTest Plan: comments only\n\nReviewed By: bhosmer\n\nDifferential Revision: D26922477\n\nfbshipit-source-id: ad630b5e1b10a2238f9b48aba656b2ffe65520a1", "pr_number": "53641", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["Merged", "cla signed"]}, "5344c3ea9e": {"title": "Remove `join_workers` from Pipeline destructor. (#53433)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53433\n\nAs described in https://github.com/pytorch/pytorch/issues/53413, the\npipeline destructor ends up hanging sometimes. The reason for this is that Pipe\nuses daemon threads and as a result these threads could be destroyed before the\nPipe destructor is done. The Pipe destructor then calls `join_workers` which\nwaits on signals from the worker threads, which might be already dead and\nresults in the main thread blocking forever.\n\nTo resolve this issue, in this PR we remove `join_workers` completely since it\nis not necessary to wait for daemon threads.\n\n#Closes: https://github.com/pytorch/pytorch/issues/53413\nghstack-source-id: 123641509\n\nTest Plan:\n1) Tested with repro in\nhttps://github.com/pytorch/pytorch/issues/53413.\n2) Hard to add a unit test for this since the bug really depends on order of\nobjects being destroyed.\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26863321\n\nfbshipit-source-id: 18fff072cabacfb10390e971eac789859d3dcc81", "pr_number": "53433", "files_changed": ["test/distributed/pipeline/sync/test_worker.py", "torch/distributed/pipeline/sync/pipeline.py", "torch/distributed/pipeline/sync/worker.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8d8a4a0624": {"title": "Remove the extra \":noindex:\" in ddp_comm_hooks.rst (#53855)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53855\n\nRemove \"noindex\" here:\n\n{F492926346}\nghstack-source-id: 123724419\n\nTest Plan:\nwaitforbuildbot\n\nThe failure on doctest does not seem to be relevant.\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26967086\n\nfbshipit-source-id: adf9db1144fa1475573f617402fdbca8177b7c08", "pr_number": "53855", "files_changed": ["docs/source/ddp_comm_hooks.rst"], "labels": ["cla signed"]}, "b69dd910e8": {"title": "[docs] Add starter content for new TorchScript language reference (#53837)", "body": "Summary:\n**Summary**\nThis commit adds a new .rst file to use for updating the language specification and prepopulates it with the updated content for the expressions section.\n\n**Test Plan**\nhttps://user-images.githubusercontent.com/4392003/110441235-638ee880-806e-11eb-83ae-3b908bf00d5b.mov\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53837\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26990801\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 3b4e711bfaa8aac4ee3a075822fed7267a818121", "pr_number": "53837", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "c15d943149": {"title": "[PyTorch] Fix broken build caused by keyword missing on Windows (#53562)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53562\n\nOn Windows when we try to build //xplat/caffe2/c10:c10Windows, it failed with an error like\n```\nstderr: buck-out\\gen\\83497cbb\\xplat\\caffe2\\c10\\c10Windows#header-mode-symlink-tree-only,headers\\c10/macros/Macros.h(189): error C2220: warning treated as error - no 'object' file generated\nbuck-out\\gen\\83497cbb\\xplat\\caffe2\\c10\\c10Windows#header-mode-symlink-tree-only,headers\\c10/macros/Macros.h(189): warning C4067: unexpected tokens following preprocessor directive - expected a newline\n```\nSee log here: https://www.internalfb.com/intern/buck/build/6eaea1f8-e237-4860-9f3b-3a8edd2207c6/\n\nThis is because Windows doesn't support `__has_attribute` keyword. Here I'm changing the ordering of `if` and `elif` so that we don't hit that line when build in Windows.\n\nTest Plan: buck build //xplat/caffe2/c10:c10Windows xplat/mode/windows\n\nReviewed By: kimishpatel, swolchok\n\nDifferential Revision: D26896510\n\nfbshipit-source-id: d52438a3df7bf742e467a919f6ab4fed14484f22", "pr_number": "53562", "files_changed": ["c10/macros/Macros.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "06cf6d37b5": {"title": "[ROCm] Enable test cases in test_data_parallel.py for ROCm (#52708)", "body": "Summary:\nEnabling the test cases because they are passing for ROCm.\n\nSigned-off-by: Kyle Chen <kylechen@amd.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52708\n\nReviewed By: albanD\n\nDifferential Revision: D26994458\n\nPulled By: mrshenli\n\nfbshipit-source-id: f0b3797c7889287a0154b1d5397df715ffb1c605", "pr_number": "52708", "files_changed": ["test/distributed/test_data_parallel.py"], "labels": ["Merged", "cla signed", "module: rocm", "oncall: distributed", "open source", "triaged"]}, "dfb5f029da": {"title": "Disable TF32 on DDP tests (#52941)", "body": "Summary:\nWhen a system has an ampere and a non-ampere card, lots of tests will fail, because results on different cards are differnet.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52941\n\nReviewed By: albanD\n\nDifferential Revision: D26994287\n\nPulled By: mrshenli\n\nfbshipit-source-id: 287537495fc13361104a4460f5bcd79a208b5d8d", "pr_number": "52941", "files_changed": ["test/distributed/test_distributed_fork.py", "test/distributed/test_distributed_spawn.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "5c2b3d7784": {"title": "[ROCm] Enable RNN test in test_c10d_spawn.py for ROCm (#52707)", "body": "Summary:\nEnabling test_rnn test because it is passing for ROCm.\n\nSigned-off-by: Kyle Chen <kylechen@amd.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52707\n\nReviewed By: albanD\n\nDifferential Revision: D26994407\n\nPulled By: mrshenli\n\nfbshipit-source-id: f7d60ab7c4f0128e5f7770f959e2b83694d18275", "pr_number": "52707", "files_changed": ["test/distributed/test_c10d_spawn.py"], "labels": ["Merged", "cla signed", "module: rocm", "oncall: distributed", "open source", "triaged"]}, "d726ce6668": {"title": "Support loading a non-DP/DDP model from a DP/DDP state_dict (#53224)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53224\n\nLoading a DP/DDP dict just needs to strip the module prefix from all items in the state dict and the metadata.\n\nOne existing example is here: https://github.com/facebookresearch/fvcore/blob/master/fvcore/common/checkpoint.py#L239.\n\n#Closes: https://github.com/pytorch/pytorch/issues/41048/\nghstack-source-id: 123722976\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test:nn -- test_load_state_dict\nbuck test mode/dev-nosan caffe2/test/distributed:c10d -- test_save_load_checkpoint\n\nReviewed By: rohan-varma, mrshenli\n\nDifferential Revision: D26798495\n\nfbshipit-source-id: 035c7d0907d7ae8f0d7ca21ec71f7f96ef8df6c8", "pr_number": "53224", "files_changed": ["test/distributed/test_c10d.py", "test/test_nn.py", "torch/nn/modules/utils.py", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed"]}, "cffe9aa617": {"title": "[libkineto] Log CUPTI errors on libkineto initialization", "body": "Summary: When libkineto is initialized from the PyTorch Profiler, if it fails we will not know why because errors are not reported. Reporting errors is not always safe, e.g. if init happens from static initialization or a dlopen library constructor function, so add a flag to specify whether to log.\n\nTest Plan: Testing in PyTorch OSS build.\n\nReviewed By: chaekit\n\nDifferential Revision: D26927500\n\nfbshipit-source-id: 2a78005239a5fcbe7e1de82e5405f04e07000fa8", "pr_number": null, "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": []}, "9f75de278f": {"title": "Move common autograd utils functions from gen_variable_type.py to api/autograd.py. (#53340)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53340\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26973914\n\nPulled By: ailzhang\n\nfbshipit-source-id: 8367a08b27b25808782c77aadc3c67d07c354957", "pr_number": "53340", "files_changed": ["tools/autograd/gen_autograd.py", "tools/autograd/gen_python_functions.py", "tools/autograd/gen_trace_type.py", "tools/autograd/gen_variable_factories.py", "tools/autograd/gen_variable_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/autograd.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/native.py", "tools/codegen/api/python.py", "tools/codegen/api/structured.py", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "aeb3e93351": {"title": "Move view handling logic to gen_inplace_or_view_type.py (#53341)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53341\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26973912\n\nPulled By: ailzhang\n\nfbshipit-source-id: ea31bdef0beac6996d509f5d45ebefa3ea8e2b89", "pr_number": "53341", "files_changed": ["caffe2/CMakeLists.txt", "tools/autograd/gen_autograd.py", "tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_type.py", "tools/codegen/api/autograd.py"], "labels": ["Merged", "cla signed"]}, "1772e26f63": {"title": "[PyTorch] Move selected_mobile_ops.h codegen function to tools (#53786)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53786\n\nTo generate `selected_mobile_ops.h` in OSS, move the header file codegen functions to `tools/lite_interpreter/gen_selected_mobile_ops_header.py` file, so OSS can reuse these functions.\nghstack-source-id: 123754437\n\nTest Plan:\n```\nbuck test //xplat/caffe2:supported_mobile_models_test\n```\n\n```\nbuck run //xplat/caffe2:gen_oplist -- --model_file_list_path @/data/users/chenlai/data/pytorch/oplist_folder/file_list_path.macro  --allow_include_all_overloads --output_dir /home/chenlai/local/data/pytorch/oplist_folder\n```\n\n`file_list_path.macro` content is:\n```\nchenlai@devvm2090:~/fbsource(45a9b7888)$ cat /data/users/chenlai/data/pytorch/oplist_folder/file_list_path.macro\n/data/users/chenlai/fbsource/buck-out/gen/aab7ed39/xplat/caffe2/supported_mobile_models_test_op_list/model_operators.yaml\n```\n\nIn output folder `/home/chenlai/local/data/pytorch/oplist_folder`, these files are generated:\n```\nselected_mobile_ops.h  selected_operators.yaml  SupportedMobileModelsRegistration.cpp\n```\n\nthe generated files are the same as before.\n\n{P282056731}\n\n{P282055046}\n\nReviewed By: dhruvbird, iseeyuan\n\nDifferential Revision: D26907868\n\nfbshipit-source-id: 9ba786f9c5674a72cad237ae7baadbe4642c51d5", "pr_number": "53786", "files_changed": ["tools/lite_interpreter/gen_selected_mobile_ops_header.py"], "labels": ["Merged", "cla signed"]}, "be344e9d88": {"title": "Update test cases generated by make_test() method to support running them in script mode. (#52748) (#53308)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53308\n\n* Update tests for test_gru_* at this moment.\n\n* Update flake8 error.\n\n* Update tests for test_gru_* at this moment.\n\n* Update flake8 error.\n\n* Update test_gru_* test cases only.\n\n* Fix flake8 issue.\n\n* Fix flake8 issue on test.\n\n* Still disable test cases created by make_test.\n\n* Update code to fix issue 'AttributeError: 'RecursiveScriptModule' object has no attribute 'forward'' for test_elman_* test cases.\n\n* Add script model support for test_lstm_* test cases.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich, malfet\n\nDifferential Revision: D26922419\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: a96432b2e7da9b142a38f87fbaf56737117462c1", "pr_number": "53308", "files_changed": ["test/onnx/model_defs/lstm_flattening_result.py", "test/onnx/model_defs/rnn_model_with_packed_sequence.py", "test/onnx/test_pytorch_onnx_onnxruntime.py"], "labels": ["Merged", "cla signed", "open source"]}, "4c1d9e58c2": {"title": "Fix copy_ export (#53046) (#53310)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53310\n\nFixes export of torch.copy_\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich, malfet\n\nDifferential Revision: D26922424\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: f509e531f5064d2be7f55e1681813f10f17475d2", "pr_number": "53310", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset11.py"], "labels": ["Merged", "cla signed", "open source"]}, "a51f130d37": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27005870\n\nfbshipit-source-id: 5d51d0e64ae3fb15d38f8a9f8479af1c86b18fa9", "pr_number": null, "files_changed": ["torch/csrc/jit/passes/onnx/helper.h"], "labels": []}, "4873641602": {"title": "Fix TCPStore wait() hang when key is previously set (#53860)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53860\n\nFixes [#53840](https://github.com/pytorch/pytorch/issues/53840)\n\nRight now [TCPStore wait([LIST_OF_KEYS_TO_AWAIT])](https://pytorch.org/docs/master/distributed.html#torch.distributed.Store.wait) will hang if any of the keys in [LIST_OF_KEYS_TO_AWAIT] has been previously set. This change will ensure that wait() is only waiting for the keys that have not been set\n\nBefore change:\n```\n# Case 1: HANG\nstore.set(\"1\", \"1\")\nstore.wait([\"1\", \"2\"])\nstore.set(\"2\", \"2\")\n\n# Case 2: SUCCEED\nstore.wait([\"1\", \"2\"])\nstore.set(\"1\", \"1\")\nstore.set(\"2\", \"2\")\n```\nAfter change:\nBoth cases work\n\nTODO: working on adding a test for wait()\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26999929\n\nPulled By: H-Huang\n\nfbshipit-source-id: 8931749923c98b520366538f785af82ef37cca8e", "pr_number": "53860", "files_changed": ["torch/lib/c10d/TCPStore.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ccab6680d5": {"title": "[not for land yet] hacky fix for x.ndim followed by sub (#53120)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53120\n\nCurrently there is a pattern which is not handled correctly by\nFX graph mode quantization:\n\n```\ndef forward(self, x):\n    ndim = x.ndim\n    # or add, mul, div, etc\n    x = torch.sub(x, ndim)\n    return x\n```\n\nThe reason this does not work is as follows:\n1. x.ndim becomes a getattr node\n2. the real world type of x.ndim is an integer, but this is not known from the graph (yet)\n3. binary ops such as `torch.sub` require quantization of inputs\n4. the framework inserts an observer to observe the output of `ndim`\n5. the observer fails because `ndim` is not a Tensor\n\nFor now, we hack a bandaid to unblock some teams, none of this is for\nland.  We will have to think of a better fix which is landable (TBD).\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx.test_getattr_with_nontensor_result\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D26756180\n\nfbshipit-source-id: c0e498766b22c23df74fbb5aaeaa237c4c944263", "pr_number": "53120", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py"], "labels": ["Merged", "cla signed", "fx"]}, "93d5807c1e": {"title": "[not for land yet]fix using size of quant layer in torch._assert (#53187)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53187\n\nBefore this diff, if we had code lik\n\n```\nx = any_quant_layer(...)\nx_size0 = x.size(0)\ntorch._assert(x_size_0 == 1)\n```\n\nThe convert code would try to insert a dequantize after `x_size0`,\nbecause it was a descendant of a quantized node and it was needed\nfor a non-quantized operation.  Since the actual type of the `size`\nfunction output is an integer, this does not make sense.\n\nFor now, this is fixed as a one-off to unblock a customer.  In the\nfuture, we may need to think more deeply about all the functions which\ncan return non-quantized types from quantized tensors and make sure\nthey are all covered.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx.test_assert_on_size_after_quant_layer\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26780690\n\nfbshipit-source-id: 44cc25c9179d460efb3f110d40b73d854d676af5", "pr_number": "53187", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py"], "labels": ["Merged", "cla signed", "fx"]}, "279b5372ab": {"title": "[not for land] fix fx quant for quant_layer -> stack -> sum (#53196)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53196\n\nBefore this PR, code patterns like this did not work:\n\n```\nx = some_quant_layer(x)\nx = torch.stack([x, ...])\nx = torch.sum(x, ...)\n```\n\nThe reason this did not work is because `torch.sum` is treated as\n\"quantized\" because of the newly added fp16 support, even though it is\nnot actually \"quantized\" for models where fp16 is not used.  We may\nneed to adjust the concept of \"quantized vs non-quantized\" into a\n\"dtype\" for the longer term fix.\n\nThe current PR is a hacky fix to unblock.  We need to clean things\nup before this is landable\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx.test_quant_sum\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D26783960\n\nfbshipit-source-id: 3be7c3c1eaa2b8fcb99a105e1b0004c9ffd3a1c1", "pr_number": "53196", "files_changed": ["test/quantization/test_quantize_fx.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py"], "labels": ["Merged", "cla signed", "fx"]}, "4884a6ab51": {"title": "fx quant: clean up names of quantize handlers (#53614)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53614\n\nEnsures that every subclass of `QuantizeHandler` has a clear name.  This\nprevents ambiguous names like `Cat`, which look like a module but are\nreally a quantize handler.\n\nTest Plan:\n```\npython test/test_quantization.py TestQuantizeFx\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D26914784\n\nfbshipit-source-id: 6dca7e27975c09f422f8e36f1d2b709bf3eaaadf", "pr_number": "53614", "files_changed": ["torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py"], "labels": ["Merged", "cla signed", "fx"]}, "7297556d5d": {"title": "Add support for single tensor in `inputs` argument for backward (#53827)", "body": "Summary:\nAlso updates the doc such that the language matches the type. For example, previously the `tensors` argument is specified as `(sequence of tensor)`, but has type annotation of `_TensorOrTensors`. Now its correctly updated to be `Sequence[Tensor] or Tensor`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53827\n\nReviewed By: albanD\n\nDifferential Revision: D26997541\n\nPulled By: soulitzer\n\nfbshipit-source-id: e1e609a4e9525139d0fe96f6157175481c90d6f8", "pr_number": "53827", "files_changed": ["test/test_autograd.py", "torch/autograd/__init__.py"], "labels": ["Merged", "cla signed"]}, "a7ddd15d15": {"title": "fix static dispatch linker error (#53859)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53859\n\nThe redispatch API wasn't linking properly when static dispatch is enabled. I'm still not sure why this wasn't caught by the static dispatch test in CI- maybe, as swolchok pointed out, we have a flag set somewhere that defers undefined symbols until runtime.\n\nBefore, building with static dispatch enabled locally + running `import torch` gave me this error:\n```\n>>> import torch\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/raid/hirsheybar/pytorch/torch/__init__.py\", line 197, in <module>\n    from torch._C import *\nImportError: /raid/hirsheybar/pytorch/torch/lib/libtorch_cpu.so: undefined symbol: _ZN2at10redispatch11logical_or_EN3c1014DispatchKeySetERNS_6TensorERKS3_\n>>>\n```\n\nPrinting the symbol:\n```\n(pytorch) hirsheybar@devfair017:/scratch/hirsheybar/pytorch$ c++filt _ZN2at10redispatch11logical_or_EN3c1014DispatchKeySetERNS_6TensorERKS3_\nat::redispatch::logical_or_(c10::DispatchKeySet, at::Tensor&, at::Tensor const&)\n```\n\nSure enough, the functions defined in `RedispatchFunctions.cpp` don't have the DispatchKeySet argument included. Adding them in this PR.\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53\n\nDifferential Revision: D26998735\n\nPulled By: bdhirsh\n\nfbshipit-source-id: c6c1104e42d13b7ec9d964b7e08d2adc8b344b78", "pr_number": "53859", "files_changed": ["tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "ae7984b1d6": {"title": "Do not use shards for single run tests (#53883)", "body": "Summary:\nDo not compute shards if whole testsuite needs to be run anyway.\nHelps avoid occasional test duplication/gaps when access to test time database is not available while one of the shards is computed\n\nFixes https://github.com/pytorch/pytorch/issues/53882\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53883\n\nReviewed By: janeyx99\n\nDifferential Revision: D27005910\n\nPulled By: malfet\n\nfbshipit-source-id: f9603db0523a3a2539118e3fec1c6874c54f8d6d", "pr_number": "53883", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["Merged", "cla signed"]}, "b00cdfe136": {"title": "Fix run_test_module logic (#53884)", "body": "Summary:\nFirst argument is either file name or test module name, but key to `CUSTOM_HANDLERS` is test module name.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53884\n\nTest Plan: Run `python3 run_test.py -i distributed/test_distributed_spawn.py`\n\nReviewed By: janeyx99\n\nDifferential Revision: D27006164\n\nPulled By: malfet\n\nfbshipit-source-id: f30b42856cd2754e5981c1c69618f84e392c986a", "pr_number": "53884", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed"]}, "d73e36a44a": {"title": "ns for fx: change API to take nn.Module instead of GraphModule (#53075)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53075\n\nThe input and output types should be `nn.Module`, to hide\nthe implementation detail that the pass is using FX.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D26740548\n\nfbshipit-source-id: d5ed445379355bebdd90d377c95fcd7e671371a3", "pr_number": "53075", "files_changed": ["torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "cc940f3580": {"title": "ns for fx: change dtype cast from once per N node to once per node", "body": "Summary:\nThis PR ensures that when we do a dtype cast for a shadow module,\nwe insert N dtype casts for N nodes, instead of combining N nodes\ninto a single dtype cast.\n\nAn example where this occurs is `cat([x, y], dim=0)`\n\n```\n// original graph\n\n[x, y] -> cat_b -> output\n\n// shadow graph with a single dtype cast, before this PR\n\n  dtype_cast -> cat_a_shadow -> output_a_shadow\n  /\n[x, y] -> cat_b -> output_b\n\n// shadow graph with multiple dtype casts, after this PR\n\n [dtype_cast_x, dtype_cast_y] -> cat_a_shadow -> output_a_shadow\n /\n[x, y] -> cat_b -> output_b\n```\n\nThe reason things worked before this PR is because `torch.dequantize`\ncan take either a single tensor or a list of tensors.  We are changing\nthis to make an upcoming addition of input loggers easier.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_prepare_model_with_stubs_multiple_dtype_casts\n```\n\nImported from OSS\n\nDifferential Revision: D26931226\n\nReviewed By: hx89\n\nPulled By: vkuzo\n\nfbshipit-source-id: e9c7d4c7942e0f59c952094d2e446b1e2c838396", "pr_number": null, "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py"], "labels": []}, "421e91dfd2": {"title": "ns for fx: add support for logging inputs", "body": "Summary:\nThis PR implements the option to log inputs for FX Numeric Suite.  The user facing api looks like\n\n```\ndef prepare_model_outputs(..., should_log_inputs : bool = False)\ndef prepare_model_with_stubs(..., should_log_inputs : bool = False)\n```\n\nThe output data now looks like\n\n```\n{\n  \"layer1\": {\n    \"node_inputs\": {\n      \"model1\": [{\n        \"values\": ...,\n        ...,\n      }],\n    },\n    \"node_outputs\": {\n      ...,\n    }\n  },\n  ...  // other layers\n}\n```\n\nOne key design decision taken here is that an input logger logs the output of previous nodes, instead of logging the input of the current node.  This matters for a signature such as `cat([x1, x2, x3])`.  We are inserting three input loggers here (for x1, x2, and x3), instead of a single input logger for `[x1, x2, x3]`.  This was chosen in order to preserve the structure of the original graph as much as possible and keep flexibility for future optimizations.\n\nTest Plan:\nTODO: fill out\n\nImported from OSS\n\nDifferential Revision: D26931225\n\nReviewed By: hx89\n\nPulled By: vkuzo\n\nfbshipit-source-id: dd692bfb5ddaaf5554f80c25e2f40b21762e4fc3", "pr_number": null, "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/ns_types.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": []}, "7d27eb8068": {"title": "ns for fx: clean up API naming (#53729)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53729\n\nAligns the names of the three core APIs to the design doc.\nNew names:\n\n```\n// weights\n_extract_weights_one_model\nextract_weights\n\n// unshadowed activations\n_add_loggers_one_model\nadd_loggers\n_extract_logger_info_one_model\nextract_logger_info\n\n// shadowed activations\nadd_shadow_loggers\nextract_shadow_logger_info\n```\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26953071\n\nfbshipit-source-id: dda6df1c26afd99dd7779e72e3eed2d3d72c8128", "pr_number": "53729", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "986e3c0a00": {"title": "ns for fx: extract common code in tests to util functions (#53748)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53748\n\nExtracts common testing patterns for FX numeric suite into\nutil functions.  No logic change.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D26967105\n\nfbshipit-source-id: 9f6cbe75bb6d2ede142929e0c9e40812006c159d", "pr_number": "53748", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "19fe8a529e": {"title": "ns for fx: move conv weight test case to new API (#53761)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53761\n\nMoves the testing of conv weight matching to new NS APIs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_weights_conv\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26967108\n\nfbshipit-source-id: af3647733f954a657e0868c2c40642018de9ea49", "pr_number": "53761", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "9c8f112ada": {"title": "ns for fx: move linear weight test case to new API (#53764)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53764\n\nMoving the linear weight test case to new FX NS APIs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_weights_linear\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26967111\n\nfbshipit-source-id: f0a90d7863d5d866e391729ec28e0e0dea339900", "pr_number": "53764", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "a71cd135ae": {"title": "ns for fx: move linear dynamic weight test case to new API (#53765)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53765\n\nMoves linear dynamic weight test case to new NS API.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_weights_linear\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26967109\n\nfbshipit-source-id: 2096a88a3005270696d536f2e1bbc87e70c07230", "pr_number": "53765", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "01c6e9360e": {"title": "ns for fx: move lstm dynamic weight test case to new API (#53772)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53772\n\nMoves the test case for extracting LSTM dynamic weights to new NS API.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_weights_lstm_dynamic\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26967104\n\nfbshipit-source-id: 0d17e7735ec361167dcf72bcb373bfc1aad84668", "pr_number": "53772", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "57bf13409a": {"title": "ns for fx: move compare activations for conv test to new API (#53776)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53776\n\nMoves the test for comparing activations for conv to new API.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_activations_conv\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26967106\n\nfbshipit-source-id: 2eb986ff19761a1e2408cb7780ac0b282cdcc523", "pr_number": "53776", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "2912ad1324": {"title": "ns for fx: move linear activation test case to new API (#53777)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53777\n\nMoves linear activation test case to new NS API\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_activations_linear\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26967107\n\nfbshipit-source-id: 83c4401b2bf79d15227b7fb3e59c54276ec5626b", "pr_number": "53777", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "7f4aff8203": {"title": "Skip dispatch for is_signed (#53847)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53847\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D26994937\n\nPulled By: carolineechen\n\nfbshipit-source-id: 8af25ecdade0b31d29fac27de6ee5f704353af10", "pr_number": "53847", "files_changed": ["aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/TensorBody.h"], "labels": ["Merged", "cla signed"]}, "804f3f9879": {"title": "[PyTorch] Remove unnecessary assert in maybe_resize_storage_cpu (#53724)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53724\n\nSee new code comment -- stealAndSetStoragePtr calls set_storage_keep_dtype.\nghstack-source-id: 123636226\n\nTest Plan: CI\n\nReviewed By: mruberry\n\nDifferential Revision: D26922164\n\nfbshipit-source-id: fe1dd2b3e5f0876b8b41694ff2fb19b9ca2bae61", "pr_number": "53724", "files_changed": ["aten/src/ATen/native/Resize.h"], "labels": ["Merged", "cla signed"]}, "90dfdef226": {"title": "[CUDA graphs] Private mempools for CUDA graphs (#51436)", "body": "Summary:\nImplements https://github.com/pytorch/pytorch/issues/51075#issuecomment-768884685 and additions discussed offline with ezyang ngimel . (Calling it \"simple\" is charitable but it's not too bad).\n\n[High level strategy](https://github.com/pytorch/pytorch/pull/51436/files#diff-acc6337586bf9cdcf0a684380779300ec171897d05b8569bf439820dc8c93bd5R57-R82)\n\nThe current design aggregates stats from private pools with the ordinary pools, which may or may not be what we want.\n\nInstead of adding PrivatePools as an internal feature of DeviceAllocator, I could inherit from DeviceAllocator (eg `DevicePrivateAllocator : public DeviceAllocator`) and create separate per-graph instances of the inherited class. I'm not sure if that would be better.\n\nGraph bindings in Python are almost unchanged from https://github.com/pytorch/pytorch/pull/48875:\n```python\n# Same bindings as 48875, but now implicitly grabs a private mempool\ngraph1.capture_begin()\ngraph1.capture_end()\n\n# pool=... is new.  It hints that allocations during graph2's capture may share graph1's mempool\ngraph2.capture_begin(pool=graph1.pool())\ngraph2.capture_end()\n\n# graph3 also implicitly creates its own mempool\ngraph3.capture_begin()\ngraph3.capture_end()\n```\n\nTest plan (other suggestions appreciated):\n\n- [x] Stop maintaining manual references for all the tensors in my existing graphs+RNG tests. If private pools somehow give bad allocations, they should start failing intermittently. They run eager ops and eager allocations mixed with graph replays, so they may expose if eager ops and replays corrupt each other.\n- [x] `test_graph_two_successive`: Capture successive graphs, with the second graph using the first graph's result. Try with and without sharing a pool. Check results, also check memory stats to confirm sharing a pool saves memory.\n- [x] `test_graph_concurrent_replay`: Capture some graphs in separate private pools, replay them concurrently in different streams, check the results to make sure they don't corrupt each other's memory. Capture some graphs with a shared pool, replay them concurrently in different streams, check results, confirm they DO corrupt each other's memory.\n- [x] `test_graph_three_successive`: A three-graph case, checking the safe and unsafe replay patterns in [Restrictions of the Strawman API](https://github.com/pytorch/pytorch/issues/51075)).\n- [x] `test_graph_memory_stats_and_use_result_after_destroy_graph`: Comprehensively check torch.cuda.memory_stats() changes that result from graph capture and delete. Check that a tensor ref created during capture and held after graph delete stays valid until the tensor itself is deleted.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51436\n\nReviewed By: mruberry\n\nDifferential Revision: D26993790\n\nPulled By: ngimel\n\nfbshipit-source-id: a992eaee1b8c23628e7b388a5a3c26e0f80e54da", "pr_number": "51436", "files_changed": ["aten/src/ATen/cuda/CUDAGraph.cpp", "aten/src/ATen/cuda/CUDAGraph.h", "aten/src/ATen/cuda/CUDAGraphsUtils.cuh", "c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "c10/cuda/CUDAGraphsC10Utils.h", "test/test_cuda.py", "torch/_C/__init__.pyi.in", "torch/csrc/cuda/Graph.cpp", "torch/cuda/__init__.py", "torch/cuda/streams.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "Reverted", "cla signed", "module: cuda graphs", "open source", "triaged"]}, "89fce74d55": {"title": "fix for method_tests() random failures (#53854)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/48125 and https://github.com/pytorch/pytorch/issues/53237\n\nThe origin of the problem was that `common_methods_invocations.method_tests()` uses `set_rng_seed(0)` which is different from the seed used at `TestCase:setUp` -> `set_rng_seed(SEED)`.\n\nAs this issue might block removing old tests I also notice that  this could also be the reason of test failures at PR https://github.com/pytorch/pytorch/issues/50655\n\nThanks\ncc mruberry, kshitij12345,  imaginary-person\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53854\n\nReviewed By: albanD\n\nDifferential Revision: D27004797\n\nPulled By: mruberry\n\nfbshipit-source-id: 66a15ed900131c782bc341b16c902972d7bb2541", "pr_number": "53854", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "high priority", "open source"]}, "f62e9156dc": {"title": "Add missing decorators in test_spectral_ops (#53736)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53456\n\nI'm confused why this wasn't picked up in CI. There's definitely at least one CI job that builds without MKL. Are spectral_ops not being run at all on that job?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53736\n\nReviewed By: albanD\n\nDifferential Revision: D27007901\n\nPulled By: mruberry\n\nfbshipit-source-id: cd93a2c48f4ccb2fd2e0e35768ee059039868a1b", "pr_number": "53736", "files_changed": ["test/test_spectral_ops.py"], "labels": ["Merged", "cla signed", "open source"]}, "d4c877b59b": {"title": "Fix typo \"informations\" -> \"information\" (#53746)", "body": "Summary:\nHey, fixing the [uncountable](https://www.oxfordlearnersdictionaries.com/definition/american_english/information) noun to the proper form.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53746\n\nReviewed By: ngimel\n\nDifferential Revision: D27012035\n\nPulled By: albanD\n\nfbshipit-source-id: dc653e739b5f6abed99b74bd2fd514b795d61b2e", "pr_number": "53746", "files_changed": ["torch/_tensor.py"], "labels": ["Merged", "cla signed", "open source"]}, "fdbd667e31": {"title": "compareSet method for HashStore and FileStore (#53803)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53062\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53803\n\nReviewed By: ngimel\n\nDifferential Revision: D27017014\n\nPulled By: H-Huang\n\nfbshipit-source-id: 736aa5ad848f5708e6581e472e48d5682bef7131", "pr_number": "53803", "files_changed": ["test/distributed/test_c10d.py", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/FileStore.hpp", "torch/lib/c10d/HashStore.cpp", "torch/lib/c10d/HashStore.hpp", "torch/lib/c10d/test/FileStoreTest.cpp", "torch/lib/c10d/test/HashStoreTest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "fe38027fc3": {"title": "[fix] torch.cat : cross-device check for out and input tensors (#53004)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52044 (`stack` dispatches to `cat`)\n\nThe way dispatcher works, currently this case happens only in CUDA kernel (CPU kernel is chosen if all inputs and out are on CPU). That is why the check is added only on the CUDA side.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53004\n\nReviewed By: albanD\n\nDifferential Revision: D27003956\n\nPulled By: mruberry\n\nfbshipit-source-id: 818ea0f76153f4fa281740f30705e5ef018413f6", "pr_number": "53004", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/cuda/Shape.cu", "test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed", "module: viewing and reshaping", "open source", "triaged"]}, "4932342363": {"title": "[Static Runtime] Fix bug in ClipRangesGatherRangesX2SigridHash (#53799)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53799\n\nFix two issues with ClipRangesGatherRangesX2SigridHash and ClipRangesGatherRangesX2SigridHashPrecompute:\n- The first issue is with the two step graph rewrite process. If step 2 doesn't happen after step 1, then we're stuck with a graph with a `fb::placeholder` op that can't run. Step 3 is added to revert step 1 so we restore the original graph if there's any `fb::placeholder` op left.\n- The second issue is with `SigridHashPrecompute`. The coupling with `freeze_module` is not ideal and limits its use to Static Runtime only. By running `ConstantPropagation` and `ConstantPooling` after splitting SigridHash, we can move all the Constant ops to the front of the graph and fusion can happen right afterwards.\n\nReviewed By: ajyu\n\nDifferential Revision: D26920008\n\nfbshipit-source-id: e4bc67c7a15181bac5dbbfbb95d861849652bddf", "pr_number": "53799", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "fe08671756": {"title": "Added cuBLAS path for torch.triangular_solve (#53147)", "body": "Summary:\nThis PR adds the cuBLAS based path for `torch.triangular_solve`\nThe device dispatching helper function was removed from native_functions.yml, it is replaced with DECLARE/DEFINE_DISPATCH.\n\n`magmaTriangularSolve` is removed and replaced with cuBLAS calls, this is not a BC-breaking change because internally MAGMA just calls the same cuBLAS function and doesn't do anything else.\n\nBatched cuBLAS is faster than batched MAGMA for matrices of size up until 512x512, after that MAGMA is faster. For batches smaller than ~8 and matrix sizes larger than 64x64 a forloop of cuBLAS calls is faster than batched version.\n\nRef. https://github.com/pytorch/pytorch/issues/47953\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53147\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27007416\n\nPulled By: mruberry\n\nfbshipit-source-id: ddfc190346e6a56b84145ed0a9af67ca9cde3506", "pr_number": "53147", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/LinearAlgebraUtils.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_linalg.py", "torch/_torch_docs.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source", "triaged"]}, "00771eff8e": {"title": "[reland] Add OpInfo for `bitwise_not` (#53181)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nNote: Reland https://github.com/pytorch/pytorch/issues/51944\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53181\n\nReviewed By: albanD\n\nDifferential Revision: D27004695\n\nPulled By: mruberry\n\nfbshipit-source-id: 92b4e8c60bb6f3c302907716de040b5c81c8db69", "pr_number": "53181", "files_changed": ["test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "open source"]}, "3078233e9a": {"title": "[Gradient Compression] Make FP16 compression as a wrapper that can be combined with other communication hooks (#53808)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53808\n\nCreate a FP16 wrapper that can combine FP16 gradient compression with any gradient compression algorithm.\n\nTest Plan:\nUnit test:\n```\nbuck test mode/dev-nosan caffe2/test/distributed:c10d -- test_fp16_compress_wrapper\n```\n\nPerformance Test on DDP QPS Benchmark: Check if AllReduce + FP16 Wrapper = FP16 Compression\n1) FP16 Compression:\nf256897690\n\n2) FP16 Wrapper + AllReduce (after patching D26960986):\nf256897289\n\nReviewed By: SciPioneer\n\nDifferential Revision: D26978832\n\nfbshipit-source-id: 0dcd18b050c02f5e9f3cff56344d1f39a04e20c0", "pr_number": "53808", "files_changed": ["docs/source/ddp_comm_hooks.rst", "test/distributed/test_c10d.py", "torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py", "torch/lib/c10d/comm.hpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "ce0fd095a8": {"title": "Implemented embedding_bag for SR (#52429)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52429\n\nImplemented embedding_bag for supporting out version in SR\n\nBefor:Milliseconds per iter: 1.15443. Iters per second: 866.226\n\n After:  Milliseconds per iter: 1.14791. Iters per second: 871.149\n\nTest Plan:\nbuck test caffe2/test:nn\nbuck test caffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nReviewed By: hlu1\n\nDifferential Revision: D26089498\n\nfbshipit-source-id: c9ba7068d5aa696c8f37a4846d8e80c6379538d2", "pr_number": "52429", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/EmbeddingBag.h", "benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "317ff429d3": {"title": "[TB] Support writing new style scalar (#53496)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53496\n\nNew style vs old style\nhttps://github.com/tensorflow/tensorboard/blob/b306651ab5baa6d362326464a703463b4683b265/tensorboard/data_compat.py#L49-L53\n\nWriting in new style can help avoid the cost of migration\nhttps://github.com/tensorflow/tensorboard/blob/b306651ab5baa6d362326464a703463b4683b265/tensorboard/data_compat.py#L46\n\n----\n\nTest Plan:\nbuck run caffe2/test:tensorboard\n\n ---\n\nReviewed By: edward-io\n\nDifferential Revision: D26879076\n\nfbshipit-source-id: 43cfe9e1ca52dad3efc10332715d39f1cc984862", "pr_number": "53496", "files_changed": ["test/expect/TestTensorBoard.test_scalar_new_style.expect", "test/test_tensorboard.py", "torch/utils/tensorboard/summary.py", "torch/utils/tensorboard/writer.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "3ce51fd5f4": {"title": "remove th_fill and th_mul dead code (#52546)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52546\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D26654127\n\nPulled By: malfet\n\nfbshipit-source-id: 68b777cd8ce2992a876dc8d22276a2afcef4830e", "pr_number": "52546", "files_changed": ["BUILD.bazel", "aten/src/TH/CMakeLists.txt", "aten/src/TH/THVector.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THVector.h", "aten/src/TH/generic/THVectorDefault.cpp", "aten/src/TH/generic/THVectorDispatch.cpp", "aten/src/TH/vector/AVX.cpp", "aten/src/TH/vector/AVX.h", "aten/src/TH/vector/NEON.cpp", "aten/src/TH/vector/VSX.cpp", "aten/src/TH/vector/simd.h", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "open source"]}, "ca4aae85fa": {"title": "[Gradient Compression] Update the docstring of fp16_compress_wrapper (#53955)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53955\n\nPer title\nghstack-source-id: 123852836\n\nTest Plan: N/A\n\nReviewed By: iseessel\n\nDifferential Revision: D27032700\n\nfbshipit-source-id: 6f9bbc028efe6cc9b54f4ec729fea745368efb2e", "pr_number": "53955", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "de70cdb66b": {"title": "Clang format default_hooks.py (#53956)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53956\n\nghstack-source-id: 123852987\n\nTest Plan: N/A\n\nReviewed By: iseessel\n\nDifferential Revision: D27032713\n\nfbshipit-source-id: 11d831fa0f08b1c8bc2e44acd144bf85a69a1211", "pr_number": "53956", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "142c6b0e55": {"title": "increase timeout for test_op_nnpi_fp16", "body": "Summary: As title. Otherwise we are getting flaky when running on devices in dev mode\n\nReviewed By: jfix71\n\nDifferential Revision: D27035924\n\nfbshipit-source-id: 4946a90bd341be63d74b7052cace3fabdefdc0c4", "pr_number": null, "files_changed": ["caffe2/contrib/fakelowp/test/test_op_nnpi_fp16.py"], "labels": []}, "39f50f468d": {"title": "matmul performance benchmarks (#51647)", "body": "Summary:\nMinor PR following up the previous PR about sparse benchmarking utils https://github.com/pytorch/pytorch/pull/48397\n\nFixes https://github.com/pytorch/pytorch/issues/44634:  Performance benchmarks for matrix-matrix and matrix-vector ops (dense-sparse, sparse-sparse, and compare to dense-dense)\n\nI ran  all benchmarks on an 2xRTX8000 machine with AMD 2970WX 24-cores for `DLMC/magnitude_pruning` dataset with different sparsity levels.\n\n ---\n<details><summary> forward tests (expand for details).\n</summary>\n\n- `sparse@sparse`\n```\n[------------------------------- cpu:matmul-forward -------------------------------]\n                           |   0.5   |   0.7   |   0.8   |   0.9   |  0.95  |   0.98\n1 threads: -------------------------------------------------------------------------\n      torch:dense@dense    |  108.1  |  100.5  |  101.3  |  108.4  |  98.4  |  187.4\n      torch:sparse@sparse  |  659.1  |  368.8  |  156.5  |   53.3  |  26.8  |   14.9\n      scipy:sparse@sparse  |  565.1  |  233.9  |  130.2  |   23.1  |  21.6  |   15.2\n\nTimes are in milliseconds (ms).\n\n[----------------------------------- cuda:matmul-forward -----------------------------------]\n                           |    0.5    |    0.7    |   0.8    |   0.9    |   0.95   |   0.98\n1 threads: ----------------------------------------------------------------------------------\n      torch:dense@dense    |   2243.5  |   4392.5  |  4419.8  |  2272.3  |  4433.9  |  8920.1\n      torch:sparse@sparse  |  21369.2  |  11877.6  |  7339.2  |  1787.2  |  1335.1  |   845.7\n\nTimes are in microseconds (us).\n\n```\n- `sparse@dense`\n```\n[------------------------------- cpu:matmul-forward -------------------------------]\n                          |   0.5   |   0.7   |   0.8   |   0.9   |   0.95  |   0.98\n1 threads: -------------------------------------------------------------------------\n      torch:dense@dense   |  105.8  |  103.8  |  103.0  |  104.4  |  104.4  |  197.0\n      torch:sparse@dense  |  119.9  |  102.4  |   84.0  |   19.7  |   16.8  |   11.6\n      scipy:sparse@dense  |  906.5  |  799.6  |  697.8  |  182.2  |  165.5  |  135.4\n\nTimes are in milliseconds (ms).\n\n[------------------------- cuda:matmul-forward --------------------------]\n                          |  0.5  |  0.7  |  0.8  |  0.9  |  0.95  |  0.98\n1 threads: ---------------------------------------------------------------\n      torch:dense@dense   |  2.2  |  4.4  |  4.4  |  2.3  |  4.5   |  2.3\n      torch:sparse@dense  |  5.7  |  6.6  |  4.5  |  1.4  |  1.4   |  1.3\n\nTimes are in milliseconds (ms).\n\n```\n- `sparse@vector`\n```\n[----------------------------------- cpu:matmul-forward ----------------------------------]\n                           |    0.5    |   0.7    |   0.8    |   0.9    |   0.95   |   0.98\n1 threads: --------------------------------------------------------------------------------\n      torch:dense@vector   |    510.6  |   505.8  |   759.6  |   782.1  |   682.4  |  764.6\n      torch:sparse@vector  |  10122.8  |  6241.1  |  7935.6  |  2076.3  |  1049.5  |  826.3\n      scipy:sparse@vector  |   1756.7  |  1033.9  |   678.2  |   343.5  |   168.5  |   65.4\n\nTimes are in microseconds (us).\n\n[-------------------------------- cuda:matmul-forward --------------------------------]\n                           |   0.5    |   0.7    |   0.8   |   0.9   |   0.95  |   0.98\n1 threads: ----------------------------------------------------------------------------\n      torch:dense@vector   |    36.1  |    21.5  |   21.6  |   21.5  |   21.6  |   21.5\n      torch:sparse@vector  |  1099.2  |  1289.4  |  775.7  |  327.1  |  285.4  |  274.0\n\nTimes are in microseconds (us).\n\n```\n</details>\n\n ---\n<details><summary> backward tests (expand for details).\n</summary>\n\n- `sparse@sparse`\n```\n[--------------------------------- cpu:matmul-backward ---------------------------------]\n                           |   0.5    |   0.7    |   0.8    |   0.9    |   0.95  |   0.98\n1 threads: ------------------------------------------------------------------------------\n      torch:dense@dense    |   246.1  |   315.0  |   306.9  |   168.6  |  290.6  |  146.9\n      torch:sparse@sparse  |  6417.5  |  4393.7  |  3012.7  |  1029.4  |  908.0  |  650.7\n\nTimes are in microseconds (us).\n\n[----------------------------- cuda:matmul-backward -----------------------------]\n                           |   0.5   |   0.7   |   0.8   |  0.9   |  0.95  |  0.98\n1 threads: -----------------------------------------------------------------------\n      torch:dense@dense    |    6.7  |   13.3  |   13.3  |   6.9  |  13.5  |   6.9\n      torch:sparse@sparse  |  143.7  |  143.4  |  119.6  |  29.5  |  29.1  |  10.9\n\nTimes are in microseconds (us).\n\n```\n- `sparse@dense`\n```\n [------------------------------ cpu:matmul-backward -------------------------------]\n                          |   0.5   |   0.7   |   0.8   |   0.9   |   0.95  |   0.98\n1 threads: -------------------------------------------------------------------------\n      torch:dense@dense   |  185.9  |  304.8  |  305.8  |  169.9  |  308.7  |  168.4\n      torch:sparse@dense  |  407.9  |  345.8  |  274.6  |  114.2  |  163.6  |  230.5\n\nTimes are in milliseconds (ms).\n\n[--------------------------- cuda:matmul-backward --------------------------]\n                          |  0.5   |  0.7   |  0.8   |  0.9  |  0.95  |  0.98\n1 threads: ------------------------------------------------------------------\n      torch:dense@dense   |   6.7  |  13.3  |  13.3  |  6.9  |  13.4  |   6.9\n      torch:sparse@dense  |  16.7  |  19.0  |  15.1  |  6.3  |   8.2  |  12.7\n\nTimes are in milliseconds (ms).\n\n```\n</details>\n\nKindly review this  PR. cc mruberry, ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51647\n\nReviewed By: albanD\n\nDifferential Revision: D27007809\n\nPulled By: mruberry\n\nfbshipit-source-id: 8c1922cb3280027ca5e3eef31bfa20500c548cfd", "pr_number": "51647", "files_changed": ["benchmarks/sparse/dlmc/README.md", "benchmarks/sparse/dlmc/__init__.py", "benchmarks/sparse/dlmc/matmul_bench.py", "benchmarks/sparse/dlmc/test.sh", "benchmarks/sparse/dlmc/utils.py", "benchmarks/sparse/matmul_dlmc_bench.py", "benchmarks/sparse/test.sh"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d46978cc55": {"title": "Refines test_orgqr_* skip (#53975)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/pull/51348 added CUDA support for orgqr but only a cuSOLVER path; the orgqr tests, however, were marked to run on builds with either MAGMA or cuSOLVER.\n\nThis PR addresses the issue by creating a skipCUDAIfNoCusolver decator and applying to the orgqr tests. It triggers ci-all because our CI build with MAGMA but no cuSOLVER is CUDA 9.2, which does run in the typical PR CI.\n\ncc IvanYashchuk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53975\n\nReviewed By: ngimel\n\nDifferential Revision: D27036683\n\nPulled By: mruberry\n\nfbshipit-source-id: f6c0a3e526bde08c44b119ed2ae5d51fee27e283", "pr_number": "53975", "files_changed": ["test/test_linalg.py", "torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed"]}, "790326d49b": {"title": "Fixed the size of the workspace array in functions calling LAPACK (#53909)", "body": "Summary:\nThe size of the workspace array should be max(1, lwork) according to LAPACK documentation. We got away with this previously because we tested only MKL which does a nice thing returning lwork >= 1.\n\nFixes https://github.com/pytorch/pytorch/issues/53454\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53909\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27017025\n\nPulled By: mruberry\n\nfbshipit-source-id: 040a8cfb4bfb98db47d0b117938856d9483b20fb", "pr_number": "53909", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "f6df18f6ca": {"title": "Clean up future imports for Python 2 (#53349)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/42919\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53349\n\nReviewed By: malfet\n\nDifferential Revision: D27039089\n\nPulled By: bugra\n\nfbshipit-source-id: 8063dc184248604506a8dbb1bcb73da8ec85bb18", "pr_number": "53349", "files_changed": ["caffe2/python/operator_test/sparse_normalize_test.py", "caffe2/quantization/server/compute_equalization_scale_test.py", "test/onnx/test_pytorch_onnx_shape_inference.py", "torch/utils/hooks.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d47d246206": {"title": "Add 'noarch' tests which only run in one CI config (#53747)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53747\n\nFixes #53743\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26971343\n\nPulled By: ezyang\n\nfbshipit-source-id: cee7aa10063ae674f741406a3af830e4b4f128df", "pr_number": "53747", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/pytorch_build_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/test.sh", "test/test_torch.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "c2f41b6b84": {"title": "Add meta device to generic device testing framework, skip NotImplementedError (#53682)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53682\n\nWith this, under the meta device, 101 tests passed and 16953 skipped.\nIt ain't much, but it's a start.\n\nSome various bits and bobs:\n- NotImplementedError suppression at test level is implemented\n  in the same way as CUDA memory leak check, i.e., by wrapping\n  test methods and monkeypatching them back in.\n- I had to reimplement assertRaises/assertRaisesRegex from scratch to\n  ignore NotImplementedError when _ignore_not_implemented_error is True.\n  The implementation relies on a small amount of private API that hasn't\n  changed since 2010\n- expectedAlertNondeterministic doesn't really work so I skipped them\n  all; there's probably a way to do it better\n\nI tested this using `pytest --disable-warnings --tb=native -k meta --sw\ntest/*.py` and a pile of extra patches to make collection actually work\n(lol).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D26955539\n\nPulled By: ezyang\n\nfbshipit-source-id: ac21c8734562497fdcca3b614a28010bc4c03d74", "pr_number": "53682", "files_changed": ["test/test_jit.py", "test/test_linalg.py", "test/test_tensor_creation_ops.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "df7c0a06d6": {"title": "[testing] assert no duplicate in method_tests for an OpInfo entry (#53492)", "body": "Summary:\nAssert no duplicate in method_tests for an OpInfo entry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53492\n\nReviewed By: izdeby\n\nDifferential Revision: D26882441\n\nPulled By: mruberry\n\nfbshipit-source-id: f0631ea2b46b74285c76365c679bd45abc917d63", "pr_number": "53492", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "700c817a6a": {"title": "Add install for libCaffe2_perfkernels_avx*.a (#53825)", "body": "Summary:\nWhen build libtorch static library, these three static libraries will be generated but won't be installed to CMAKE_INSTALL_LIBDIR:\n- libCaffe2_perfkernels_avx2.a\n- libCaffe2_perfkernels_avx512.a\n- libCaffe2_perfkernels_avx.a\n\nThis PR will fix this issue.\n\nPlease be noted that after this fix there still have static libraries missing in CMAKE_INSTALL_LIBDIR, but they belong to third_party repo, and we need to fix in the corresponding repo:\n- libfoxi_loader.a\n- libonnx.a\n- libonnx_proto.a\n- libfmt.a\n- libnccl_static.a\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53825\n\nReviewed By: ngimel\n\nDifferential Revision: D27013844\n\nPulled By: malfet\n\nfbshipit-source-id: 8a84cc72b6ae87393ca26c4e474f5526a7b18ab2", "pr_number": "53825", "files_changed": ["caffe2/perfkernels/CMakeLists.txt"], "labels": ["Merged", "cla signed", "open source"]}, "8734e88f0b": {"title": "delete has no more data after the key (#53886)", "body": "Summary:\nThe tcpstore delete key implementation inadvertendly set \"moreData\" when sending the key when it was in fact the last message.\n\nThank you, PetrochukM, for the reproducing example which was instrumental in developing the fix (and is the blueprint for the test case).\n\nFixes https://github.com/pytorch/pytorch/issues/53872\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53886\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27011846\n\nPulled By: H-Huang\n\nfbshipit-source-id: 5c460d1e4d095a8bc267bf63613b556856ced3e8", "pr_number": "53886", "files_changed": ["torch/lib/c10d/TCPStore.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "4f62c622b3": {"title": "Cleanup of unused list in adam.py (#53874)", "body": "Summary:\nCode cleanup.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53874\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27036819\n\nPulled By: ngimel\n\nfbshipit-source-id: c267e20c8d91224cd3c01b302a75f43aa309b560", "pr_number": "53874", "files_changed": ["torch/optim/adam.py"], "labels": ["Merged", "cla signed", "open source"]}, "da10ccd35f": {"title": "Implements cpu_kernel_multiple_outputs and torch.frexp (#51097)", "body": "Summary:\nClose https://github.com/pytorch/pytorch/issues/51108\nRelated https://github.com/pytorch/pytorch/issues/38349\n\nThis PR implements the `cpu_kernel_multiple_outputs` to support returning multiple values in a CPU kernel.\n```c++\nauto iter = at::TensorIteratorConfig()\n  .add_output(out1)\n  .add_output(out2)\n  .add_input(in1)\n  .add_input(in2)\n  .build();\n\nat::native::cpu_kernel_multiple_outputs(iter,\n  [=](float a, float b) -> std::tuple<float, float> {\n    float add = a + b;\n    float mul = a * b;\n    return std::tuple<float, float>(add, mul);\n  }\n);\n```\n\nThe `out1` will equal to `torch.add(in1, in2)`, while the result of `out2` will be `torch.mul(in1, in2)`.\nIt helps developers implement new torch functions that return two tensors more conveniently, such as NumPy-like functions [divmod](https://numpy.org/doc/1.18/reference/generated/numpy.divmod.html?highlight=divmod#numpy.divmod) and [frexp](https://numpy.org/doc/stable/reference/generated/numpy.frexp.html#numpy.frexp).\n\nThis PR adds `torch.frexp` function to exercise the new functionality provided by `cpu_kernel_multiple_outputs`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51097\n\nReviewed By: albanD\n\nDifferential Revision: D26982619\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: cb61c7f2c79873ab72ab5a61cbdb9203531ad469", "pr_number": "51097", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/Loops.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/tensor_iterator_test.cpp", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_namedtuple_return_api.py", "test/test_unary_ufuncs.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: TensorIterator", "module: reductions", "open source", "triaged"]}, "7e39a40300": {"title": "Fix typo in torchvision_models.py (#53968)", "body": "Summary:\naccross -> across\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53968\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27035761\n\nPulled By: ngimel\n\nfbshipit-source-id: 94fac6f2e27648e70652fd29f7800e60b211acd5", "pr_number": "53968", "files_changed": ["benchmarks/functional_autograd_benchmark/torchvision_models.py"], "labels": ["Merged", "cla signed", "open source"]}, "5b62b0d9bc": {"title": "[RPC] Fix typo in rref_context.cpp (#53978)", "body": "Summary:\nuntill -> until\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53978\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27039043\n\nPulled By: rohan-varma\n\nfbshipit-source-id: c9178e79fe8b2a3dc61665148fe55dba5adb0abf", "pr_number": "53978", "files_changed": ["torch/csrc/distributed/rpc/rref_context.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "4bb34c2a75": {"title": "Update Binary Ops with scalar lists (#49249)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49249\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D25502939\n\nPulled By: izdeby\n\nfbshipit-source-id: b16e23063b37521be549e83cb17676e3afc4ddb3", "pr_number": "49249", "files_changed": ["aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu", "test/test_foreach.py"], "labels": ["Merged", "cla signed"]}, "b5cdb53af1": {"title": "Add division logic to a slow/fast path (#49250)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49250\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D25502938\n\nPulled By: izdeby\n\nfbshipit-source-id: bdd583464eb15d7cb30fd0c22d119cc4b31cbf8d", "pr_number": "49250", "files_changed": ["aten/src/ATen/native/cuda/ForeachBinaryOpList.cu", "aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu", "test/test_foreach.py"], "labels": ["Merged", "cla signed"]}, "7ff4955de5": {"title": "[doc] Fix documentation for tensorsolve (#53320)", "body": "Summary:\nThis PR fixes a typo in the explanation of `dims` for `linalg.tensorsolve`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53320\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27048736\n\nPulled By: anjali411\n\nfbshipit-source-id: db230b21191cc9cfb73b967cd15305fe74178c2b", "pr_number": "53320", "files_changed": ["torch/linalg/__init__.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "7f88840495": {"title": "Fix prefix store timeout bug (#53928)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53928\n\nHashStoreTest was taking forever to run. Turns out it was because a default timeout is set when creating Store() and setTimeout for prefixStore is not actually able to change the timeout of the underlying store.\n\nAfter removing the default timeout and updating setTimeout, this will save ~10 minutes for all of the gcc_test CI runs.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D27025275\n\nPulled By: H-Huang\n\nfbshipit-source-id: 650c8c1eb8b166da1d412ed88e765747a2ca2069", "pr_number": "53928", "files_changed": ["torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/test/HashStoreTest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "33e3deed4f": {"title": "add OneDNN relu backward and reshape backward (#49455)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49455\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D26006886\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: c81ef115205171b80652800a76170dd759905e28", "pr_number": "49455", "files_changed": ["aten/src/ATen/native/mkldnn/Relu.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_mkldnn.py", "tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed", "open source"]}, "793a29a7d5": {"title": "add OneDNN batch_norm backward (#50460)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50460\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D26006887\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 472398772af01a31594096ccc714fd487ed33dd4", "pr_number": "50460", "files_changed": ["aten/src/ATen/native/mkldnn/Normalization.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_mkldnn.py"], "labels": ["Merged", "cla signed", "open source"]}, "67f765328b": {"title": "scripts: Change promote pypi to be more flexible (#53774)", "body": "Summary:\nPromotion to PyPI should be more flexible to allow any package to be\npromoted to PyPI.\n\nAfter we re-added a version suffix to cuda 10.2 it means that this\nscript needs to have the flexibility to designate which platform and\nwhich version suffix will actually be uploaded to PyPI\n\nShould coincide with https://github.com/pytorch/builder/pull/678\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53774\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27052347\n\nPulled By: seemethere\n\nfbshipit-source-id: 71129cc5afbd7de448c970ef721bc979c3420586", "pr_number": "53774", "files_changed": ["scripts/release/promote/prep_binary_for_pypi.sh", "scripts/release/promote/wheel_to_pypi.sh"], "labels": ["Merged", "cla signed", "releng"]}, "65087dd1d4": {"title": "Fix broken link from load_inline to new test location (#53701)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53701\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27047406\n\nPulled By: ezyang\n\nfbshipit-source-id: 0be6e669cf41527d3ffeb101e5f36db07e41b4af", "pr_number": "53701", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed", "open source"]}, "8f98b87212": {"title": "Update Kineto revision (#53940)", "body": "Summary:\nUpdate Kineto revision\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53940\n\nTest Plan: CI\n\nReviewed By: gdankel, ngimel\n\nDifferential Revision: D27027834\n\nPulled By: ilia-cher\n\nfbshipit-source-id: f5515720c641fde8a8b80c38fa4cbb611f76f36e", "pr_number": "53940", "files_changed": ["third_party/kineto", "torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["Merged", "cla signed"]}, "274b96b878": {"title": "Move as_view/increment_version to its separate key. (#53342)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53342\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26973913\n\nPulled By: ailzhang\n\nfbshipit-source-id: bc7fc25d1a3a1f20cdfa1d7126fa559a84d194a4", "pr_number": "53342", "files_changed": ["BUILD.bazel", "aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/VariableFallbackKernel.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/TensorImpl.cpp", "caffe2/CMakeLists.txt", "tools/autograd/gen_autograd.py", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_trace_type.py", "tools/autograd/gen_variable_factories.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/InplaceOrViewType.cpp", "tools/build_variables.bzl", "tools/codegen/model.py"], "labels": ["Merged", "cla signed"]}, "b9fdf72174": {"title": "Fix doc (#53996)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53996\n\nFixes issue: #52479\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D27051056\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: ff5d2fc3599571346e2323fa893c1e238097a164", "pr_number": "53996", "files_changed": ["torch/_jit_internal.py"], "labels": ["Merged", "cla signed"]}, "e91aeb0470": {"title": "[4/n][torch/elastic][upstream] Move torchelastic/metrics to torch/distributed/elastic/metrics (#53870)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53870\n\nMove torchelastic/metrics to torch/distributed/elastic/metrics\n\nTest Plan: buck test mode/dev-nosan //pytorch/elastic/torchelastic/...\n\nReviewed By: kiukchung\n\nDifferential Revision: D26970901\n\nfbshipit-source-id: 0e0a211fe509b7bc3ab10adfefba81cd71b6db37", "pr_number": "53870", "files_changed": ["test/distributed/elastic/metrics/__init__.py", "test/distributed/elastic/metrics/api_test.py", "torch/distributed/elastic/metrics/__init__.py", "torch/distributed/elastic/metrics/api.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "ee35060888": {"title": "Fix sharding algo + test it (#53942)", "body": "Summary:\nThis PR:\n1. moves sharding algorithm from run_test.py to framework_utils.py (let me know if you have a better place for it)\n2. adds tests for the algorithm in test_testing.py\n3. fixes the algorithm so that it doesn't tack on the unknown jobs all to the shard with the minimum time, but instead distributes them around the shards.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53942\n\nTest Plan: python test/test_testing.py -k TestFrameworkUtils\n\nReviewed By: samestep\n\nDifferential Revision: D27047223\n\nPulled By: janeyx99\n\nfbshipit-source-id: 824b20009c0bb707aa5361de445cdec795d5e3f1", "pr_number": "53942", "files_changed": ["mypy-strict.ini", "test/run_test.py", "test/test_testing.py", "torch/testing/_internal/framework_utils.py"], "labels": ["Merged", "cla signed"]}, "08f04c0db2": {"title": "Test forward reference annotations (#53713)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53713\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D26946847\n\nPulled By: ansley\n\nfbshipit-source-id: 2f99247c4b54ee06dcb54b23fdcee3537643cad4", "pr_number": "53713", "files_changed": ["test/fx/test_future.py", "test/test_fx.py"], "labels": ["Merged", "cla signed", "fx"]}, "2c5579702a": {"title": "[PyTorch Mobile] Add module size to logged metadata (#53578)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53578\n\nWe want to be able to log the loaded module size to the scuba table `qpl_metrics/pytorch`. Hence, adding the `model_size` field to the logged metadata when logging a module load success event.\n\nghstack-source-id: 123980964\n\nTest Plan: xcheng16 How should this be tested?\n\nReviewed By: xcheng16, raziel\n\nDifferential Revision: D26902971\n\nfbshipit-source-id: a7c2e9120706bd31f76f6572c8503d4acf8a89e2", "pr_number": "53578", "files_changed": ["torch/csrc/jit/mobile/import.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "603097be18": {"title": "OneDNN MaxPooling: reduce memory use for inference path (#52728)", "body": "Summary:\nFor OneDNN MaxPooling training, it will save indices as a workspace for backward, but for inference, indices are not necessary, this PR will make check to avoid saving indices to reduce memory use for inference path.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52728\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27062435\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9e70268a8ba491a7914b980079c0945d753cd4f3", "pr_number": "52728", "files_changed": ["aten/src/ATen/native/mkldnn/Pooling.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "4dd1c72dde": {"title": "Treat Scalar parameter as if it is constant (#53582)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53582\n\nWe will pass `Scalar` by reference in the following commit,\ni.e. `const Scalar&`.\nghstack-source-id: 123965970\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D26904444\n\nfbshipit-source-id: 7f58ee4e38dcd860f0d1120cab4e82f35ca3770f", "pr_number": "53582", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/TensorFactories.cpp"], "labels": ["Merged", "cla signed"]}, "2ecb2c7931": {"title": "Pass Scalar by reference (#53583)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53583\n\n`Scalar` takes 32 bytes due to `c10::complex<double>`\nrequires aligning to 16 bytes. Passing Scalar by reference\nshows about 1% improvements on instruction count.\n\nAll the changes in this commit are codemoded except for\nthe following 4 files (which code-gen signatures):\n```\ntools/codegen/api/cpp.py\ntools/codegen/api/native.py\ntools/codegen/api/structured.py\ncaffe2/contrib/aten/gen_op.py\n```\n\n# Codemode\n\n## Main Step\n\nFor the codemod part, here is the main command used:\n```\nfastmod --extensions h '([a-zA-Z_+]\\([^)]*,?\\s*)Scalar (\\w+)' '${1}const Scalar& ${2}'\nfastmod --extensions h '([a-zA-Z_+]\\([^)]*,?\\s*)optional<Scalar> (\\w+)' '${1}const optional<Scalar>& ${2}'\nfastmod --extensions cpp '([a-zA-Z_+]\\([^)]*,?\\s*)Scalar (\\w+)' '${1}const Scalar& ${2}'\nfastmod --extensions cpp '([a-zA-Z_+]\\([^)]*,?\\s*)optional<Scalar> (\\w+)' '${1}const optional<Scalar>& ${2}'\n```\n\nAs you can tell, it codemods both `Scalar` and `optional<Scalar>`.  Apply these commands iteratively until reaching a fix-point (since one method signature might contain multiple `Scalar` parameter).\n\nIn retrospect, excluding `thrid_party` and `torch/csrc/jit` would be a good idea. (I revert it manually later, see https://github.com/pytorch/pytorch/pull/53479 as an reference).\n\n## Pre-Step\n\nPrior to applying the main command,  as some `Scalar` are presented as `at::Scalar` or `c10::Scalar`, so I codemod some of them in advance. Here is an incomplete list:\n```\nfastmod --extensions h '([a-zA-Z_+]\\([^)]*,?\\s*)at::Scalar (\\w+)' '${1}const at::Scalar& ${2}'\nfastmod --extensions cpp '([a-zA-Z_+]\\([^)]*,?\\s*)at::Scalar (\\w+)' '${1}const at::Scalar& ${2}'\nfastmod --extensions h '([a-zA-Z_+]\\([^)]*,?\\s*)c10::optional<Scalar> (\\w+)' '${1}const c10::optional<Scalar>& ${2}'\nfastmod --extensions cpp '([a-zA-Z_+]\\([^)]*,?\\s*)c10::optional<Scalar> (\\w+)' '${1}const c10::optional<Scalar>& ${2}'\n```\n\n## Fixup\nThere are a couple of post codemod fixup. For example, `const Scalar` will be codemoded into `const const Scalar&`. `at:Scalar` will be codemoded into `at::const Scalar&`  (if `Pre-step` is not done comprehensively). Here is an incomplete list:\n```\nfastmod --extensions cpp 'const const Scalar' 'const Scalar'\nfastmod --extensions h 'const const c10::optional<Scalar>' 'const c10::optional<Scalar>'\nfastmod --extensions cpp 'const const c10::optional<Scalar>' 'const c10::optional<Scalar>'\nfastmod 'at::const Scalar&' 'const at::Scalar&'\n```\n\n## Supplementary\n\n`cu` and `mm` files also need to be codemoded, for example:\n\n```\nfastmod --extensions cu 'at::const Scalar&' 'const at::Scalar&'\nfastmod --extensions mm '([a-zA-Z_+]\\([^)]*,?\\s*)Scalar (\\w+)' '${1}const Scalar& ${2}'\n```\n\nFunction pointers are not codemoded. Here is an incomplete list:\n\n```\n# Cover case: using index_fill_fn = void(*)(TensorIterator & iter, int64_t dim, int64_t self_dim_size, int64_t self_dim_stride, Scalar source);\nfastmod --extensions h '(void\\s*\\(\\s*\\*\\s*\\)\\([^)]*,?\\s*)Scalar (\\w+)' '${1}const Scalar& ${2}'\n\n# Cover case: using softplus_fn = void (*)(TensorIterator&, Scalar, Scalar);\nfastmod --extensions h '(void\\s*\\(\\s*\\*\\s*\\)\\([^)]*,?\\s*)Scalar([, \\)])' '${1}const Scalar&${2}'\nfastmod --extensions cpp '(void\\s*\\(\\s*\\*\\s*\\)\\([^)]*,?\\s*)Scalar([, \\)])' '${1}const Scalar&${2}'\nfastmod --extensions h '(void\\s*\\(\\s*\\*\\s*\\)\\([^)]*,?\\s*)optional<Scalar>([, \\)])' '${1}const optional<Scalar>&${2}'\n```\n\nSome corner cases needs to be manually fixed.\n\nghstack-source-id: 123970306\n\nTest Plan: Imported from OSS\n\nReviewed By: smessmer\n\nDifferential Revision: D26904445\n\nfbshipit-source-id: 8d8a002af4b5125f153a32f03c6956be7ae5671d", "pr_number": "53583", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/ScalarOps.cpp", "aten/src/ATen/ScalarOps.h", "aten/src/ATen/TensorIndexing.cpp", "aten/src/ATen/TensorIndexing.h", "aten/src/ATen/TensorOperators.h", "aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/core/ivalue.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/Bucketization.cpp", "aten/src/ATen/native/BucketizationUtils.h", "aten/src/ATen/native/CPUBlas.h", "aten/src/ATen/native/ConstantPadNd.cpp", "aten/src/ATen/native/Fill.cpp", "aten/src/ATen/native/Fill.h", "aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/Lerp.cpp", "aten/src/ATen/native/Lerp.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebra.h", "aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/PackedSequence.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/PointwiseOps.h", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/Pow.h", "aten/src/ATen/native/QuantizedLinear.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cpu/BlasKernel.cpp", "aten/src/ATen/native/cpu/FillKernel.cpp", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cpu/LerpKernel.cpp", "aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp", "aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp", "aten/src/ATen/native/cpu/PowKernel.cpp", "aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/BinaryAddSubKernel.cu", "aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu", "aten/src/ATen/native/cuda/Blas.cu", "aten/src/ATen/native/cuda/Bucketization.cu", "aten/src/ATen/native/cuda/FillKernel.cu", "aten/src/ATen/native/cuda/ForeachBinaryOpList.cu", "aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu", "aten/src/ATen/native/cuda/ForeachPointwiseOp.cu", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/Lerp.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/PointwiseOpsKernel.cu", "aten/src/ATen/native/cuda/PowKernel.cu", "aten/src/ATen/native/cuda/RangeFactories.cu", "aten/src/ATen/native/cuda/ReduceNormKernel.cu", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/SparseMM.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/metal/MetalPrepackOpContext.h", "aten/src/ATen/native/metal/MetalPrepackOpRegister.cpp", "aten/src/ATen/native/metal/ops/MetalAddmm.mm", "aten/src/ATen/native/metal/ops/MetalBinaryElementwise.mm", "aten/src/ATen/native/metal/ops/MetalClamp.mm", "aten/src/ATen/native/mkl/LinearAlgebra.cpp", "aten/src/ATen/native/mkldnn/BinaryOps.cpp", "aten/src/ATen/native/mkldnn/Relu.cpp", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qelu.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/qthreshold.cpp", "aten/src/ATen/native/quantized/cpu/quantized_ops.h", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.h", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/ATen/native/vulkan/VulkanAten.cpp", "aten/src/ATen/native/vulkan/VulkanConvolution.cpp", "aten/src/ATen/native/vulkan/VulkanConvolution.h", "aten/src/ATen/native/vulkan/VulkanOpContext.cpp", "aten/src/ATen/native/vulkan/VulkanOpContext.h", "aten/src/ATen/native/vulkan/ops/Arithmetic.cpp", "aten/src/ATen/native/vulkan/ops/Clamp.cpp", "aten/src/ATen/native/vulkan/ops/Convolution.cpp", "aten/src/ATen/native/vulkan/ops/Convolution.h", "aten/src/ATen/native/vulkan/ops/Mm.cpp", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Convolution.h", "aten/src/ATen/native/xnnpack/Linear.cpp", "aten/src/ATen/native/xnnpack/Linear.h", "aten/src/ATen/native/xnnpack/OpContext.cpp", "aten/src/ATen/native/xnnpack/OpContext.h", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/test/extension_backend_test.cpp", "caffe2/contrib/aten/aten_op_template.h", "caffe2/contrib/aten/gen_op.py", "test/cpp_extensions/msnpu_extension.cpp", "tools/autograd/templates/python_torch_functions.cpp", "tools/codegen/api/cpp.py", "tools/codegen/api/native.py", "tools/codegen/api/structured.py", "torch/csrc/api/include/torch/linalg.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/autograd/utils/wrap_outputs.h", "torch/csrc/utils/python_arg_parser.h"], "labels": ["Merged", "cla signed"]}, "e8e570e9c5": {"title": "[MacOS] Cross compile stub when building for M1 on x86 (#54046)", "body": "Summary:\nAlso rename `CROSS_COMPILE_ARM` to `CROSS_COMPILE_ARM64`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54046\n\nReviewed By: walterddr\n\nDifferential Revision: D27071928\n\nPulled By: malfet\n\nfbshipit-source-id: 9143cd5d110ed67f0609f0a4bbb20922012ee665", "pr_number": "54046", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".jenkins/pytorch/macos-build.sh", "setup.py"], "labels": ["Merged", "cla signed"]}, "bea3cb7069": {"title": "remove aliasMultinomial decode from TH and THC (#52585)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52585\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D26654125\n\nPulled By: albanD\n\nfbshipit-source-id: 6a745080021623a2472dae7862cde91b949983ee", "pr_number": "52585", "files_changed": ["BUILD.bazel", "aten/src/TH/CMakeLists.txt", "aten/src/TH/THTensor.h", "aten/src/TH/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.cpp", "aten/src/TH/generic/THTensorRandom.h", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCTensorRandom.cu", "aten/src/THC/THCTensorRandom.cuh", "aten/src/THC/THCTensorRandom.h", "aten/src/THC/generic/THCTensorRandom.cu", "aten/src/THC/generic/THCTensorRandom.h", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "open source"]}, "b27e678dfb": {"title": "[RELAND] [CUDA graphs] Private mempools for CUDA graphs (#54038)", "body": "Summary:\nResubmit of https://github.com/pytorch/pytorch/pull/51436.\n\nApparently some non-public windows builds run cuda tests on the default stream, so I changed a few capture tests to manually ensure all captures happen on non-default streams.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54038\n\nReviewed By: mruberry\n\nDifferential Revision: D27068649\n\nPulled By: ngimel\n\nfbshipit-source-id: 4284475fa40ee38c0f8faff05a2faa310cf8a207", "pr_number": "54038", "files_changed": ["aten/src/ATen/cuda/CUDAGraph.cpp", "aten/src/ATen/cuda/CUDAGraph.h", "aten/src/ATen/cuda/CUDAGraphsUtils.cuh", "c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "c10/cuda/CUDAGraphsC10Utils.h", "test/test_cuda.py", "torch/_C/__init__.pyi.in", "torch/csrc/cuda/Graph.cpp", "torch/cuda/__init__.py", "torch/cuda/streams.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed", "module: cuda graphs", "open source", "triaged"]}, "2c4a64589b": {"title": "fix mkldnn_add in-place behavior (#51687)", "body": "Summary:\nThere are the following two patterns to call add in-pace.\n\n```python\ntorch.add(a, b, out=a) # (1) a in-placed\ntorch.add(a, b, out=b) # (2) b in-placed\n```\n\nIf a and b are mkldnn Tensor, the value is different from expected in case (2).\n\n**Sample code to reproduce the behavior:**\n\n```python\nimport torch\n\ntorch.manual_seed(4)\na = torch.randn(4, 4)\nb = torch.randn(4, 4)\nb.fill_(1.0)\n\na_mkl = a.to_mkldnn()\nb_mkl = b.to_mkldnn()\n\ntorch.add(b, a, alpha=1.0, out=a)\ntorch.add(b_mkl, a_mkl, alpha=1.0, out=a_mkl)\n\nprint(a)\nprint(a_mkl)\n```\n\n**Results:**\n\nActual:\n\n```python\ntensor([[ 0.0586,  2.2632,  0.8162,  1.1505],\n        [ 1.1075,  0.7220, -1.6021,  1.6245],\n        [ 0.1316,  0.7949,  1.3976,  1.6699],\n        [ 0.9463,  1.0467, -0.7671, -1.1205]])\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]], layout=torch._mkldnn)\n```\n\nExpected:\n\n```python\ntensor([[ 0.0586,  2.2632,  0.8162,  1.1505],\n        [ 1.1075,  0.7220, -1.6021,  1.6245],\n        [ 0.1316,  0.7949,  1.3976,  1.6699],\n        [ 0.9463,  1.0467, -0.7671, -1.1205]])\ntensor([[ 0.0586,  2.2632,  0.8162,  1.1505],\n        [ 1.1075,  0.7220, -1.6021,  1.6245],\n        [ 0.1316,  0.7949,  1.3976,  1.6699],\n        [ 0.9463,  1.0467, -0.7671, -1.1205]], layout=torch._mkldnn)\n```\n\nThis is because `dnnl::sum` called in `mkldnn_add` has the following specifications:\n\n[oneDNN doc : Sum](https://oneapi-src.github.io/oneDNN/dev_guide_sum.html)\n\n> The sum primitive supports in-place operation, meaning that the src0 tensor can be used as both input and output.\n> In-place operation overwrites the original data. Using in-place operation requires the memory footprint of the\n> output tensor to be either bigger than or equal to the size of the dst memory descriptor used for primitive creation.\n\nbut, case 2) are added to the first argument.\nSo, we modified it so that a and b are swapped and passed to \"sum\" in case (2).\n\n**Environment**\n\u30fbCPU : Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz\n\u30fbbuild USE_MKLDNN=1\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51687\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27062172\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: bf76d36f9fdb1b4337d71d87bcdbaf4edb11f12f", "pr_number": "51687", "files_changed": ["aten/src/ATen/native/mkldnn/BinaryOps.cpp", "test/test_mkldnn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "6332fd6255": {"title": "enable sc1090 and sc1091 (#54069)", "body": "Summary:\nSC1090/1091 are important to prevent accidental delete/move of utility shell scripts\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54069\n\nTest Plan: CI\n\nReviewed By: samestep\n\nDifferential Revision: D27084094\n\nPulled By: walterddr\n\nfbshipit-source-id: 16deb83fce691eba0263978374564d172bc8d371", "pr_number": "54069", "files_changed": [".jenkins/caffe2/bench.sh", ".jenkins/caffe2/build.sh", ".jenkins/caffe2/test.sh", ".jenkins/pytorch/.shellcheckrc", ".jenkins/pytorch/build-asan.sh", ".jenkins/pytorch/build-mobile-code-analysis.sh", ".jenkins/pytorch/build-mobile.sh", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/codegen-test.sh", ".jenkins/pytorch/common.sh", ".jenkins/pytorch/docker-build-test.sh", ".jenkins/pytorch/docs-test.sh", ".jenkins/pytorch/macos-build-test.sh", ".jenkins/pytorch/macos-build.sh", ".jenkins/pytorch/macos-common.sh", ".jenkins/pytorch/macos-lite-interpreter-build-test.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/short-perf-test-cpu.sh", ".jenkins/pytorch/short-perf-test-gpu.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-build.sh", ".jenkins/pytorch/win-test.sh", ".jenkins/run-shellcheck.sh"], "labels": ["Merged", "cla signed"]}, "1f5b9170aa": {"title": "Faster backwards for cumsum and cumprod (#53711)", "body": "Summary:\nProvides a faster formula for `cumprod` in the case when the input has zeros. This formula is non-differentiable, so we leave the previous formula for the cases when `at::GradMode::is_enabled()`.\n\nThis new formula gives up to x10 and x30 speed-ups in CPU and GPU (see the benchmarks below).\n\nThe `cumsum` backward formula was rewritten so that no copies are necessary. We also removed a double negation in its formula. This gives a significant speed-up in CPU, while being almost as efficient as the formula with copies in GPU. We can see this speed-up when comparing the \"No zeros\" part of the benchmark.\n\nBenchmarks:\n\nnb. It is worth noting that the script tests the forward and the backward for `cumprod`, so the speed-ups should be even larger than those announced here.\n<details>\n<summary>Script</summary>\n\n```python\nfrom IPython import get_ipython\nimport torch\nfrom itertools import product\n\ntorch.manual_seed(13)\ntorch.set_num_threads(1)\n\nipython = get_ipython()\n\ncpu = torch.device('cpu')\ncuda = torch.device('cuda')\n\ndef run_test(ndims, size, size_prod, zeros, device):\n    print(f\"ndims: {ndims}, tensor_size: {size}, size_prod: {size_prod}, zeros: {zeros}, device: {device}\")\n\n    for dim in range(ndims):\n        sizes = ndims * [size]\n        sizes[dim] = size_prod\n        tensor = torch.rand(*sizes, device=device)\n        with torch.no_grad():\n            if zeros:\n                # Set 0.1 of them to zero\n                p_drop = 0.1\n                mask = torch.full_like(tensor, 1.0 - p_drop)\n                tensor = tensor * torch.bernoulli(mask)\n            else:\n                tensor = tensor + 1e-3\n        tensor.requires_grad_()\n        grad = torch.ones_like(tensor)\n        # We test both forward + backward, meaning that the speed-up is actually greater than reported\n        # That being said, this is more realistic than doing `retain_graph=True`\n        command = \"torch.autograd.grad([tensor.cumprod(dim)], [tensor], grad_outputs=[grad])\"\n        if device == cuda:\n            command += \"; torch.cuda.synchronize()\"\n        ipython.magic(f\"timeit {command}\")\n    print()\n\nfor device, zeros in product([cuda, cpu], [True, False]):\n    run_test(3, 300, 10, zeros, device)\n    run_test(3, 300, 100, zeros, device)\n    if device == cuda:\n        run_test(3, 300, 300, zeros, device)\n```\n\n</details>\n\n<details>\n<summary>CPU This PR  (Some regression small tensors, x4 speed-up large tensors)</summary>\n\n```\nZeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cpu\n28.2 ms \u00b1 12.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n29.8 ms \u00b1 78.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n24.5 ms \u00b1 29.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cpu\n414 ms \u00b1 3.63 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n428 ms \u00b1 4.12 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n382 ms \u00b1 3.18 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nNo Zeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cpu\n3.11 ms \u00b1 9.72 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n3.83 ms \u00b1 3.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.08 ms \u00b1 10.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cpu\n92.2 ms \u00b1 113 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n101 ms \u00b1 101 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n87 ms \u00b1 170 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n```\n</details>\n\n<details>\n<summary>CUDA This PR (7-30x speed-up)</summary>\n\n```\n\nZeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cuda\n1.46 ms \u00b1 2.07 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.48 ms \u00b1 3.51 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.93 ms \u00b1 8.07 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cuda\n10.5 ms \u00b1 914 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10.6 ms \u00b1 509 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n11.7 ms \u00b1 864 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 300, zeros: True, device: cuda\n30.3 ms \u00b1 5.16 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n30.6 ms \u00b1 6.44 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n32.2 ms \u00b1 2.34 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nNo Zeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cuda\n248 \u00b5s \u00b1 335 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n252 \u00b5s \u00b1 186 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n438 \u00b5s \u00b1 254 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cuda\n2.1 ms \u00b1 193 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n2.16 ms \u00b1 380 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n2.59 ms \u00b1 398 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 300, zeros: False, device: cuda\n6.3 ms \u00b1 857 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n6.39 ms \u00b1 288 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.15 ms \u00b1 233 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\n</details>\n\n<details>\n<summary>CPU master</summary>\n\n```\nZeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cpu\n8.27 ms \u00b1 12.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10.8 ms \u00b1 13.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n28.2 ms \u00b1 74.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cpu\n1.53 s \u00b1 116 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.95 s \u00b1 4.38 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.86 s \u00b1 3.58 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nNo Zeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cpu\n3.42 ms \u00b1 20 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.25 ms \u00b1 3.65 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n4.34 ms \u00b1 3.04 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cpu\n104 ms \u00b1 148 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n117 ms \u00b1 99.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n94.8 ms \u00b1 125 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n```\n\n</details>\n\n<details>\n<summary>CUDA master</summary>\n\n```\nZeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: True, device: cuda\n912 \u00b5s \u00b1 431 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.05 ms \u00b1 2.46 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n2.74 ms \u00b1 381 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: True, device: cuda\n71.3 ms \u00b1 7.91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n85.4 ms \u00b1 9.82 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n119 ms \u00b1 6.21 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 300, zeros: True, device: cuda\n646 ms \u00b1 103 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n776 ms \u00b1 81.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n917 ms \u00b1 160 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nNo Zeros:\nndims: 3, tensor_size: 300, size_prod: 10, zeros: False, device: cuda\n301 \u00b5s \u00b1 893 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n308 \u00b5s \u00b1 236 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n592 \u00b5s \u00b1 140 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 100, zeros: False, device: cuda\n2.61 ms \u00b1 375 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n2.68 ms \u00b1 524 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n3.38 ms \u00b1 736 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 300, size_prod: 300, zeros: False, device: cuda\n7.89 ms \u00b1 848 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n8.03 ms \u00b1 517 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.24 ms \u00b1 405 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\n</details>\n\ncc nikitaved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53711\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27059662\n\nPulled By: anjali411\n\nfbshipit-source-id: be610d5590c0199b4412dff66fac47666faaff9d", "pr_number": "53711", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c0fafcc766": {"title": "Don't actually print anomalies in TTRR (#54078)", "body": "Summary:\nThis PR disables the bulk of the output for test time regression reporting, since it's obscuring more important signal (especially in cases where shards are shifting around).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54078\n\nTest Plan:\n```\npython test/test_testing.py\n```\n\nReviewed By: ezyang, walterddr\n\nDifferential Revision: D27088987\n\nPulled By: samestep\n\nfbshipit-source-id: 06a4eeb75641552bad2ab4b9154a8c70c57b0d68", "pr_number": "54078", "files_changed": ["test/test_testing.py", "torch/testing/_internal/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "665d5e2a4f": {"title": "[PyTorch][JIT] Audit interpreter for extra copies (#54029)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54029\n\nI found what appear to be some missed moves and/or extra copies in the JIT interpreter.\nghstack-source-id: 123958682\n\nTest Plan:\nExisting CI for correctness\n\nRan AdIndexer inline_cvr local_ro model benchmark with static_runtime off via\n`env bin=/tmp/ptvsc2_predictor_bench.StaticDispatchModeFile static_runtime=0 caffe2=0 scripts/swolchok/static_runtime/inline_cvr/run_local_ro.sh`\n\nbefore:\n```\nI0315 14:25:23.916893 3075680 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.01635. Iters per second: 983.914\nI0315 14:26:05.536207 3080560 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.01689. Iters per second: 983.395\nI0315 14:26:47.510561 3083335 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.02697. Iters per second: 973.737\nI0315 14:27:29.024830 3086767 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.01326. Iters per second: 986.918\nI0315 14:28:10.849496 3091323 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.023. Iters per second: 977.517\n```\n\nafter:\n```\nI0315 14:17:43.280469 3046242 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 0.997838. Iters per second: 1002.17\nI0315 14:18:24.244606 3046861 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.00173. Iters per second: 998.269\nI0315 14:19:05.208899 3051998 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.00187. Iters per second: 998.136\nI0315 14:19:46.103854 3055392 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 1.00073. Iters per second: 999.27\nI0315 14:20:27.011411 3056062 PyTorchPredictorBenchLib.cpp:215] PyTorch run finished. Milliseconds per iter: 0.999121. Iters per second: 1000.88\n```\n\n(This was just a convenient workload I had handy; the plan of record is to use static runtime for inline_cvr inference AIUI.)\n\nReviewed By: dhruvbird, walterddr\n\nDifferential Revision: D27060762\n\nfbshipit-source-id: 5567206d7c2d9ae99776ce5524caf09ec2035e87", "pr_number": "54029", "files_changed": ["torch/csrc/jit/runtime/interpreter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3c457043fb": {"title": "Also propagate storage_access_should_throw_ when copying tensor metadata (#53816)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53816\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27036574\n\nPulled By: ezyang\n\nfbshipit-source-id: 71e61b0aa3d46159c9af1112c262cbfa7eaa1879", "pr_number": "53816", "files_changed": ["c10/core/TensorImpl.cpp", "test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "d47fd3df81": {"title": "Compute type_equal() without reference to backend() (#53823)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53823\n\nArgument for correctness: type_equal previous compared if backends\nare equal.  Backend is computed by translation from dispatch key.\nI verified that computeDispatchKey never computed a weird\ndispatch key (e.g., AutogradXLA), so that dispatchKeyToBackend\nwas effectively injective.  Then it is always valid to compare\nthe arguments of an injective function for equality, rather than\nthe output of the injective function.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27036575\n\nPulled By: ezyang\n\nfbshipit-source-id: 6aeafc89f287da0bc0065bd21c1adb5e272dbb81", "pr_number": "53823", "files_changed": ["c10/core/TensorOptions.h"], "labels": ["Merged", "cla signed"]}, "4878415688": {"title": "Make storage access error NotImplementedError (#53972)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53972\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27036573\n\nPulled By: ezyang\n\nfbshipit-source-id: 5cc7d9e124bd27ca4041feb56b5007d9408d622a", "pr_number": "53972", "files_changed": ["c10/core/TensorImpl.cpp"], "labels": ["Merged", "cla signed"]}, "524cb0a514": {"title": "[PyTorch Mobile] Dedup method names in bytecode serialization (#53677)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53677\n\nWhen serializing bytecode, we serialize it based on methods. It may happen that there are multiple instances of a class. In such a case, the methods inside the class may be serialized multiple times.\n\nTo reduce the duplication, we cache the qualified name of the methods, so that one method is serialized only once.\n\nTest Plan: existing unittests and CI\n\nReviewed By: dhruvbird, raziel\n\nDifferential Revision: D26933945\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 8a9833949fa18f7103a5a0be19e2028040dc7717", "pr_number": "53677", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4b00bce156": {"title": "[Gradient Compression] Introduce fp16_compress_wrapper in ddp_comm_hooks.rst (#54052)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54052\n\nIntroduce `fp16_compress_wrapper`, which can give some speedup on top of some gradient compression algorithms like PowerSGD.\n\nghstack-source-id: 124001805\n\nTest Plan: {F509205173}\n\nReviewed By: iseessel\n\nDifferential Revision: D27076064\n\nfbshipit-source-id: 4845a14854cafe2112c0caefc1e2532efe9d3ed8", "pr_number": "54052", "files_changed": ["docs/source/ddp_comm_hooks.rst"], "labels": ["Merged", "cla signed"]}, "dc070605f1": {"title": "TST Replaces assertEqualIgnoreTypes with assertEqual in test_indexing (#53115)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38095 and https://github.com/pytorch/pytorch/issues/50006\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53115\n\nReviewed By: mruberry\n\nDifferential Revision: D27086086\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 7a6af6bcf3d7ce9ba96d47a24a40f451d00f0e67", "pr_number": "53115", "files_changed": ["test/test_indexing.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "8533a485ea": {"title": "Fix SIGSEGV in CudaIPCTypes.cpp. (#53080)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53080\n\nAs described in https://github.com/pytorch/pytorch/issues/51619,\nProcessGroupShareTensorTest was failing due to segfaults in CudaIPCTypes.cpp.\nThere were two issues that had to be fixed for this:\n\n1. The ref_counter_files_ map was looked up and the result was used without\nchecking whether or not the appropriate key existed in the map. This would\nresult in default construction in the map if the key didn't exist resulting in\na nullptr being stored in the map.\n2. ~CudaIPCSentData uses the global cuda_ipc_global_entities variable. But as\npart of destroying cuda_ipc_global_entities, ~CudaIPCSentData is called which\naccesses an already destroyed cuda_ipc_global_entities. This is now avoided by\nclearing all shared blocks in ~CudaIPCGlobalEntities to ensure they are all\ncleaned up before the destructor exits.\n\n#Closes: https://github.com/pytorch/pytorch/issues/51619\nghstack-source-id: 122812319\n\nTest Plan: Run `python test/distributed/test_c10d_spawn.py -v ProcessGroupShareTensorTest`\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26742332\n\nfbshipit-source-id: 6de4c4533f5bca673e6e171af32d034bd6ade5bb", "pr_number": "53080", "files_changed": ["torch/csrc/CudaIPCTypes.cpp", "torch/csrc/CudaIPCTypes.h"], "labels": ["Merged", "cla signed"]}, "c4f50162be": {"title": "[typing] suppress errors in `fbcode/caffe2` - batch 2", "body": "Test Plan: Sandcastle\n\nDifferential Revision: D27082725\n\nfbshipit-source-id: a920b4eb62ff07d8e80fa2b9e3fd340cb44b689f", "pr_number": null, "files_changed": ["caffe2/python/mint/app.py"], "labels": []}, "252916ab61": {"title": "Update TensorPipe submodule (#54070)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54070\n\nTest Plan: Export to CircleCI\n\nReviewed By: mrshenli\n\nDifferential Revision: D27084375\n\nfbshipit-source-id: 9e67916ad5abf91ccb62f8cbce6197e1e7fbc8d6", "pr_number": "54070", "files_changed": ["third_party/tensorpipe"], "labels": ["Merged", "cla signed", "fb-exported"]}, "b936abd840": {"title": "fix nest openmp performance bug in thnn_conv2d (#52577)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52577\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D27063800\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 000e17b722b2b1d48e1012b3fa222729e26777fb", "pr_number": "52577", "files_changed": ["aten/src/ATen/ParallelOpenMP.h"], "labels": ["Merged", "cla signed", "open source"]}, "04d5278cb6": {"title": "[Static Runtime] Only run ReplaceWithCopy pass when enable_out_variant is true (#54111)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54111\n\nIf we only run the ReplaceWithCopy pass when enable_out_variant is true, there is no need register a default op implementation.\n\nReviewed By: edvgha\n\nDifferential Revision: D27036077\n\nfbshipit-source-id: f615f5d8b84629044af1c554421ea5e505e93239", "pr_number": "54111", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "11a135ec82": {"title": "Remove _th_take (#52665)", "body": "Summary:\nThese definitions of TH functions were left in the codebase after they were ported to ATen in https://github.com/pytorch/pytorch/pull/45283 and https://github.com/pytorch/pytorch/pull/45430\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52665\n\nReviewed By: mruberry\n\nDifferential Revision: D26655236\n\nPulled By: ailzhang\n\nfbshipit-source-id: eb106b72dfb814bd1fb4d240a1ede621ef4261b2", "pr_number": "52665", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/LegacyTHFunctionsCUDA.h"], "labels": ["Merged", "cla signed", "open source"]}, "ce15f312a8": {"title": "[PyTorch] Align function parameters across declaration and definition for max pool 2d (#54105)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54105\n\nThis is preparing XNNPACK to be enabled in Windows. For some reason Windows clang doesn't think functions taking `float` and `const float` to have the same signature and thus throwing link errors like:\n```\nlld-link: error: undefined symbol: bool __cdecl at::native::xnnpack::use_max_pool2d(class at::Tensor const &, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, bool, float, float)\n>>> referenced by C:\\open\\fbsource\\buck-out\\gen\\f84e6a81\\xplat\\caffe2\\pt_ops_full_template_registration\\aten\\src\\ATen\\native\\Pooling.cpp:127\n>>>               libpt_ops_fullWindows.lib(out.obj):(class at::Tensor __cdecl at::native::max_pool2d(class at::Tensor const &, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, bool))\n\nlld-link: error: undefined symbol: class at::Tensor __cdecl at::native::xnnpack::max_pool2d(class at::Tensor const &, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, bool, float, float)\n>>> referenced by C:\\open\\fbsource\\buck-out\\gen\\f84e6a81\\xplat\\caffe2\\pt_ops_full_template_registration\\aten\\src\\ATen\\native\\Pooling.cpp:129\n>>>               libpt_ops_fullWindows.lib(out.obj):(class at::Tensor __cdecl at::native::max_pool2d(class at::Tensor const &, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, class c10::ArrayRef<__int64>, bool))\n```\n\nDeclaration: `src/ATen/native/xnnpack/Engine.h`\nDefinition: `src/ATen/native/xnnpack/MaxPooling.cpp`\nReference: `src/ATen/native/Pooling.cpp`\n\nTest Plan: build succeeded\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27097201\n\nfbshipit-source-id: ab557f608713840ee0a65b252fa875624ddd502f", "pr_number": "54105", "files_changed": ["aten/src/ATen/native/xnnpack/Engine.h", "aten/src/ATen/native/xnnpack/Shim.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "8f1af02f35": {"title": "[PyTorch][mobile] Audit mobile interpreter for extra copies (#54031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54031\n\nSimilar to D27060762 (https://github.com/pytorch/pytorch/commit/665d5e2a4f81e32c57e8f2d4212a1f2856155423), caught some probably-unintended copies.\nghstack-source-id: 124040889\n\nTest Plan: CI?\n\nReviewed By: walterddr, iseeyuan\n\nDifferential Revision: D27061818\n\nfbshipit-source-id: f4a77cb5c21cd3ebce7b7e82764e4361467bab91", "pr_number": "54031", "files_changed": ["torch/csrc/jit/mobile/interpreter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4a24c552cc": {"title": "[PyTorch] Fix string copy in WARN path for both interpreters (#54076)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54076\n\nIf we don't constrain ourselves to use `torch::jit::pop`, we can avoid copying a string or moving IValues around.\nghstack-source-id: 124040891\n\nTest Plan:\nexisting tests\n\nspot-checked regular interpreter assembly; seems better\n\nReviewed By: dhruvbird, walterddr\n\nDifferential Revision: D27087204\n\nfbshipit-source-id: 7cf355dbcec31409bdb37afa09d7df85cf2a7e4b", "pr_number": "54076", "files_changed": ["torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ce40ff5c64": {"title": "Avoid DDP race condition with find_unused_parameters=True when all params are used (#53160)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53159.\n\nSee comments for a description of the race condition. Thanks to ptrblck xwang233 and especially zasdfgbnm for lots of help isolating the problem and discussing the fix.\n\nPRing for discussion. We can try to concoct a dedicated test for the problem if you want. The ingredients are:\n- DDP(..., find_unused_parameters=True)\n- Use all the DDP-ed model's params in forward such that the \"lazy local used work wait()\" path will be taken in backward\n- Queue up a lot of asynchronous dummy work just before backward(), so stream work gets pushed far into the future relative to CPU work\n\nBenchmark:\nBert model, When find_unused_parameters=true, latency (sec) per iteration P50: trunk-1.265sec, this PR-1.263sec, if add blocking copy before calling local_used_.fill(i)-1.236 sec\nBert model, When find_unsued_parameters=false, latency (sec) per iteration P50: trunk-1.00sec, this PR-1.026sec\nResnet50 model, accuracy is also matched with trunk when find_unused_parameters=true and false\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53160\n\nReviewed By: albanD\n\nDifferential Revision: D26916766\n\nPulled By: zhaojuanmao\n\nfbshipit-source-id: 3e0ed91b7b5c42e2f2c82e12d4d2940fdc89e023", "pr_number": "53160", "files_changed": ["torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "91747a5e93": {"title": "add tests for ddp with activation check points (#52894)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52894\n\nadd two success cases and two failure cases for ddp with activation check points when grad_as_bucket_view = true and false\n\nTest Plan: unit tests\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26679895\n\nfbshipit-source-id: a6f6cb22b4903ed8b1f7b8ed4fe8b13e102d8c21", "pr_number": "52894", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "0806126aad": {"title": "[fx][trivial] Add TestConstFold coverage to test_fx (#54072)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54072\n\natt\n\nTest Plan: Adding coverage\n\nDifferential Revision: D27085591\n\nfbshipit-source-id: 8c5ea5a52be619249f23a938ddb0a3aed1ada0f7", "pr_number": "54072", "files_changed": ["test/test_fx.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "f30a7a2739": {"title": "Add export-historic-test-times option to dump S3 test times into a JSON file (#54083)", "body": "Summary:\nThis will allow for future work to use the test times file (which will save computation time and also allow for more consistency). (Step one to fixing https://github.com/pytorch/pytorch/issues/53882)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54083\n\nTest Plan:\nexport CIRCLE_JOB=your-favorite-circleci-job e.g., pytorch_linux_xenial_cuda10_2_cudnn7_py3_gcc7_test2\n`python test/run_test.py --export-historic-test-times` OR\n`python test/run_test.py --export-historic-test-times .your-favorite-file`\n\nWhen opening either .pytorch-test-times or .your-favorite-file, you should see something like:\n```\n{\"commit\": \"2d559a09392aabb84dfb4a498010b2f01d99818c\", \"job_times\": {\"distributed/test_distributed_spawn\": 583.5889999999973, \"distributed/test_data_parallel\": 4.866999999999997, \"test_binary_ufuncs\": 171.1569999999998, \"test_numpy_interop\": 2.5649999999999995, \"test_public_bindings\": 0.011,...}}\n```\n\nNote that no tests will be run when this option is specified.\n\nReviewed By: walterddr\n\nDifferential Revision: D27091351\n\nPulled By: janeyx99\n\nfbshipit-source-id: e191d739268d86de0a0ba0eea0006969859d1940", "pr_number": "54083", "files_changed": [".gitignore", "test/run_test.py"], "labels": ["Merged", "cla signed"]}, "7e7533b2e2": {"title": "Delete denseTypeIdWithDefault and toDense (#54016)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54016\n\nI managed to convince myself that typeIdWithDefault was sufficient for\nthe sparse constructor case.  Here is the reasoning.\n\nThe surface reading of the use site of denseTypeIdWithDefault is\nto convert what could be a sparse dispatch key into the dense version\nso we can properly allocate underlying dense tensors for the sparse\nconstructor call.  But WHERE does this dispatch key come from?\nInspection of call sites reveals that dispatch key is provided by\ntorch::tensors::get_default_dispatch_key().  This key is NEVER\nsparse, as that would correspond to setting sparse tensors to be\nthe default tensor via torch.set_default_tensor_type() (which is\nforbidden, and even if it worked most of everything in PyTorch would\nbreak).  That means that typeIdWithDefault is a sufficient replacmenet.\n\nWith denseTypeIdWithDefault removed, we can also delete toDense\nas this was the sole use of that function.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27109511\n\nPulled By: ezyang\n\nfbshipit-source-id: c698eff0ab54c0c101fe9f55be3b7657584c4372", "pr_number": "54016", "files_changed": ["c10/core/Backend.h", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed"]}, "a2a7179695": {"title": "Fix bug in assertRaises NotImplemented handling when no exception is thrown (#54126)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54126\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski, mruberry\n\nDifferential Revision: D27109510\n\nPulled By: ezyang\n\nfbshipit-source-id: ba5a4de85ca00f81724f3d4e645797e8f32aa3b1", "pr_number": "54126", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "2e7311ef25": {"title": "First step to refactoring S3 reading logic (#53755)", "body": "Summary:\nThis is an initial attempt in refactoring and consolidating our S3 read logic for print_test_stats.py, test_history.py, and run_test.py. This way, boto3 and botocore do not need to be imported in various places throughout the code base, and duplicated logic (such as the many type definitions) can exist in one place: `tools/stat_utils/s3_stat_parser.py`. walterddr contributed to this PR by moving print_test_stats.py to the tools folder and the corresponding tests a subfolder within tools.\n\n**NOTE: this removes those tests from CI as the new `tools/test/test_stats.py` is not in the test/ directory as the other tests in TESTS in run_test.py.**\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53755\n\nTest Plan:\nThis refactoring change should not break anything, so running the files as before should work as they did previously.\nTo make sure that print_test_stats.py still functions: run `python tools/test/test_stats.py` and make sure all tests pass.\nTo make sure that test_history.py works, run the example commands from `tools/test_history.py --help` and check that their output matches that shown. Note that the script will continue printing for a while, so don't be alarmed.\n\nSome next steps:\n- Actually coming up with similarities among the three current use cases and further refactoring/consolidating of functions (e.g., combining simplify and get_cases)\n- Moving more parsing logic to s3_stat_parser.py to have better abstraction between our files\n- Adding tests for s3_stat_parser.py when there is more functionality in it\n\nReviewed By: agolynski, samestep\n\nDifferential Revision: D27030285\n\nPulled By: janeyx99\n\nfbshipit-source-id: e664781324ef7c0c30943bfd7f17c895075ef7a7", "pr_number": "53755", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", "mypy-strict.ini", "mypy.ini", "test/run_test.py", "test/test_testing.py", "tools/print_test_stats.py", "tools/stats_utils/__init__.py", "tools/stats_utils/s3_stat_parser.py", "tools/test/test_stats.py", "tools/test_history.py", "torch/testing/_internal/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "fd5c1123e4": {"title": "wrap AliasDb in Python (#51336)", "body": "Summary:\nAlso added a wrapper tlemo 's graphviz export to string.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51336\n\nReviewed By: ezyang\n\nDifferential Revision: D26150809\n\nPulled By: eellison\n\nfbshipit-source-id: 9beafce5cbdc1785b986b71c3cd986c1087faa11", "pr_number": "51336", "files_changed": ["test/jit/test_python_bindings.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/python/python_ir.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source", "triaged"]}, "79534867ac": {"title": "Migrate about 100 kernel to C10 full dispatcher (#54109)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54109\n\nCodemod command generated by https://github.com/pytorch/pytorch/pull/54098\n\nghstack-source-id: 124114894\n\nTest Plan: CI\n\nReviewed By: smessmer\n\nDifferential Revision: D27100359\n\nfbshipit-source-id: 8338405274a2a020856af6e4a35a2fb21438f2a8", "pr_number": "54109", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/FunctionOfAMatrixUtils.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/RangeFactories.cu", "aten/src/ATen/native/cuda/ScanKernels.cu", "aten/src/ATen/native/cuda/Sorting.cu", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/mkl/SpectralOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qreduction.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "8cc06e3ca3": {"title": "Disable CUDA RPC tests that use new device in user-function outputs (#54023)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54023\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D27059107\n\nPulled By: mrshenli\n\nfbshipit-source-id: e878511942f2e2577b2f0b8e7711d70582537851", "pr_number": "54023", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e442d5c8a5": {"title": "Disallow CUDA RPC to use new devices in output tensors (#54024)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54024\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D27059108\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1997ce8b130220786883b54c8a32e99989f70f22", "pr_number": "54024", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.h", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "957700be7e": {"title": "Improved aten::to performance from inline cvr remote_request_only (#53800)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53800\n\ncopy_impl improvement:\n\nBefore: 1732 ns\nAfter:.    1159 ns\n\nremote_request_only\n\nBefore: Milliseconds per iter: 1.24185. Iters per second: 805.252\n             0.161477 ms.    13.5036%. aten::to (155 nodes)\n\nAfter:     Milliseconds per iter: 1.14195. Iters per second: 875.696\n             0.113893 ms.     10.339%. aten::to (155 nodes)\n\nTest Plan: buck test caffe2:ATen-core-test\n\nReviewed By: ajyu\n\nDifferential Revision: D26967349\n\nfbshipit-source-id: d8f8dc5e8e3df1cec57fa098b21119ec9568e4a5", "pr_number": "53800", "files_changed": ["aten/src/ATen/native/Copy.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "407d60ee91": {"title": "Upgrade actions/setup-python from v1 to v2 (#54202)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54202\n\nTest Plan: The lint and clang-format workflows in CI.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27134223\n\nPulled By: samestep\n\nfbshipit-source-id: 7f38240696e31f1a479e93f6b326b9d13e3ddf9c", "pr_number": "54202", "files_changed": [".github/workflows/clang_format.yml", ".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "8f61b13e80": {"title": "[Pytorch Mobile] Optimize Non Forward for Mobile (#53314)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53314\n\nIntroduction of api for optimizing non forward functions for mobile. As of this diff, all functions that you say to optimize will be preserved, and those functions will be run through canonical optimization. The intention is to stack each further optimization onto separate diffs since they touch multiple files, and it seems like it'd be a nightmare to review.\nghstack-source-id: 123909414\n\nTest Plan:\ntorch.utils.mobile_optimizer.optimize_for_mobile(net, methods_to_optimize=[\"forward\", \"foo\"]) runs fine\n\ntorch.utils.mobile_optimizer.optimize_for_mobile(net, methods_to_optimize={\"foo\"}) optimizes just foo if the model doesnt define forward otherwise optimizes foo and forward\n\ntorch.utils.mobile_optimizer.optimize_for_mobile(net, methods_to_optimize=[\"forward\"]) runs fine\n\ntorch.utils.mobile_optimizer.optimize_for_mobile(net) runs fine if the model defines forward, Throws otherwise\n\nReviewed By: kimishpatel\n\nDifferential Revision: D26618689\n\nfbshipit-source-id: 5bff1fb3f3f6085c4a649a8128af9c10f0fa9400", "pr_number": "53314", "files_changed": ["torch/_C/__init__.pyi.in", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp", "torch/utils/mobile_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "94b22b5b3b": {"title": "try catch test upload failures (#54194)", "body": "Summary:\nException during send report shouldn't fail the entire pipeline\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54194\n\nTest Plan: CI\n\nReviewed By: samestep\n\nDifferential Revision: D27128457\n\nPulled By: walterddr\n\nfbshipit-source-id: 5404b542bc1a14c6f6c4d8586c1643c8c65e6d1f", "pr_number": "54194", "files_changed": ["tools/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "7d1e1c7e0d": {"title": "Pyre-ify torch.jit.interface's (#54084)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54084\n\nTest Plan: Sandcastle\n\nReviewed By: derekmod-fb\n\nDifferential Revision: D27075597\n\nfbshipit-source-id: 992592c88320df61e3a65eb0ac4ba5705b0b5802", "pr_number": "54084", "files_changed": ["torch/testing/_internal/distributed/nn/api/remote_module_test.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed", "oncall: jit"]}, "06cb9293c5": {"title": "Add GitHub Actions workflow to test tools (#54207)", "body": "Summary:\nThis PR closes https://github.com/pytorch/pytorch/issues/52866 by adding a GitHub Actions workflow to run the tests in the dir introduced by https://github.com/pytorch/pytorch/issues/53755. It also uses `actions/setup-python@v2`, assuming that https://github.com/pytorch/pytorch/issues/54202 will be merged.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54207\n\nTest Plan: The added \"Test tools\" GHA workflow in CI.\n\nReviewed By: walterddr\n\nDifferential Revision: D27135159\n\nPulled By: samestep\n\nfbshipit-source-id: c8c5e2e2ac2491baab1b1f1ed4f44b4c3266ee8d", "pr_number": "54207", "files_changed": [".github/workflows/test_tools.yml"], "labels": ["Merged", "cla signed"]}, "8ecb2d35bc": {"title": "Add ability to override _reduce_ex_ function of DataPipe (#52858)", "body": "Summary:\nRequired for `torchdata` graph functions\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52858\n\nReviewed By: H-Huang\n\nDifferential Revision: D26736348\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 1735e88374090422e6365d07d5b84075e371500c", "pr_number": "52858", "files_changed": ["torch/utils/data/dataset.py"], "labels": ["Merged", "cla signed"]}, "74993dcf7b": {"title": "Add repeats to Timer.collect_callgrind(...) (#53295)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53295\n\nA lot of the time spent in `collect_callgrind` is spinning up Valgrind and executing the initial `import torch`. In most cases the actual run loop is a much smaller fraction. As a result, we can reuse the same process to do multiple replicates and do a much better job amortizing that startup cost. This also tends to result in more stable measurements: the kth run is more repeatable than the first because everything has been given a chance to settle into a steady state. The instruction microbenchmarks lean heavily on this behavior. I found that in practice doing several `n=100` replicates to be more reliable than one monolithic 10,000+ iteration run. (Since rare cases like memory consolidation will just contaminate that one replicate, as opposed to getting mixed into the entire long run.)\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D26907093\n\nPulled By: robieta\n\nfbshipit-source-id: 72e5b48896911f5dbde96c8387845d7f9882fdb2", "pr_number": "53295", "files_changed": ["test/benchmark_utils/test_benchmark_utils.py", "torch/_C/__init__.pyi.in", "torch/csrc/Module.cpp", "torch/utils/benchmark/utils/timer.py", "torch/utils/benchmark/utils/valgrind_wrapper/compat_bindings.cpp", "torch/utils/benchmark/utils/valgrind_wrapper/timer_callgrind_template.cpp", "torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "ac78d05d05": {"title": "[Kineto] Update rev for fix to #53848 (#54226)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53848.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54226\n\nReviewed By: ilia-cher\n\nDifferential Revision: D27144893\n\nPulled By: gdankel\n\nfbshipit-source-id: f3609de540fd62c58f60f19cdca88f0dbf3ee8ca", "pr_number": "54226", "files_changed": ["third_party/kineto"], "labels": ["Merged", "cla signed"]}, "a4f0f8b1e9": {"title": "[distributed] add base processgroup::options (#53662)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53662\n\nAdd a base processgroup::options so that we can do inheritance and\nprovide\na universal option API in python\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26968856\n\nPulled By: wanchaol\n\nfbshipit-source-id: 858f4b61b27aecb1943959bba68f8c14114f67d8", "pr_number": "53662", "files_changed": ["test/cpp/rpc/test_e2e_process_group.cpp", "test/cpp/rpc/test_e2e_tensorpipe.cpp", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/frontend.cpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/lib/c10d/test/ProcessGroupNCCLErrorsTest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "f4a044ca1d": {"title": "[distributed] add options field in ProcessGroupGloo/NCCL (#54090)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54090\n\nThis PR adds an options field to both ProcessGroupGloo/NCCL so that we\nhave a constant `options` field even after the initialization of\nProcessGroup, which gives us the ability to inspect the options during\nconstruction of specific ProcessGroup. Also use options inside different\nmethods instead of separate fields.\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27093670\n\nPulled By: wanchaol\n\nfbshipit-source-id: b02d9394290e9be88b21bddb94d4de7993b4a2e3", "pr_number": "54090", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "fef0219f7e": {"title": "[ROCM] Fix hipfft transform type error (#53411)", "body": "Summary:\nThis PR enable some failing unit tests for fft in pytorch on ROCM.\n\nThe reason these tests were failing was due to an error in how hipfft was executed for different transform types for float inputs causing a mismatch error when compared to baselines.\n\nWe solved the problem by calling hipfft with the right config for each transformation type.\n\nThere PR doesnot enable all fft tests. There are still other issues that need to be resolved before that can happen.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53411\n\nReviewed By: albanD\n\nDifferential Revision: D27008323\n\nPulled By: mruberry\n\nfbshipit-source-id: 649c65d0f12a889a426ec475f7d8fcc6f1d81bd3", "pr_number": "53411", "files_changed": ["aten/src/ATen/native/cuda/SpectralOps.cu", "test/test_spectral_ops.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "ef9ee46756": {"title": "Avoid modifying rebuild buckets state in no_grad context (#54159)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54159\n\nSee https://github.com/pytorch/pytorch/issues/54059 for discussion.\n\nIn short, users might want to run evaluation on a single rank\nin `torch.no_grad()` mode. When this happens, we need to make\nsure that we skip all rebuild bucket logics, as the forward only\nruns on one rank and not all peers can sure the bucket configuration\nsync communication.\n\nTest Plan: Imported from OSS\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27119666\n\nPulled By: mrshenli\n\nfbshipit-source-id: 4b2f8cce937cdd893e89d8d10c9267d255ba52ea", "pr_number": "54159", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ca429fedd3": {"title": "[StaticRuntime] Fuse SigridTransforms + ListUnpack (#53920)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53920\n\nFusing SigridTransforms + ListUnpack allows for enabling out variant for SigridTransforms so that the output tensors can be managed by the MemoryPlanner in Static Runtime.\n\nThe speedup comes from three parts 1) get rid of memory allocation inside SigridTransforms itself, 2) memory deallocation cost (outside SigridTransforms, inside MemoryPlanner), 3) get rid of ListUnpack. However, in 3) we still need to pay the cost of constructing `vector<Tensor>` for outputs and a round of refcount bumps for all the output TensorImpls.\n\nReviewed By: ajyu\n\nDifferential Revision: D26220546\n\nfbshipit-source-id: 651bdfb850225511c43b8f50083b13e8dec46bcc", "pr_number": "53920", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/passes.cpp", "torch/csrc/jit/runtime/static/passes.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "255b103c1b": {"title": "[WIP] Function to retrieve inspect.Signature instances for PyTorch ops (#53830)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53830\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D26982802\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 18fddc9f3f34b09e173de59f2fe886f8eedd000e", "pr_number": "53830", "files_changed": ["test/test_fx.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/init.cpp", "torch/csrc/jit/python/python_ir.cpp", "torch/fx/operator_schemas.py"], "labels": ["Merged", "cla signed", "fx", "oncall: jit"]}, "a27f46bbe3": {"title": "[FX] Experimental type annotation pass using Python signatures (#53831)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53831\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D26982804\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 17db9f71e729206f29ee231e34723d9616f128b7", "pr_number": "53831", "files_changed": ["test/test_fx_experimental.py", "torch/fx/experimental/schema_type_annotation.py", "torch/fx/graph.py"], "labels": ["Merged", "cla signed", "fx"]}, "72c7983f23": {"title": "Remove __get__ from Tensor stub. (#54208)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54208\n\nIt seems like it was added to suppress some errors in LazyModules, but I think we should solve those more directly with some type ignores in more surgical places.\n\nFixes #54087.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27137363\n\nPulled By: ezyang\n\nfbshipit-source-id: 017cafcc3350e73cd62436078835b97cd9b3b929", "pr_number": "54208", "files_changed": ["tools/pyi/gen_pyi.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/linear.py"], "labels": ["Merged", "cla signed"]}, "2d8795c552": {"title": "[FX] Normalize torch. namespace ops (#53832)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53832\n\nTest Plan: Imported from OSS\n\nReviewed By: jfix71, Chillee\n\nDifferential Revision: D26982801\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 96ac8efe2b3c644cfb7328168f6db089d3756aa2", "pr_number": "53832", "files_changed": ["torch/fx/experimental/normalize.py", "torch/fx/operator_schemas.py"], "labels": ["Merged", "cla signed", "fx"]}, "133000fe7a": {"title": "[distributed] add processgroup options as argument (#53663)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53663\n\nThis add the processgroup option as an optional argument to new_group\nand init_processgroup, this allows user to pass in a initialized\nprocessgroup option for gloo and nccl.\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26968857\n\nPulled By: wanchaol\n\nfbshipit-source-id: 2ff73a009120b85e83ecde7c69956b731902abc2", "pr_number": "53663", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/distributed_c10d.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c618dc13d2": {"title": "Use type-erased union for Buffer. (#322)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/tensorpipe/pull/322\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54145\n\nIn order to merge the channel hierarchies, we need a generic `Buffer` type, that can wrap either a `CpuBuffer` or a `CudaBuffer`.\nThe constraints are that, since this type is used by the channels, it cannot explicitly refer to `CudaBuffer`. We propose here a type-erasure based solution, with small-buffer optimization to avoid heap-allocating the wrapped concrete buffer.\nghstack-source-id: 124131499\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27001339\n\nfbshipit-source-id: 26d7dc19d69d7e3336df6fd4ff6ec118dc17c5b6", "pr_number": "54145", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": ["Merged", "Reverted", "cla signed", "oncall: distributed"]}, "2f3b194dc2": {"title": "Add cusolver potrf and potrfBatched to the backend of torch.cholesky decomposition (#53104)", "body": "Summary:\nThis PR adds cusolver potrf and potrfBatched to the backend of torch.cholesky and torch.linalg.cholesky.\n\nCholesky heuristics:\n\n- Use cusolver potrf for batch_size 1\n- Use magma_xpotrf_batched for batch_size >= 2\n- if magma is not available, use loop of cusolver potrf for batch_size >= 2\n\ncusolver potrf batched currently has some nan output issue, we will switch to cusolver potrf batched after it's fixed\n\nSee also https://github.com/pytorch/pytorch/issues/42666 #47953\n\nTodo:\n\n- [x] benchmark and heuristic\n\nClose https://github.com/pytorch/pytorch/pull/53992\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53104\n\nReviewed By: agolynski\n\nDifferential Revision: D27113963\n\nPulled By: mruberry\n\nfbshipit-source-id: 1429f63891cfc6176f9d8fdeb5c3b0617d750803", "pr_number": "53104", "files_changed": ["aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/CUDASolver.h", "aten/src/ATen/cuda/Exceptions.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h", "caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed", "open source"]}, "564456ac44": {"title": "Added autograd support for torch.orgqr (#52637)", "body": "Summary:\nThis PR adds autograd support for `torch.orgqr`.\n\nSince `torch.orgqr` is one of few functions that expose LAPACK's naming and all other linear algebra routines were renamed a long time ago, I also added a new function with a new name and `torch.orgqr` now is an alias for it.\n\nThe new proposed name is `householder_product`. For a matrix `input` and a vector `tau` LAPACK's orgqr operation takes columns of `input` (called Householder vectors or elementary reflectors) scalars of `tau` that together represent Householder matrices and then the product of these matrices is computed. See https://www.netlib.org/lapack/lug/node128.html.\nOther linear algebra libraries that I'm aware of do not expose this LAPACK function, so there is some freedom in naming it. It is usually used internally only for QR decomposition, but can be useful for deep learning tasks now when it supports differentiation.\n\nResolves https://github.com/pytorch/pytorch/issues/50104\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52637\n\nReviewed By: agolynski\n\nDifferential Revision: D27114246\n\nPulled By: mruberry\n\nfbshipit-source-id: 9ab51efe52aec7c137aa018c7bd486297e4111ce", "pr_number": "52637", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/linalg.rst", "test/test_linalg.py", "test/test_ops.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "torch/_torch_docs.py", "torch/csrc/api/include/torch/linalg.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/linalg/__init__.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: autograd", "module: linear algebra", "oncall: jit", "open source", "triaged"]}, "444552e7f9": {"title": "Optimize alias_analysis node lookup (#54115)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54115\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27104047\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 0ef4e78be9ea7081b63ab2303711746bf09653eb", "pr_number": "54115", "files_changed": ["torch/csrc/jit/ir/alias_analysis.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "9f86b656ba": {"title": "Resubmit: Adding parallel support for the LLVM backend. (#54122)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54122\n\nTest Plan:\n* USE_TBB=1 ATEN_THREADING=TBB python setup.py develop --cmake\n  * USE_TBB=1 ATEN_THREADING=NATIVE python setup.py develop --cmake\n  * USE_TBB=1 ATEN_THREADING=OMP python setup.py develop --cmake\n  * cd build; ninja bin/tensorexpr_bench\n  * bin/test_tensorexpr --gtest_filter=\"*Parallel*\"\n\nReviewed By: bertmaher\n\nDifferential Revision: D27109802\n\nPulled By: zheng-xq\n\nfbshipit-source-id: db159466d0b46357bcf0fbefb36094bee312368c", "pr_number": "54122", "files_changed": ["benchmarks/cpp/tensorexpr/CMakeLists.txt", "benchmarks/cpp/tensorexpr/bench_parallel.cpp", "benchmarks/cpp/tensorexpr/bench_reduce.cpp", "test/cpp/tensorexpr/test_llvm.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.h", "torch/csrc/jit/tensorexpr/llvm_jit.cpp", "torch/csrc/jit/tensorexpr/llvm_jit.h", "torch/csrc/jit/tensorexpr/stmt.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "dc35848804": {"title": "[PyTorch] Rename XPLAT_MOBILE_BUILD to TEMPLATE_SELECTIVE_BUILD (#54217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54217\n\nAs title. Find all XPLAT_MOBILE_BUILD usage: [search result](https://www.internalfb.com/intern/codesearch/?bunny_arg=XPLAT_MOBILE_BUILD&bunny_command=fbgs&lucky=0&q=repo%3Afbcode%20case%3Ainsensitive%20regex%3Aoff%20XPLAT_MOBILE_BUILD&source=redirect) and replace.\n\n Since template selective build is added in OSS, rename the macro to make it clearer.\n\nT86478520 to follow up to unify XPLAT_MOBILE_BUILD (rename to TEMPLATE_SELECTIVE_BUILD), C10_MOBILE and BUILD_LITE_INTERPRETER macros.\nghstack-source-id: 124206354\n\nTest Plan: CI\n\nReviewed By: dhruvbird, iseeyuan\n\nDifferential Revision: D27112046\n\nfbshipit-source-id: 6f89b168c1f39c5449c8ed6538d887ea066a2225", "pr_number": "54217", "files_changed": ["aten/src/ATen/Dispatch.h", "aten/src/ATen/core/op_registration/op_allowlist.h"], "labels": ["Merged", "cla signed"]}, "a52e295cbb": {"title": "Add MyPY to lint GHA workflow (#54067)", "body": "Summary:\nAlso disable test_run_mypy from  test_type_hints.py as it is running as part of GHA\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54067\n\nReviewed By: ezyang\n\nDifferential Revision: D27091530\n\nPulled By: malfet\n\nfbshipit-source-id: 9cfe397260aba34aeb055676855db383cd06f76d", "pr_number": "54067", "files_changed": [".github/workflows/lint.yml", "test/test_type_hints.py"], "labels": ["Merged", "cla signed", "module: typing"]}, "cba8516b52": {"title": "make internal forwardAD methods on at::Tensor internal (#54099)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54099\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D27117838\n\nPulled By: albanD\n\nfbshipit-source-id: ede96529a4b099dea9cf885d0bf2cb352aa30fa5", "pr_number": "54099", "files_changed": ["aten/src/ATen/core/Formatting.cpp", "aten/src/ATen/native/AutogradComposite.cpp", "aten/src/ATen/templates/TensorBody.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/autograd_meta.cpp", "torch/csrc/autograd/saved_variable.cpp"], "labels": ["Merged", "cla signed"]}, "a425eb2135": {"title": "Add size check for forward grads (#54100)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54100\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D27117842\n\nPulled By: albanD\n\nfbshipit-source-id: ccb6abac38d7fca31bea72cbbf3bba38c6030c37", "pr_number": "54100", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/autograd_meta.cpp"], "labels": ["Merged", "cla signed"]}, "09b4af2f0f": {"title": "Remove legacy from optional-related function names (#54101)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54101\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D27117839\n\nPulled By: albanD\n\nfbshipit-source-id: 1f50b06ff9b0be8301f6ea9eca14f73a3a5fa137", "pr_number": "54101", "files_changed": ["torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["Merged", "cla signed"]}, "004db37358": {"title": "properly make AutogradMeta/DifferentiableViewMeta attributes internal (#54102)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54102\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D27117841\n\nPulled By: albanD\n\nfbshipit-source-id: bb047cf1878ccff81d677ceb02e98e784760c3ec", "pr_number": "54102", "files_changed": ["torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["Merged", "cla signed"]}, "cc92117aad": {"title": "cleanup static_cast of AutogradMeta (#54103)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54103\n\nThe goal is to reduce the spread of static casts in the autograd code as per the comment in https://github.com/pytorch/pytorch/pull/49097#discussion_r543695091\nI wasn't sure how to use a virtual method here but a simple method in impl clean it up quite nicely.\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski\n\nDifferential Revision: D27117840\n\nPulled By: albanD\n\nfbshipit-source-id: 5f277dde34ccf6bc20f76583b906ff3528cde5aa", "pr_number": "54103", "files_changed": ["torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["Merged", "cla signed"]}, "f0056f89a4": {"title": "Final kernel launch checks (#54214)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54214\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D27138004\n\nfbshipit-source-id: 4448ad8242eb721d0ce02b35a65236226eed9a31", "pr_number": "54214", "files_changed": ["aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/THC/THCTensorSort.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "04a2506091": {"title": "Fixed the size of the workspace array in functions calling MAGMA (#54009)", "body": "Summary:\nThe size of the workspace arrays should not be less than 1. This PR fixes lstsq calls to LAPACK and MAGMA. Also `max(1, ...)` guards were added to a few other functions (symeig, svd).\nROCm testing is enabled for lstsq, pinv, pinverse.\n\nFixes https://github.com/pytorch/pytorch/issues/53976\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54009\n\nReviewed By: ejguan\n\nDifferential Revision: D27155845\n\nPulled By: mruberry\n\nfbshipit-source-id: 04439bfa82a5bdbe2297a6d62b6e68ba1c30e4a2", "pr_number": "54009", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "test/test_linalg.py"], "labels": ["Merged", "Reverted", "cla signed", "module: linear algebra", "module: rocm", "open source", "triaged"]}, "d85faf8d8e": {"title": "Cleanup mypy lint job (#54260)", "body": "Summary:\nUpdate to checkout v2\nDelete \"Get HEAD commit SHA\" step\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54260\n\nReviewed By: samestep\n\nDifferential Revision: D27160678\n\nPulled By: malfet\n\nfbshipit-source-id: d1afe4f1cf0046cfb93de583ee123b4db5b25f9a", "pr_number": "54260", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "3b1e3103ca": {"title": "Remove usage of onEachDevice from legacy profiler (#54125)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54125\n\nFixes https://github.com/pytorch/pytorch/issues/48987\n\nTest Plan:\npython setup.py clean\nTORCH_CUDA_ARCH_LIST=\"6.0\" USE_CUDA=1 USE_MKLDNN=1 BLAS=MKL BUILD_BINARY=1 python setup.py develop install --cmake 2>&1 | tee ~/output.txt\npython test/test_profiler.py -v\n\npython setup.py clean\nUSE_CUDA=0 USE_MKLDNN=1 BLAS=MKL BUILD_BINARY=1 python setup.py develop install --cmake 2>&1 | tee ~/output.txt\npython test/test_profiler.py -v\n\n+ CI\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27109481\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 3fba8bc55deafeed1ab4680b311e927f40eaf99c", "pr_number": "54125", "files_changed": ["torch/autograd/profiler.py", "torch/csrc/autograd/profiler_legacy.cpp", "torch/csrc/autograd/profiler_legacy.h", "torch/csrc/distributed/rpc/utils.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "0645e2b490": {"title": "Use shard file if present, improve functions used for sharding (#54210)", "body": "Summary:\nStep 2 to fixing https://github.com/pytorch/pytorch/issues/53882 :)\n\nThis changes TARGET_DET_LIST and sharding automation by checking if there's already cached data from the commit in `.pytorch-test-times`. If not, it pulls data from S3 and updates the file to have the stats. This way, S3 pulling does not need to happen more than once for the same commit.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54210\n\nTest Plan:\nthe following methods should run the same set of tests.\nFirst `export CIRCLE_JOB=pytorch_linux_xenial_cuda10_2_cudnn7_py3_gcc7_test2` or your favorite CIRCLE JOB.\n\n1. Pull data first and use it:\nDownload the data from S3 and write it to the cache file with `python test/run_test.py --export-historic-test-times .pytorch-test-times`\nNow run `python test/run_test.py --shard 1 10`\n\n2. Make the sharding job pull data:\nDelete the file you just created: `rm .pytorch-test-times`\nNow run `python test/run_test.py --shard 1 10`\n\nReviewed By: walterddr\n\nDifferential Revision: D27136849\n\nPulled By: janeyx99\n\nfbshipit-source-id: 51a42c4e2fa3f8cf15e682679dd3eb6130aad927", "pr_number": "54210", "files_changed": [".gitignore", "test/run_test.py", "test/test_testing.py", "torch/testing/_internal/framework_utils.py"], "labels": ["Merged", "cla signed"]}, "a95abc4648": {"title": "Test tools/test_history.py (#54259)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54259\n\nTest Plan:\nThe main point of this is to be run in our \"Test tools\" GitHub Actions workflow. To test locally:\n```\nmypy --config=mypy-strict.ini\npython tools/test/test_test_history.py\n```\n\nReviewed By: seemethere\n\nDifferential Revision: D27164519\n\nPulled By: samestep\n\nfbshipit-source-id: 46f90e62e2d4d0c413b202419e509d471bad43de", "pr_number": "54259", "files_changed": [".github/workflows/test_tools.yml", "mypy-strict.ini", "tools/test/test_test_history.py", "tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "90bbe0b38b": {"title": "cmake: auto-detect ccache to speed up developer builds (#49389)", "body": "Summary:\nhttps://ccache.dev/ is a compiler cache that speeds up subsequent builds. Auto-detecting ccache ensures that it is used on systems where it is available, greatly improving build times for developers. There is no risk in enabling ccache in practice. Please refer to https://ccache.dev/ for a short summary / motivation\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49389\n\nReviewed By: ejguan\n\nDifferential Revision: D27169957\n\nPulled By: malfet\n\nfbshipit-source-id: 673b60bbceb0d323901c8a992a75792c6da9b805", "pr_number": "49389", "files_changed": ["CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["Merged", "cla signed", "module: build", "open source", "triaged"]}, "8cd4dac78f": {"title": "Move mypy wrapper to tools (#54268)", "body": "Summary:\nThis PR\n\n- moves `torch/testing/_internal/mypy_wrapper.py` (and its accompanying tests from `test/test_testing.py`) to `tools`,\n- removes the now-unused `test_run_mypy` from `test/test_type_hints.py`, and\n- replaces the hardcoded list of `mypy` configs (previously duplicated across `mypy_wrapper.py` and `.github/workflows/lint.yml`) with a simpler glob\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54268\n\nTest Plan:\nShould also be run in the \"Test tools\" GHA workflow in CI:\n```\npython tools/test/test_mypy_wrapper.py\n```\n\nReviewed By: janeyx99\n\nDifferential Revision: D27168095\n\nPulled By: samestep\n\nfbshipit-source-id: a8dc18407b5e4c103ace23a636b0a8534951905a", "pr_number": "54268", "files_changed": [".github/workflows/lint.yml", ".github/workflows/test_tools.yml", "mypy-strict.ini", "test/test_testing.py", "test/test_type_hints.py", "tools/README.md", "tools/mypy_wrapper.py", "tools/test/test_mypy_wrapper.py", "torch/testing/_internal/mypy_wrapper.py"], "labels": ["Merged", "cla signed"]}, "75498164fe": {"title": "Remove nonexistent files (#54276)", "body": "Summary:\nSince both these files were deleted back in time, we shouldn't be running them anymore, as this was the old sharding strategy (see https://github.com/pytorch/pytorch/issues/50660).\n```\ntest_python_nn.bat\ntest_python_all_except_nn.bat\n```\n\nI believe we intend to run all the python files, so I added a call for that instead.\n\nNote: I don't believe there is a single unsharded test build, though, so should I instead just assume that all windows tests will be sharded?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54276\n\nReviewed By: ejguan\n\nDifferential Revision: D27173045\n\nPulled By: janeyx99\n\nfbshipit-source-id: a7562c1479e18bd63f192f02129a42911a73a70b", "pr_number": "54276", "files_changed": [".jenkins/pytorch/win-test-helpers/test_python.bat", ".jenkins/pytorch/win-test.sh"], "labels": ["Merged", "cla signed"]}, "04e0cbf5a9": {"title": "Add padding='same' mode to conv{1,2,3}d (#45667)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/45667\n\nFirst part of #3867 (Pooling operators still to do)\n\nThis adds a `padding='same'` mode to the interface of `conv{n}d`and `nn.Conv{n}d`. This should match the behaviour of `tensorflow`. I couldn't find it explicitly documented but through experimentation I found `tensorflow` returns the shape `ceil(len/stride)` and always adds any extra asymmetric padding onto the right side of the input.\n\nSince the `native_functions.yaml` schema doesn't seem to support strings or enums, I've moved the function interface into python and it now dispatches between the numerically padded `conv{n}d` and the `_conv{n}d_same` variant. Underscores because I couldn't see any way to avoid exporting a function into the `torch` namespace.\n\nA note on asymmetric padding. The total padding required can be odd if both the kernel-length is even  and the dilation is odd. mkldnn has native support for asymmetric padding, so there is no overhead there, but for other backends I resort to padding the input tensor by 1 on the right hand side to make the remaining padding symmetrical. In these cases, I use `TORCH_WARN_ONCE` to notify the user of the performance implications.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D27170744\n\nPulled By: jbschlosser\n\nfbshipit-source-id: b3d8a0380e0787ae781f2e5d8ee365a7bfd49f22", "pr_number": "45667", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Pool.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/Functions.cpp", "aten/src/ATen/templates/Functions.h", "c10/util/overloaded.h", "test/cpp/api/modules.cpp", "test/test_nn.py", "torch/csrc/api/include/torch/enum.h", "torch/csrc/api/include/torch/nn/functional/conv.h", "torch/csrc/api/include/torch/nn/modules/conv.h", "torch/csrc/api/include/torch/nn/options/conv.h", "torch/csrc/api/src/enum.cpp", "torch/csrc/api/src/nn/modules/conv.cpp", "torch/csrc/jit/tensorexpr/external_functions.cpp", "torch/nn/functional.py", "torch/nn/modules/conv.py", "torch/testing/_internal/common_nn.py"], "labels": ["Merged", "cla signed", "module: convolution", "module: nn", "open source", "triaged"]}, "53d8778b4d": {"title": "Update clang-format linux hash and yaml import calls (#53932)", "body": "Summary:\nFixing Bandit security issues.\n- yaml_load: Use of unsafe yaml load. Allows instantiation of arbitrary objects. Consider yaml.safe_load().\nTest ID: B506\nSeverity: MEDIUM\nConfidence: HIGH\nFile: ./caffe2/contrib/aten/gen_op.py\nMore info: https://bandit.readthedocs.io/en/latest/plugins/b506_yaml_load.html\n235 if __name__ == '__main__':\n236     decls = yaml.load(read(os.path.join(args.yaml_dir, 'Declarations.yaml')), Loader=Loader)\n237     factory_methods = find_factory_methods(decls)\n\n- Blacklist: Use of insecure MD2 (https://github.com/pytorch/pytorch/commit/6149a26adb9bcbee2965ea6cc2d1d47fe0569c95), MD4 (https://github.com/pytorch/pytorch/commit/fc7f0269808581499571c5db8af87311c943cd4e), MD5 (https://github.com/pytorch/pytorch/commit/7ea9d9af4e82d20c7c6cee5edd3c52f9bcb42821), or SHA1 hash function.\nTest ID: B303\nSeverity: MEDIUM\nConfidence: HIGH\nFile: ./tools/clang_format_utils.py\nMore info: https://bandit.readthedocs.io/en/latest/blacklists/blacklist_calls.html#b303-md5\n36\n37     hash = hashlib.sha1()\n38\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53932\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27072017\n\nPulled By: malfet\n\nfbshipit-source-id: 2fef0119388797aee3cacdc880fc345bd2ba68ce", "pr_number": "53932", "files_changed": ["caffe2/contrib/aten/gen_op.py", "test/test_namedtuple_return_api.py", "tools/autograd/gen_python_functions.py", "tools/autograd/load_derivatives.py", "tools/clang_format_hash/linux64/clang-format-linux64", "tools/clang_format_hash/mac/clang-format-mojave", "tools/clang_format_utils.py", "tools/clang_tidy.py", "tools/codegen/gen.py", "tools/codegen/selective_build/selector.py", "tools/setup_helpers/generate_code.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "bbb06c05a8": {"title": "remove type_hint_tests and convert the files to use the new test style (#53167)", "body": "Summary:\nThis is a follow-up PR of https://github.com/pytorch/pytorch/issues/52408 and move/convert all files under `test/type_hint_tests/*.py` to use the new test style.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53167\n\nReviewed By: ejguan\n\nDifferential Revision: D27081041\n\nPulled By: walterddr\n\nfbshipit-source-id: 56508083800a5e12a7af88d095ca26229f0df358", "pr_number": "53167", "files_changed": ["mypy.ini", "test/test_typing.py", "test/type_hint_tests/module_list.py", "test/type_hint_tests/namedtuple.py", "test/type_hint_tests/opt_size.py", "test/type_hint_tests/size.py", "test/type_hint_tests/tensor_copy.py", "test/type_hint_tests/torch_cuda_random.py", "test/type_hint_tests/torch_optim.py", "test/typing/reveal/module_list.py", "test/typing/reveal/namedtuple.py", "test/typing/reveal/opt_size.py", "test/typing/reveal/size.py", "test/typing/reveal/tensor_copy.py", "test/typing/reveal/torch_optim.py"], "labels": ["Merged", "cla signed", "module: typing", "open source", "triaged"]}, "19792b45db": {"title": "add a pytest.ini file (#53152)", "body": "Summary:\nThis shall fix the first three items of https://github.com/pytorch/pytorch/issues/52984 by adding a pytest.ini configuration file.\n\n#### `--tb=short`\nIn a failure, pytest will not show the entire traceback nor the docstring.\n\n- Without `--tb=short`:\n<details>\n\n```\n$ pytest test/test_typing.py -k tensor_copy\n================================================================== test session starts ===================================================================\nplatform linux -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /home/guilhermel/git/pytorch, configfile: pytest.ini\nplugins: hypothesis-5.38.1, typeguard-2.10.0\ncollected 8 items / 7 deselected / 1 selected\n\ntest/test_typing.py F                                                                                                                              [100%]\n\n======================================================================== FAILURES ========================================================================\n______________________________________________________________ test_reveal[tensor_copy.py] _______________________________________________________________\n\npath = '/home/guilhermel/git/pytorch/test/typing/reveal/tensor_copy.py', reveal = 'int '\nexpected_reveal = \"/home/guilhermel/git/pytorch/test/typing/reveal/tensor_copy.py:11: note: Revealed type is 'torch.tensor.Tensor'\", lineno = 11\n\n    def _test_reveal(path: str, reveal: str, expected_reveal: str, lineno: int) -> None:\n        if reveal not in expected_reveal:\n>           raise AssertionError(_REVEAL_MSG.format(lineno, expected_reveal, reveal))\nE           AssertionError: Reveal mismatch at line 11\nE\nE           Expected reveal: \"/home/guilhermel/git/pytorch/test/typing/reveal/tensor_copy.py:11: note: Revealed type is 'torch.tensor.Tensor'\"\nE           Observed reveal: 'int '\n\ntest/test_typing.py:156: AssertionError\n================================================================ short test summary info =================================================================\nFAILED test/test_typing.py::test_reveal[tensor_copy.py] - AssertionError: Reveal mismatch at line 11\n```\n\n</details>\n\n- With `--tb=short`:\n<details>\n\n```\n$ pytest test/test_typing.py -k tensor_copy\n================================================================== test session starts ===================================================================\nplatform linux -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\nrootdir: /home/guilhermel/git/pytorch, configfile: pytest.ini\nplugins: hypothesis-5.38.1, typeguard-2.10.0\ncollected 8 items / 7 deselected / 1 selected\n\ntest/test_typing.py F                                                                                                                              [100%]\n\n======================================================================== FAILURES ========================================================================\n______________________________________________________________ test_reveal[tensor_copy.py] _______________________________________________________________\ntest/test_typing.py:156: in _test_reveal\n    raise AssertionError(_REVEAL_MSG.format(lineno, expected_reveal, reveal))\nE   AssertionError: Reveal mismatch at line 11\nE\nE   Expected reveal: \"/home/guilhermel/git/pytorch/test/typing/reveal/tensor_copy.py:11: note: Revealed type is 'torch.tensor.Tensor'\"\nE   Observed reveal: 'int '\n```\n\n</details>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53152\n\nReviewed By: agolynski\n\nDifferential Revision: D26846808\n\nPulled By: walterddr\n\nfbshipit-source-id: d16c951b370b0643c8bbedca73d5184c6b65aba7", "pr_number": "53152", "files_changed": ["pytest.ini"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "bfd009836e": {"title": "[torch.special] Add special.erf{c, inv} (#53260)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/50345\n\nAlso adds `overrides` entry for module and the newly added functions.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53260\n\nReviewed By: agolynski\n\nDifferential Revision: D27114342\n\nPulled By: mruberry\n\nfbshipit-source-id: b1dd88f373db251bb71df12d33b160382138f63f", "pr_number": "53260", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/special.rst", "torch/_torch_docs.py", "torch/csrc/api/include/torch/special.h", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py", "torch/special/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: special", "oncall: jit", "open source"]}, "acf03b13f1": {"title": "[Static Runtime] Check for number of uses of op inputs > 1 in ReplaceWithCopy (#54230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54230\n\nThe comments in the code explained why this change is needed.\n\nReviewed By: bwasti\n\nDifferential Revision: D27145406\n\nfbshipit-source-id: 2a61a42f22dfadfad59ee6c3be3e9e9d19e90ac3", "pr_number": "54230", "files_changed": ["benchmarks/static_runtime/deep_wide_pt.h", "benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "8f755b9ed0": {"title": "initial draft for assert_tensors_(equal|allclose) in torch.testing (#53820)", "body": "Summary:\nAddresses https://github.com/pytorch/pytorch/issues/53618#issuecomment-795896298\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53820\n\nReviewed By: agolynski\n\nDifferential Revision: D27113912\n\nPulled By: mruberry\n\nfbshipit-source-id: 2a37522eaa37e90bf7b116f3964b06b46068cab7", "pr_number": "53820", "files_changed": ["torch/testing/asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "a84afb3a7c": {"title": "Use type-erased union for Buffer. (#54251)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54251\n\nPull Request resolved: https://github.com/pytorch/tensorpipe/pull/324\n\nIn order to merge the channel hierarchies, we need a generic `Buffer` type, that can wrap either a `CpuBuffer` or a `CudaBuffer`.\nThe constraints are that, since this type is used by the channels, it cannot explicitly refer to `CudaBuffer`. We propose here a type-erasure based solution, with small-buffer optimization to avoid heap-allocating the wrapped concrete buffer.\n\nThis is a new version of D27001339 (https://github.com/pytorch/pytorch/commit/c618dc13d2aa23625cb0d7ada694137532a4fa33) which broke PyTorch OSS build.\n\nTest Plan: CI\n\nReviewed By: lw, mrshenli\n\nDifferential Revision: D27156053\n\nfbshipit-source-id: 4244302af33a3be91dcd06093c0d6045d081d3cc", "pr_number": "54251", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "third_party/tensorpipe", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": ["Merged", "ci/all", "cla signed", "fb-exported", "oncall: distributed"]}, "f2b4b0e9eb": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27184963\n\nfbshipit-source-id: 65355a12697c8bd996b86947e3e0aeb0ee4eff3f", "pr_number": null, "files_changed": ["torch/csrc/distributed/rpc/utils.cpp"], "labels": []}, "887759c9b9": {"title": "Changes to autograd/custom functions to handle optional arguments (#54270)", "body": "Summary:\nSmall changes to autograd to support optional Tensor values.\nOn MLC device, we use Autograd Custom Functions to override the autograd engine for a specific operation. We do something like:\n\n```\nat::Tensor AtenMLCAutogradTypeDefault::abs(const at::Tensor & self) {\n  torch_mlc::mlclogger() << \"MLC bridge autograd MLC : abs\" << std::endl;\n  torch_mlc::AutoNonAtenMLCAutogradTypeDefault guard(true);\n  return MLCAbsFunction::apply(self);\n}\n\nTORCH_LIBRARY_IMPL(aten, AutogradMLC, m) {\n  m.impl(\"abs\", static_cast<at::Tensor (*)(const at::Tensor &)>(&AtenMLCAutogradTypeDefault::abs));\n}\n```\nWhat I noticed is that the existing code does not always work for optional Tensor types. This PR fixes it. Let me know if you have a better way to deal with this issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54270\n\nReviewed By: ejguan\n\nDifferential Revision: D27171623\n\nPulled By: albanD\n\nfbshipit-source-id: 3aa8d59ee8da3cc943ad5e73521c2755d1ff2341", "pr_number": "54270", "files_changed": ["torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/custom_function.h"], "labels": ["Merged", "cla signed", "module: autograd", "open source", "triaged"]}, "e0aebe241d": {"title": "Refactor tensor_new.cpp to use TensorOptions instead of DispatchKey (#54034)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54034\n\nFixes #53544\n\nI had to touch a bunch of lines but the refactoring was fairly\nmechanical.  Here's how it works.\n\nThe basic concept behind this PR is that tensor_new.cpp was previously\nabusing DispatchKey when it actually meant TensorOptions.  The provided\nDispatchKey argument to most of the constructor functions typically\ncomes from torch::tensors::get_default_dispatch_key();  it doesn't\nreally make sense for people to set the default dispatch key, but\nthis got grandfathered in due to the old API set_default_tensor_type\n(where the \"Type\" concept got refactored into \"DispatchKey\" concept\nover time).  See also #53124.  But the upshot is that, semantically,\nwhat we refer to as the default dispatch key really is more like\ntorch.set_default_tensor_type(torch.Tensor) versus\ntorch.set_default_tensor_type(torch.cuda.Tensor): clearly the user\nwants to do something about *construction* of the tensor, and\nTensorOptions captures that exactly.\n\nSo, how exactly to translate from one to the other?\n- Sources (things that used to PRODUCE DispatchKey)\n  - Most top level functions take a DispatchKey as their argument.  I\n    use the new function dispatchKeyToTensorOptions to convert it into\n    a TensorOptions\n  - typeIdWithDefault now produces a TensorOptions (probably could do\n    with a rename, though I didn't)\n- Sinks (things that used to CONSUME DispatchKey)\n  - Previously, the function options() was typically used to convert the\n    DispatchKey into a TensorOptions.  Now its replacement build_options\n    just takes a TensorOptions and sets some extra fields on it.\n    Irritatingly, I can't just replace\n    `build_options(options, scalar_type, device)` with\n    `options.dtype(scalar_type).device(device)` because the semantics\n    are slightly different: if device is nullopt, we should preserve\n    the usage of the device specified in options (what options.device()\n    does is overwrite the device unconditionally; e.g., if device is\n    nullopt, unset device from options)\n  - The other major sink for DispatchKey was `internal_new_from_data`,\n    but it turns out it only really extracts the device type from\n    the dispatch key.  Now it just pulls out the device from\n    TensorOptions.\n- To actually do the translation of DispatchKey to TensorOptions, I\n  introduce new functions dispatchKeyToLayout (replicating\n  layout_from_backend--there are still a few uses of this function\n  so I couldn't delete it) and dispatchKeyToDeviceType (replacing\n  computeDeviceType)\n- In all internal functions, whenever DispatchKey is taken as an argument,\n  I instead take TensorOptions as an argument, and pass it along.\n- Anywhere `legacyExtractDispatchKey(other.key_set())` equality was\n  previously used, I now do `other.options().type_equal()`, which\n  is the intended BC for doing \"backend to backend\" comparisons\n- There are a few places in the sparse constructors where we allocated\n  a tensor for values, and then read out the dispatch key from the\n  result to allocate the keys.  As best as I can tell, this is totally\n  equivalent to just passing in the options to both values and indices\n  (the only difference is dtype, which is captured via a separate\n  argument)\n\nThis refactor doesn't really go far enough: for example, there are now\nfunctions that take both TensorOptions and ScalarType, when really\nthe TensorOptions can capture this all.  I kept it solely just\ns/DispatchKey/TensorOptions/ to reduce the number of possible bugs;\nalso, a lot of this will be mooted by a proper fix to #53124.\n\nEven with this limited refactor, the payoff is sweet.  I can delete:\n\n- backendToCPU\n- backendToXPU\n- backendToCUDA\n- backendToHIP\n- backendToBackendOfDeviceType\n\nThe reason I can do this is because I can simply overwrite layout in TensorOptions\nto do the conversion, rather than having to type out each backend case\nexplicitly.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27109509\n\nPulled By: ezyang\n\nfbshipit-source-id: 91d16cfbc390127770362ac04fb43f7e070077e9", "pr_number": "54034", "files_changed": ["c10/core/Backend.h", "c10/core/TensorOptions.h", "test/test_sparse.py", "torch/csrc/Module.cpp", "torch/csrc/autograd/python_legacy_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_new.h"], "labels": ["Merged", "cla signed"]}, "49f1336106": {"title": "Add Tensor::is_cpu, genericize TensorIterator (#54079)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54079\n\nFixes https://github.com/pytorch/pytorch/issues/53815\n\nInstead of testing if something is CUDA, we instead test if something\nis not CPU.  This in the general theming of \"Don't be so darn CUDA\ncentric\".\n\nIntruigingly, we didn't have a is_cpu() method on Tensor.  Which seems\nlike a big oversight and one of the reasons how we ended up in this\nmess.  So in it goes.  Maybe we should also get this for Python bindings\nas well (but in that case, should probably look into redoing all of the\nis_X bindings so they aren't done manually).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27109507\n\nPulled By: ezyang\n\nfbshipit-source-id: abbe72c2e688c452ffe098d206cb79938b5824b1", "pr_number": "54079", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.cpp", "c10/core/TensorImpl.h", "test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "645a3e9a92": {"title": "Fix inaccurate dispatch tables (#54127)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54127\n\nDuring the meta tensor bringup, I found all of these operators\nadvertised that they worked on all backends (DefaultBackend/Math)\nbut actually they only worked on CPU/CUDA.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27109508\n\nPulled By: ezyang\n\nfbshipit-source-id: 0f474ecf4aba8b8207f2910bdc962bf581f53853", "pr_number": "54127", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "cc7a28d727": {"title": "Refactor Unary Ops tests (#49712)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/49712\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D25673712\n\nPulled By: izdeby\n\nfbshipit-source-id: 4420d5d129026195097d914e410b75b144bea795", "pr_number": "49712", "files_changed": ["aten/src/ATen/native/cuda/ForeachUnaryOp.cu", "test/test_foreach.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "05a03a6c8c": {"title": "[FX][EZ] Fix type correctness on GraphModule.graph (#54305)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54305\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D27181176\n\nPulled By: jamesr66a\n\nfbshipit-source-id: ed91cfed193984249c07a5bafc7aa732bfe0194d", "pr_number": "54305", "files_changed": ["torch/fx/graph_module.py"], "labels": ["Merged", "cla signed", "fx"]}, "fa07d0c8eb": {"title": ".github: Add workflow to build libtorch (#53292)", "body": "Summary:\nBased on https://github.com/pytorch/pytorch/issues/50633 and https://github.com/pytorch/pytorch/issues/51243.\n\nThings left to do:\n\n- [x] modify `.github/scripts/generate_binary_build_matrix.py` further\n  - [x] add option for not iterating over Python version\n  - [x] add `LIBTORCH_VARIANT`\n  - [x] add option for cxx11\n  - [x] fix the artifact uploading\n  - [x] remove `pull_request` hook before merging\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53292\n\nTest Plan: [CI](https://github.com/pytorch/pytorch/actions/runs/665781075).\n\nReviewed By: seemethere\n\nDifferential Revision: D27189150\n\nPulled By: samestep\n\nfbshipit-source-id: ec91e1f0b75b8c93613d55801585ed975697be03", "pr_number": "53292", "files_changed": [".github/scripts/generate_binary_build_matrix.py", ".github/workflows/build_linux_conda.yml", ".github/workflows/build_linux_libtorch.yml", ".github/workflows/build_linux_wheels.yml", "mypy-strict.ini"], "labels": ["Merged", "cla signed"]}, "f1cbd10276": {"title": "[PyPer] Port c2 add to pt (#54229)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54229\n\nBecause caffe2 add uses Eigen for add with broadcasting which is not well supported by OSS PyTorch, it's easier to just keep the `c2_add_out` internal for now. Caffe2 does use mkl add when the input dims of A and B are the same and there is no broadcasting needed.\n\nReviewed By: bertmaher\n\nDifferential Revision: D27036279\n\nfbshipit-source-id: 49f0ec5407ea1f641896f054cad2283faed81687", "pr_number": "54229", "files_changed": ["caffe2/operators/elementwise_ops_utils.cc", "caffe2/operators/elementwise_ops_utils.h", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "679f07a017": {"title": "Backup .circleci/config.yml before regenerating (#54345)", "body": "Summary:\nIf you accidentally modify `.circleci/config.yml` directly and then run `.circleci/regenerate.sh`, it clobbers your changes. This PR saves the previous contents of `.circleci/config.yml` to a temporary file, whose name is printed to the console due to the `-x` already present in the script.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54345\n\nTest Plan:\n```\n$ echo \"418 I'm a teapot\" > .circleci/config.yml\n$ .circleci/regenerate.sh\n```\n\nBefore:\n```\n++ dirname .circleci/regenerate.sh\n+ cd .circleci\n++ mktemp\n+ NEW_FILE=/var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.vW7yBQT2\n+ ./generate_config_yml.py\n+ cp /var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.vW7yBQT2 config.yml\n```\n```\n$ echo ':('\n:(\n```\n\nAfter:\n```\n++ dirname .circleci/regenerate.sh\n+ cd .circleci\n++ mktemp\n+ OLD_FILE=/var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.54GhUh7w\n+ cp config.yml /var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.54GhUh7w\n++ mktemp\n+ NEW_FILE=/var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.aV87RTvQ\n+ ./generate_config_yml.py\n+ cp /var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.aV87RTvQ config.yml\n```\n```\n$ cat /var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.54GhUh7w\n418 I'm a teapot\n$ echo ':D'\n:D\n```\n\nReviewed By: janeyx99\n\nDifferential Revision: D27195142\n\nPulled By: samestep\n\nfbshipit-source-id: fcd9e4ac102ec3523d96f772eedbd42123364e26", "pr_number": "54345", "files_changed": [".circleci/regenerate.sh"], "labels": ["Merged", "cla signed"]}, "6a4d2c61d5": {"title": "Allow linking against vcomp on Windows (#54132)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54054\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54132\n\nReviewed By: zou3519\n\nDifferential Revision: D27181524\n\nPulled By: malfet\n\nfbshipit-source-id: b79b34afb7edcc594d9b5907c5a7505b9cc5683b", "pr_number": "54132", "files_changed": ["README.md", "cmake/Modules/FindOpenMP.cmake"], "labels": ["Merged", "cla signed", "open source"]}, "27048c1dfa": {"title": "Remove legacy constructor calls from _torch_ folder. (#53889)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53146\nRelated to https://github.com/pytorch/pytorch/issues/47112\n\nAs mentioned in https://github.com/pytorch/pytorch/issues/47112, the plan is to:\n\n1. Verify that all `torch.Tensor()` scenarios are covered by other functions\n2. Scrub internal `torch.Tensor()` uses\n3. Update the docs and throw `TORCH_WARN_ONCE` if someone uses `torch.Tensor()`\n\nIn this PR, I replaced all occurrences of `torch.Tensor` present in the _torch_ folder.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53889\n\nReviewed By: walterddr, zou3519\n\nDifferential Revision: D27190743\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 7ecc201d57935b8dbb98ae3718b60d95cb55a010", "pr_number": "53889", "files_changed": ["torch/_torch_docs.py", "torch/distributions/kumaraswamy.py", "torch/linalg/__init__.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/modules/activation.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/linear.py", "torch/nn/modules/normalization.py", "torch/nn/modules/rnn.py", "torch/nn/modules/sparse.py", "torch/nn/parameter.py", "torch/nn/utils/prune.py", "torch/onnx/symbolic_opset9.py", "torch/quantization/fx/quantize.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/testing/_internal/common_nn.py", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py"], "labels": ["Merged", "cla signed", "fx", "oncall: distributed", "open source"]}, "270d675f86": {"title": "update distributed doc table for alltoall nccl (#54277)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54277\n\nalltoall already supported in nccl backend, so update the doc to reflect it.\n\nTest Plan: Imported from OSS\n\nReviewed By: divchenko\n\nDifferential Revision: D27172904\n\nPulled By: wanchaol\n\nfbshipit-source-id: 9afa89583d56b247b2017ea2350936053eb30827", "pr_number": "54277", "files_changed": ["docs/source/distributed.rst"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "9d9986fd10": {"title": "Support for Half / bfloat16 / index_select and better testing (#53898)", "body": "Summary:\nAdded the support for half / bfloat / bool for `index_select`, as suggested by ngimel in\nhttps://github.com/pytorch/pytorch/issues/49707#issuecomment-788140578\n\nFor the tests to pass, I also added the support for `index_add`.\n\nI added `OpInfo` tests for `index_add` and more thorough forward tests for `index_select` to test these changes.\n\nWhile doing so, I found that the support for scalar types in the derivative of `index_add` was not correct, so I corrected it.\n\nResolves https://github.com/pytorch/pytorch/issues/49707\n\nIt should also resolve similar issues that I encountered when porting `index_copy`, `take` and `put`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53898\n\nReviewed By: mruberry\n\nDifferential Revision: D27193294\n\nPulled By: ngimel\n\nfbshipit-source-id: 5a0af2c62a0cf24f3cc9c74f230ab4f3712bbb7a", "pr_number": "53898", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "4fa47e5e7d": {"title": "Support non-tensor inputs and outputs for checkpointed functions. (#52422)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52422\n\nAs mentioned in https://github.com/pytorch/pytorch/issues/52415,\n`torch.utils.checkpoint` doesn't support checkpointing for functions which have\nnon-tensor inputs and outputs.\n\nThis PR resolves this issue by ensuring the autograd machinery ignores the\nnon-tensor inputs and outputs and processes the tensors accordingly.\nghstack-source-id: 124406867\n\nTest Plan:\n1) unit test\n2) waitforbuildbot\n\nReviewed By: albanD\n\nDifferential Revision: D26507228\n\nfbshipit-source-id: 0a5a1591570814176185362e83ad18dabd9c84b0", "pr_number": "52422", "files_changed": ["test/test_autograd.py", "test/test_utils.py", "torch/autograd/function.py", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/custom_function.h", "torch/csrc/autograd/python_function.cpp", "torch/utils/checkpoint.py"], "labels": ["Merged", "cla signed"]}, "08e4312559": {"title": "Tag distributed team for review for /torch/nn/parallel (#54221)", "body": "Summary:\nThis folder contains the DDP python interface as well as several misc. communication files.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54221\n\nReviewed By: agolynski\n\nDifferential Revision: D27149068\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 0c23ea9a0d1dfc2719a2008e182ea75f2058d7dc", "pr_number": "54221", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed"]}, "8294bff20d": {"title": "[StaticRuntime] Copy version of reshape/flatten (#54353)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54353\n\nThe current implementation of reshape/flatten is problematic because whether the output is sometimes a tensor view and sometimes not. It entirely depends on the graph ir and input shapes. Replacing them with the copy version makes it deterministic and the output is always a tensor.\n\nReviewed By: ajyu, edvgha\n\nDifferential Revision: D26358525\n\nfbshipit-source-id: ee7571317b061221a8d50083676cded388ce6f87", "pr_number": "54353", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "cffa70d36d": {"title": "Merge channel hierarchies. (#54333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54333\n\nPull Request resolved: https://github.com/pytorch/tensorpipe/pull/326\n\nPull Request resolved: https://github.com/pytorch/tensorpipe/pull/312\n\nThis is a first step towards cross-device type transfers: eventually,\nchannels will not connect devices of a given type between two hosts,\nbut possibly heterogeneous pairs of devices. Hence, the distinction\nbetween CPU-to-CPU and GPU-to-GPU channels will not make much sense\nanymore, and we can afford to simplify the Pipe's code quite bit.\n\nThe main change here is that the `channel::Channel` and\n`channel::Context` classes are not templated (on the buffer type)\nanymore. Instead, a channel's `send`/`recv` methods act on generic\n`Buffer`s and the actual unpacking is done in the\n`ChannelBoilerplate`. The\n`channel::CpuContext`/`channel::CudaContext` (respectively\n`channel::CudaContext`/`channel::CudaChannel`) aliases now simply\nresolve to `channel::Context` (respectively `channel::Channel`). A\nsubsequent diff will get rid of the aliases altogether.\n\nThe Pipe is being simplified: all the duplication due to having\nseparate hierarchies is gone, which gets rid of a lot of boiler plate\ntemplate code. Note that previously, two channels with the same name\ncould potentially coexist, provided one was a CPU channel and the\nother a GPU channel. This is not the case anymore, though it should\nnot matter.\nIn its current state, the Pipe still needs to pick a channel based on\nwhether that channel acts on CPU or GPU buffers. This is solved by\nintroducing the temporary method\n`bool channel::Context::supportsDeviceType(DeviceType t)`. When\niterating through available channels to select one for a given tensor,\nthe Pipe now discards channels that do not support the tensor's\n`DeviceType`. This leads to having a single ordered list of channels,\nwhich in practice is two separate lists (one for CPU, one for GPU)\nmerged together. This will change soon as we initialize only one\nchannel per `DeviceType`.\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D26958187\n\nPulled By: beauby\n\nfbshipit-source-id: 3e3f7921166892d468fa78cfad3199277588021c", "pr_number": "54333", "files_changed": ["third_party/tensorpipe", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "f48a9712b7": {"title": "Rewrite functional.tensordot to be TorchScript-able (#53672)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53487\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53672\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D26934392\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: f842af340e4be723bf90b903793b0221af158ca7", "pr_number": "53672", "files_changed": ["test/jit/test_unsupported_ops.py", "test/test_jit.py", "torch/functional.py", "torch/jit/_builtins.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "fc58b3f146": {"title": "Skips failing pinv and pinverse test (#54392)", "body": "Summary:\nThis will unblock the CI failing due to https://github.com/pytorch/pytorch/issues/54381.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54392\n\nReviewed By: ngimel\n\nDifferential Revision: D27221925\n\nPulled By: mruberry\n\nfbshipit-source-id: 5b6e6f21428fd7d97cc75300e3a1aca2a40fbb24", "pr_number": "54392", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "ab8e9188dc": {"title": "add --gpu-max-threads-per-block=256 to hipMAGMA build (#54161)", "body": "Summary:\nAs of ROCm version 4.0.1, the HIP compiler default for max threads per block is 256 but is subject to change in future releases.  To protect against changes, hipMAGMA should be built with the previously-assumed default.  This change is necessary here in PyTorch until upstream magma project utilizes `__launch_bounds__` or some other means of controlling launch bounds.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54161\n\nReviewed By: zou3519\n\nDifferential Revision: D27194829\n\nPulled By: malfet\n\nfbshipit-source-id: 8be2cff3b38786526954b627ff6ab02b510040a1", "pr_number": "54161", "files_changed": [".circleci/docker/common/install_rocm.sh"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "7b939d934e": {"title": "Simplifes OpInfo test matrix to reduce test time (#53255)", "body": "Summary:\nThis PR:\n\n- Updates the structure of the SampleInput class to require the \"input\" attribute be a tensor\n- Limits unary ufuncs to test only the uint8, long, float16, bfloat16, float and cfloat dtypes by default\n- Limits variant testing to the float dtype\n- Removes test_variant_consistency from test_unary_ufuncs.py since it's now redundant with variant testing in test_ops.py\n- Adds backwards supported testing to clarify failures that were coming from variant testing\n\nThis should decrease test e2e time.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53255\n\nReviewed By: ngimel\n\nDifferential Revision: D27043643\n\nPulled By: mruberry\n\nfbshipit-source-id: 91d6b483ad6e2cd1b9ade939d42082980ae14217", "pr_number": "53255", "files_changed": ["test/test_fx.py", "test/test_ops.py", "test/test_shape_ops.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "4ffafbac40": {"title": "Make test_cpp_extensions_aot handle lack of pytest more gracefully (#53740)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53740\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26956603\n\nPulled By: ezyang\n\nfbshipit-source-id: 09ff60b29c4bd64044f4c9f0b7e17ffed33c30db", "pr_number": "53740", "files_changed": ["test/test_cpp_extensions_aot.py"], "labels": ["Merged", "cla signed"]}, "36ce673f16": {"title": "Disable the fusion group which is not supported by XPU device. (#54239)", "body": "Summary:\nThe XPU device doesn't support the fusion group. Disable it for XPU devices.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54239\n\nReviewed By: zou3519\n\nDifferential Revision: D27188735\n\nPulled By: ezyang\n\nfbshipit-source-id: f28f62148e7aa12e8b08345df7eb0133216ce6a5", "pr_number": "54239", "files_changed": ["torch/csrc/jit/codegen/fuser/executor.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "d226985257": {"title": "Read out layout from options directly, rather than via backend (#54074)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54074\n\nI don't see why this shouldn't work.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53\n\nDifferential Revision: D27086594\n\nPulled By: ezyang\n\nfbshipit-source-id: 1d5f1997017ec48c4140f43e44f0d8a3df28ac7f", "pr_number": "54074", "files_changed": ["tools/codegen/api/python.py"], "labels": ["Merged", "cla signed"]}, "635595f706": {"title": "Change sharding in ci (#54228)", "body": "Summary:\nStep three (landing this should fix https://github.com/pytorch/pytorch/issues/53882)!\n\nModifying CI to compute job times during build so that the exported job times can be used for sharding future test jobs.\nThe builds that are exempted from this:\n- `bazel` (no python tests so no need)\n- `libtorch` (no python stuff so no need)\n- `onnx` (the test shards are not calculated the same way)\n- `asan` (runs into error I don't know how to debug/we can debug later: [logs](https://app.circleci.com/pipelines/github/pytorch/pytorch/288019/workflows/57f95f67-1a1b-44a0-9b02-9652b57f2a5f/jobs/11693962)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54228\n\nTest Plan: CI\n\nReviewed By: samestep\n\nDifferential Revision: D27192978\n\nPulled By: janeyx99\n\nfbshipit-source-id: 3cb20d14f4989e61873043b81dfd6b0f82d17ccd", "pr_number": "54228", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".jenkins/pytorch/build-asan.sh", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/win-test-helpers/build_pytorch.bat", ".jenkins/pytorch/win-test-helpers/test_python_first_shard.bat", ".jenkins/pytorch/win-test-helpers/test_python_jit_legacy.bat", ".jenkins/pytorch/win-test-helpers/test_python_second_shard.bat", "test/run_test.py"], "labels": ["Merged", "cla signed"]}, "2355f61f19": {"title": "Add logging for debugging S223170", "body": "Summary: more context in T86752810. Add info for tensor lengths size to see if it fails on in complete batch\n\nTest Plan: manually created failed run: f258719092\n\nReviewed By: aartibasant\n\nDifferential Revision: D27181049\n\nfbshipit-source-id: 341c020a3430c410f9726d92315efb80d36e9452", "pr_number": null, "files_changed": ["caffe2/operators/lengths_reducer_ops.h"], "labels": []}, "ef472d5b31": {"title": "Back out \"[PT QNNPACK] Temporarily disable input pointer caching\" (#52917)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52917\n\nOriginal commit changeset: f6ceef606994\n\nTest Plan:\nFB:\nThis was an attempt to fix ig crashes but we root caused it to pthreadpool changes. Thus this is not needed anymore.\n\nReviewed By: AshkanAliabadi\n\nDifferential Revision: D26485737\n\nfbshipit-source-id: 5d689231cccd11d911b571f8486a19d646352698", "pr_number": "52917", "files_changed": ["aten/src/ATen/native/quantized/cpu/qnnpack/src/conv-run.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "b6bbb41fd8": {"title": "Temporary disable TestNumbaIntegration.test_from_cuda_array_interface* (#54430)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54430\n\nsee https://github.com/pytorch/pytorch/issues/54429\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27232636\n\nPulled By: pbelevich\n\nfbshipit-source-id: 15fb69828a23cb6f3c173a7863bd55bf4973f408", "pr_number": "54430", "files_changed": ["test/test_numba_integration.py"], "labels": ["Merged", "cla signed"]}, "afb560065c": {"title": "[testing] OpInfo for sgn and sign (#53885)", "body": "Summary:\nReference https://github.com/pytorch/pytorch/issues/42515\n\nTODO:\n* [x] Check rendered docs. https://11525594-65600975-gh.circle-artifacts.com/0/docs/generated/torch.sgn.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53885\n\nReviewed By: ejguan\n\nDifferential Revision: D27114318\n\nPulled By: mruberry\n\nfbshipit-source-id: 678179d87741aacd3b50f03dc460207c5aa29589", "pr_number": "53885", "files_changed": ["test/test_torch.py", "test/test_unary_ufuncs.py", "torch/_torch_docs.py", "torch/testing/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "19f77700ec": {"title": "clean up typos in submodule (#54372)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54372\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27233797\n\nPulled By: walterddr\n\nfbshipit-source-id: f8d321199b6ae8b482e2ac3f10575402551365ef", "pr_number": "54372", "files_changed": [".gitmodules"], "labels": ["Merged", "cla signed"]}, "a46d56f988": {"title": "Update tensorpipe submodule. (#54412)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54412\n\nReviewed By: lw\n\nDifferential Revision: D27230317\n\nPulled By: beauby\n\nfbshipit-source-id: 9e8380584cdd0f5750047005416202a23abe738c", "pr_number": "54412", "files_changed": ["third_party/tensorpipe"], "labels": ["Merged", "cla signed"]}, "6e7a3c1fdd": {"title": "Clang-format distributed.py (#54402)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54402\n\nghstack-source-id: 124497872\n\nTest Plan: N/A\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27225942\n\nfbshipit-source-id: 277f466554fbc034fb76de161bf4b3b7c431daf7", "pr_number": "54402", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8bb07c7e21": {"title": "[CI]Install older cmath during Windows build (#54431)", "body": "Summary:\nBased on peterjc123 analysis, `cmath` after https://github.com/microsoft/STL/commit/26bbe2ad50cd7003b8220cfec2bff16dbc032ca8#diff-3fa97ceb95d524432661f01d4b34509c6d261a2f7f45ddcf26f79f55b3eec88a renders a lot of CUDA fail to compile with:\n```\nerror: calling a __host__ function(\"__copysignf\") from a __host__ __device__ function(\"c10::guts::detail::apply_impl< ::at::native::AUnaryFunctor< ::>  &,     ::std::tuple<float >  &, (unsigned long long)0ull > \") is not allowed\n```\nWorkaround for https://github.com/pytorch/pytorch/issues/54382\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54431\n\nReviewed By: anjali411\n\nDifferential Revision: D27234299\n\nPulled By: malfet\n\nfbshipit-source-id: b3f1fef941341222cc10cb27346fcf4a1d522a0c", "pr_number": "54431", "files_changed": [".circleci/config.yml", ".circleci/scripts/vs_install_cmath.ps1", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed"]}, "263cd5cf98": {"title": "Disable all cu92 in scheduled-ci (#54421)", "body": "Summary:\nsince we no longer support cuda9.2 disabling scheduled ci for those\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54421\n\nReviewed By: janeyx99\n\nDifferential Revision: D27234293\n\nPulled By: walterddr\n\nfbshipit-source-id: 923e32c0229ea861bce6ff473501892bd4e5bec1", "pr_number": "54421", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml", ".jenkins/pytorch/test.sh"], "labels": ["Merged", "cla signed"]}, "1e09880b45": {"title": "Add support for list insertion for mutation removal (#54271)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54271\n\nTest Plan:\npython test/test_jit.py TestRemoveMutation.test_lists_insert\n\nImported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D27180031\n\nfbshipit-source-id: ba4388b6688cf83caf70901934f4adacd22d8ca6", "pr_number": "54271", "files_changed": ["test/jit/test_remove_mutation.py", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/passes/remove_mutation.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "0f628d1503": {"title": "[ROCm][doc] add ROCm section for building from source (#53845)", "body": "Summary:\nInstructions for compiling PyTorch from source for ROCm were missing now that PyTorch 1.8 announced beta support for ROCm.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53845\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27237916\n\nPulled By: malfet\n\nfbshipit-source-id: c8be92fd76ea8df7e9f6944c0036568189f58808", "pr_number": "53845", "files_changed": ["README.md"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "f1e72a7e18": {"title": "add uncommit change detector (#54373)", "body": "Summary:\nwarn if uncommit changes exists in .circleci/config.yml, unlike other generated code, .circleci/config.yml actually commits to the repo. (this is a follow up of https://github.com/pytorch/pytorch/issues/54345)\n\ntwo options I am open to\n1. abort regenerate if detected\n2. print out backed up temp filename\n\nAlso remove the `-x` since it is currently very verbose\n```\n++ dirname .circleci/regenerate.sh\n+ cd .circleci\n++ mktemp\n+ OLD_FILE=/var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.54GhUh7w\n+ cp config.yml /var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.54GhUh7w\n++ mktemp\n+ NEW_FILE=/var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.aV87RTvQ\n+ ./generate_config_yml.py\n+ cp /var/folders/vw/ryb6j4d97xs1t_14024b710h0000gn/T/tmp.aV87RTvQ config.yml\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54373\n\nTest Plan:\n1.\n```\n$ echo \"418 I'm a teapot\" > .circleci/config.yml\n$ .circleci/regenerate.sh\n$ .circleci/regenerate.sh\n```\nResult:\n```\n$ .circleci/regenerate.sh\nUncommitted change detected in .circleci/config.yml\nIt has been backed up to /var/folders/89/brnr1wt970130lk0m52605mw0000gn/T/tmp.2VOp4BPo\nNew config generated in .circleci/config.yml\n$ .circleci/regenerate.sh  #-- second time there's no uncommitted changes\nNew config generated in .circleci/config.yml\n```\n\n2.\n```\n$ echo \"418 I'm a teapot\" > .circleci/config.yml\n$ git add .circleci/config.yml\n$ .circleci/regenerate.sh\n$ .circleci/regenerate.sh\n```\nResult:\n```\n$ .circleci/regenerate.sh\nUncommitted change detected in .circleci/config.yml\nIt has been backed up to /var/folders/89/brnr1wt970130lk0m52605mw0000gn/T/tmp.2VOp4BPo\nNew config generated in .circleci/config.yml\n$ .circleci/regenerate.sh #-- second time there's still uncommitted changes b/c git split staged vs unstaged changes\nUncommitted change detected in .circleci/config.yml\nIt has been backed up to /var/folders/89/brnr1wt970130lk0m52605mw0000gn/T/tmp.2ruMAynI\nNew config generated in .circleci/config.yml\n```\n\nReviewed By: samestep\n\nDifferential Revision: D27234394\n\nPulled By: walterddr\n\nfbshipit-source-id: 6364cc1f6f71a43424a63ca6fce9d2ba69437741", "pr_number": "54373", "files_changed": [".circleci/regenerate.sh"], "labels": ["Merged", "cla signed"]}, "9fef25e579": {"title": "[Pytorch Mobile] optimize_for_mobile: Remove dropout from any function (#53846)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53846\n\nTheres already a varient of removeDropout that takes in a graph. So just switch to calling that one. It doesnt error check that the module isnt in training mode (because it doenst have a module) but optimize_for_mobile guarantees the cloned_module is in eval mode.\nghstack-source-id: 124544216\n\nTest Plan: called optimize on forward and foo, both contained dropouts, both dropouts removed. Called both functions afterwords to verify they ran and gave the same output. {P308987364}\n\nReviewed By: kimishpatel\n\nDifferential Revision: D26986251\n\nfbshipit-source-id: 085e08cbaa982aa08803a718fee4380af5f86b78", "pr_number": "53846", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/xnnpack_rewrite.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4919fecf23": {"title": "Delete dead TensorOptions::key_set (#54004)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54004\n\nAccording to\n`glean-search find-decls --refs 'c10::TensorOptions::key_set'`\nthere are no uses of this function\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27047971\n\nPulled By: ezyang\n\nfbshipit-source-id: 63662dd7ab27753ecb79c45c152c2cad1160dab2", "pr_number": "54004", "files_changed": ["c10/core/TensorOptions.h"], "labels": ["Merged", "cla signed"]}, "2130f4ccc4": {"title": "Use c10::ArrayRef instead of std::vector for the jit::unpickle's tensor_table. (#54428)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54428\n\nUsing c10::ArrayRef as the parameter type makes the API more flexible and allows the caller to leverage small-buffer optimizations (e.g. c10::SmallVector, std::array) for performance critical cases.\n\nTest Plan: No behavioral changes. Run the existing unit and integration tests.\n\nReviewed By: suo\n\nDifferential Revision: D27232222\n\nfbshipit-source-id: 7b13bc6bd02257097ca119077028fbccc68cc925", "pr_number": "54428", "files_changed": ["torch/csrc/distributed/autograd/rpc_messages/cleanup_autograd_context_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/rref_backward_req.cpp", "torch/csrc/distributed/rpc/python_remote_call.cpp", "torch/csrc/distributed/rpc/rref_proto.cpp", "torch/csrc/distributed/rpc/script_call.cpp", "torch/csrc/distributed/rpc/script_remote_call.cpp", "torch/csrc/distributed/rpc/script_resp.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/jit/serialization/import_legacy.cpp", "torch/csrc/jit/serialization/pickle.cpp", "torch/csrc/jit/serialization/pickle.h", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/jit/serialization/unpickler.h"], "labels": ["Merged", "cla signed", "fb-exported", "module: bc-breaking", "oncall: distributed", "oncall: jit"]}, "5a27199149": {"title": "Add device_of overload for optional<Tensor> (#54262)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54262\n\nregister_dispatch_key.py might generate device_of call over\noptional<Tensor> if it happened to be the first Tensor-like\nargument.\n\nghstack-source-id: 124535550\n\nTest Plan: Test together with next diff in stack\n\nReviewed By: ezyang\n\nDifferential Revision: D27164093\n\nfbshipit-source-id: 3b0400d5d603338e884218498106f6481e53f194", "pr_number": "54262", "files_changed": ["aten/src/ATen/DeviceGuard.h"], "labels": ["Merged", "cla signed"]}, "edfc787df4": {"title": "Migrate kernels with Tensor? to C10 full dispatcher (#54263)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54263\n\nCodemod commands generated by https://github.com/pytorch/pytorch/pull/54223\n\nSignatures of the following 8 methods in LegacyTHFunctionsCUDA.h are\nmanually changed.\n\n```\n_thnn_multi_margin_loss_forward\n_thnn_multi_margin_loss_backward\n_thnn_nll_loss_forward\n_thnn_nll_loss_backward\n_thnn_nll_loss2d_forward\n_thnn_nll_loss2d_backward\n_thnn_conv2d_forward\n_thnn_conv_depthwise2d_forward\n```\n\nghstack-source-id: 124539990\n\nTest Plan: buck build //caffe2/aten/...\n\nReviewed By: smessmer\n\nDifferential Revision: D27164092\n\nfbshipit-source-id: 59062179ffd958ca253cbf63fdd495799b9a9586", "pr_number": "54263", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/LegacyNNDefinitions.cpp", "aten/src/ATen/native/Linear.cpp", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/native/LossNLL.cpp", "aten/src/ATen/native/LossNLL2d.cpp", "aten/src/ATen/native/NNPACK.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/SummaryOps.cpp", "aten/src/ATen/native/VariableMethodStubs.cpp", "aten/src/ATen/native/cuda/DepthwiseConv3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/Loss.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/layer_norm_kernel.cu", "aten/src/ATen/native/cudnn/BatchNorm.cpp", "aten/src/ATen/native/cudnn/ConvPlaceholders.cpp", "aten/src/ATen/native/cudnn/RNN.cpp", "aten/src/ATen/native/group_norm.cpp", "aten/src/ATen/native/layer_norm.cpp", "aten/src/ATen/native/miopen/BatchNorm_miopen.cpp", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/miopen/RNN_miopen.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/mkldnn/Linear.cpp", "aten/src/ATen/native/mkldnn/Normalization.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp"], "labels": ["Merged", "cla signed"]}, "92770d25cd": {"title": "fix comparison of narrow type with wide type in loop condition (#53951)", "body": "Summary:\nfix Semmle warning: Comparison of narrow type with wide type in loop condition\n\nFor example there is below piece of code:\nfor (int i=0; i<array.size(); ++i) {}\n\nThe problem is that array.size() return type is size_t can be larger type than int depending on the implementation so there is chance that i overflows (for very large array that array size is beyond the range of integer) and this loop will never be terminated.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53951\n\nReviewed By: zou3519\n\nDifferential Revision: D27181495\n\nPulled By: malfet\n\nfbshipit-source-id: 0612c5cedcdc656c193085e7fbb87dd163f20688", "pr_number": "53951", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/Utils.h", "aten/src/ATen/core/type.cpp", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/ConstantPadNd.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/Embedding.cpp", "aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/ForeachUtils.h", "aten/src/ATen/native/FractionalMaxPool3d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/PackedSequence.cpp", "aten/src/ATen/native/QuantizedLinear.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorIteratorReduce.cpp", "aten/src/ATen/native/TestOps.cpp", "aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest2d.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_nearest3d.cpp", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "aten/src/ATen/test/NamedTensor_test.cpp", "c10/benchmark/intrusive_ptr_benchmark.cpp", "c10/mobile/CPUProfilingAllocator.cpp", "c10/test/core/impl/SizesAndStrides_test.cpp", "c10/util/irange.h", "caffe2/core/blob_serialization.cc", "caffe2/core/operator_schema.cc", "caffe2/core/prof_dag_counters.cc", "caffe2/operators/generate_proposals_op_test.cc", "caffe2/operators/generate_proposals_op_util_boxes.h", "caffe2/operators/generate_proposals_op_util_nms.h", "caffe2/operators/generate_proposals_op_util_nms_test.cc", "caffe2/operators/half_float_ops_test.cc", "caffe2/operators/string_ops_test.cc", "caffe2/operators/text_file_reader_utils.cc", "caffe2/operators/text_file_reader_utils_test.cc", "caffe2/perfkernels/embedding_lookup.cc", "caffe2/perfkernels/embedding_lookup_idx.cc", "caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup.cc", "caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup_idx.cc", "caffe2/perfkernels/math_cpu_avx2.cc", "caffe2/perfkernels/math_cpu_base.cc", "caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc", "caffe2/quantization/server/norm_minimization.cc", "caffe2/queue/blobs_queue.cc", "caffe2/transforms/common_subexpression_elimination.cc", "caffe2/transforms/pattern_net_transform.cc", "caffe2/utils/eigen_utils.h", "caffe2/utils/math_test.cc", "modules/observers/net_observer_reporter_print.cc"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2668149b8c": {"title": "Export torch::jit::toIValue (#54449)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54448\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54449\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27243154\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: fc21d6ce251b868356ad8ea13ae891fb56e311ce", "pr_number": "54449", "files_changed": ["torch/csrc/jit/python/pybind_utils.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "decbdf7b0b": {"title": "Get rid of {Cpu,Cuda}{Channel,Context} in tensorpipe_agent. (#54432)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54432\n\nFollowing the merge of channel hierarchies, here comes the promised\nclean up.\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27232442\n\nfbshipit-source-id: 540dc6bc18a9a415b676e06e75530d729daf2d5b", "pr_number": "54432", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "7e33dc3498": {"title": "[PyTorch] Avoid extra intrusive_ptr copy in IValue::toIntrusivePtr (#54124)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54124\n\nNo need to have an extra temporary intrusive_ptr (`p`) just to do an `incref`.\nghstack-source-id: 124150644\n\nTest Plan:\nexisting tests for correctness; inspect assembly for\nc10::IValue::toObject to double-check & see that it's a bit shorter\n\nReviewed By: smessmer\n\nDifferential Revision: D27109183\n\nfbshipit-source-id: 497706190867eeac0fb1d309d0ecc97cf8d65b08", "pr_number": "54124", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "3959d393b8": {"title": "[PyTorch][JIT] Less shared_ptr use in dictConstruct (#54110)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54110\n\ndictConstruct doesn't need to make its caller have a `shared_ptr<DictType>`. It also doesn't need to do extra `shared_ptr` copies into the `key_type` and `value_type` locals.\nghstack-source-id: 124150642\n\nTest Plan: fitsships\n\nReviewed By: ezyang\n\nDifferential Revision: D27101782\n\nfbshipit-source-id: 3c632ad9d8f1bd7bdf37f517a86aca27bd41548a", "pr_number": "54110", "files_changed": ["torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/vararg_functions.cpp", "torch/csrc/jit/runtime/vararg_functions.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "18c04a3f0f": {"title": "Avoid dispatch overhead in call to mkldnn convolution (#52614)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52614\n\nThis can speed up models by 5% (~.5-1% from the base, but ~5% after they've been sped up with mkldnn).\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D26696693\n\nPulled By: eellison\n\nfbshipit-source-id: bfed55242524a4c2f1ae5d63e76d6803016d986d", "pr_number": "52614", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/normalize_ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "81c6e5fb38": {"title": "use reshape when possible in broadcasting (#53326)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53326\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D26897275\n\nPulled By: eellison\n\nfbshipit-source-id: 44278633a1e6429db43443ca689b97d5a077a15c", "pr_number": "53326", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c411017a41": {"title": "Only allow hub.load() from original repo. (#54451)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54451\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27243825\n\nPulled By: ailzhang\n\nfbshipit-source-id: 2f65a82064d83b71224b4280ddfaabfa8ec9aec3", "pr_number": "54451", "files_changed": ["test/test_utils.py", "torch/hub.py"], "labels": ["Merged", "cla signed"]}, "52abd3bd7b": {"title": "[Static Runtime] Fix bug in reshape_copy (#54467)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54467\n\n`at::native::copy_` requires src/dest to have the same sizes, which isn't true in reshape.\n\nTest Plan: Added new test cases to cover this case.\n\nReviewed By: ajyu\n\nDifferential Revision: D27249617\n\nfbshipit-source-id: 2c95175fa8564b3c648979445ad4314f97818852", "pr_number": "54467", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "5870346173": {"title": "Port index_copy from TH to ATen (#52203)", "body": "Summary:\nThe design of the `TensorIterator` was similar to that in https://github.com/pytorch/pytorch/pull/50578\n\nResolves https://github.com/pytorch/pytorch/issues/24670\nResolves https://github.com/pytorch/pytorch/issues/24523\n\nTimings:\n<details>\n<summary>Script</summary>\n\n```python\nfrom IPython import get_ipython\nimport torch\n\ntorch.manual_seed(13)\ntorch.set_num_threads(1)\n\nipython = get_ipython()\n\ncpu = torch.device('cpu')\ncuda = torch.device('cuda')\n\ndef run_test(ndims, size, index_len, device):\n    print(f\"ndims: {ndims}, tensor_size: {size}, index_len: {index_len}, device: {device}\")\n\n    x = torch.rand(*([size] * ndims), device=device)\n    index = torch.randint(size, (index_len,), dtype=torch.long, device=device)\n    for d in range(ndims):\n        shape_t = [size] * d + [index_len] + [size] * (ndims - d - 1)\n        t = torch.rand(*shape_t, device=device)\n        command = \"x.index_copy(d, index, t)\"\n        if device == cuda:\n            command = command + \"; torch.cuda.synchronize()\"\n        ipython.magic(f\"timeit {command}\")\n    print()\n\nrun_test(3, 700, 10, cpu)\nrun_test(3, 700, 100, cpu)\nrun_test(3, 700, 700, cpu)\nrun_test(2, 10000, 10000, cpu)\n\nrun_test(3, 700, 10, cuda)\nrun_test(3, 700, 100, cuda)\nrun_test(3, 700, 700, cuda)\nrun_test(2, 10000, 10000, cuda)\n```\n\n</details>\n\n<details>\n<summary>CPU ATen</summary>\n\n```\nndims: 3, tensor_size: 700, index_len: 10, device: cpu\n327 ms \u00b1 309 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n329 ms \u00b1 456 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n378 ms \u00b1 1.44 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nndims: 3, tensor_size: 700, index_len: 100, device: cpu\n348 ms \u00b1 1.52 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n359 ms \u00b1 330 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n526 ms \u00b1 686 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nndims: 3, tensor_size: 700, index_len: 700, device: cpu\n560 ms \u00b1 19 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n552 ms \u00b1 2.61 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n932 ms \u00b1 2.52 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nndims: 2, tensor_size: 10000, index_len: 10000, device: cpu\n163 ms \u00b1 5.05 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n302 ms \u00b1 5.75 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n</details>\n\n<details>\n<summary>CUDA ATen</summary>\n\n```\nndims: 3, tensor_size: 700, index_len: 10, device: cuda\n9.63 ms \u00b1 441 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.65 ms \u00b1 230 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n12.4 ms \u00b1 881 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 700, index_len: 100, device: cuda\n10.8 ms \u00b1 1.51 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n11 ms \u00b1 417 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n21.2 ms \u00b1 18.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nndims: 3, tensor_size: 700, index_len: 700, device: cuda\n19 ms \u00b1 4.42 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n17.8 ms \u00b1 493 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n25.8 ms \u00b1 1.22 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nndims: 2, tensor_size: 10000, index_len: 10000, device: cuda\n5.59 ms \u00b1 109 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10 ms \u00b1 25.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\n</details>\n\n<details>\n<summary>CPU TH</summary>\n\n```\nndims: 3, tensor_size: 700, index_len: 10, device: cpu\n333 ms \u00b1 2.42 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n327 ms \u00b1 1.04 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n366 ms \u00b1 753 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nndims: 3, tensor_size: 700, index_len: 100, device: cpu\n336 ms \u00b1 1.24 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n345 ms \u00b1 914 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n884 ms \u00b1 4.32 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nndims: 3, tensor_size: 700, index_len: 700, device: cpu\n441 ms \u00b1 3.58 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n514 ms \u00b1 1.17 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n7.46 s \u00b1 6.46 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\nndims: 2, tensor_size: 10000, index_len: 10000, device: cpu\n141 ms \u00b1 233 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n1.13 s \u00b1 855 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\n</details>\n\n<details>\n<summary>CUDA TH</summary>\n\n```\nndims: 3, tensor_size: 700, index_len: 10, device: cuda\n9.64 ms \u00b1 390 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.68 ms \u00b1 3.26 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n13.9 ms \u00b1 928 ns per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nndims: 3, tensor_size: 700, index_len: 100, device: cuda\n11.6 ms \u00b1 1.38 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n12.1 ms \u00b1 3.72 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n30.3 ms \u00b1 27.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nndims: 3, tensor_size: 700, index_len: 700, device: cuda\n27.2 ms \u00b1 19.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n30.6 ms \u00b1 43.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n146 ms \u00b1 204 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\nndims: 2, tensor_size: 10000, index_len: 10000, device: cuda\n6.5 ms \u00b1 3.99 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n64.7 ms \u00b1 55.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n```\n\n</details>\n\nAccording to these we see a slight performance improvement across both CPU and GPU.\n\ncc: nikitaved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52203\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27066572\n\nPulled By: mruberry\n\nfbshipit-source-id: 6101e461cf731afa3db042a383b723d3d6bfdc26", "pr_number": "52203", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/TH/generic/THTensorMath.h", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.h", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_python_functions.py", "tools/autograd/gen_variable_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "77ccd4f9a3": {"title": "[5/n][torch/elastic][upstream] Move torchelastic/agent to torch/distributed/elastic/agent (#54343)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54343\n\nMove torchelastic/agent to torch/distributed/elastic/agent\n\nTest Plan:\nbuck test mode/dev-nosan //pytorch/elastic/torchelastic/...\n      buck test mode/dev-nosan //caffe2/test/distributed/elastic/agent/server/test/...\n\nReviewed By: kiukchung, wilson100hong\n\nDifferential Revision: D27173271\n\nfbshipit-source-id: 26761acc3f962af2afffcc3c7a237f3b6d65e531", "pr_number": "54343", "files_changed": ["test/distributed/elastic/agent/server/test/__init__.py", "test/distributed/elastic/agent/server/test/api_test.py", "test/distributed/elastic/agent/server/test/local_elastic_agent_test.py", "torch/distributed/elastic/agent/server/__init__.py", "torch/distributed/elastic/agent/server/api.py", "torch/distributed/elastic/agent/server/local_elastic_agent.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "349a17f1c0": {"title": "Replace some tensor.device().is_cpu() calls with direct tensor.is_cpu() (#54397)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54397\n\nI was supposed to have done this in https://github.com/pytorch/pytorch/pull/54079\nbut apparently I forgot to push these changes before landing, so here's\nthe clean up.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27235382\n\nPulled By: ezyang\n\nfbshipit-source-id: ffcce5abc78251c81c230992bac70b8973906ace", "pr_number": "54397", "files_changed": ["aten/src/ATen/TensorIterator.cpp"], "labels": ["Merged", "cla signed"]}, "c00d66f73c": {"title": "Move compute_native_function_declaration to its own dest module (#54419)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54419\n\nI'm planning to break it into some helper functions, so let's put it in its own module first.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27235378\n\nPulled By: ezyang\n\nfbshipit-source-id: c03c5440d2d753859e2c5ec2b2c8b1b82870f03a", "pr_number": "54419", "files_changed": ["tools/codegen/dest/__init__.py", "tools/codegen/dest/native_functions.py", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "6d0027197c": {"title": "Delete all unnecessary singular Math entries (#54436)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54436\n\nAn operator entry with no dispatch table implicitly generates a Math\nentry, so you don't need to define one yourself.  I also added\nsome asserts in the codegen to fail on these cases.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27235381\n\nPulled By: ezyang\n\nfbshipit-source-id: f8c905090b863120f4f3656c37e2b7f26e8bb9ef", "pr_number": "54436", "files_changed": ["aten/src/ATen/native/README.md", "aten/src/ATen/native/native_functions.yaml", "tools/codegen/model.py"], "labels": ["Merged", "cla signed"]}, "c22fc448cd": {"title": "[Gradient Compression] Remove cuda.syncrhonize in batched powerSGD (#54482)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54482\n\n`cuda.synchronize` is unnecessary for `batched_powerSGD_hook`.\nghstack-source-id: 124607761\n\nTest Plan:\nf259607860\nf259563921\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27254314\n\nfbshipit-source-id: 4744c07a6f0c8939e766ffa935ddbf3c47e85d18", "pr_number": "54482", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8518b0ee55": {"title": "[PyTorch] Update Bazel build for TensorPipe (#54416)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54416\n\nOnce D27230990 lands, we'll need this for TensorPipe to be built with Bazel.\nghstack-source-id: 124512701\n\nTest Plan: None for now.\n\nReviewed By: beauby\n\nDifferential Revision: D27231000\n\nfbshipit-source-id: 474cc1b23118703ecb47ed4b8e0c5b000572eae8", "pr_number": "54416", "files_changed": ["third_party/tensorpipe.BUILD"], "labels": ["Merged", "cla signed"]}, "1041fdd069": {"title": "Grammatically update tech docs (#54370)", "body": "Summary:\nSmall grammatical update to nn.rst\n\n![Screenshot 2021-03-20 at 11 44 29](https://user-images.githubusercontent.com/80534697/111867047-d868f900-8971-11eb-8cc2-0ae7d2c59229.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54370\n\nReviewed By: radkris-git\n\nDifferential Revision: D27243944\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 08d8061d9e74ffaf95c8a610107a8632259474ca", "pr_number": "54370", "files_changed": ["docs/source/nn.rst"], "labels": ["Merged", "cla signed", "open source"]}, "f2a38a0edd": {"title": "Enabled BFloat16 support for argmax & argmin on both CPU & CUDA (#52582)", "body": "Summary:\n1. Enabled `BFloat16` support for `argmax` & `argmin` on both CPU & CUDA\n2. Added `OpInfo`s for `argmax` & `argmin`\n3. Enabled `test_argminmax_multiple` for `float16`. It can't be enabled for `bfloat16`, as comparison is done with numpy, which doesn't currently support `bfloat16`.\n4. Enabled `test_dim_arg_reduction_scalar` for `float16` & `bfloat16`.\n5. Enabled `test_reduction_vectorize_along_output` for `bfloat16`.\n6. Enabled `test_reduction_vectorize_along_input_corner` for `bfloat16`.\n7. Enabled `test_dim_reduction` for both `float16` and `bfloat16`, except that both of them don't support `prod` on CPU.\n8. Unskipped `TestCommonCPU.test_variant_consistency_jit` for dtype `bfloat16` for `amax` & `amin`, as they're passing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52582\n\nReviewed By: anjali411\n\nDifferential Revision: D27204704\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: cdad5df494d070f8e1a8fb83939441a91124b4d9", "pr_number": "52582", "files_changed": ["aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "test/test_reductions.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "fee470d8ef": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27257117\n\nfbshipit-source-id: 6fdda695987892a74137d3afa720979c8d5c68bb", "pr_number": null, "files_changed": ["torch/csrc/jit/passes/normalize_ops.cpp"], "labels": []}, "ba9f12d235": {"title": "Fix minor whitespace typo in tools/test_history.py (#54504)", "body": "Summary:\noops\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54504\n\nTest Plan:\n```\ntools/test_history.py --help\n```\n\nReviewed By: walterddr\n\nDifferential Revision: D27262271\n\nPulled By: samestep\n\nfbshipit-source-id: 0f47e9a69d35a605c558f6c86e3e2ca98720ff86", "pr_number": "54504", "files_changed": ["tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "f3c00047ce": {"title": "Reset Optimizer counter while deserializing netWithBackwardOptions", "body": "Summary: Add ability to reset optimizer counter..\n\nTest Plan: will wait for integration tests to run on diff.\n\nDifferential Revision: D27248286\n\nfbshipit-source-id: a608df1bd61b64eb317c9ffd9cfdd804c5288f6d", "pr_number": null, "files_changed": ["caffe2/python/optimizer.py"], "labels": []}, "acffa604cc": {"title": "disable cu112 test on windows (#54512)", "body": "Summary:\nCurrently cu112 test on windows is broken. see\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/288940/workflows/c6612fe8-8396-4266-88d8-2ad2736c994c/jobs/11744008/steps\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54512\n\nReviewed By: janeyx99\n\nDifferential Revision: D27265585\n\nPulled By: walterddr\n\nfbshipit-source-id: 49e212d6c332a9725e6f2a78faf41198d4a21ac5", "pr_number": "54512", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "ci/all", "cla signed"]}, "583c4bf7d3": {"title": "[Pytorch Mobile] optimize_for_mobile: Fuse Add Relu on any function (#54441)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54441\n\nSimilar to previous dropout one\nghstack-source-id: 124544176\n\nTest Plan: Printed graphs before and after fusion. verified input outputs stayed the same {P299343882}\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27014352\n\nfbshipit-source-id: d0a9548f8743472bdd7e194efd8e8d5fe53b95b6", "pr_number": "54441", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/xnnpack_rewrite.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "21a9a93eb4": {"title": "gdb special command to print tensors (#54339)", "body": "Summary:\nThis is something which I wrote because it was useful during my debugging sessions, but I think it might be generally useful to other people as well so I took the liberty of proposing an official `pytorch-gdb` extension.\n\n`pytorch-gdb` is a gdb script written in python. Currently, it contains only one command: `torch-tensor-repr`, which prints a human-readable repr of an `at::Tensor` object. Example:\n```\nBreakpoint 1, at::native::neg (self=...) at [...]/pytorch/aten/src/ATen/native/UnaryOps.cpp:520\n520     Tensor neg(const Tensor& self) { return unary_op_impl(self, at::neg_out); }\n(gdb) # the default repr of 'self' is not very useful\n(gdb) p self\n$1 = (const at::Tensor &) 0x7ffff72ed780: {impl_ = {target_ = 0x5555559df6e0}}\n(gdb) torch-tensor-repr self\nPython-level repr of self:\ntensor([1., 2., 3., 4.], dtype=torch.float64)\n```\n\nThe idea is that by having an official place where to put these things, `pytorch-gdb` will slowly grow other useful features and make the pytorch debugging experience nicer and faster.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54339\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27253674\n\nPulled By: ezyang\n\nfbshipit-source-id: dba219e126cc2fe66b2d26740f3a8e3b886e56f5", "pr_number": "54339", "files_changed": [".gdbinit", "CONTRIBUTING.md", "tools/gdb/pytorch-gdb.py", "torch/csrc/utils.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "e5b97777e3": {"title": "[ROCm] allow PYTORCH_ROCM_ARCH in cpp_extension.py (#54341)", "body": "Summary:\nAllows extensions to override ROCm gfx arch targets.  Reuses the same env var used during cmake build for consistency.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54341\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27244010\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 279e1a41ee395a0596aa7f696b6e908cf7f5bb83", "pr_number": "54341", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["Merged", "module: rocm", "open source"]}, "1b792a7f15": {"title": "Fix Flake8 (#54540)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/54339 broke Flake8. This PR fixes it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54540\n\nTest Plan:\n```\nflake8\n```\n\nReviewed By: walterddr\n\nDifferential Revision: D27274171\n\nPulled By: samestep\n\nfbshipit-source-id: 4b440d72b4b5615f45e6fcb25f7a4c0423add272", "pr_number": "54540", "files_changed": ["tools/gdb/pytorch-gdb.py"], "labels": ["Merged", "cla signed"]}, "35186eb983": {"title": "Update TensorPipe submodule (#54507)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54507\n\nTest Plan: CircleCI\n\nReviewed By: mrshenli\n\nDifferential Revision: D27262943\n\nfbshipit-source-id: cffecd01756180325147d4fb85fbe9bc78727884", "pr_number": "54507", "files_changed": ["third_party/tensorpipe"], "labels": ["Merged", "cla signed", "fb-exported"]}, "c06d979731": {"title": "[Static Runtime] Name refactoring to make MemoryPlanning more readable (#54045)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54045\n\nTest Plan: Imported from OSS\n\nReviewed By: bwasti\n\nDifferential Revision: D27233880\n\nfbshipit-source-id: 43b38901d8cfea0941a1a2934997a08027b57b6d", "pr_number": "54045", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "792f5ffb83": {"title": "Also strip slow_test (#54528)", "body": "Summary:\nSince `_test1`, `_test2` and `_build` and `test` are all stripped, `slow_test` should be stripped as well. This way, the _slow_test stats will be considered as a part of all stats relating to a particular build job, though currently, it doesn't do much because the jobs don't share a common stemmed name--the build has `_gcc7` while the slow_test CI job does not.\n\nThis makes me think...do we omit the `gcc7` intentionally? Are there other things I should strip, e.g., `multigpu_test`?\n\nSee:\nci/circleci: pytorch_linux_xenial_cuda10_2_cudnn7_py3_slow_test\nci/circleci: pytorch_linux_xenial_cuda10_2_cudnn7_py3_gcc7_test1\nci/circleci: pytorch_linux_xenial_cuda10_2_cudnn7_py3_gcc7_test2\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54528\n\nReviewed By: samestep\n\nDifferential Revision: D27270393\n\nPulled By: janeyx99\n\nfbshipit-source-id: ffb7289cfe4dba52ded67f50a89f3e75e7bad68d", "pr_number": "54528", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed"]}, "7e3cf1ee24": {"title": "[pytorch] Add native support for segment reduce step1: API definition (#53727)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53727\n\nThis is first diff to add native support for segment reduction in PyTorch. It provides similar functionality like torch.scatter or \"numpy.ufunc.reduceat\".\n\nThis diff mainly focuses on API layer to make sure future improvements will not cause backward compatibility issues. Once API is settled, here are next steps I am planning:\n- Add support for other major reduction types (e.g. min, sum) for 1D tensor\n- Add Cuda support\n- Backward support\n- Documentation for the op\n- Perf optimizations and benchmark util\n- Support for multi dimensional tensors (on data and lengths) (not high priority)\n- Support for 'indices' (not high priority)\n\nTest Plan: Added unit test\n\nReviewed By: ngimel\n\nDifferential Revision: D26952075\n\nfbshipit-source-id: 8040ec96def3013e7240cf675d499ee424437560", "pr_number": "53727", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/SegmentReduce.cpp", "aten/src/ATen/native/SegmentReduce.h", "aten/src/ATen/native/native_functions.yaml", "test/test_segment_reductions.py", "tools/build_variables.bzl", "torch/overrides.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "4a74b0f2dd": {"title": "Fix logic in TestFX.test_get_torch_func_signature_exhaustive (#54510)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54510\n\nTest Plan: Imported from OSS\n\nReviewed By: ansley\n\nDifferential Revision: D27264670\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 0ef6395dacde3eb2a4b9c7eeff760a1be38b6dfe", "pr_number": "54510", "files_changed": ["test/test_fx.py"], "labels": ["Merged", "cla signed"]}, "5754816597": {"title": "fix SC2126 introduced error (#54545)", "body": "Summary:\nSC2126 suggested from diff CI is wrong. reverting last commit in https://github.com/pytorch/pytorch/issues/54373\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54545\n\nReviewed By: samestep\n\nDifferential Revision: D27276006\n\nPulled By: walterddr\n\nfbshipit-source-id: 1a9823e9ad6c6509a36896df88d599546826f4e9", "pr_number": "54545", "files_changed": [".circleci/regenerate.sh"], "labels": ["Merged", "cla signed"]}, "66a3614b47": {"title": "Fix typo in .github/workflows/lint.yml (#54551)", "body": "Summary:\nFixes a minor typo introduced in https://github.com/pytorch/pytorch/issues/51796.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54551\n\nTest Plan: None, since this only changes a comment.\n\nReviewed By: seemethere\n\nDifferential Revision: D27278347\n\nPulled By: samestep\n\nfbshipit-source-id: 34a781cce0cb4e93a68821d6006bbf05b0bbe2f0", "pr_number": "54551", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "91d37d7d2f": {"title": "[CI] Install compatible cmath for Win binary builds (#54527)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54527\n\nReviewed By: walterddr\n\nDifferential Revision: D27269528\n\nPulled By: malfet\n\nfbshipit-source-id: 4afdc706598f3a6ad296468dfb77a70433ae7d0f", "pr_number": "54527", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml"], "labels": ["Merged", "ci/all"]}, "33b95c6bac": {"title": "Add __torch_function__ support for torch.nn.functional.embedding (#54478)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54478\n\nFixes #54292\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27264179\n\nPulled By: ezyang\n\nfbshipit-source-id: cd267e2e668fdd8d7f958bf70a0b93e058ec7c23", "pr_number": "54478", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed"]}, "789dc6d445": {"title": "[NCCL] Add more details for checkForNCCLErrors (#54117)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54117\n\nhttps://github.com/pytorch/pytorch/pull/45950 enhanced our NCCL logging errors so that we add some basic debug information about what when wrong when erroring out with a NCCL error.\n\nHowever, that PR only used the added function for `C10D_NCCL_CHECK` which is used to check the return values of NCCL calls. However, in ProcessGroupNCCL we also have `checkForNCCLErrors` which checks for errors on nccl communicators, and in case of errors it would be good to have this logging there too.\n\nAlso renames the function s/errorMessage/getNcclErrorDetailStr\nghstack-source-id: 124662592\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27100497\n\nfbshipit-source-id: fec3663ffa3e92bae8391ef4f77054abb4bb9715", "pr_number": "54117", "files_changed": ["torch/lib/c10d/NCCLUtils.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5cd8a77e01": {"title": "Skip inplace autograd test if inplace variant doesn't exist (#54460)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54413\n\n1. Skip inplace autograd test for an op if its inplace variant does not exist.\n2. For ops that don't have an inplace variant, remove redundant `supports_inplace_autograd=False` assignments in their `OpInfo`s.\n3. Ops having inplace variants that do not support autograd should not have `supports_inplace_autograd=False` entries removed from their `OpInfo`s.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54460\n\nReviewed By: ngimel\n\nDifferential Revision: D27255938\n\nPulled By: mruberry\n\nfbshipit-source-id: f15334b09e68995e9f26adc2ff3e59c292689ee8", "pr_number": "54460", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "5105250e16": {"title": "[FX] Add docs for shape propagation (#54554)", "body": "Summary:\nFixes #{i54538}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54554\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27281263\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 2fd3914f0e24be0b6a18ad7715f3336dcf7949ba", "pr_number": "54554", "files_changed": ["torch/fx/passes/shape_prop.py"], "labels": ["Merged", "cla signed", "fx", "open source"]}, "afe339d7dd": {"title": "[static runtime] support DictConstruct (#54438)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54438\n\nAugust 1x model has DictConstruct in the graph (P331168321)\nThese can be easily removed with jit pass, but to easily measure the improvement\nand run replayer with the model in the meantime, enable DictConstruct in static runtime\n\nTest Plan:\n```\n./sigrid/predictor/scripts/pytorch/pyper_inference_e2e_local_replayer_test.sh \\\n    cpu 218841466_0 7449 /data/users/ansha/tmp/adfinder/august_1x/ /data/users/ansha/tmp/adfinder/august_1x/filtered_requests_inline_cvr_100\n```\n\n```\nTEST trace\nTotal num requests                                   100\nNum exceptions                                         0\nLatency us avg                                    180965\nLatency us p25                                     89785\nLatency us p50                                    131240\nLatency us p75                                    146621\nLatency us p90                                    158378\nLatency us p95                                    166628\nLatency us p99                                   1886680\nLatency us p100                                  3803252\nServer latency us avg                              91554\nServer latency us p25                              51447\nServer latency us p50                              86371\nServer latency us p75                              95229\nServer latency us p90                             102706\nServer latency us p95                             116023\nServer latency us p99                             557017\nServer latency us p100                            716319\nNum rankUnits avg                                     28\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27236682\n\nfbshipit-source-id: 1da49a836dd7533480e77797338baa9edcb65fb5", "pr_number": "54438", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "a4ca394f8a": {"title": "Revert \"Revert D26907093: Add repeats to Timer.collect_callgrind(...)\" (#54484)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54484\n\nRe-land of https://github.com/pytorch/pytorch/pull/53295. (With fixed unit tests.)\n\nThis reverts commit 0dc5abfaa9cac9266791788839d896b14600d123.\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27255201\n\nPulled By: robieta\n\nfbshipit-source-id: 4e9fed7522631d66c5cd7e27ace9b5ffc3a0bbfc", "pr_number": "54484", "files_changed": ["test/benchmark_utils/test_benchmark_utils.py", "torch/_C/__init__.pyi.in", "torch/csrc/Module.cpp", "torch/utils/benchmark/utils/timer.py", "torch/utils/benchmark/utils/valgrind_wrapper/compat_bindings.cpp", "torch/utils/benchmark/utils/valgrind_wrapper/timer_callgrind_template.cpp", "torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py"], "labels": ["Merged", "cla signed"]}, "0d81528a47": {"title": "Definition infrastructure for instruction count ubenchmarks (#53296)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53296\n\nPart 1 of the instruction count microbenchmarks. This PR is focused on benchmark definition machinery. (Though you can run `main.py` to see it in action.) A summary of the system is given in the README.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D26907092\n\nPulled By: robieta\n\nfbshipit-source-id: 0f61457b3ce89aa59a06bf1f0e7a74ccdbf17090", "pr_number": "53296", "files_changed": ["benchmarks/instruction_counts/README.md", "benchmarks/instruction_counts/core/__init__.py", "benchmarks/instruction_counts/core/api.py", "benchmarks/instruction_counts/core/expand.py", "benchmarks/instruction_counts/core/types.py", "benchmarks/instruction_counts/core/utils.py", "benchmarks/instruction_counts/definitions/__init__.py", "benchmarks/instruction_counts/definitions/setup.py", "benchmarks/instruction_counts/definitions/standard.py", "benchmarks/instruction_counts/main.py", "benchmarks/instruction_counts/worker/__init__.py", "benchmarks/instruction_counts/worker/main.py", "mypy-strict.ini"], "labels": ["Merged", "cla signed"]}, "d3f784244e": {"title": "fix comparison of narrow type with wide type in loop condition part2 (#54471)", "body": "Summary:\nFollow up PR of https://github.com/pytorch/pytorch/issues/53951.\nThis PR fixes remaining semmle warning: comparison of narrow type with wide type in loop condition\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54471\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27262493\n\nPulled By: malfet\n\nfbshipit-source-id: 05765758da79699936af11de237c3ff3d34373d6", "pr_number": "54471", "files_changed": ["aten/src/ATen/ExpandUtils.cpp", "aten/src/ATen/native/MaxUnpooling.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/TH/THGeneral.cpp", "caffe2/perfkernels/math_cpu_avx2.cc", "caffe2/perfkernels/math_cpu_base.cc"], "labels": ["Merged", "cla signed", "open source"]}, "bac566bf61": {"title": "torch.square : OpInfo and minor fixes (#52551)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nAdd `out` variant to be consistent with Unary Ops.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52551\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27233482\n\nPulled By: mruberry\n\nfbshipit-source-id: fef6f241849a12c46028bd1aad8f5ecc1dc65ea1", "pr_number": "52551", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_binary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c371542efc": {"title": "testing: dont skip test_ops suite for operators testing against scipy (#54186)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54152\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54186\n\nReviewed By: ngimel\n\nDifferential Revision: D27287024\n\nPulled By: mruberry\n\nfbshipit-source-id: 3e19b94b138fb56a7cb2c1c13af3587a5b6d937a", "pr_number": "54186", "files_changed": ["test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "05c8ddfe05": {"title": "[AutoAccept][Codemod][FBSourceGoogleJavaFormatLinter] Daily `arc lint --take GOOGLEJAVAFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27288729\n\nfbshipit-source-id: 84c9f4cffdabd3c1967e3279ec123867d8eded00", "pr_number": null, "files_changed": ["android/pytorch_android_torchvision/src/main/java/org/pytorch/torchvision/TensorImageUtils.java", "android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java"], "labels": []}, "67e4618037": {"title": "Add arg layer_norm_eps to transformer layers (#54494)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/44367\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54494\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27264321\n\nPulled By: jbschlosser\n\nfbshipit-source-id: ed264d253b2df2d6f1d80898464f4f26022482ec", "pr_number": "54494", "files_changed": ["torch/nn/modules/transformer.py"], "labels": ["Merged", "cla signed", "open source"]}, "2f5db68797": {"title": "Make nightly checkout work with generated testing py (#54477)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54477\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27263065\n\nPulled By: ezyang\n\nfbshipit-source-id: 7fa653fb334ff91c9100cf5adcabab6b30533a89", "pr_number": "54477", "files_changed": ["tools/nightly.py"], "labels": ["Merged", "cla signed"]}, "f9ca0d87a7": {"title": "Teach Python TS frontend to parse complex literals (#52881)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52881\n\n**This PR adds:**\n1. logic to parse complex constants (complex literals of the form `bj`)\n2. logic to parse complex lists\n3. support for complex constructors: `complex(tensor/int/float/bool, tensor/int/float/bool)`\n4. Limited operator support\n     - `add`, `sub`, `mul`, `torch.tensor`, `torch.as_tensor`\n\n**Follow-up work:**\n1. Add complex support for unary and other registered ops.\n2. support complex constructor with string as input (this is supported in Python eager mode).\n3. Test all emitXYZ for all XYZ in `ir_emitter.cpp` (currently only emitConst, emitValueToTensor are tested). e.g., test loops etc.\n4. onnx doesn't support complex tensors, so we should error out with a clear and descriptive error message.\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27245059\n\nPulled By: anjali411\n\nfbshipit-source-id: af043b5159ae99a9cc8691b5a8401503fa8d6f05", "pr_number": "52881", "files_changed": ["aten/src/ATen/core/interned_strings.h", "aten/src/ATen/core/ivalue.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_subgraph_matcher.cpp", "test/jit/test_complex.py", "test/jit/test_list_dict.py", "test/test_jit.py", "torch/csrc/jit/frontend/function_schema_parser.cpp", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/lexer.h", "torch/csrc/jit/frontend/schema_matching.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/tree_views.h", "torch/csrc/jit/ir/attributes.h", "torch/csrc/jit/ir/constants.cpp", "torch/csrc/jit/ir/constants.h", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/ir.h", "torch/csrc/jit/ir/irparser.cpp", "torch/csrc/jit/ir/node_hashing.cpp", "torch/csrc/jit/ir/subgraph_matcher.cpp", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/runtime/register_ops_utils.cpp", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/jit/frontend.py", "torch/jit/supported_ops.py"], "labels": ["Merged", "cla signed", "module: complex", "oncall: jit"]}, "12a61a172e": {"title": "Fix missing class in cpp tensor documentation (#54488)", "body": "Summary:\nThe given example in the documentation does not compile due to the missing `torch::`. It is correct in the tutorial about [writing a custom extension ](https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54488\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27267000\n\nPulled By: glaringlee\n\nfbshipit-source-id: 86a46d656c1a4fa4098287a6a43a38d1ef80171e", "pr_number": "54488", "files_changed": ["docs/cpp/source/notes/tensor_basics.rst"], "labels": ["Merged", "cla signed", "open source"]}, "2b07bcf9eb": {"title": "[operator benchmarks] Added more interpolation test cases (#54584)", "body": "Summary:\nDescription:\n- Added uint8 nearest test case\n- Added 3d vectorization test case\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54584\n\nReviewed By: malfet\n\nDifferential Revision: D27291303\n\nPulled By: fmassa\n\nfbshipit-source-id: 236ee5af351c8dc34ec3cdb7dda662c77feb8cf0", "pr_number": "54584", "files_changed": ["benchmarks/operator_benchmark/pt/interpolate_test.py"], "labels": ["Merged", "cla signed", "open source"]}, "b032316c41": {"title": "Improve `nn.Sequential` documentation (#53380)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53380\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D26849861\n\nPulled By: ansley\n\nfbshipit-source-id: 2add8c73ae421332ed1c03340806e25656bafabb", "pr_number": "53380", "files_changed": ["docs/source/jit.rst", "torch/nn/modules/container.py"], "labels": ["Merged", "cla signed"]}, "145bc5cd51": {"title": "Rename Math to CompositeImplicitAutograd (#54466)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54466\n\nI had to very carefully audit all the use sites since there are a lot\nof other uses of the string Math; I did most of the conversion by\ngrepping for all occurrences of Math and then doing a search\nreplace.\n\nI also updated documentation for clarity.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27253239\n\nPulled By: ezyang\n\nfbshipit-source-id: afb485d07ff39575742a4f0e1e205179b60bc953", "pr_number": "54466", "files_changed": ["BUILD.bazel", "aten/src/ATen/core/boxing/KernelFunction.cpp", "aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/native/README.md", "aten/src/ATen/native/native_functions.yaml", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "test/test_dispatch.py", "tools/codegen/gen.py", "tools/codegen/model.py", "torch/_python_dispatcher.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/utils/python_dispatch.cpp"], "labels": ["Merged", "cla signed"]}, "556fc8d418": {"title": "skip test_symeig if MAGMA not detected (#54526)", "body": "Summary:\nAdd proper way to skip test_symeig. In case MAGMA is not detected, skip the test_symeig properly.\nAdded skipCUDAIfNoMagma decorator.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54526\n\nReviewed By: malfet\n\nDifferential Revision: D27293640\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 245f86540af0e37c8795e80dc003e1ca4c08cd5b", "pr_number": "54526", "files_changed": ["test/test_vmap.py"], "labels": ["Merged", "cla signed", "open source"]}, "d371a9f9c5": {"title": "Change ScatterGather kernel names on dtype dispatch. (#54498)", "body": "Summary:\nChanged `ScatterGather` kernel name when `dtype` dispatching to a more meaningful name than `\"method_name\"`.\n\nhttps://github.com/pytorch/pytorch/blob/2a53897114e984f5433e9e495cccb1417b86a8d3/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp#L146-L148\n\nhttps://github.com/pytorch/pytorch/blob/2a53897114e984f5433e9e495cccb1417b86a8d3/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp#L241-L243\n\nMaybe, a more specific name, based on who is calling (e.g. `gather_cpu_kernel`, `scatter_cpu_kernel`), would be better. Any thoughts?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54498\n\nReviewed By: malfet\n\nDifferential Revision: D27291514\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 123b77296e685ee34031da661c78e201a10757db", "pr_number": "54498", "files_changed": ["aten/src/ATen/native/cpu/ScatterGatherKernel.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "796be045bb": {"title": "Refactor gradcheck (#53857)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53857\n\nThis PR basically just factors a lot of the logic out from the main gradcheck function into their own individual functions. It aims to avoid any behavior change (but we may not have enough tests to actually verify this). Refactorings that lead to any behavior chang are done in the next PR in this stack.\n\nThe rationale for this change is 1) to make the main gradcheck function cleaner to read, and 2) also allow us to reuse the same pieces when we add the fast gradcheck.\n\nMaybe this PR is also a good place to add some tests for gradcheck, i.e., make sure gradcheck fails when it should fail, as to make sure that we are indeed not changing any logic. This will also help us make sure our fast_gradcheck does all the necessary checks:\nSo far existing tests are:\n- test_gradcheck_fail_when_no_differentiable_outputs_and_num_grad_not_zero` (test_autograd)\n- test_gradcheck_single_input (test_autograd)\n- test_gradcheck_sparse_input (test_autograd)\n- test_gradcheck_nondeterministic (test_autograd)\n- test_gradcheck (test_overrides)\n\nFull coverage would potentially require adding the following missing tests (for each test for both raise_exception=True/False) - Methodology for getting the list below is that for every type of error message we spit out, we make sure we can hit it:\n- complex:\n  - when numerical != analytical when tested with imag grad_out\n- check_inputs\n  - ~when inputs are not dense, but check_sparse_nnz is false~\n  - ~when none of the inputs require grad~\n  - ~(warning) when inputs are not double precision~\n  - ~when layout is not mkldnn(aka has strides) and input has a dimension with stride 0.~\n- check_no_differentiable_outputs:\n  - ~when none of the outputs are differentiable, but numerical gradient is not zero~\n- check_outputs:\n  - ~when sparse outputs (always raise)~\n  - ~when mkldnn outputs (always raise)~\n- test_batched_grad\n  - ~when encounter runtime error while computing batched grad (print big message)~\n  - when not allclose (print out big message)\n- test_backward_mul_by_grad_output\n  - ~when layout of grad_input is not the same as input~\n  - ~when grad_input is sparse and has incorrect sparse_dim/dense_dim~\n  - ~when backward not multiplied by grad_output (sparse/non-sparse case)~\n  - when grad is incorrect type/size\n- test_undefined_grad\n  - ~when encounter runtime error while running backward~\n  - when we complete backward but grad inputs (the output of .grad()) is not none\n- check_analytical_jacobian_attributes (for both complex/non complex)\n  - when grad input is incorrect dtype/size\n\nTest Plan: Imported from OSS\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27201571\n\nPulled By: soulitzer\n\nfbshipit-source-id: 86670a91e65740d57dd6ada7c6b4512786d15962", "pr_number": "53857", "files_changed": ["test/test_autograd.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "673ed4623e": {"title": "Gradcheck small fixes (#53916)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53916\n\nThis PR fixes some bugs that are made more clear by the previous refactor.\n- make sure gradcheck returns false when its supposed to fail and when raise_exception=False.\n- make sure when test_batched_grad fails, it returns false when raise_exception=False\n\nRemoving checkIfNumericalAnalyticAreClose made sense here to me because underneath its really doing `torch.allclose`, and using that directly instead of adding another opaque function to call seemed to make the code more clear.\n\nTODO:\n- ~add a test to see if when torch.allclose fails, we indeed return false.~\n- ~uncomment test from previous PR.~\n\nTest Plan: Imported from OSS\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27201692\n\nPulled By: soulitzer\n\nfbshipit-source-id: 8b8dc37c59edb7eebc2e8db6f8839ce98a81d78b", "pr_number": "53916", "files_changed": ["test/test_autograd.py", "test/test_overrides.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "7605ce4ed8": {"title": "[PyTorch] Enable test_lite_interpreter_runtime running in android (#54579)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54579\n\n## Summary\n\n1. Eliminate a few more tests when BUILD_LITE_INTERPRETER is on, such that test_lite_interpreter_runtime can build and run on device.\n2. Remove `#include <torch/torch.h>`, because it's not needed.\n\n## Test plan\n\nSet the BUILD_TEST=ON `in build_android.sh`, then run\n` BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh x86`\n\npush binary to android device:\n```\n adb push ./build_android_x86/bin/test_lite_interpreter_runtime /data/local/tmp\n```\n\nReorganize the folder in `/data/local/tmp` so the test binary and model file is like following:\n```\n/data/local/tmp/test_bin/test_lite_interpreter_runtime\n/data/local/tmp/test/cpp/lite_interpreter_runtime/sequence.ptl\n```\nsuch that the model file is in the correct path and can be found by the test_lite_interpreter_runtime.\n\n![image](https://user-images.githubusercontent.com/16430979/112276332-d89d1900-8c3d-11eb-91de-7bf10d1e418d.png)\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27300720\n\nPulled By: cccclai\n\nfbshipit-source-id: d9526c7d3db8c0d3e76c5a4d604c6877c78afdf9", "pr_number": "54579", "files_changed": ["aten/src/ATen/CMakeLists.txt", "modules/module_test/CMakeLists.txt", "test/cpp/lite_interpreter_runtime/test_lite_interpreter_runtime.cpp"], "labels": ["Merged", "cla signed"]}, "ac33432606": {"title": "Fixed out= variants of non-symmetric eigendecomposition and QR decomposition (#54056)", "body": "Summary:\nThis PR modifies the behavior of _out variants of `torch.eig`, `torch.qr`, `torch.linalg.qr`  to match the description here https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch\nWith this PR result and input tensors must be on the same device and have the same \"type kind\".\n\nTested with OpInfo's `supports_out=True`.\n\nRef. https://github.com/pytorch/pytorch/issues/42666\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54056\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27230275\n\nPulled By: mruberry\n\nfbshipit-source-id: 3fe1ce6c0e2c20bdfd6742305a20f3cf3632a4d6", "pr_number": "54056", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source", "triaged"]}, "1442a92741": {"title": "Ensure local_used_maps_tmp is distinct from local_used_maps_[i] (#54474)", "body": "Summary:\nFollowup/hotfix for https://github.com/pytorch/pytorch/pull/53160. rohan-varma and zhaojuanmao were seeing https://github.com/pytorch/pytorch/pull/53160/files#diff-9273e5ff7b40f30d6a4444d1c7be9fe9a5c2068070c68af4e7b0ac2d4cff0923R582 fire in some internal workloads, indicating `local_used_maps_tmp` wasn't actually being created as a distinct temporary, in other words, `local_used_maps_[i]` was already pinned for some reason. This seems like a bug with the CPU allocator: [`local_used_maps_` should not have been pinned on construction](https://github.com/pytorch/pytorch/blob/9be4c75fa0399a292e95ef8f3c79457e4b5b2338/torch/lib/c10d/reducer.cpp#L180-L183). We should [investigate that separately](https://github.com/pytorch/pytorch/pull/53160/files#r599188373).\n\nIn the meantime, the present PR should ensure `local_used_maps_tmp` is always distinct from `local_used_maps_[i]` (and therefore prevents the race condition described in https://github.com/pytorch/pytorch/pull/51360) even if `local_used_maps_[i]`is already pinned.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54474\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27268039\n\nPulled By: rohan-varma\n\nfbshipit-source-id: ab9af3dd845098bde788cb28a9217caea246ddfa", "pr_number": "54474", "files_changed": ["torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "347ab5d8b8": {"title": "Update Kineto submodule (#54621)", "body": "Summary:\nUpdate Kineto submodule to the latest rev.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54621\n\nTest Plan: CI\n\nReviewed By: gdankel\n\nDifferential Revision: D27303589\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 7cea96f779981acd36d10290a537601e2f361720", "pr_number": "54621", "files_changed": ["third_party/kineto"], "labels": ["Merged", "cla signed"]}, "6f8328ef44": {"title": "[special] Add special.entr (#53500)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/50345\n\nTODO:\n\n* [x] Verfiy docs rendering (https://11397990-65600975-gh.circle-artifacts.com/0/docs/special.html)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53500\n\nReviewed By: ngimel\n\nDifferential Revision: D27287096\n\nPulled By: mruberry\n\nfbshipit-source-id: 6b3dfd53e811a0f023ee444a0b56176f825d39e9", "pr_number": "53500", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/special.rst", "tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/special.h", "torch/overrides.py", "torch/special/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "fe2c1268b7": {"title": "More name refactoring of memory planning codes to make it more readable (#54272)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54272\n\nTest Plan: Imported from OSS\n\nReviewed By: bwasti\n\nDifferential Revision: D27233881\n\nfbshipit-source-id: f257f16ac0684df055961e539f17d002cb8f1bfe", "pr_number": "54272", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "64d31e3f45": {"title": "Add double tensor type to DivFakeFp16 Op (#54636)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54636\n\nTest Plan: The model will be rerun after the diff lands...\n\nReviewed By: hx89\n\nDifferential Revision: D27310244\n\nfbshipit-source-id: 88575237596a59996da14a49a8459f8b3d0ee66a", "pr_number": "54636", "files_changed": ["caffe2/contrib/fakelowp/elementwise_fp16_fake_op.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "9f336bdf10": {"title": "Fixes new tf32 failures in test_nn.py (#52871)", "body": "Summary:\nAlso modify the `tf32_on_and_off` decorator to make it support function without `device` argument.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52871\n\nReviewed By: ngimel\n\nDifferential Revision: D27286674\n\nPulled By: mruberry\n\nfbshipit-source-id: 14f6d558271bd6a1d0bc40691c170d47e81de1ff", "pr_number": "52871", "files_changed": ["test/test_nn.py", "torch/testing/_internal/common_cuda.py", "torch/testing/_internal/common_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "947ab84fd2": {"title": "enable_and_enhance_bf16_threshold (#54384)", "body": "Summary:\nenable_and_enhance_bf16_threshold\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54384\n\nReviewed By: ngimel\n\nDifferential Revision: D27286323\n\nPulled By: mruberry\n\nfbshipit-source-id: 517fa94764d8202bbcbf94011d2d48f716fbd01b", "pr_number": "54384", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "aten/src/ATen/native/cpu/Activation.cpp", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2662e34e92": {"title": "Add PyTorchDeploy predictor model type (#54120)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54120\n\nConstruct InterpreterManager inside PyTorchDeployModel\n - add ReadAdapterInterface to deploy::Package\n\nImplement PyTorchDeployModel::makePrediction for FeatureStore Examples\n- Basic test of loading and executing 'simple' model\n\nTest Plan: ran unit tests locally and CI\n\nDifferential Revision: D26961744\n\nfbshipit-source-id: fce72bc83b9005500d9b7ce3fab2ed466f73d6ed", "pr_number": "54120", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "c0bcd5a58f": {"title": "Remove NestedTensor from DefaultBackend alias (#54559)", "body": "Summary:\nKernels such as \"add\" are registered to DefaultBackend. At a minimum NestedTensor is not compatible with structured kernels due to missing fields such as size, which can therefore cause difficult to catch bugs when being passed into a function without a NestedTensor-specific kernel.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54559\n\nReviewed By: ezyang\n\nDifferential Revision: D27283591\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: fad7c03ca3b2190f2f90039dd2872184e9bc5049", "pr_number": "54559", "files_changed": ["c10/core/DispatchKeySet.cpp"], "labels": ["Merged", "cla signed"]}, "55dfb4a575": {"title": "Update CODEOWNERS for distributed training (#54661)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54661\n\nMy username was somehow deleted, and I couldn't receive review requests.\nghstack-source-id: 124853153\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27320286\n\nfbshipit-source-id: c38ea3adb2e8197f949a806127d20982299a2851", "pr_number": "54661", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed"]}, "6cdabb2e40": {"title": "Update .gitignore to ignore NFS handle files (#54618)", "body": "Summary:\nIgnore NFS handle files starting with .nfs*.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54618\n\nReviewed By: malfet\n\nDifferential Revision: D27304405\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 9abeed796fec0a4ff416eacea450f3f8e2813b32", "pr_number": "54618", "files_changed": [".gitignore"], "labels": ["Merged", "cla signed"]}, "9029d0d7d8": {"title": "Introduce a fluent API to construct tensors from external data. (#54530)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54530\n\nThis diff introduces the following changes and improvements:\n\n- Introduces a new fluent API to construct tensors from external data as an alternative to `from_blob` overloads. See below for an example.\n- Leverages several small-buffer optimizations which result in %50 reduction in tensor construction times.\n- Exposes a new (lightweight) way to construct tensors by passing a naked `context` and `context_deleter` pair as an alternative to the existing `deleter` parameter.\n- Updates the existing `from_blob` overloads to internally use the fluent API.\n\n```\n// Example 1\nat::Tensor tensor = at::for_blob(data, sizes)\n  .strides(strides)\n  .context(context, [](void *ctx) { delete static_cast<Ctx*>(ctx); })\n  .options(...)\n  .target_device(...)\n  .make_tensor();\n\n// Example 2\nat::Tensor tensor = at::for_blob(data, sizes).make_tensor();\n\n// Example 3\nat::Tensor tensor = at::for_blob(data, sizes)\n  .deleter(...)\n  .make_tensor();\n```\n\nTest Plan:\nBelow are the folly Benchmark results for the following two equivalent operations:\n\n```\n// The fluent API\nat::Tensor tensor = at::for_blob(data, sizes)\n  .deleter([buffer](void*) mutable { buffer.reset(); })\n  .options(dtype(c10::ScalarType::Float))\n  .make_tensor();\n\n// The original `from_blob` overload\nat::Tensor tensor = at::from_blob(\n  data,\n  sizes,\n  [buffer](void*) mutable { buffer.reset(); },\n  dtype(c10::ScalarType::Float));\n```\n\n```\n============================================================================\nscripts/balioglu/from_blob_exp/main.cpp         relative  time/iter  iters/s\n============================================================================\nfluent                                                     298.34ns    3.35M\nfrom_blob                                         55.19%   540.51ns    1.85M\n============================================================================\n```\n\nVarious similar experiments show an approximate %50 reduction in tensor construction times.\n\nReviewed By: ezyang\n\nDifferential Revision: D27269344\n\nfbshipit-source-id: e6bd0b78384bf89fd24f22254008180329000363", "pr_number": "54530", "files_changed": ["aten/src/ATen/templates/Functions.cpp", "aten/src/ATen/templates/Functions.h", "c10/core/Storage.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "93bbbeccf7": {"title": "Make SharedCache thread-safe (#53750)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53731\n\nMake SharedCache thread-safe by using explicit locks instead of relying on atomicity of certain Python operations\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53750\n\nReviewed By: malfet\n\nDifferential Revision: D27304793\n\nPulled By: albanD\n\nfbshipit-source-id: 7c62babe4357bed57df3056fbda6801fb6168846", "pr_number": "53750", "files_changed": ["torch/multiprocessing/reductions.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "267fc27d39": {"title": "Ensure torch.futures.wait_all exits early on error. (#53953)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53953\n\ntorch.futures.wait_all, would wait for all specified futures to\ncomplete before it returned. As a result, if there was an error it would still\nwait for a long time (ex: long running RPCs) before it returned an error to the\nuser.\n\nThis PR ensures `wait_all` returns and error as soon as any future runs into an\nerror and doesn't wait for all futures to complete.\n\nI removed the logic _invoke_rpc_python_udf which raised an error in the unwrap\nfunction, because ideally the error should be set on the Future and not be\nraised to the user only when `wait()` is called. As an example, in the case of\n`wait_all`, the user never calls `wait()` on the future that errored out but a\nfuture down the chain and we should propagate these errors via `setError`\ninstead.\nghstack-source-id: 124721216\n\nTest Plan:\n1) Unit test added.\n2) waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D27032362\n\nfbshipit-source-id: c719e2277c27ff3d45f1511d5dc6f1f71a03e3a8", "pr_number": "53953", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "test/cpp/jit/test_misc.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/futures/__init__.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "4bf90558e0": {"title": "[Gradient Compression] Add logging for gradient compression stats. (#54647)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54647\n\nRegularly log stats showing effect of gradient compression when using the PowerSGD DDP communication hook.\n\nTest Plan:\nbuck run mode/dev-nosan scripts/wayi/torch:power_sgd\n\nPlay with the layer sizes of the input model (you can just use linear layers for convenience), and check the log that shows compression stats. For convenience, you can change `logging.info` to `print` locally.\n\nYou can create some test diffs on top of this diff, to show that the compression stats are correct in different cases.\n\nRun with power_sgd script:\n{F537381542}\n\nDiff with example using a simple linear model: D27299934\nsample output:\n{F538486535}\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27240254\n\nfbshipit-source-id: 9e142b2f7957cc874804f799b7bb3bffdf824858", "pr_number": "54647", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "f251bb40c1": {"title": "Cancel redundant GHA workflows (#54685)", "body": "Summary:\nThis PR adds a lightweight workflow which runs when any of our GitHub Actions lint or test workflows start (currently just the three listed in the YAML in this PR's diff), and cancels redundant ones (e.g. if a PR author pushes several commits in rapid succession). Currently this isn't particularly impactful, but it would become more so if/when we add heavier workflows that run on PRs.\n\nInitially we tried using [`technote-space/auto-cancel-redundant-workflow`](https://github.com/technote-space/auto-cancel-redundant-workflow) instead of [`potiuk/cancel-workflow-runs`](https://github.com/potiuk/cancel-workflow-runs), but for some reason it the former doesn't seem to work even if triggered by `workflow_run` with the `TARGET_RUN_ID` input set appropriately.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54685\n\nTest Plan: janeyx99 and I tested this in a separate GitHub repo, and confirmed that it successfully cancels redundant `push`-triggered workflows on the source repo and `pull_request`-triggered workflows from forks.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27327999\n\nPulled By: samestep\n\nfbshipit-source-id: c5793a7660d21361381e0f033d314f2d603f70ec", "pr_number": "54685", "files_changed": [".github/workflows/cancel_redundant_workflows.yml"], "labels": ["Merged", "Reverted", "cla signed"]}, "d12118c0aa": {"title": "Handle stride > 1 with im2col in CUDA thnn conv2d (#54080)", "body": "Summary:\nThe fallback thnn 2d convolution uses `im2col` to get patches and `gemm` to implement convolution .\nI has a shortcut to use `gemm` directly for kernel size 1, but this only works for stride == 1 and padding == 0.\nThis PR adds checks for stride == 1 and padding == 0 to determining whether `im2col` can be skipped.\n\nFixes https://github.com/pytorch/pytorch/issues/54036\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54080\n\nReviewed By: ejguan\n\nDifferential Revision: D27170482\n\nPulled By: zou3519\n\nfbshipit-source-id: 055d6502239d34945934de409d78144d8a5c56f4", "pr_number": "54080", "files_changed": ["aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/THCUNN/generic/SpatialConvolutionMM.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "53596cdb73": {"title": "Remove hacky wrapper for about 100 kernels (#54367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54367\n\nCodemod commands generated by https://github.com/pytorch/pytorch/pull/54098\nghstack-source-id: 124804544\n\nTest Plan: buck build //caffe2/aten/...\n\nReviewed By: smessmer\n\nDifferential Revision: D27210057\n\nfbshipit-source-id: 368dc77843468cfc44535488a040dbc2cb67208d", "pr_number": "54367", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/Cross.cpp", "aten/src/ATen/native/Lerp.cpp", "aten/src/ATen/native/Linear.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/TriangularOps.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/Lerp.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/RangeFactories.cu", "aten/src/ATen/native/cuda/SparseMM.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/TriangularOps.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/THCUNN/generic/MultiLabelMarginCriterion.cu", "aten/src/THCUNN/generic/MultiMarginCriterion.cu", "benchmarks/cpp/tensorexpr/bench_approx.cpp", "benchmarks/static_runtime/deep_wide_pt.h", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "71b9f2dd76": {"title": "Add GHA to cancel redundant GHA workflows except on master (#54689)", "body": "Summary:\nRelands https://github.com/pytorch/pytorch/issues/54685 with the fix to filter out master\n\nTested with samestep in other repository.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54689\n\nReviewed By: walterddr\n\nDifferential Revision: D27330804\n\nPulled By: janeyx99\n\nfbshipit-source-id: 06d8199af6173eedca2e7db4a1fd7b9a143d29d2", "pr_number": "54689", "files_changed": [".github/workflows/cancel_redundant_workflows.yml"], "labels": ["Merged", "cla signed"]}, "9c60fc9cd9": {"title": "Fix broken javadoc URL in README (#54434)", "body": "Summary:\nThe link in the README was broken\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54434\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328733\n\nPulled By: nairbv\n\nfbshipit-source-id: 12ebb6f66983f9348a90b9738fbd9f3f2660c2d1", "pr_number": "54434", "files_changed": ["android/README.md"], "labels": ["Merged", "cla signed"]}, "68bdeef2ce": {"title": "[CMake] Simplify CPU architecture detection logic (#54637)", "body": "Summary:\nCMAKE_SYSTEM_PROCESSOR set to x86_64(on Linux) or AMD64 (https://github.com/pytorch/pytorch/commit/5ec224496b1e42bd24d17ec4b75e4e4d3ae71ef2)(on Windows) indicates build is running on x86_64 architecture, while `CMAKE_SYSTEM_PROCESSOR` set to aarch64 or arm64 means we running on ARMv8+ architecture.\nDelete `i[3-6]86` pattern as 32-bit builds are no longer supported\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54637\n\nReviewed By: ezyang\n\nDifferential Revision: D27311897\n\nPulled By: malfet\n\nfbshipit-source-id: 26989fc9b54a96d70c768ab03ca4528506ee7808", "pr_number": "54637", "files_changed": ["CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "dfc7fa03e5": {"title": "lu_backward: more numerically stable and with complex support. (#53994)", "body": "Summary:\nAs per title.\n\nNumerical stability increased by replacing inverses with solutions to systems of linear triangular equations.\n\nUnblocks computing `torch.det` for FULL-rank inputs of complex dtypes via the LU decomposition once https://github.com/pytorch/pytorch/pull/48125/files is merged:\n```\nLU, pivots = input.lu()\nP, L, U = torch.lu_unpack(LU, pivots)\ndet_input = P.det() * torch.prod(U.diagonal(0, -1, -2), dim=-1)  # P is not differentiable, so we are fine even if it is complex.\n```\n\nUnfortunately, since `lu_backward` is implemented as `autograd.Function`, we cannot support both autograd and scripting at the moment.\nThe solution would be to move all the lu-related methods to ATen, see https://github.com/pytorch/pytorch/issues/53364.\n\nResolves https://github.com/pytorch/pytorch/issues/52891\nTODOs:\n* extend lu_backward for tall/wide matrices of full rank.\n* move lu-related functionality to ATen and make it differentiable.\n* handle rank-deficient inputs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53994\n\nReviewed By: pbelevich\n\nDifferential Revision: D27188529\n\nPulled By: anjali411\n\nfbshipit-source-id: 8e053b240413dbf074904dce01cd564583d1f064", "pr_number": "53994", "files_changed": ["tools/autograd/gen_variable_type.py", "torch/_autograd_functions.py", "torch/_tensor.py", "torch/functional.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "complex_autograd", "module: complex", "module: linear algebra", "open source", "triaged"]}, "6b7652e26c": {"title": "[DDP logging] Prefer use of c10::Join (#54649)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54649\n\nSome operator<< code manually implemented string join in C++, turns\nout there is a c10 util for this. Use the util instead of rolling our own.\nghstack-source-id: 124840043\n\nTest Plan: Ci\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27316705\n\nfbshipit-source-id: 5118097f84be2f38a503d8f81faa38c8d95ec17a", "pr_number": "54649", "files_changed": ["c10/util/Logging.h"], "labels": ["Merged", "cla signed"]}, "9db4802184": {"title": "[fuser] Support bfloat16 (#54571)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54571\n\nSupports bfloat16 via a similar method to half: upconvert inputs to\nfp32, do math, then downconvert outputs to bf16.\n\nResource strings are mostly derived from cuda-11 headers.\n\nFixes #53918, for the legacy fuser at least.\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328987\n\nPulled By: bertmaher\n\nfbshipit-source-id: 5c0eae44164623faa0c75cb818e8bf0211579fdc", "pr_number": "54571", "files_changed": ["test/test_jit_fuser.py", "torch/csrc/jit/codegen/fuser/codegen.cpp", "torch/csrc/jit/codegen/fuser/cuda/resource_strings.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "fd58ececab": {"title": "Pin autocanceling GHA repo to specific commit (#54738)", "body": "Summary:\nThis way, if malicious code gets committed and the tag moves forward, we would be at risk. This does mean that we would have to manually update the SHA if there are desirable upgrades to the repository.\n\nWe are pinning it to this commit: https://github.com/potiuk/cancel-workflow-runs/commit/a81b3c4d59c61e27484cfacdc13897dd908419c9\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54738\n\nReviewed By: samestep\n\nDifferential Revision: D27346792\n\nPulled By: janeyx99\n\nfbshipit-source-id: 5641a78567c3cd61dce35dfa2fd4918f255a7681", "pr_number": "54738", "files_changed": [".github/workflows/cancel_redundant_workflows.yml"], "labels": ["Merged", "cla signed"]}, "52a8075f16": {"title": "ns for fx: add support for lstm activation matching (#53779)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53779\n\nMoves the test case for LSTM activation matching to new NS APIs.\n\nThis requires adding the ability to log non-Tensor types.\nSince we need Loggers to be scriptable and TorchScript does\nnot support `Union`, we collect statistics in a separate collector\nif we have an RNN.  Note: this can scale to a small N of\nreturn types, but not to a large N.  If the N becomes large in\nthe future, we will solve it then.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26967110\n\nfbshipit-source-id: afe60b44fdec28a328813b4f342cf4fe04820baa", "pr_number": "53779", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "3dc8ba27a5": {"title": "ns for fx: move shadow activations conv test to new API (#53818)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53818\n\nMoves testing of conv for shadow activations to new NS API\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_shadow_activations_conv\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26982732\n\nfbshipit-source-id: 9e8709a76363fbcdf84413e5d4a6c8a0889cb97b", "pr_number": "53818", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "cfe7364809": {"title": "ns for fx: move shadow activations linear test to new API (#53819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53819\n\nMoves the linear tests for shadow activations to new API.\nIn order to do so, adds logic for fp32 to fp32 dtype cast,\nwhich is an identity.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_shadow_activations_linear\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26982734\n\nfbshipit-source-id: b6203228abf3cdf74ab0638468a6df77658aa662", "pr_number": "53819", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "9e8e744efe": {"title": "ns for fx: move shadow lstm test to new API (#53828)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53828\n\nMoves LSTM shadow activations test to new API. In order\nto enable this, adds support for passing two args instead\nof one arg when copying a subgraph from A to B.\n\nSince this was the last test of the old API, deletes\nthe old test case.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels.test_compare_shadow_activations_lstm_dynamic\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D26982733\n\nfbshipit-source-id: 03f580688dd37f3ccd688d9f444e9e79cfa84734", "pr_number": "53828", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "test/test_quantization.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "454832e5fa": {"title": "ns for fx: create subgraph type (#54253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54253\n\nCreates an `NSSubgraph` type for representing a subgraph instance,\nand modifies the NS code to use it. This will enable us to add\nmore information to the subgraph instance definition without\nhaving to change all the callsites.\n\nTest Plan:\n```\nmypy torch/quantization\npython test/test_quantization.py TestFXGraphMatcher\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27158198\n\nfbshipit-source-id: 548785dd90144e2da256c23af990620c778e7cfe", "pr_number": "54253", "files_changed": ["torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/ns_types.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "182d8c375c": {"title": "ns for fx: add partial support for subgraphs with base_op_node (#54254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54254\n\nIn fp16 emulation, we now have patterns such as\n\n```\n... -> dequantize -> linear -> relu -> to(torch.float16) -> ...\n```\n\nThis PR adds support for\n* specifying a subgraph's \"base_op_node\", which is the node with the op\nwhich should be matched to related nodes. In the example above,\n\"base_op_node\" would be the linear node, and it would be the second\nnode in the matched pattern.\n* matching these fusion patterns and properly setting \"base_op_node\"\nbased on pattern and index\n* using \"base_op_node\" instead of \"start_node\" throughout the NS\ncodebase wherever the intent is to match subgraphs or create names\nfor subgraphs.\n\nAt the end of this PR, matching unshadowed activations with an example\nfp16 emulation pattern works e2e.\n\nI'm saving the following work for future PRs (soon), mostly to keep\nPR size manageable:\n* adding weight matching (will require some changes to function which\nextracts weights)\n* adding shadowed activation matching (will require some changes to\nshadow copying)\n* adding input logging for these patterns (will likely require some changes as well)\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_linear_fp16\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27158199\n\nfbshipit-source-id: 49fc445395452fda62e3c7a243544190f9af691c", "pr_number": "54254", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/ns_types.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "0a18211989": {"title": "ns for fx: add weight matching for linear fp16 emulation (#54257)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54257\n\nMakes the NS weight extraction fuction work correctly with\nfp16 emulation patterns for linear.  We navigate to the\nweight correctly, and cast it to `torch.float16` before returning.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_linear_fp16\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27159370\n\nfbshipit-source-id: 95f555298e3153e4783c64b3d8c83b9d3fdffa12", "pr_number": "54257", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "b7b481bd07": {"title": "[PyTorch] Enable template build at aten operator level (#53801)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53801\n\n## Summary\n\nEnable partial explicit Aten level sources list for lite interpreter. More aten level source list will be added.\n\n1. Use `gen_selected_mobile_ops_header.py ` to generate `selected_mobile_ops.h`. Currently, it only includes selected operators, and dtypes is all.\n2. Add a custom target includes only `seleteted_mobile_ops.h`, and add it to `torch_cpu` dependency, when `BUILD_LITE_INTERPRETER` is enabled.\n\nAs a note, the current input yaml file is slightly different than the one use in internal. Will align these two yaml as next step.\n\n**Android**\nx86:\n`SELECTED_OP_LIST=/Users/chenlai/Documents/pytorch/experiemnt/deeplabv3_scripted.yaml BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh x86`\n\nlibpytorch_jni_lite.so -- 3.4 MB\n\narmeabi-v7a\n`SELECTED_OP_LIST=/Users/chenlai/Documents/pytorch/experiemnt/deeplabv3_scripted.yaml BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh armeabi-v7a`\nlibpytorch_jni_lite.so -- 2.5 MB\n\n**iOS:**\n```\n(base) chenlai@chenlai-mp install % du -sh *\n 15M\tinclude\n 57M\tlib\n2.8M\tshare\n```\n\n```\n(base) chenlai@chenlai-mp lib % ls -lh\ntotal 117296\n-rw-r--r--  1 chenlai  staff   3.2M Mar 15 22:03 libXNNPACK.a\n-rw-r--r--  1 chenlai  staff   913K Mar 15 22:03 libc10.a\n-rw-r--r--  1 chenlai  staff   4.6K Mar 15 22:03 libclog.a\n-rw-r--r--  1 chenlai  staff    42K Mar 15 22:03 libcpuinfo.a\n-rw-r--r--  1 chenlai  staff   1.5M Mar 15 22:03 libeigen_blas.a\n-rw-r--r--  1 chenlai  staff    44K Mar 15 22:03 libpthreadpool.a\n-rw-r--r--  1 chenlai  staff   166K Mar 15 22:03 libpytorch_qnnpack.a\n-rw-r--r--  1 chenlai  staff   384B Mar 15 22:03 libtorch.a\n-rw-r--r--  1 chenlai  staff    51M Mar 15 22:03 libtorch_cpu.a\n```\n\n### **Master (Baseline):**\n\n**Android**\nx86:\n`SELECTED_OP_LIST=/Users/chenlai/Documents/pytorch/experiemnt/deeplabv3_scripted.yaml BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh x86`\n\nlibpytorch_jni_lite.so -- 3.8 MB\n\narmeabi-v7a\n`SELECTED_OP_LIST=/Users/chenlai/Documents/pytorch/experiemnt/deeplabv3_scripted.yaml BUILD_LITE_INTERPRETER=1 ./scripts/build_pytorch_android.sh armeabi-v7a`\nlibpytorch_jni_lite.so -- 2.8 MB\n\n**iOS:**\n```\n(base) chenlai@chenlai-mp install % du -sh *\n 15M\tinclude\n 58M\tlib\n2.8M\tshare\n```\n\n```\n(base) chenlai@chenlai-mp lib % ls -lh\ntotal 119600\n-rw-r--r--  1 chenlai  staff   3.2M Mar  4 23:16 libXNNPACK.a\n-rw-r--r--  1 chenlai  staff   910K Mar  4 23:16 libc10.a\n-rw-r--r--  1 chenlai  staff   4.6K Mar  4 23:16 libclog.a\n-rw-r--r--  1 chenlai  staff    42K Mar  4 23:16 libcpuinfo.a\n-rw-r--r--  1 chenlai  staff   1.5M Mar  4 23:16 libeigen_blas.a\n-rw-r--r--  1 chenlai  staff    44K Mar  4 23:16 libpthreadpool.a\n-rw-r--r--  1 chenlai  staff   166K Mar  4 23:16 libpytorch_qnnpack.a\n-rw-r--r--  1 chenlai  staff   384B Mar  4 23:16 libtorch.a\n-rw-r--r--  1 chenlai  staff    52M Mar  4 23:16 libtorch_cpu.a\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: dhruvbird\n\nDifferential Revision: D27074814\n\nPulled By: cccclai\n\nfbshipit-source-id: 762b5ad5b87b6a262444392fd089249c4837ba18", "pr_number": "53801", "files_changed": ["caffe2/CMakeLists.txt", "tools/lite_interpreter/gen_selected_mobile_ops_header.py"], "labels": ["Merged", "cla signed"]}, "5e62da2efd": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27356622\n\nfbshipit-source-id: f03ad23a2847b3cbaf61e16055393cbbfbc215ae", "pr_number": null, "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/python_functions.cpp"], "labels": []}, "1126d51de9": {"title": "Remove useless contiguous calls from torch.matmul (#54616)", "body": "Summary:\nThis reduces the memory usage of matmul significantly for expanded batch size.\n\nThis reduces the peak memory usage of\n```\na = torch.rand(1, 1024, 1024, device=\"cuda\")\nb = torch.rand(1024, 1024, 1, device=\"cuda\")\n\nout = torch.matmul(a, b)\n```\nFrom 4GB to 16MB which is not too bad.\n\nIt also fixes the same problem when `b` is not batched.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54616\n\nReviewed By: ailzhang\n\nDifferential Revision: D27327056\n\nPulled By: albanD\n\nfbshipit-source-id: 4bb5f4015aeab4174148512f3c5b8d1ffa97bf54", "pr_number": "54616", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "test/test_cuda.py"], "labels": ["Merged", "cla signed"]}, "d4045e9aa1": {"title": "initial commit to refactor all s3 access codes to s3_stats_parser (#54681)", "body": "Summary:\nFirst step to move all S3 related operations into S3 parser utils.\nin the end we provide APIs from s3_stats_parser:\n1. downloading data as reports and uploading data as reports\n2. filter by job name\n\nand handle all compression, formatting inside.\n\nTODO\n- [ ] Refactor out upload into s3_stats_parser\n- [ ] Remove all S3/BOTO related checkers and try/catch blocks outside of s3_stats_parser\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54681\n\nTest Plan:\n1. Running tools/test/* covers the refactoring logic (test_test_history.py and test_stats.py as entrypoint and both using the 2 new APIs in s3_stats_parser after the refactoring.\n2. print_test_stats.py's main argparse entrypoint is covered by CI step Report Test Result step.\n3. run `python test/run_test.py --export-past-test-times` before and after this PR should result in the same file content in .pytorch-test-times\n\nReviewed By: ailzhang\n\nDifferential Revision: D27346742\n\nPulled By: walterddr\n\nfbshipit-source-id: fb40162e631e007fed9d5821fe4f190bda2cb52e", "pr_number": "54681", "files_changed": ["test/run_test.py", "tools/print_test_stats.py", "tools/stats_utils/s3_stat_parser.py", "tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "645119eaef": {"title": "Lowering NLLLoss/CrossEntropyLoss to ATen code (#53789)", "body": "Summary:\n* Lowering NLLLoss/CrossEntropyLoss to ATen dispatch\n* This allows the MLC device to override these ops\n* Reduce code duplication between the Python and C++ APIs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53789\n\nReviewed By: ailzhang\n\nDifferential Revision: D27345793\n\nPulled By: albanD\n\nfbshipit-source-id: 99c0d617ed5e7ee8f27f7a495a25ab4158d9aad6", "pr_number": "53789", "files_changed": ["aten/src/ATen/native/LossNLL.cpp", "aten/src/ATen/native/native_functions.yaml", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d.expect", "test/onnx/expect/TestOperators.test_softmaxcrossentropy_3d_none.expect", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/nn/functional.py", "torch/onnx/symbolic_opset12.py", "torch/testing/_internal/common_nn.py", "torch/testing/_internal/jit_metaprogramming_utils.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "f6634be4c2": {"title": "Fix OpInfo failing without scipy (#54735)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54735\n\nOne of the tests didn't wrap scipy call with TEST_SCIPY. Also, the wrapper function seems unnecessary and requires lambdas to be created.\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27351349\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 029e273785b11e01d6be7b816469654de6583deb", "pr_number": "54735", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "2be1b486ce": {"title": "Drop Python 2 support in common_device_type.py (#54691)", "body": "Summary:\nHey!\n\nJust stumbled across these Python 2 fragments while reading the source code and thought it could be removed, since the Python 2 support has already been dropped.\n\nmruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54691\n\nReviewed By: mruberry\n\nDifferential Revision: D27344439\n\nPulled By: ailzhang\n\nfbshipit-source-id: 926303bfff9afa6dabd2efb5e98f9d0d9ef83dc7", "pr_number": "54691", "files_changed": ["torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed", "open source"]}, "20d8fe83cd": {"title": "[TSAN] Suppress data races in caffe2/c10/util/Logging.cpp", "body": "Summary:\nThis suppresses some data races reported by TSAN. See the associated\ntask(s) below for context, including sample stack traces caused by these races\nand reproduction instructions.\n\nThis diff is automatically generated. Therefore, the way it makes suppressions\nmay not be as beautiful as if written by hand. *However, we don't have the\nresources to manually adjust these diffs, nor do we have the capacity to\nactually fix the bugs*; we just want to get the existing bugs\nout of the way so we can enable TSAN across the fleet. If you are a reviewer\nplease do one of the following:\n\n1. Accept the diff as is, and you may follow up with more changes (or fix the\n   bugs) later.\n2. Fix the data races in a different diff and land it within a reasonable amount\n   of time (e.g. a week), and comment about it here.\n3. Comment to suggest us a different code location(s) to suppress these data\n   races.\n\nTest Plan: Unit tests were automatically run as part of https://www.internalfb.com/intern/sandcastle/job/22517998509525934/\n\nReviewed By: ezyang\n\nDifferential Revision: D26094360\n\nfbshipit-source-id: 06c285570bcf7a1491d8f17d1885d065ef0bc537", "pr_number": null, "files_changed": ["c10/util/Logging.cpp"], "labels": []}, "70dd2a2bdd": {"title": "Add myself on all native_functions.yaml code reviews (#54595)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54595\n\nSeeing a lot of misuse of DefaultBackend, want to try to\nnip some of these in code review.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27301721\n\nPulled By: ezyang\n\nfbshipit-source-id: 1a39426cb6cac5c7f322df6f8a69ccb463f1b258", "pr_number": "54595", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed"]}, "13b1ca9466": {"title": "Rename DefaultBackend to CompositeExplicitAutograd (#54470)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54470\n\n```\ngit grep -l 'DefaultBackend' | xargs sed -i 's/DefaultBackend/CompositeExplicitAutograd/g'\n```\n\nPlus a quick fixup in native/README.md\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27253240\n\nPulled By: ezyang\n\nfbshipit-source-id: 964df951ea8b52fa72937f3cc66aeaf49a702e6f", "pr_number": "54470", "files_changed": ["BUILD.bazel", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/native/README.md", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "aten/src/ATen/native/native_functions.yaml", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "test/test_dispatch.py", "tools/autograd/gen_variable_type.py", "tools/codegen/dest/register_dispatch_key.py", "tools/codegen/gen.py", "tools/codegen/model.py", "torch/_python_dispatcher.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/utils/python_dispatch.cpp"], "labels": ["Merged", "cla signed"]}, "db3a9d7f8a": {"title": "Fix __torch_function__ tests. (#54492)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54492\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27292567\n\nPulled By: ezyang\n\nfbshipit-source-id: dc29daea967c6d8aaf63bdbcb4aff0bb13d7a5f7", "pr_number": "54492", "files_changed": ["test/test_overrides.py", "torch/nn/functional.py"], "labels": ["Merged", "cla signed", "open source"]}, "0435059ddf": {"title": "docs: fix docstring signature in `all_reduce_multigpu` (#54665)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/43500\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54665\n\nReviewed By: ezyang\n\nDifferential Revision: D27340481\n\nPulled By: rohan-varma\n\nfbshipit-source-id: d53c36b41dd26c7a791d3674a5b4b67daaadae13", "pr_number": "54665", "files_changed": ["torch/distributed/distributed_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "df70e2fde5": {"title": "Refactor get analytical jacobian (#54049)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54049\n\nThe goal of this is to factor out the core logic of getting the analytical jacobian which is effectively doing `f(grad_out) = grad_out^T J = grad_input`. This allows us to test a lot of logic that was not possible before because now we can replace f with whatever we want in order to simulate potential issues that gradcheck is designed to catch.\n\nEdit: I realize a lot of things this PR was originally aiming to allow is actually possible with hooks, hence the tests have already been added in a earlier PR in the stack. But this is still slightly useful for reducing code duplication when adding the new fast gradcheck code (more details below)\n\nAfter this change, `get_analytical_jacobian` is only responsible for gathering a list of rows that are later combined into a single Jacobian tensor. This means we don't have to perform any checks for correctness of the dtypes/size at this step\n\nWe factor out that logic into a separate function, `combine_jacobian_rows`, which handles the list of rows -> single Tensor step for each jacobian, and the error checking it entails. (This allows this code to be shared between the fast/slow versions.)\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27307240\n\nPulled By: soulitzer\n\nfbshipit-source-id: 65bb58cda000ed6f3114e5b525ac3cae8da5b878", "pr_number": "54049", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "module: deprecation"]}, "14a2501786": {"title": "Update max-version in setup.py to 3.9 (#54690)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54690\n\nReviewed By: seemethere\n\nDifferential Revision: D27330462\n\nPulled By: malfet\n\nfbshipit-source-id: db332acf5aa5bff67af2bef777935f2387bc963c", "pr_number": "54690", "files_changed": ["setup.py"], "labels": ["Merged", "cla signed"]}, "ee73c752c6": {"title": "Delete unnecessary empty file (#54796)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54796\n\nReviewed By: albanD\n\nDifferential Revision: D27370733\n\nPulled By: iramazanli\n\nfbshipit-source-id: 5f78e9250a545afb91b4bc7b14daa7135a2b6a1b", "pr_number": "54796", "files_changed": [".nojekyll"], "labels": ["Merged", "cla signed"]}, "593295daac": {"title": "Migrate kernels with TensorOptions to C10 full dispatcher (#54539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54539\n\nCodemod commands generated by https://github.com/pytorch/pytorch/pull/54468\n\nghstack-source-id: 125018630\n\n# Facebook:\nThe following 2 files are changed on fb side:\n```\n// Should be hidden\n```\n\nTest Plan: buck build //caffe2/aten/...\n\nReviewed By: smessmer\n\nDifferential Revision: D27273744\n\nfbshipit-source-id: 35c1bff63189477645008caaf0dc794096e3fcc4", "pr_number": "54539", "files_changed": ["aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/QuantizedLinear.cpp", "aten/src/ATen/native/SobolEngineOps.cpp", "aten/src/ATen/native/SoftMax.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/SummaryOps.cpp", "aten/src/ATen/native/TensorConversions.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/MultinomialKernel.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/layer_norm_kernel.cu", "aten/src/ATen/native/cudnn/ConvShared.cpp", "aten/src/ATen/native/cudnn/RNN.cpp", "aten/src/ATen/native/group_norm.cpp", "aten/src/ATen/native/layer_norm.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/TensorFactories.cpp", "aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qchannel_shuffle.cpp", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "triaged"]}, "416ba5c48f": {"title": "Merge CUDA Streams and Events (#53902)", "body": "Summary:\n-----------\n- Updates current_stream and default stream API's to take `optional[device]` argument\n- Adds parsing logic to replace `torch.cuda.Stream` and `torch.cuda.Event` -> `torch.classes.cuda.Stream` and `torch.classes.cuda.Event` for JIT\n- Merges StreamContext manager for both Eager and JIT.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53902\n\nTest Plan:\n------\nRun JIT tests:\npython test/test_jit.py -v TestCUDA\n\nRun eager tests:\npython test/test_cuda.py -v TestCuda\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27285996\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 45d9fee9a582b5f4c82330f5f99eb88584804270", "pr_number": "53902", "files_changed": ["test/cpp/jit/tests_setup.py", "test/jit/test_cuda.py", "torch/_utils.py", "torch/csrc/jit/cuda/cuda.h", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/register_cuda_ops.cpp", "torch/cuda/__init__.py", "torch/cuda/_utils.py", "torch/jit/__init__.py", "torch/jit/cuda.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "394b720e38": {"title": "Fix raw_deleter() bug with PYTORCH_NO_CUDA_MEMORY_CACHING=1 (#54775)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54775\n\nThanks danpovey for reporting. Fixes https://github.com/pytorch/pytorch/issues/54770\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27363730\n\nPulled By: ezyang\n\nfbshipit-source-id: 81777aff7d9194b060fb076ef97cf788f2a4f43e", "pr_number": "54775", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp"], "labels": ["Merged", "cla signed"]}, "f22bad752d": {"title": "Move some variable ops out of VariableTypeManual. (#54459)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54459\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D27321820\n\nPulled By: ailzhang\n\nfbshipit-source-id: e45392d2332f3c4bc31f20a500f58cdcd75f9ddf", "pr_number": "54459", "files_changed": ["aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/core/VariableHooksInterface.h", "aten/src/ATen/native/VariableMethodStubs.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/TensorBody.h", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "ba1f640928": {"title": "Optimize memory usage in logsumexp_out (#51239)", "body": "Summary:\nPartly fixes https://github.com/pytorch/pytorch/issues/31837.\n\n### Update: This is ready for review.\n\nCurrently, `torch.logsumexp(input, out=result)` internally creates 2 intermediate tensors with same shape as `input` tensor. This causes unnecessary OOM problems when tensor size is large.\n\nThese tensors come from the following:\n1. `self - maxes` will create a new tensor with shape of `self`\n2. `at::exp` will create another tensor with the shape of `self`\n\nTo get rid of this problem, we can use `(self-maxes).exp_()` that performs exp operation in-place. This would reduce memory need from `~3 x input.shape` to `~2 x input.shape` (`self-maxes` is still there)\n\nI think we can't get rid of having a single intermediate tensor with shape of `input` because of `self - maxes` as we have to keep `self` intact. The only scenario would be to have a `torch.Tensor.logsumexp_` method that can do in-place operations on tensor itself. However, I didn't see any in-place method example for reduction operations, so it might not be a good fit.\n\nThis is my first contribution here, please let me know if I'm missing anything!\n\nThanks!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51239\n\nReviewed By: anjali411\n\nDifferential Revision: D27363147\n\nPulled By: ezyang\n\nfbshipit-source-id: 696fa8764b74386a80b4aa33104f3f9ca57ed712", "pr_number": "51239", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "0e320ddb36": {"title": "Lazily initialize alias db constant prop (#54640)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54640\n\nIf we are running constant propagation on a graph that doesn't have any operators with constant inputs and any mutable inputs/outputs, we do not need to initialize an alias db. This is going to be used to speed up symbolic shape analysis.\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27340863\n\nPulled By: eellison\n\nfbshipit-source-id: 087b2a33b42c58fa5dae405d652b056d0f1d72e7", "pr_number": "54640", "files_changed": ["torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/constant_propagation.h", "torch/quantization/quantize_jit.py", "torch/testing/_internal/jit_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "2620bce42a": {"title": "[ROCM] load only hipfft separately past rocm4.1 (#54349)", "body": "Summary:\nThis PR is a follow up to https://github.com/pytorch/pytorch/pull/53408.\n\nIt only loads hipfft if the version is rocm 4.1 or after and stops loading rocfft. This was done to resolve some issues observed in our internal ci due to conflicts.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54349\n\nReviewed By: ezyang\n\nDifferential Revision: D27374252\n\nPulled By: ngimel\n\nfbshipit-source-id: 724e80df5011ea8fabd81739e18ae8a13d3a7ea0", "pr_number": "54349", "files_changed": ["cmake/Dependencies.cmake", "cmake/public/LoadHIP.cmake"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "71201340c6": {"title": "Remove 13 hacky wrapper not required (#54793)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54793\n\nghstack-source-id: 125033229\n\nTest Plan:\nbuck build //caffe2/aten/...\nBUILD_TENSOREXPR_BENCHMARK=ON BUILD_STATIC_RUNTIME_BENCHMARK=ON python setup.py install\n\nGenerated `build/aten/src/ATen/NativeFunctions.h` is same\n\nReviewed By: smessmer\n\nDifferential Revision: D27369943\n\nfbshipit-source-id: 5171bad44290a4ecf62a8f4deab17252c5bd0852", "pr_number": "54793", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "316804e373": {"title": "[test_c10d] Add wait in nccl high priority stream test (#54714)", "body": "Summary:\nAdd wait in test_pass_nccl_options_high_priority_stream\nafter the all reduce operation.\nWithout wait, the allreduce operation might be still running and the\ncomparison of result might not be valid.\n\nSigned-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54714\n\nReviewed By: ezyang\n\nDifferential Revision: D27379544\n\nfbshipit-source-id: 6393d25f8f3d5635c5d34c9b3aac8b801315b48e", "pr_number": "54714", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "f612d4eb58": {"title": "Add 'remote_parameters' and 'get_module_rref' to RemoteModule docs. (#54645)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54645\n\nHad to replace RRef[..] with just RRef in the return signature since\nsphynx seemed to completely mess up rendering RRef[..]\nghstack-source-id: 125024783\n\nTest Plan: View locally.\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27314609\n\nfbshipit-source-id: 2dd9901e79f31578ac7733f79dbeb376f686ed75", "pr_number": "54645", "files_changed": ["docs/source/rpc.rst", "torch/distributed/nn/api/remote_module.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8eb896ce99": {"title": "Improve error message while setting error twice. (#54464)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54464\n\nIn case where we accidentaly set an error twice on a Future, we get a\ncryptic error like this:\n\n```\nException in thread pool task: !completed() INTERNAL ASSERT FAILED at \"aten/src/ATen/core/ivalue_inl.h\":534, please report a bug to PyTorch.\n```\n\nThis PR, updates the error message to include some additional information about\nwhat the previous error was.\nghstack-source-id: 125039478\n\nTest Plan:\n1) unit test\n2) waitforbuildbot\n\nReviewed By: swolchok\n\nDifferential Revision: D27249758\n\nfbshipit-source-id: 517cf3837fb7b7821312e101e8813844c188f372", "pr_number": "54464", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/test/ivalue_test.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "d63dd07f06": {"title": "Add JIT support for cmath unary ops (#54089)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54089\n\n**This PR adds:**\n1. support for the following [cmath](https://docs.python.org/3/library/cmath.html) functions:\n     - Power and logarithmic functions (`cmath.{exp, log, log10, sqrt}`)\n     - Trigonometric functions (`cmath.{sin, cos, tan, asin, acos, atan}`)\n     - Hyperbolic functions (`cmath.{sinh, cos, tanh, asinh, acosh, atanh}`)\n     - `cmath.phase()`\n2. `abs()`\n\n**Future work:**\n1. support\n    - `cmath.{polar, rect}`\n    - classification functions (`cmath.{isfinite, isnan, isinf, isclose}`)\n    - constants (`cmath.{pi, e, inf, nan, infj, nanj}`)\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27339149\n\nPulled By: anjali411\n\nfbshipit-source-id: fe1a019c95adbc9f27f7948eb28c0c3b93d8c026", "pr_number": "54089", "files_changed": ["test/jit/test_complex.py", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/jit/_builtins.py"], "labels": ["Merged", "cla signed", "module: complex", "oncall: jit"]}, "1d5cc6c53d": {"title": "Move requires_grad_/backward out of VariableTypeManual. (#54543)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54543\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D27321819\n\nPulled By: ailzhang\n\nfbshipit-source-id: 991c83e134d109e270c872b4b79026dcb732d77a", "pr_number": "54543", "files_changed": ["aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/core/VariableHooksInterface.h", "aten/src/ATen/native/VariableMethodStubs.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/TensorBody.h", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "65781f94ad": {"title": "Enable faulthandler for distributed tests. (#54531)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54531\n\nEnabling faulthandler will intercept signals like SIGSEGV, SIGFPE,\nSIGABRT, SIGBUS and SIGKILL and dump the entire python traceback before the\nprocess goes down.\n\nThis can help us in debugging flaky tests where a process crashes and we need\nto debug what happened.\nghstack-source-id: 125045894\n\nTest Plan:\n1) Tested locally to see traceback is produced.\n2) waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27271048\n\nfbshipit-source-id: ca12125a9da6cdfc7bac5619ad1c7e116666014b", "pr_number": "54531", "files_changed": ["torch/testing/_internal/common_distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed"]}, "6eaf96961d": {"title": "[codemod] fix tautological imports", "body": "Test Plan: waitforsandcastle\n\nReviewed By: koronthaly\n\nDifferential Revision: D27310963\n\nfbshipit-source-id: 9ca0a6468e00d481b1583ab98578dc70f80bb3bf", "pr_number": null, "files_changed": ["caffe2/python/examples/imagenet_trainer.py", "torch/__init__.py"], "labels": []}, "e5634f5f25": {"title": "More types for torch (#54037)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54037\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D27054755\n\nfbshipit-source-id: f21985e201b35bdb83269595cdcf5e1e64837e52", "pr_number": "54037", "files_changed": ["torch/distributed/__init__.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "e70f3d1189": {"title": "Nasty little hack to preserve NotImplementedError raised in interpreter (#54627)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54627\n\nThis is the simplest little fix to get interpreter to preserve\nNotImplementedError, so that the test suite doesn't start choking\non meta tensors not working in interpreter.  It is sound and correct\nbut doesn't work for other c10::Error subclasses with special handling.\nA more proper fix is requested at\nhttps://github.com/pytorch/pytorch/issues/54612\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: wenleix, ngimel\n\nDifferential Revision: D27328666\n\nPulled By: ezyang\n\nfbshipit-source-id: 483bef062de5a907d20e2d9e25eafe2d5197cf8d", "pr_number": "54627", "files_changed": ["torch/csrc/jit/runtime/interpreter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "90e70ace9b": {"title": "Fix some more native_functions.yaml mistakes (#54597)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54597\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27328667\n\nPulled By: ezyang\n\nfbshipit-source-id: 79ddfda28e05d4cbcbed37a969f2577ea7c292fb", "pr_number": "54597", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "05fa570bbc": {"title": "Add empty_generic, which allocates an empty tensor in a device-generic way (#54703)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54703\n\nThe trick is that this function takes in the allocator and dispatch key\nexplicitly; so you still need to know where to find the appropriate\nallocator.  The plan is to use this for meta tensors, but you probably\ncould also use this for empty_cuda as well.  It also takes in arguments\npost optional resolution, which can save a few instructions if you want\nto call this function directly (no uses yet).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27338814\n\nPulled By: ezyang\n\nfbshipit-source-id: 131c97922d245e9a2de547527123b464bddb2f99", "pr_number": "54703", "files_changed": ["aten/src/ATen/Utils.cpp", "aten/src/ATen/Utils.h"], "labels": ["Merged", "cla signed"]}, "b5ab348253": {"title": "Fix missing format string qualifier (#54705)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54705\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27338808\n\nPulled By: ezyang\n\nfbshipit-source-id: b21c931c2306e525bc444766bc203bb303868dbf", "pr_number": "54705", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "ed560cf2c6": {"title": "Disambiguate where 'Doesn't run' error message comes from (#54706)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54706\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: wenleix, anjali411\n\nDifferential Revision: D27338812\n\nPulled By: ezyang\n\nfbshipit-source-id: 76321e49f2a8140595c89775afbecd5717e31c2e", "pr_number": "54706", "files_changed": ["torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed"]}, "c9e0aab2bf": {"title": "Make convolution_overrideable default implementation raise NotImplementedError (#54707)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54707\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27338807\n\nPulled By: ezyang\n\nfbshipit-source-id: b18c39a09d130626709408c08034c260c34e2bc5", "pr_number": "54707", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["Merged", "cla signed"]}, "6445c9a1cb": {"title": "Avoid testing device in cdist when called in a \"Math\" context (#54708)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54708\n\ncdist advertises itself as Math but actually it error checks that the inputs\nare CPU/CUDA in cdist_impl, which is invoked from a composite context in some\nsituations. I worked around this by ensuring that when cdist_impl was called in\nthis way, we DON'T do the device checks, but the entire function is a little\njanky and I filed an issue about it at #54096\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27338813\n\nPulled By: ezyang\n\nfbshipit-source-id: 1202b02c58584a33dc32a5270e59e5f0af6398c5", "pr_number": "54708", "files_changed": ["aten/src/ATen/native/Distance.cpp"], "labels": ["Merged", "cla signed"]}, "c782949e17": {"title": "Make the fuser raise NotImplementedError when unknown device is hit (#54709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54709\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D27338815\n\nPulled By: ezyang\n\nfbshipit-source-id: 5cbaf3c19b9b85cc3f171f3b405d0cd98f832e65", "pr_number": "54709", "files_changed": ["torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "f067972527": {"title": "Make memory overlap a little less precise so it works with null data ptr (#54710)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54710\n\nI'm going to make meta tensors have storage (but DataPtr is always\nnull) so that I can accurately report memory overlap error checking, but\nI now have a problem which is that if memory overlap test looks at the\nactual data pointer, everything is going to look like it aliases!  A\nmore conservative test is to just see if the Storage objects themselves\nalias, and assume that the data pointers are unique if they don't.\n\nThe loss of precision stems from if you unsafely have two distinct\nstorage objects that point to the same data pointer.  This situation\nis pretty rare and so I think it is worth it (and I am hoping no tests\ntrigger by this.)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27338810\n\nPulled By: ezyang\n\nfbshipit-source-id: 5ebaf81c22824494c47c1ae78982d9c0e5cba59f", "pr_number": "54710", "files_changed": ["aten/src/ATen/MemoryOverlap.cpp"], "labels": ["Merged", "cla signed"]}, "2309173143": {"title": "Compute Tensor::toString() without reference to backend (#54711)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54711\n\nJust print the dispatch key directly.  The format here doesn't really\nmake sense but you'll still get something like CPUFloatTensor (because\nthe dispatch key is just CPU).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27338811\n\nPulled By: ezyang\n\nfbshipit-source-id: f459c5f7c006c06df4913ab33697eae89c46d83f", "pr_number": "54711", "files_changed": ["aten/src/ATen/core/Tensor.cpp"], "labels": ["Merged", "cla signed"]}, "7caa464631": {"title": "Implement public API InferenceMode and its error handling (#53343)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53343\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang, nikithamalgifb\n\nDifferential Revision: D26973911\n\nPulled By: ailzhang\n\nfbshipit-source-id: 0ebdac7a3cd554822d26d5a40f539b6e2aaec61d", "pr_number": "53343", "files_changed": ["c10/core/InferenceMode.cpp", "c10/core/InferenceMode.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/core/impl/LocalDispatchKeySet.h", "test/cpp/api/CMakeLists.txt", "test/cpp/api/grad_mode.cpp", "test/cpp/api/inference_mode.cpp", "test/cpp/api/support.h", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_type.py", "tools/codegen/model.py", "torch/csrc/autograd/InferenceMode.h", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/functions/utils.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/script.h"], "labels": ["Merged", "Reverted", "cla signed"]}, "695eef05a4": {"title": "optimizer exploration - v1 and v2 + fix position_weighted optimizer + decoupled weight decay (#54042)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54042\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53881\n\n1. Fix position_weighted optimizer: Position weighted layer uses default optimizer but is actually gradient_slice, which will cause problem if we do not handle it properly in the new optimizier. The solution is to use sparseadagrad when it is gradient_slices.\n2. Optimizer implementation of v1 and v2: using 1st momentum with/without bias_correction.\n3. also implemented decoupled weight decay in the new optimizer.\n\nTest Plan:\nbuck test //caffe2/caffe2/fb/dper/layer_models/tests/split_1:sparse_nn_test_2 -- test_mlp_optimization\n\nbuck test //caffe2/caffe2/python:optimizer_test -- TestDecayAdagrad\n\nbuck test //caffe2/caffe2/python/operator_test:decay_adagrad_test\n\nctr_mbl_feed work flow: f255731660\noc work flow: f255739503\n\nReviewed By: 0x10cxR1\n\nDifferential Revision: D26839668\n\nfbshipit-source-id: 2b6881c1a88540ef5766be40f5e80001257e2199", "pr_number": "54042", "files_changed": ["caffe2/python/operator_test/decay_adagrad_test.py", "caffe2/python/optimizer.py", "caffe2/python/optimizer_test.py", "caffe2/sgd/decay_adagrad_op.cc", "caffe2/sgd/decay_adagrad_op.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "eec48303c0": {"title": "Make index_add take a scalar argument alpha (#54176)", "body": "Summary:\n```\nindex_add(Tensor self, int dim, Tensor index, Tensor source) -> Tensor\n```\nnow becomes\n```\nindex_add(Tensor self, int dim, Tensor index, Tensor source, Scalar alpha=1) -> Tensor\n```\nGenerally, this sounds useful and harmless, and inside PyTorch, we are already needing this feature in `add_out_dense_sparse_cuda`, see the `SparseCUDATensorMath.cu` change in this PR.\n\n**Test not added yet. Will add if after discussion we believe this is a good idea.**\n- [ ] TODO: add test\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54176\n\nReviewed By: ngimel\n\nDifferential Revision: D27319198\n\nPulled By: mruberry\n\nfbshipit-source-id: fe43be082d1230c87c5313458213d5252be2ff23", "pr_number": "54176", "files_changed": ["aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "test/test_torch.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "enhancement", "open source", "triaged"]}, "0527d14248": {"title": "[numpy] Add torch.take_along_dim (#52833)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/38349\n\nWrapper around the existing `torch.gather` with broadcasting logic.\n\nTODO:\n* [x] Add Doc entry (see if phrasing can be improved)\n* [x] Add OpInfo\n* [x] Add test against numpy\n* [x] Handle broadcasting behaviour and when dim is not given.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52833\n\nReviewed By: malfet\n\nDifferential Revision: D27319038\n\nPulled By: mruberry\n\nfbshipit-source-id: 00f307825f92c679d96e264997aa5509172f5ed1", "pr_number": "52833", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "test/test_indexing.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: numpy", "open source", "triaged"]}, "01b1557014": {"title": "enable bf16 vec copy (#54671)", "body": "Summary:\nEnable bf16 vectorized copy.\n\nBFloat16's copy get 2x performance for fp32 as our expectation.\n\nBFloat16's vec copy dose not show performance gain compare with scalar version with op benchmark. This should caused by the memory system of operator. The system will really \"read/write\" a scalar at one time, although the code is written in scalar version.\n\nbenchmarks code:\n```\nimport torch\nimport torch.utils.benchmark as benchmark\n\n# x = torch.empty(10 * 18304 * 1024 * 16, dtype=torch.bfloat16)\nx = torch.empty(10 * 18304 * 1024 * 16, dtype=torch.float)\ndef copy(tensors):\n    for t in tensors:\n        x.copy_(t)\n\ntensors = []\nfor i in range(2):\n    # l3 cache size 36608k = 18304 bfloat16 * 2 byte(per bfloat16)\n    # tensors.append(torch.rand(10 * 18304 * 1024 * 16).bfloat16())\n    tensors.append(torch.rand(10 * 18304 * 1024 * 16))\n\nt0 = benchmark.Timer(\n    stmt='copy(tensors)',\n    setup='from __main__ import copy',\n    globals={'tensors': tensors},\n    num_threads=1)\n\nprint(t0.timeit(20))\n```\n\nBefore this comit:\nfp32:\n  3.84 s\n  1 measurement, 20 runs , 1 thread\nbf16:\n  1.89 s\n  1 measurement, 20 runs , 1 thread\n\nAfter:\nfp32:\n  3.71 s\n  1 measurement, 20 runs , 1 thread\nbf16:\n  1.85 s\n  1 measurement, 20 runs , 1 thread\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54671\n\nReviewed By: ailzhang\n\nDifferential Revision: D27325350\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 1a3b8ca17b4c60dbb3e86bf196f63e0a05228c65", "pr_number": "54671", "files_changed": ["aten/src/ATen/native/cpu/CopyKernel.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "07350da3b4": {"title": "enable bf16 for cat serial kernel (#54674)", "body": "Summary:\ncat 10 2-D tensors at dim=1\n\n|                     | shape                    | serial kernel | copy kernel |\n| ------------ | ------------- | ------------ | ------------- |\n| fp32          | 1024 * 16k           |   105.45 ms    | 102.41 ms     |\n| fp32          | 1024 * (100 + i)  |   324.75 us     | 448.66 us      |\n| bf16          | 1024 * 16k           |   49.82 ms       | 51.39 ms       |\n| bf16          | 1024 * (100 + i)  |   164.74 us      | 244.64 us      |\n\ni = {0, ..., 9}\n\nbenchmark code\n```\nimport torch\nimport torch.utils.benchmark as benchmark\ndef cat(*args, dim=0):\n    return torch.cat(args, dim)\n\ntensors = []\nfor i in range(10):\n    tensors.append(torch.rand(1024, 16 *1024))\n    # tensors.append(torch.rand(1024, 16 *1024).bfloat16())\n    # tensors.append(torch.rand(1024, 100 + i))\n    # tensors.append(torch.rand(1024, 100 + i).bfloat16())\n\nt0 = benchmark.Timer(\n    stmt='cat(*tensors, dim=1)',\n    setup='from __main__ import cat',\n    globals={'tensors': tensors},\n    num_threads=1)\n\nprint(t0.blocked_autorange())\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54674\n\nReviewed By: ailzhang\n\nDifferential Revision: D27325347\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 7a0f4bf8d92dbf8e725fdd2e8a2c901188811d6f", "pr_number": "54674", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/cpu/CatKernel.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "b7c5d57563": {"title": "[testing] support op with args/kwargs in test_unary_ufunc (#52194)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51242\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52194\n\nReviewed By: ngimel\n\nDifferential Revision: D27385139\n\nPulled By: mruberry\n\nfbshipit-source-id: 63118dee33a138ef13810465d2d2d9fa194dfb28", "pr_number": "52194", "files_changed": ["test/test_ops.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "9e6877c5c5": {"title": "Port torch.outer method_tests() to OpInfos (#54798)", "body": "Summary:\nAn attempt to make an OpInfo-based test for torch.outer (aka toch.ger).\n\nAs a part of https://github.com/pytorch/pytorch/issues/54261 effort.\n\nmruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54798\n\nReviewed By: ngimel\n\nDifferential Revision: D27384891\n\nPulled By: mruberry\n\nfbshipit-source-id: 0c90f84a388d2addc8de37d0c1713d8598211555", "pr_number": "54798", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "4e5af53d29": {"title": "Deprecate legacy constructor `torch.Tensor()` (#54414)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/47112\n\nThis pull request is the final step in [the proposed plan](https://github.com/pytorch/pytorch/issues/47112#issuecomment-789972007) for deprecating `torch.Tensor()` constructor. Specifically, it **updates the docs and throws `TORCH_WARN_ONCE` if someone uses `torch.Tensor()`**.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54414\n\nReviewed By: ailzhang\n\nDifferential Revision: D27325267\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 5442572603d340b89e8cc5a886a330dd9b13550a", "pr_number": "54414", "files_changed": ["docs/source/tensors.rst", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "86b1f4e9f2": {"title": "fix silent correctness bug with channels_last usage of upsample cuda kernels (#54744)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54744\n\nFixes https://github.com/pytorch/pytorch/issues/54590\n\nAfter the porting the upsample operators to be structured, they now forward memory_format information to the output. This is a problem for the cuda kernels, which are not implemented to deal with `torch.channels_last` memory format. The operators are:\n* upsample_nearest2d\n* upsample_bilinear2d\n* upsample_nearest3d\n* upsample_trilinear3d\n\nThis fix just allocates a temporary, contiguous output tensor when that happens, writes the results to the temporary and copies the results back to the output tensor.\n\nI held off on adding tests to get the fix out quickly, but I wrote a script and ran some manual tests, that basically just asserts that the outputs are the same for cpu and cuda, for some threshold. I ran it for all 4 operators:\n```\nimport torch\n\ndef basically_equal(t1, t2):\n    epsilon = 1e-4\n    diffs = torch.abs(t1 - t2)\n    print(torch.all(diffs < 1e-4))\n\n# upsample 2d\na = torch.arange(48).reshape(2, 2, 3, 4).contiguous(memory_format=torch.channels_last).float()\n\nout_cpu = torch.nn.functional.interpolate(a, scale_factor=2, mode='nearest')\nout_cuda = torch.nn.functional.interpolate(a.to('cuda'), scale_factor=2, mode='nearest')\n\nbasically_equal(out_cpu, out_cuda.to(\"cpu\"))\n\nout_cpu = torch.nn.functional.interpolate(a, scale_factor=2, mode='bilinear', align_corners=True)\nout_cuda = torch.nn.functional.interpolate(a.to('cuda'), scale_factor=2, mode='bilinear', align_corners=True)\n\nbasically_equal(out_cpu, out_cuda.to(\"cpu\"))\n\n# upsample 3d\na = torch.arange(96).reshape(2, 2, 2, 3, 4).contiguous(memory_format=torch.channels_last_3d).float()\n\nout_cpu = torch.nn.functional.interpolate(a, scale_factor=3, mode='nearest')\nout_cuda = torch.nn.functional.interpolate(a.to('cuda'), scale_factor=3, mode='nearest')\n\nbasically_equal(out_cpu, out_cuda.to(\"cpu\"))\n\nout_cpu = torch.nn.functional.interpolate(a, scale_factor=3, mode='trilinear', align_corners=True)\nout_cuda = torch.nn.functional.interpolate(a.to('cuda'), scale_factor=3, mode='trilinear', align_corners=True)\n\nbasically_equal(out_cpu, out_cuda.to(\"cpu\"))\n```\n\nprints\n```\ntensor(True)\ntensor(True)\ntensor(True)\ntensor(True)\n```\n\nOne thing that was weird- `upsample_bilinear2d` and `upsample_trilinear3d` were only accurate across cpu/cuda with an epsilon of `1e-4`. That tentatively sounds close enough to say that cuda isn't \"wrong\" (?), but that's not exactly \"equal\"... and I also ran the script before my change, and `bilinear2d` and `trilinear3d` were also the same across cpu/cuda with an epsilon of `1e-4`.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27351393\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b33f46e4855dc8b49b363770190b639beebbf5a7", "pr_number": "54744", "files_changed": ["aten/src/ATen/native/cuda/UpSampleNearest2d.cu", "aten/src/ATen/native/cuda/UpSampleNearest3d.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed"]}, "6d2bf76bba": {"title": "Using latest windows CUDA exe (#54596)", "body": "Summary:\nUsing latest cuda_11.2.2_461.33_win10 to fix cu112 test failures.\nthis should fix https://github.com/pytorch/pytorch/issues/51980.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54596\n\nReviewed By: seemethere\n\nDifferential Revision: D27365008\n\nPulled By: walterddr\n\nfbshipit-source-id: 682e79888d9f10c0a5b227d66165ea50c47ba0f9", "pr_number": "54596", "files_changed": [".circleci/config.yml", ".circleci/scripts/windows_cuda_install.sh", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "ci/all", "cla signed"]}, "2fd1eb3a9f": {"title": "make all arguments in test_history.py optional kwarg (#54797)", "body": "Summary:\nThis is to make it more flexible to be reused when pulling test stats other than by-test-case.\nAlso it makes it less likely to use it wrong with positional arguments.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54797\n\nTest Plan: see the updated tools/test/test_test_history.py examples.\n\nReviewed By: samestep\n\nDifferential Revision: D27371903\n\nPulled By: walterddr\n\nfbshipit-source-id: 0ee02d654684315b44f5942904b857053d27e954", "pr_number": "54797", "files_changed": ["tools/stats_utils/s3_stat_parser.py", "tools/test_history.py"], "labels": ["Merged", "cla signed"]}, "94efb48e16": {"title": "Adds the cfloat dtype to the eager and jit variant consistency tests (#54854)", "body": "Summary:\nPer title. One skip for addmm was needed. Either it or the jit test doesn't seem to handle a complex literal properly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54854\n\nReviewed By: anjali411\n\nDifferential Revision: D27395651\n\nPulled By: mruberry\n\nfbshipit-source-id: 0bfadf0a8500f26d3a89f56f104fb44561f594d9", "pr_number": "54854", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "1f36ce6e4d": {"title": "Restore storage on meta tensors; increase meta coverage (#53973)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53973\n\nTwo parts to this PR; I had to put them together because adding support for X causes more test code to be exercised, which in turn may require a fix for Y.\n\nThe first part is restoring the concept of storage to meta tensors.  Previously, meta tensors had a nullptr storage (e.g., `meta_tensor.storage()` is an error.) As I was increasing the coverage of meta tensors, I started running into test cases (specifically memory overlap tests) that were failing because not having storage meant I couldn't check for memory overlap. After some discussion, we decided that it would make sense for meta tensors to model this as well (we already model strides, so getting accurate view information also seems useful). This PR does that by:\n\n* Rewrite all of the factory functions in MetaTensor.cpp to use the generic versions (which are very carefully written to not actually poke at the data pointer, so everything works out). The key idea here is we give meta tensors a special allocator, MetaAllocator, which always returns a nullptr even if you ask for a nonzero number of bytes. resize_ is also made generic; the normal variant can be used directly rather than having to instruct it to avoid resizing storage\n* Turn on memory overlap checking in TensorIterator even for meta tensors\n* Although meta tensors now have storage, the concept of meta storage is NOT exposed to Python land (as it would imply I would have to codegen MetaFloatStorage, MetaDoubleStorage, etc. classes). So `x.storage()` still raises an error and I have a cludge in `__deepcopy__` to break storage sharing upon deep copy (this is wrong, but no tests exercise this at the moment).\n\nThe second part is adding more support for the most used functions in the test suite.\n\n* Inplace operations have very simple meta functions. I added `fill_`, `zero_`, `random_`, `uniform_` and `normal_`. In the case of random, I take advantage of pbelevich's templates for defining random kernels, so that I can reuse the common scaffolding, and then just register a noop stub that actually does the RNG. (Look, another structured kernels tiny variant!)\n* `copy_` is now implemented. Copying into a meta tensor is always OK, but copying out of a meta tensor raises an error (as we don't know what the \"correct\" data to copy out is in this case)\n* `empty_strided` usage from structured kernels now is implemented (TBH, this could have been done as soon as `empty_strided` was added)\n* Meta was missing in a few places in TensorOptions/DispatchKey utility functions, so I added them\n* Autograd engine now correctly homes meta tensors with CPU tensors (they have -1 device index so CUDA queues wouldn't work anyway)\n* `apply_`, `map_` and `map2_` are special cased to no-op on meta tensor self. These count as inplace operations too but they are implemented a little differently.\n\nGetting more meta function support triggers a number of bugs in the test suite, which I then fix:\n\n- Linear algebra functions sometimes don't report NotImplementedError because they get swallowed by catch all try blocks. This is tracked in https://github.com/pytorch/pytorch/issues/53739\n- dlpack obviously doesn't work with meta tensors, I just disabled the test\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: D27036572\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski, bdhirsh\n\nPulled By: ezyang\n\nfbshipit-source-id: 7005ecf4feb92a643c37389fdfbd852dbf00ac78", "pr_number": "53973", "files_changed": [".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/win-test.sh", "aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Fill.cpp", "aten/src/ATen/native/MetaTensor.cpp", "aten/src/ATen/native/README.md", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/native_functions.yaml", "c10/core/DispatchKeySet.cpp", "c10/core/StorageImpl.h", "c10/core/TensorOptions.h", "test/test_linalg.py", "test/test_torch.py", "tools/codegen/dest/register_dispatch_key.py", "torch/_tensor.py", "torch/csrc/DynamicTypes.cpp", "torch/csrc/autograd/engine.cpp", "torch/csrc/utils/tensor_apply.cpp"], "labels": ["Merged", "cla signed"]}, "3ddc6174da": {"title": "Raise error in clip_grad_norm_ if norm is non-finite (#53843)", "body": "Summary:\n**BC-breaking note**: This change throws errors for cases that used to silently pass. The old behavior can be obtained by setting `error_if_nonfinite=False`\n\nFixes https://github.com/pytorch/pytorch/issues/46849\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53843\n\nReviewed By: malfet\n\nDifferential Revision: D27291838\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 216d191b26e1b5919a44a3af5cde6f35baf825c4", "pr_number": "53843", "files_changed": ["test/cpp/api/nn_utils.cpp", "test/test_cuda.py", "test/test_nn.py", "torch/csrc/api/include/torch/nn/utils/clip_grad.h", "torch/nn/utils/clip_grad.py"], "labels": ["Merged", "cla signed", "module: NaNs and Infs", "module: bc-breaking", "module: nn", "module: norms and normalization", "open source", "triaged"]}, "717e70a824": {"title": "(BE) Refactor get-test-times-from-S3 into s3_stat_parser (#54808)", "body": "Summary:\nMoves more s3 parsing code to s3_stat_parser.py. This is another step in modularizing the parsing code more correctly. I will also be using this exact function in future slowTest code.\n\nAlso replaces some Any's in the code to be Report.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54808\n\nTest Plan:\n.pytorch-test-times generated before the code and after this code is the same.\nCI should pass, specifically the test tools GHA.\n\nReviewed By: walterddr\n\nDifferential Revision: D27375783\n\nPulled By: janeyx99\n\nfbshipit-source-id: bec28551668b2eb3fdd60d802200993e493eac83", "pr_number": "54808", "files_changed": ["test/run_test.py", "tools/stats_utils/s3_stat_parser.py"], "labels": ["Merged", "cla signed"]}, "68af6d9565": {"title": "Use custom sqrt if stdc++ does not fall back to C99 csqrt (#54820)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54820\n\ntemplate implementation of std::sqrt() in libstdc++ yields incorrect results for `std::complex(-std::abs(x), -0.0)`, see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=89991\nFor example:\n```\n#include <iostream>\n#include <complex>\nint main() {\n  std::cout << std::sqrt(std::complex<float>(-1.0f, -0.0f)) << std::endl;\n}\n```\nprints `(0, -1)` if libstdc++ is compiled to use C99 csqrt/csqrtf fallback, but `(0, 1)` if configured not to use it.\n\nTest Plan: CI\n\nReviewed By: luciang\n\nDifferential Revision: D27379302\n\nfbshipit-source-id: 03f614fdb7ff734139736a2a5f6872cee0173bee", "pr_number": "54820", "files_changed": ["c10/util/complex_math.cpp", "c10/util/complex_math.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "56f12e6199": {"title": "Add annotations to PRs from forks (#54779)", "body": "Summary:\nWe've been using [pytorch/add-annotations-github-action](https://github.com/pytorch/add-annotations-github-action) to add annotations to PRs when they fail Flake8 or clang-tidy. Up until now, though, that functionality has only worked on PRs in pytorch/pytorch itself, not on PRs from forks. This PR fixes that using a technique from [this GitHub blog post](https://securitylab.github.com/research/github-actions-preventing-pwn-requests/) (also linked in a comment in this diff).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54779\n\nTest Plan: janeyx99 and I tested this in the same GitHub repo used to test https://github.com/pytorch/pytorch/issues/54685 and https://github.com/pytorch/pytorch/issues/54693, including with PRs from forks.\n\nReviewed By: walterddr\n\nDifferential Revision: D27364777\n\nPulled By: samestep\n\nfbshipit-source-id: a830d372d7bb3b2529fc633b707b44f2b6cf9baa", "pr_number": "54779", "files_changed": [".github/workflows/add_annotations.yml", ".github/workflows/lint.yml"], "labels": ["Merged", "Reverted", "cla signed"]}, "12a454788b": {"title": "docs: fix parameter in `torch.take` (#54667)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/43495\nhttps://11812612-65600975-gh.circle-artifacts.com/0/docs/generated/torch.take.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54667\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328252\n\nPulled By: zou3519\n\nfbshipit-source-id: 5812ebdaba063ca0a9c0f4a9becd00a570d84d30", "pr_number": "54667", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "open source"]}, "84232b762b": {"title": "docs: add `reset_peak_memory_stats` in cuda.rst (#54668)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/41808\nhttps://11812999-65600975-gh.circle-artifacts.com/0/docs/cuda.html\n\nOne question: does `reset_peak_stats` exist in `torch.cuda` ?\nI can't find anywhere.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54668\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328444\n\nPulled By: zou3519\n\nfbshipit-source-id: 098024d43da98e3249aa9aa71cb10126095504a4", "pr_number": "54668", "files_changed": ["docs/source/cuda.rst", "torch/cuda/memory.py"], "labels": ["Merged", "cla signed", "open source"]}, "475251631b": {"title": "docs: reference links to serialization.html (#54659)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/54311\nhttps://11811979-65600975-gh.circle-artifacts.com/0/docs/generated/torch.save.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54659\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328281\n\nPulled By: zou3519\n\nfbshipit-source-id: b88d02e5407238a338d537d013a297ae9cdf922b", "pr_number": "54659", "files_changed": ["docs/source/notes/serialization.rst", "torch/serialization.py"], "labels": ["Merged", "cla signed", "open source"]}, "7eef0c3ab5": {"title": "docs: add functional group_norm (#54673)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/34209\nhttps://11813548-65600975-gh.circle-artifacts.com/0/docs/nn.functional.html#normalization-functions\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54673\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328211\n\nPulled By: zou3519\n\nfbshipit-source-id: 75c49849377047502962157239857ed99afe6d1e", "pr_number": "54673", "files_changed": ["docs/source/nn.functional.rst"], "labels": ["Merged", "cla signed", "open source"]}, "02f5c50828": {"title": "docs: separate autosummary for flatten layers (#54663)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/46881\nhttps://11815123-65600975-gh.circle-artifacts.com/0/docs/generated/torch.nn.Flatten.html#torch.nn.Flatten\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54663\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328367\n\nPulled By: zou3519\n\nfbshipit-source-id: de1651a670181db8ea8ab16624c17ba08a88eb5d", "pr_number": "54663", "files_changed": ["docs/source/nn.rst"], "labels": ["Merged", "cla signed", "open source"]}, "6dedecc77c": {"title": "docs: add `memory_format` in torch.empty (#54664)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/43504\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54664\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328504\n\nPulled By: zou3519\n\nfbshipit-source-id: 6c3e11473ada34f7e9fae7bae366328e50f71b0e", "pr_number": "54664", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "open source"]}, "74e01c1dd9": {"title": "docs: change to FloatTensor for `requires_grad=True` (#54658)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/54506\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54658\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328321\n\nPulled By: zou3519\n\nfbshipit-source-id: d29fa266a1cb2b6d8566055dfb6ce001edde9d96", "pr_number": "54658", "files_changed": ["torch/autograd/grad_mode.py"], "labels": ["Merged", "cla signed", "open source"]}, "9ef53f7e0f": {"title": "docs: remove extra backticks in `narrow_copy` (#54669)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/41590\nhttps://11813004-65600975-gh.circle-artifacts.com/0/docs/tensors.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54669\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328228\n\nPulled By: zou3519\n\nfbshipit-source-id: 9a4a9bc4b265b0e82cf91f94dbbfd842fc42cdcb", "pr_number": "54669", "files_changed": ["torch/_tensor_docs.py"], "labels": ["Merged", "cla signed", "open source"]}, "1551bcc670": {"title": "change logging.warn to logging.warning (#51727)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51727\n\nlogging.warn() is deprecated since Python 3.3 in favor of logging.warning()\n\nReviewed By: yinghai\n\nDifferential Revision: D25785598\n\nfbshipit-source-id: 391d834fe607cd571ee147445aa0a98910535099", "pr_number": "51727", "files_changed": ["caffe2/python/experiment_util.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "3187a71bbe": {"title": "[test] vc toolchain modification (#54589)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54502\nNeeds to be merged after https://github.com/pytorch/builder/pull/684\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54589\n\nReviewed By: walterddr\n\nDifferential Revision: D27402066\n\nPulled By: seemethere\n\nfbshipit-source-id: 68f92485d89edf2c3315de8c57447f180679c77d", "pr_number": "54589", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/vs_install.ps1", ".circleci/verbatim-sources/job-specs/binary-job-specs.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "ci/all", "cla signed", "open source", "triaged"]}, "67d44377e3": {"title": "Remove hacky wrapper for about 100 kernels (#54751)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54751\n\nCodemod commands generated by https://github.com/pytorch/pytorch/pull/54098\nghstack-source-id: 125141211\n\nTest Plan:\nbuck build //caffe2/aten/...\nBUILD_TENSOREXPR_BENCHMARK=ON BUILD_STATIC_RUNTIME_BENCHMARK=ON python setup.py install\n\nReviewed By: smessmer\n\nDifferential Revision: D27353530\n\nfbshipit-source-id: 66f83edfb1016ca0040fb603e43604cd2db02c4c", "pr_number": "54751", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/AdaptiveAveragePooling.cpp", "aten/src/ATen/native/AdaptiveAveragePooling3d.cpp", "aten/src/ATen/native/AdaptiveMaxPooling2d.cpp", "aten/src/ATen/native/AdaptiveMaxPooling3d.cpp", "aten/src/ATen/native/AveragePool2d.cpp", "aten/src/ATen/native/AveragePool3d.cpp", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Bucketization.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/FractionalMaxPool2d.cpp", "aten/src/ATen/native/FractionalMaxPool3d.cpp", "aten/src/ATen/native/GatedLinearUnit.cpp", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/LossMultiLabelMargin.cpp", "aten/src/ATen/native/NamedTensor.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/Bucketization.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/ScanKernels.cu", "aten/src/ATen/native/cuda/Shape.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/mkldnn/Pooling.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "51fa25443f": {"title": "[PyTorch][easy] Move strings into class_::defineMethod (#54533)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54533\n\nThere were some forgotten moves here. Since the values are\nnot otherwise used, let's just not give them names.\nghstack-source-id: 124674348\n\nTest Plan: CI\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27271991\n\nfbshipit-source-id: 793dd4576db659b3b9b973a4e09ee3133cf41dfe", "pr_number": "54533", "files_changed": ["torch/custom_class.h"], "labels": ["Merged", "cla signed"]}, "ff537b77ff": {"title": "[PyTorch][easy] Move more strings in torch::class_ (#54547)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54547\n\nThese arguments to `BuiltinOpFunction`'s ctor don't need to be copied.\nghstack-source-id: 124690196\n\nTest Plan: CI\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27277318\n\nfbshipit-source-id: 68f1f545ca977b2e1cabc91620da31719bf81e1a", "pr_number": "54547", "files_changed": ["torch/custom_class.h"], "labels": ["Merged", "cla signed"]}, "8cf97cbb55": {"title": "[ROCm] add 4.1 to nightly builds (#54635)", "body": "Summary:\nDepends on https://github.com/pytorch/builder/pull/685.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54635\n\nReviewed By: malfet\n\nDifferential Revision: D27368700\n\nPulled By: walterddr\n\nfbshipit-source-id: 35ac59bed8450e7e69b1a4ba74955a72d729487a", "pr_number": "54635", "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/config.yml"], "labels": ["Merged", "ci/binaries", "module: rocm", "open source", "triaged"]}, "7c8b0f2600": {"title": "Test torch.chain_matmul for complex dtype (#54885)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54885\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D27400936\n\nPulled By: anjali411\n\nfbshipit-source-id: 415d843d7c55f4d84a8e9faab926a4895e1544d0", "pr_number": "54885", "files_changed": ["test/test_autograd.py", "test/test_linalg.py"], "labels": ["Merged", "cla signed", "complex_autograd", "module: complex"]}, "4541f60390": {"title": "Gloo-only CPU-based monitored barrier (#53773)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53773\n\nCloses https://github.com/pytorch/pytorch/issues/52876\n\nImplements a barrier by doing send/recv to rank 0, and rank 0 waits for these requests and on timeout, throws an exception indicating which rank did not join in the given timeout.\n\nThis barrier is only intended for CPU use cases and built into process group gloo, and will be used for debugging synchronization/hang issues.\n\nTest Plan: Added UT\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26921357\n\nfbshipit-source-id: 7c16e861b4b8ea2bdd67a36b3de7b1029af7d173", "pr_number": "53773", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d185719455": {"title": "Expose dist.monitored_barrier() API (#53787)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53787\n\nPer title, exposes a python-based monitored barrier API that we can use as part of debugability and may be useful for user applications.\nghstack-source-id: 125124315\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D26965127\n\nfbshipit-source-id: 6c7826e63758462e3e5111f28cced54cba76a758", "pr_number": "53787", "files_changed": ["torch/distributed/distributed_c10d.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "028d2d6e63": {"title": "[NCCL] Enhance watchdog to log exceptions (#54557)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54557\n\nWhen looping through the nccl communicator cache checking for errors, enhance the watchdog to log exceptions that are set on the communicator.\n\nThis will allow for better debugability since the NCCL error will be logged when the watchdog receives errors for the communicators and aborts them appropriately.\n\nTested by forcing a NCCL error with NCCL_BLOCKING_WAIT=1 and verifying that the exception is indeed logged.\nghstack-source-id: 125124310\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27106699\n\nfbshipit-source-id: 1d2bd9f057a3796ce15dd8a4ce34cf6899eee45c", "pr_number": "54557", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d5564618d0": {"title": "[NCCL][Blocking Wait] Log set exceptions when checking for exceptions in (#54558)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54558\n\nIn blocking wait's polling synchronization loop, we frequently call checkAndSetException() as part of isCompleted() to check the status of nccl operations. It would be useful to log here in case we encounter any exceptions (which are later thrown by `checkAndThrowException`).\n\nAlso slightly refactors code previously added to make use of a helper function to get the error message given an `std::exception_ptr`.\nghstack-source-id: 125124314\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D27136202\n\nfbshipit-source-id: 256eb63c5c2a84be909722d3fd7377ad9303fa11", "pr_number": "54558", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "49b07ac5d1": {"title": "Enable complex autograd for `index`, add `index` and `index_put` OpInfos (#54562)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53605\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54562\n\nReviewed By: malfet\n\nDifferential Revision: D27300086\n\nPulled By: anjali411\n\nfbshipit-source-id: 23e8335e6e4c8f10888b5c54a040880c5b499215", "pr_number": "54562", "files_changed": ["test/test_fx.py", "tools/autograd/gen_variable_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "e829754992": {"title": "[PyTorch] Inline Tensor keyset-checking methods & similar getters (#54806)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54806\n\nThese are all very small key set checks (or similar getters\nlike `dtype()`, and we clearly want them to be inlinable -- we've even\nmade them non-virtual for perf in TensorImpl and said so in\ncomments. Don't make LTO work to figure that out.\nghstack-source-id: 125060650\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D27375016\n\nfbshipit-source-id: 5c3dbfa38fa493c8f7e0ac4e5acd3598d5896558", "pr_number": "54806", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.cpp"], "labels": ["Merged", "cla signed"]}, "1a0b77e7c4": {"title": "Suggest TORCH_LIBRARY_FRAGMENT in duplicate TORCH_LIBRARY error message (#54883)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54883\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27400592\n\nPulled By: ezyang\n\nfbshipit-source-id: 45d6a3a890979cce1b07e933f5335f3fa3a375a2", "pr_number": "54883", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp"], "labels": ["Merged", "cla signed"]}, "f4dfa02c03": {"title": "Add documentation for torch.jit.Attribute and torch.jit.annotate (#54485)", "body": "Summary:\nThis is to prepare for new language reference spec that needs to describe `torch.jit.Attribute` and `torch.jit.annotate`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54485\n\nReviewed By: SplitInfinity, nikithamalgifb\n\nDifferential Revision: D27406843\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 98983b9df0f974ed69965ba4fcc03c1a18d1f9f5", "pr_number": "54485", "files_changed": ["docs/source/jit.rst", "torch/jit/__init__.py", "torch/jit/_script.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "1bccd48465": {"title": "Allow creating SugaredValue for a complex valued IValue and deserialization logic for \"infj\" and \"nanj\" global constants (#54328)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54328\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27369134\n\nPulled By: anjali411\n\nfbshipit-source-id: aec26750a6fc8917ee15306684b743d13a91570c", "pr_number": "54328", "files_changed": ["test/jit/test_complex.py", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/python_print.cpp"], "labels": ["Merged", "cla signed", "module: complex", "oncall: jit"]}, "fbaad8c0f9": {"title": "[PyTorch] TensorIterator::output should return const reference (#54811)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54811\n\nCallers can make a refcount bump themselves if they need one.\nghstack-source-id: 125136516\n\nTest Plan: CI\n\nReviewed By: ngimel\n\nDifferential Revision: D27377210\n\nfbshipit-source-id: ea58c7190fe2d7896432e403ecb1c59761aa319d", "pr_number": "54811", "files_changed": ["aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed"]}, "5c12d97d96": {"title": "Add script to export a JSON of slow test case times (#54907)", "body": "Summary:\nThis PR introduces a script to spit our a list of slow tests into a file `.pytorch-slow-tests`. The format is currently JSON, and is simply a dictionary with entries that look like: `(\"test_case_name (__main__.test_suite)\" -> average time in seconds)`. This is one of the steps in maintaining a list of slow tests so we could retire the manual slowTest labeling process.\n\nThe script reads data from the previous day's viable/strict's data (to ensure we have fully uploaded data), and aggregates the test times for **passed** test cases. It then filters the individual test cases to exclude those faster than 60 seconds.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54907\n\nTest Plan:\n`python tools/export_slow_test.py`\nCheck that `.pytorch-slow-tests` contains data. Mine looks like:\n```\n{\n    \"test_matmul_4d_4d_complex_cpu (__main__.TestAutogradDeviceTypeCPU)\": 91.22675,\n    \"test_unary_ops (__main__.TestTEFuser)\": 68.6,\n    \"test_fn_gradgrad_unfold_cpu_complex128 (__main__.TestGradientsCPU)\": 82.49153333333334,\n    \"test_conv1d_basic (__main__.TestXNNPACKConv1dTransformPass)\": 94.0914375,\n    \"test_ddp_uneven_inputs (__main__.TestDistBackendWithFork)\": 134.4995,\n    \"test_pdist_norm_large_cuda (__main__.TestTorchDeviceTypeCUDA)\": 60.2634,\n    \"test_cusparse_multiple_threads_same_device (__main__.TestCuda)\": 97.9022,\n    \"test_fn_gradgrad_unfold_cuda_complex128 (__main__.TestGradientsCUDA)\": 130.7222,\n    \"test_ddp_uneven_inputs (__main__.TestDistBackendWithSpawn)\": 136.08133333333333,\n    \"test_jit_cuda_archflags (__main__.TestCppExtensionJIT)\": 112.80733333333333,\n    \"test_lobpcg_ortho_cuda_float64 (__main__.TestLinalgCUDA)\": 63.8312,\n    \"test_matmul_4d_4d_complex_cuda (__main__.TestAutogradDeviceTypeCUDA)\": 62.1062,\n    \"test_inverse_many_batches_cuda_complex128 (__main__.TestLinalgCUDA)\": 1434.505,\n    \"test_inverse_many_batches_cuda_complex64 (__main__.TestLinalgCUDA)\": 1403.846,\n    \"test_inverse_many_batches_cuda_float32 (__main__.TestLinalgCUDA)\": 2081.614,\n    \"test_inverse_many_batches_cuda_float64 (__main__.TestLinalgCUDA)\": 1410.788,\n    \"test_matrix_exp_analytic_cuda_complex128 (__main__.TestLinalgCUDA)\": 172.167,\n    \"test_matrix_exp_analytic_cuda_complex64 (__main__.TestLinalgCUDA)\": 172.57,\n    \"test_matrix_exp_analytic_cuda_float32 (__main__.TestLinalgCUDA)\": 258.61,\n    \"test_matrix_exp_analytic_cuda_float64 (__main__.TestLinalgCUDA)\": 174.793,\n    \"test_inverse_many_batches_cpu_complex128 (__main__.TestLinalgCPU)\": 666.464,\n    \"test_inverse_many_batches_cpu_complex64 (__main__.TestLinalgCPU)\": 667.26,\n    \"test_inverse_many_batches_cpu_float32 (__main__.TestLinalgCPU)\": 1100.719,\n    \"test_inverse_many_batches_cpu_float64 (__main__.TestLinalgCPU)\": 651.037,\n    \"test_matrix_exp_analytic_cpu_complex128 (__main__.TestLinalgCPU)\": 72.965,\n    \"test_matrix_exp_analytic_cpu_complex64 (__main__.TestLinalgCPU)\": 74.184,\n    \"test_matrix_exp_analytic_cpu_float32 (__main__.TestLinalgCPU)\": 128.768,\n    \"test_matrix_exp_analytic_cpu_float64 (__main__.TestLinalgCPU)\": 72.138,\n    \"test_conv1d_with_relu_fc (__main__.TestXNNPACKConv1dTransformPass)\": 123.728,\n    \"test_fn_gradgrad_linalg_householder_product_cuda_complex128 (__main__.TestGradientsCUDA)\": 60.708,\n    \"test_lobpcg (__main__.TestAutograd)\": 120.408,\n    \"test_collect_callgrind (__main__.TestBenchmarkUtils)\": 206.896,\n    \"test_collect_cpp_callgrind (__main__.TestBenchmarkUtils)\": 122.507,\n    \"test_proper_exit (__main__.TestDataLoader)\": 172.356,\n    \"test_proper_exit (__main__.TestDataLoaderPersistentWorkers)\": 172.02,\n    \"testNBit (__main__.operator_test.fused_nbit_rowwise_conversion_ops_test.TestNBitGreedyFused)\": 96.9435,\n    \"IntegerDivider (__main__.TestCUDAIntegerDivider)\": 156.73700000000002\n}\n```\n\nReviewed By: walterddr, malfet\n\nDifferential Revision: D27412861\n\nPulled By: janeyx99\n\nfbshipit-source-id: ec3d327e0dc6c93093e8b1c8454e3166b0649909", "pr_number": "54907", "files_changed": [".gitignore", "tools/export_slow_tests.py", "tools/stats_utils/s3_stat_parser.py"], "labels": ["Merged", "cla signed"]}, "f9097c43b9": {"title": "Support mix of int32 and int64 offsets/indices for EmbeddingBag and its variants (#53655)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53655\n\nCurrently EmbeddingBag and it variants support either int32 or int64 indices/offsets. We have use cases where there are mix of int32 and int64 indices which are not supported yet. To avoid introducing too many branches we could simply cast offsets type to indices type when they are not the same.\n\nTest Plan: unit tests\n\nReviewed By: qizzzh\n\nDifferential Revision: D26820202\n\nfbshipit-source-id: 3e8f09523329ea12393ea92ee9a6315aa40a0b7f", "pr_number": "53655", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "test/test_nn.py", "torch/nn/modules/sparse.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "46e7f6773f": {"title": "[Static Runtime] Check for inplace ops explicitly in ReplaceWithCopy (#54657)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54657\n\nThe constraint checked in D27145406 (https://github.com/pytorch/pytorch/commit/acf03b13f157f8f0190d7ef70591490a2e8585b4) is too tight for the adindexer model and as a result, 5 ops (4 aten::narrow + 1 aten::premute) are not replaced with the copy version and resulted in perf regression. This diff checks for inplace ops explicitly and only applies the input constraint to graphs with inplace ops.\n\nTest Plan: Contbuild\n\nReviewed By: ajyu\n\nDifferential Revision: D27253145\n\nfbshipit-source-id: 23e2b1a018c84dd0fc2880fddd9c41bc0422b8eb", "pr_number": "54657", "files_changed": ["benchmarks/static_runtime/deep_wide_pt.h", "benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/passes.cpp", "torch/csrc/jit/runtime/static/passes.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "0269a5f481": {"title": "Re-enable `cmath.sqrt(complex(-1,-0.0))` test (#54923)", "body": "Summary:\nBoth JITed and plan `cmath.sqrt(complex(-1, -0.0))` should return `-1j` after https://github.com/pytorch/pytorch/pull/54820 has been resolved.\n\nAlso, use f-string instead of `.format` method\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54923\n\nReviewed By: anjali411\n\nDifferential Revision: D27415117\n\nPulled By: malfet\n\nfbshipit-source-id: 52e182feca50b690684de87c99df0ad6bef1ab44", "pr_number": "54923", "files_changed": ["test/jit/test_complex.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "2503028ff5": {"title": "Fix ConvTranspose with padding as a list of values (#54911)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54452\n\nThe assertion that fails in the issue is necessary to appease mypy. Instead, I fix `_ntuple` to always return a `tuple`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54911\n\nReviewed By: H-Huang\n\nDifferential Revision: D27411088\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 7f5045c58dd4f5f3b07b4826d9b4ca85606c5bce", "pr_number": "54911", "files_changed": ["test/test_nn.py", "torch/nn/modules/utils.py"], "labels": ["Merged", "cla signed", "open source"]}, "c690ed0ae8": {"title": "Fix override for __iter__ (#54702)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54702\n\nThis fixes subclassing for __iter__ so that it returns an iterator over\nsubclasses properly instead of Tensor.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D27352563\n\nPulled By: ezyang\n\nfbshipit-source-id: 4c195a86c8f2931a6276dc07b1e74ee72002107c", "pr_number": "54702", "files_changed": ["test/test_overrides.py", "torch/_tensor.py", "torch/overrides.py"], "labels": ["Merged", "cla signed", "open source"]}, "6c31f56bf4": {"title": "[Gradient Compression] Add cuda.syncrhonize back to batched powerSGD (#54838)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54838\n\nRealize that an explicit sync is somehow still needed for batched PowerSGD hook. I find that a job failure can be fixed by this change.\n\nThe sync was once removed by #54482.\n\nTest Plan:\nf260900882\nf260899693\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27384738\n\nfbshipit-source-id: 3efd738b9fd375e2ceb36ed3a6bf99cd8ce8ff95", "pr_number": "54838", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7c0941ee63": {"title": "Clang-format powerSGD_hook.py (#54839)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54839\n\nghstack-source-id: 125089465\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27384796\n\nfbshipit-source-id: 8312059f6a47d60ca29f75041141bb88804e1b32", "pr_number": "54839", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "6c8d783830": {"title": "Generate no-op meta functions for all inplace operations (#54901)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54901\n\nSome subtleties:\n- Need to make sure not to clobber composite definitions when\n  deciding when to generate\n- I was lazy and so I didn't make inplace on TensorList work,\n  nor did I make inplace functions that returned void work\n- A few tests started complaining that these noop meta functions\n  weren't raising the errors they needed.  This is tracked\n  in https://github.com/pytorch/pytorch/issues/54897\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27407232\n\nPulled By: ezyang\n\nfbshipit-source-id: 5e706a267496368acdafd128942c310954e43d29", "pr_number": "54901", "files_changed": ["test/test_nn.py", "test/test_torch.py", "test/test_type_promotion.py", "tools/codegen/dest/register_dispatch_key.py", "torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed"]}, "9b9e19a808": {"title": "Fix test_variant_consistency_jit_addmm for complex types (#54917)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54917\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D27411483\n\nPulled By: anjali411\n\nfbshipit-source-id: 95a2241ff326a7ab8b8d3abe0ad100074c23e47a", "pr_number": "54917", "files_changed": ["torch/csrc/jit/passes/decompose_ops.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: complex", "oncall: jit"]}, "728d18f976": {"title": "Enable USE_KINETO (#51273)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51273\n\nReviewed By: malfet\n\nDifferential Revision: D26119144\n\nfbshipit-source-id: eab0d17789c1eab89a7369f0574d3b4c2767c98a", "pr_number": "51273", "files_changed": ["torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "75ed6fbd91": {"title": "Fix CUDA 11.2 jobs for Windows (#54955)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/pull/54589#issuecomment-810255467\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54955\n\nReviewed By: walterddr\n\nDifferential Revision: D27434722\n\nPulled By: agolynski\n\nfbshipit-source-id: b99f24be679da65e5894e1a21e3cb2a62320fdda", "pr_number": "54955", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "cla signed", "open source"]}, "c9d0c855f7": {"title": "[special] Alias for special.expm1 and special.exp2 (#54670)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/50345\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54670\n\nReviewed By: H-Huang\n\nDifferential Revision: D27401440\n\nPulled By: mruberry\n\nfbshipit-source-id: 02b1fd0e8ffd3f5a017d6b6b9229b76b92b4b745", "pr_number": "54670", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/special.rst", "torch/_torch_docs.py", "torch/csrc/api/include/torch/special.h", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py", "torch/special/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "9f93d82907": {"title": "OpInfo: Add opinfo for cum{min,max} and minor fixes (#54762)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54762\n\nReviewed By: H-Huang\n\nDifferential Revision: D27390171\n\nPulled By: mruberry\n\nfbshipit-source-id: 9376dafa3bd2228786756f62fed01565134228fa", "pr_number": "54762", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d60874354f": {"title": "[docs] Add updated TorchScript language reference section for types (#53673)", "body": "Summary:\n**Summary**\nThis commit adds information about type annotation and inference to\nthe updated language specification. It will be rebased on top of https://github.com/pytorch/pytorch/issues/52494\nafter it lands.\n\n**Test Plan**\nContinuous integration.\n\nScreen capture:\nhttps://user-images.githubusercontent.com/4392003/110560184-66371f80-80fa-11eb-803a-923cf8de25ff.mov\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53673\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27413001\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: b54b300b4b1f10537ec06e2ee9eeb6d2b1f1810b", "pr_number": "53673", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "d49beba071": {"title": "[pyper] out variant of sigrid_transforms_torch_bind + ListUnpack (#54761)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54761\n\nTest Plan:\nRegen adindexer model that uses sigrid_transforms_torch_bind: /mnt/public/ansha/adindexer/merge20210323/adindexer_pt_traced_merge.pt\n\n```\nMKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --scripted_model=adindexer_pt_traced_merge.pt --pt_inputs=/data/users/ansha/tmp/adindexer/merge2/container_precomputation_bs1.pt --iters=30000 --warmup_iters=300000 --num_threads=1 --pred_net=c2_net_merge.pb --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --pt_optimize_memory=1\n```\n\nBefore ms/iter: 0.0647056\nAfter ms/iter: 0.0581197\n\nReviewed By: hlu1\n\nDifferential Revision: D27239617\n\nfbshipit-source-id: dffe6cbaf3a783c41605c97c5947a36e3b1b1f3b", "pr_number": "54761", "files_changed": ["torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "46c27ea84d": {"title": "Enabling OneDNN for group conv (#54890)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50042\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54890\n\nReviewed By: ejguan\n\nDifferential Revision: D27405252\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 7f4880ff07a51b83f796e218eb0df048ad4725ce", "pr_number": "54890", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["Merged", "cla signed"]}, "f5d6b90c35": {"title": "Add a missing sys import in test/distributed/rpc/test_tensorpipe_agent.py (#54925)", "body": "Summary:\n`sys` is used a couple of lines below.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54925\n\nReviewed By: agolynski\n\nDifferential Revision: D27434941\n\nPulled By: H-Huang\n\nfbshipit-source-id: b03c9373ee77e7a158964f619b29967fa55226d0", "pr_number": "54925", "files_changed": ["test/distributed/rpc/test_tensorpipe_agent.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "18e61d1ce9": {"title": "Improve placeholder matching in subgraph rewriter (#54958)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54958\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D27431889\n\nPulled By: ansley\n\nfbshipit-source-id: 8b1b4f2f0202305530b9648b6b770f9e2ecacfe2", "pr_number": "54958", "files_changed": ["test/fx/test_subgraph_rewriter.py", "torch/fx/subgraph_rewriter.py"], "labels": ["Merged", "cla signed", "fx"]}, "eafa235582": {"title": "Clarify and document commit choice for CI jobs (#54967)", "body": "Summary:\nPRs https://github.com/pytorch/pytorch/issues/53652 and https://github.com/pytorch/pytorch/issues/54693 attempted to increase the consistency of our choice of commit (head vs merge) for CI on PRs, and have so far been unsuccessful. This PR takes a less ambitious approach to the problem by clarifying the choice in one specific way (see the following paragraph) and documenting it in `CONTRIBUTING.md`.\n\nIn addition to documentation, this PR also removes the current behavior of our GHA jobs that checkout the PR tip instead of the merge commit. At first glance, this behavior seems to increase consistency (by eliminating the special-case for `ghstack` PRs), but in reality, it actually just means that for non-`ghstack` PRs, the question \"Which commit is used in CI?\" has *two* answers instead of just one; see the description of https://github.com/pytorch/pytorch/issues/53652 for more details.\n\nOnce merged, this PR will unblock other PRs that add modify our GHA workflows in breaking ways, such as https://github.com/pytorch/pytorch/issues/54737.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54967\n\nTest Plan: None.\n\nReviewed By: walterddr, seemethere\n\nDifferential Revision: D27435913\n\nPulled By: samestep\n\nfbshipit-source-id: 405fb419cf015cf88107d5eb2498cfb5bcb7ce33", "pr_number": "54967", "files_changed": [".github/workflows/build_linux_conda.yml", ".github/workflows/build_linux_libtorch.yml", ".github/workflows/build_linux_wheels.yml", ".github/workflows/clang_format.yml", ".github/workflows/lint.yml", ".github/workflows/test_tools.yml", "CONTRIBUTING.md"], "labels": ["Merged", "cla signed"]}, "5bcbbf5373": {"title": "Lint trailing newlines (#54737)", "body": "Summary:\n*Context:* https://github.com/pytorch/pytorch/issues/53406 added a lint for trailing whitespace at the ends of lines. However, in order to pass FB-internal lints, that PR also had to normalize the trailing newlines in four of the files it touched. This PR adds an OSS lint to normalize trailing newlines.\n\nThe changes to the following files (made in 54847d0adb9be71be4979cead3d9d4c02160e4cd) are the only manually-written parts of this PR:\n\n- `.github/workflows/lint.yml`\n- `mypy-strict.ini`\n- `tools/README.md`\n- `tools/test/test_trailing_newlines.py`\n- `tools/trailing_newlines.py`\n\nI would have liked to make this just a shell one-liner like the other three similar lints, but nothing I could find quite fit the bill. Specifically, all the answers I tried from the following Stack Overflow questions were far too slow (at least a minute and a half to run on this entire repository):\n\n- [How to detect file ends in newline?](https://stackoverflow.com/q/38746)\n- [How do I find files that do not end with a newline/linefeed?](https://stackoverflow.com/q/4631068)\n- [How to list all files in the Git index without newline at end of file](https://stackoverflow.com/q/27624800)\n- [Linux - check if there is an empty line at the end of a file [duplicate]](https://stackoverflow.com/q/34943632)\n- [git ensure newline at end of each file](https://stackoverflow.com/q/57770972)\n\nTo avoid giving false positives during the few days after this PR is merged, we should probably only merge it after https://github.com/pytorch/pytorch/issues/54967.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54737\n\nTest Plan:\nRunning the shell script from the \"Ensure correct trailing newlines\" step in the `quick-checks` job of `.github/workflows/lint.yml` should print no output and exit in a fraction of a second with a status of 0. That was not the case prior to this PR, as shown by this failing GHA workflow run on an earlier draft of this PR:\n\n- https://github.com/pytorch/pytorch/runs/2197446987?check_suite_focus=true\n\nIn contrast, this run (after correcting the trailing newlines in this PR) succeeded:\n\n- https://github.com/pytorch/pytorch/pull/54737/checks?check_run_id=2197553241\n\nTo unit-test `tools/trailing_newlines.py` itself (this is run as part of our \"Test tools\" GitHub Actions workflow):\n```\npython tools/test/test_trailing_newlines.py\n```\n\nReviewed By: malfet\n\nDifferential Revision: D27409736\n\nPulled By: samestep\n\nfbshipit-source-id: 46f565227046b39f68349bbd5633105b2d2e9b19", "pr_number": "54737", "files_changed": [".circleci/windows-jni/include/jni.h", ".github/workflows/lint.yml", ".jenkins/pytorch/perf_test/test_cpu_speed_torch.sh", ".jenkins/pytorch/perf_test/test_cpu_speed_torch_tensor.sh", "android/test_app/app/src/main/res/layout/activity_main.xml", "aten/src/ATen/NumericUtils.h", "aten/src/ATen/cpu/vec256/vsx/vec256_complex_double_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vsx_helpers.h", "aten/src/ATen/cudnn/cudnn-wrapper.h", "aten/src/ATen/div_rtn.h", "aten/src/ATen/native/Cross.h", "aten/src/ATen/native/batch_norm.h", "aten/src/ATen/native/cpu/CrossKernel.cpp", "aten/src/ATen/native/cuda/CrossKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cuh", "aten/src/ATen/native/quantized/cpu/kernels/README.md", "aten/src/ATen/native/quantized/cpu/qnnpack/CONTRIBUTING.md", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/ATen/test/cuda_complex_math_test.cu", "aten/src/ATen/test/pow_test.cpp", "aten/src/THC/generated/THCTensorMathCompareTBool.cu", "aten/src/THC/generated/THCTensorMathCompareTByte.cu", "aten/src/THC/generated/THCTensorMathCompareTChar.cu", "aten/src/THC/generated/THCTensorMathCompareTDouble.cu", "aten/src/THC/generated/THCTensorMathCompareTFloat.cu", "aten/src/THC/generated/THCTensorMathCompareTHalf.cu", "aten/src/THC/generated/THCTensorMathCompareTInt.cu", "aten/src/THC/generated/THCTensorMathCompareTLong.cu", "aten/src/THC/generated/THCTensorMathCompareTShort.cu", "benchmarks/README.md", "benchmarks/fastrnns/README.md", "c10/util/Optional.cpp", "c10/util/Unicode.cpp", "caffe2/core/common_test.cc", "caffe2/core/nomnigraph/tests/AlgorithmsTest.cc", "caffe2/onnx/device.cc", "caffe2/operators/roi_align_op.cc", "caffe2/opt/optimizer.cc", "caffe2/python/CMakeLists.txt", "caffe2/python/convert.py", "caffe2/python/fakelowp/init_shared_libs.py", "caffe2/python/onnx/README.md", "caffe2/python/onnx/backend_cpp_rep.py", "caffe2/python/onnx/test_onnxifi.py", "caffe2/python/onnx/tests/__init__.py", "caffe2/python/onnx/tests/test_utils.py", "caffe2/python/operator_test/mean_op_test.py", "caffe2/python/rnn/__init__.py", "caffe2/python/serialized_test/SerializedTestCoverage.md", "caffe2/python/trt/data/class_labels.txt", "caffe2/python/trt/test_trt.py", "caffe2/python/trt/transform.py", "caffe2/utils/simple_queue_test.cc", "cmake/Modules/FindAtlas.cmake", "cmake/Modules/FindHiredis.cmake", "cmake/Modules/FindNumPy.cmake", "cmake/Modules/FindNuma.cmake", "cmake/Modules/FindOpenBLAS.cmake", "cmake/Modules/FindRocksDB.cmake", "cmake/Modules/FindSnappy.cmake", "cmake/Modules/FindVSX.cmake", "cmake/Modules_CUDA_fix/upstream/FindCUDA/parse_cubin.cmake", "cmake/public/gflags.cmake", "cmake/public/glog.cmake", "cmake/public/utils.cmake", "docs/caffe2/DOXYGEN.md", "docs/cpp/Makefile", "docs/source/community/persons_of_interest.rst", "docs/source/dlpack.rst", "docs/source/jit_builtin_functions.rst", "docs/source/notes/cpu_threading_torchscript_inference.svg", "docs/source/notes/randomness.rst", "docs/source/special.rst", "docs/source/storage.rst", "docs/source/torch.nn.intrinsic.quantized.rst", "docs/source/torch.nn.qat.rst", "docs/source/torch.quantization.rst", "ios/TestApp/.clang-format", "ios/TestApp/Gemfile", "ios/TestApp/TestApp/Assets.xcassets/AppIcon.appiconset/Contents.json", "ios/TestApp/TestApp/Assets.xcassets/Contents.json", "ios/TestApp/fastlane/Fastfile", "modules/observers/CMakeLists.txt", "mypy-strict.ini", "scripts/apache_header.txt", "scripts/apache_python.txt", "scripts/diagnose_protobuf.py", "scripts/release_notes/common.py", "scripts/xcode_build.rb", "test/benchmark_utils/callgrind_artifacts.json", "test/cpp/api/dataloader.cpp", "test/cpp/api/init_baseline.h", "test/optim/test.lua", "tools/README.md", "tools/autograd/templates/InplaceOrViewType.cpp", "tools/clang_format_hash/linux64/clang-format-linux64", "tools/clang_format_hash/mac/clang-format-mojave", "tools/test/test_trailing_newlines.py", "tools/trailing_newlines.py", "tools/update_disabled_tests.sh", "torch/_C/_cudnn.pyi", "torch/_C/_functions.pyi", "torch/_C/_nn.pyi.in", "torch/csrc/deploy/CMakeLists.txt", "torch/csrc/jit/serialization/source_range_serialization_impl.h", "torch/csrc/python_dimname.cpp", "torch/csrc/python_dimname.h", "torch/csrc/python_headers.h", "torch/deploy.h", "torch/optim/_multi_tensor/__init__.pyi", "torch/optim/_multi_tensor/adadelta.pyi"], "labels": ["Merged", "cla signed"]}, "0e43a73f76": {"title": "Support needsOutputs for RecordFunction and ObserverUtil improvements (#54442)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54442\n\nAdded needsOutputs support to RecordFunction, improved ObserverUtil functions to handle list data. Minor refactor names to be consistent.\n\nTo get output data from kernel calls, we need to temporarily capture them before passing them to the record function. Then the results are released to function return. We handle two cases, for unboxed and boxed kernels. The boxed version is fairly simple since all outputs are stored in the stack object. For unboxed kernel calls, we added a `ReturnValue` utility class to properly handle the different return values of unboxed kernels.\n\nFor optimization, this intermediate capture is only enabled for observers that request `needsOutputs(true)` and should not affect other observers or when the observer is not enabled.\n\nTest Plan:\n```\n=> buck build //caffe2/test/cpp/jit: --show-output\n=> buck-out/gen/caffe2/test/cpp/jit/jit --gtest_filter=RecordFunctionTest*\nCUDA not available. Disabling CUDA and MultiCUDA tests\nNote: Google Test filter = RecordFunctionTest*-*_CUDA:*_MultiCUDA\n[==========] Running 7 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 7 tests from RecordFunctionTest\n[ RUN      ] RecordFunctionTest.TracedTestInputsOutputs\n[       OK ] RecordFunctionTest.TracedTestInputsOutputs (226 ms)\n[ RUN      ] RecordFunctionTest.SampledCallbacks\n[       OK ] RecordFunctionTest.SampledCallbacks (771 ms)\n[ RUN      ] RecordFunctionTest.RecordFunctionGuard\n[       OK ] RecordFunctionTest.RecordFunctionGuard (0 ms)\n[ RUN      ] RecordFunctionTest.Callbacks\n[       OK ] RecordFunctionTest.Callbacks (2 ms)\n[ RUN      ] RecordFunctionTest.ShouldRun\n[       OK ] RecordFunctionTest.ShouldRun (0 ms)\n[ RUN      ] RecordFunctionTest.Basic\n[       OK ] RecordFunctionTest.Basic (1 ms)\n[ RUN      ] RecordFunctionTest.OperatorNameOverload\n[       OK ] RecordFunctionTest.OperatorNameOverload (1 ms)\n[----------] 7 tests from RecordFunctionTest (1001 ms total)\n\n[----------] Global test environment tear-down\n[==========] 7 tests from 1 test case ran. (1002 ms total)\n[  PASSED  ] 7 tests.\n\n```\n\nReviewed By: ilia-cher\n\nDifferential Revision: D25966661\n\nfbshipit-source-id: 707886e1f212f40ba16a1fe292ea7dd33f2646e3", "pr_number": "54442", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "test/cpp/jit/test_misc.cpp"], "labels": ["Merged", "Reverted", "cla signed", "fb-exported", "oncall: jit"]}, "4694452d08": {"title": "[complex] `masked_fill`: Complex Autograd support and update masked_scatter skips. (#54244)", "body": "Summary:\nReference Issue: https://github.com/pytorch/pytorch/issues/33152\nPrevious PR : https://github.com/pytorch/pytorch/pull/52035, https://github.com/pytorch/pytorch/pull/52483\n\nFixes : https://github.com/pytorch/pytorch/issues/53608\nFixes : https://github.com/pytorch/pytorch/issues/53523\n\n**Note**: This PR is based on `ci-all/*` branch to ascertain that we don't break the master again.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54244\n\nReviewed By: H-Huang\n\nDifferential Revision: D27429147\n\nPulled By: anjali411\n\nfbshipit-source-id: 97945998b6911c2e7fd3f8db6cbd8963e5d6f21f", "pr_number": "54244", "files_changed": ["tools/autograd/gen_variable_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: complex", "open source"]}, "920eb01e2e": {"title": "Add scatter_add to amp docs (#54908)", "body": "Summary:\nUpdates docs to reflect https://github.com/pytorch/pytorch/pull/52133.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54908\n\nReviewed By: agolynski\n\nDifferential Revision: D27431302\n\nPulled By: H-Huang\n\nfbshipit-source-id: fa3dc6267bc73c81cdd96f986c971daee1922cb5", "pr_number": "54908", "files_changed": ["docs/source/amp.rst"], "labels": ["Merged", "cla signed", "open source"]}, "0e543b2b00": {"title": "Provide a decorator to set/unset nccl blocking wait for tests (#54740)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54740\n\nAdds a simple helper decorator to set/unset nccl blocking wait for\ntests. This will make it easier than having to manually set/unset the\nos.environ vars every time.\nghstack-source-id: 125233693\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27277222\n\nfbshipit-source-id: c289b9d05e2f6328d672810b07501979b6e177c6", "pr_number": "54740", "files_changed": ["torch/testing/_internal/common_distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3f1cd2e3a0": {"title": "test_c10d: Run tests with nccl_async_error_handling (#54741)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54741\n\nSimilar to what we did for distributed_test.py, let MultiProcessTests that run collecticve comm. tests with nccl blocking run under nccl_async_error_handling. This will better simulate real-world training scenarios.\nghstack-source-id: 125233692\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27277389\n\nfbshipit-source-id: a6c6e9abcf3a53b03ea8b9f8fb63b78e0cb6e81e", "pr_number": "54741", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "23b15ef98a": {"title": "test_c10d: use with_nccl_blocking_wait decorator (#54742)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54742\n\nUses with_nccl_blocking_wait decorator for test_c10d.\nghstack-source-id: 125233691\n\nTest Plan: ci\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27277835\n\nfbshipit-source-id: 063de32646b19d18969e9d60cb9a31a40d73d6a7", "pr_number": "54742", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ea37fe34ff": {"title": "[PyTorch] Avoid refcount bump in TensorArg (#54934)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54934\n\nIt looks like the vast majority of usage is just borrowing a pre-existing Tensor.\nghstack-source-id: 125216052\n\nTest Plan: Existing CI.\n\nReviewed By: hlu1\n\nDifferential Revision: D27415131\n\nfbshipit-source-id: d5a8dc4ca5d48ca3eaa3664655b724094e61f371", "pr_number": "54934", "files_changed": ["aten/src/ATen/TensorUtils.h", "aten/src/ATen/native/cudnn/AffineGridGenerator.cpp", "aten/src/ATen/native/cudnn/BatchNorm.cpp", "aten/src/ATen/native/cudnn/GridSampler.cpp"], "labels": ["Merged", "cla signed"]}, "dde7fff0e9": {"title": "[PyTorch] Avoid refcount bumps in addmm_out_cuda_impl (#54935)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54935\n\nBunch of avoidable copying of Tensor objects, which results in a refcount bump.\nghstack-source-id: 125216023\n\nTest Plan:\nCompared percentage of self time spent in addmm_out_cuda_impl while running the following sample:\n\n```\n+import torch\n+import torch.nn as nn\n+\n+m = nn.Linear(1024, 1024).cuda().half()\n+x = torch.randn(16, 1024).cuda().half()\n+while True: y = m(x)\n```\n\nin perf record, decreased from 0.74% to 0.56%.\n\nReviewed By: ngimel\n\nDifferential Revision: D27420388\n\nfbshipit-source-id: d2c5e4c4899cd02c60c45735b2d72c4ed913f6e8", "pr_number": "54935", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["Merged", "cla signed"]}, "d490e0120f": {"title": "[PyTorch] One less refcount bump in linear() (#54936)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54936\n\nAnother case where we can use `MaybeOwned<Tensor>` to save a bump at a small cost.\nghstack-source-id: 125218488\n\nTest Plan: Existing CI\n\nReviewed By: ngimel\n\nDifferential Revision: D27421117\n\nfbshipit-source-id: 16bb31ec38817be1f889360e2abfd0d9596e2943", "pr_number": "54936", "files_changed": ["aten/src/ATen/native/Linear.cpp"], "labels": ["Merged", "cla signed"]}, "1bf57430f1": {"title": "Remove hacky wrappers for 21 operators (#54819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54819\n\n20 of them contain both optional Tensor and output position.\n\nHacky wrapper for `_convolution_mode` was added in\n04e0cbf5a9f073a1b73195537c12fb332c2fddd9 after hacky wrappers\nare removed for optional<Tensor>.\n\nCodemod commands are generated by a hacked version of\nhttps://github.com/pytorch/pytorch/pull/54223 and\nhttps://github.com/pytorch/pytorch/pull/54098.\nghstack-source-id: 125278883\n\nTest Plan:\nbuck build //caffe2/aten/...\nBUILD_TENSOREXPR_BENCHMARK=ON BUILD_STATIC_RUNTIME_BENCHMARK=ON python setup.py install\n\nReviewed By: smessmer\n\nDifferential Revision: D27378819\n\nfbshipit-source-id: b925ed0510a83e3976383aaeec8b7de438b23bf3", "pr_number": "54819", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/LegacyNNDefinitions.cpp", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/native/LossNLL.cpp", "aten/src/ATen/native/LossNLL2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/cuda/Loss.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "2df4acd025": {"title": "Remove hacky wrapper for about 70 kernels (#54898)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54898\n\nCodemod commands generated by https://github.com/pytorch/pytorch/pull/54098\nghstack-source-id: 125278884\n\nTest Plan:\nbuck build //caffe2/aten/...\nBUILD_TENSOREXPR_BENCHMARK=ON BUILD_STATIC_RUNTIME_BENCHMARK=ON python setup.py install\n\nReviewed By: smessmer\n\nDifferential Revision: D27404868\n\nfbshipit-source-id: cb6593c0d1a2dee4e65f0baa08f32a76cf7f5339", "pr_number": "54898", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Col2Im.cpp", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/DilatedMaxPool2d.cpp", "aten/src/ATen/native/DilatedMaxPool3d.cpp", "aten/src/ATen/native/FractionalMaxPool3d.cpp", "aten/src/ATen/native/Im2Col.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/MaxUnpooling.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/ReflectionPad.cpp", "aten/src/ATen/native/ReplicationPadding.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/cuda/Col2Im.cu", "aten/src/ATen/native/cuda/ConvolutionMM2d.cu", "aten/src/ATen/native/cuda/DepthwiseConv2d.cu", "aten/src/ATen/native/cuda/DepthwiseConv3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/Im2Col.cu", "aten/src/ATen/native/cuda/MaxUnpooling.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "f956b7524e": {"title": "lazily init AliasDb and add `changed` status to CSE (#54776)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54776\n\nReviewed By: H-Huang\n\nDifferential Revision: D27422064\n\nPulled By: Krovatkin\n\nfbshipit-source-id: dfeb61001f60a2080246e128d8b7f83bbc584801", "pr_number": "54776", "files_changed": ["torch/csrc/jit/passes/common_subexpression_elimination.cpp", "torch/csrc/jit/passes/common_subexpression_elimination.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3baeeb3f57": {"title": "Added Azure Pipelines build steps for PyTorch (#54039)", "body": "Summary:\nThis PR adds Azure Pipelines build steps for PyTorch. There are 3 pipelines that are added.\n\n1) CI Build\n    - Runs when a PR is opened or when new commits to an open PR is added. This build must succeed before the PR can be merged.\n    - Currently only TestTorch unit tests are run.\n    - Only the CI Build configurations are run.\n2) Daily Build\n    - Runs once a day during inactive hours to ensure the current PyTorch repo performs as expected.\n    - Runs all unit tests.\n        - Note: I do not have access to the current [determine-from](https://github.com/pytorch/pytorch/blob/b9e900ee525eeaa601aefdeadaf89c6e1a3085e8/test/run_test.py#L737) unit tests that are skipped on Windows builds. This `determine-from` filter can be added once a clear way to skip certain unit tests given the build configuration is explained.\n    - Runs on All Build configurations.\n3) Official Build\n    - Runs once a day during inactive hours to publish official PyTorch artifacts to Azure DevOps Artifacts for consumption.\n    - No unit tests are run.\n    - Runs in three stages: Build, Verify, Publish, where PyTorch is built, then its wheel is installed in a clean Conda environment for verification, and then the wheel is published to Azure Artifacts as a Universal Package.\n    - Runs on All Build configurations.\n\nUbuntu builds run on Docker with the specified Dockerfile configuration. Windows builds run directly on configured Windows VMs (CPU, CUDA/cuDNN)\n\nCI Build configurations:\n1. Ubuntu 18.04\n    1. Python 3.9\n\t\ta. CUDA 11.2/cuDNN 8.1.0\n\t2. Python 3.8\n\t\ta. CPU\n2. Windows 2019\n\t1. Python 3.8\n\t\tb. CUDA 10.2/cuDNN 7.6.5\n\t2. Python 3.7\n\t\ta. CPU\n\nAll Build configurations:\n1. Ubuntu 18.04\n\t1. Python 3.9\n\t\ta. CUDA 11.2/cuDNN 8.1.0\n\t2. Python 3.8\n\t\ta. CPU\n\t\tb. CUDA 10.2/cuDNN 8.1.0\n\t3. Python 3.7\n\t\ta. CPU\n\t\tb. CUDA 10.1/cuDNN 7.6.5\n\n2. Windows 2019\n\t1. Python 3.9\n\t\ta. CUDA 11.2/cuDNN 8.1.0\n\t2. Python 3.8\n\t\ta. CPU\n\t\tb. CUDA 10.2/cuDNN 7.6.5\n\t3. Python 3.7\n\t\ta. CPU\n        b. CUDA 10.1/cuDNN 7.6.4\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54039\n\nReviewed By: ezyang\n\nDifferential Revision: D27373310\n\nPulled By: malfet\n\nfbshipit-source-id: 06dcfe2d99da0e9876b6deb224272800dae46028", "pr_number": "54039", "files_changed": [".azure_pipelines/build-pipeline.yml", ".azure_pipelines/daily-pipeline.yml", ".azure_pipelines/job_templates/build-verify-publish-template-unix.yml", ".azure_pipelines/job_templates/build-verify-publish-template-win.yml", ".azure_pipelines/job_templates/common-packages.yml", ".azure_pipelines/job_templates/prepare-build-template.yml", ".azure_pipelines/job_templates/set-environment-variables.yml", ".azure_pipelines/verify-pipeline.yml", "docker/pytorch/ubuntu_cpu_gpu/Dockerfile"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b907d6e3b6": {"title": "[ROCm] skip some tests to enable 4.1 CI upgrade (#54536)", "body": "Summary:\nSkips the tests indicated as failing in https://github.com/pytorch/pytorch/issues/54535.\n\nDuring the ROCm CI upgrade from 4.0.1 to 4.1, some tests regressed. Specifically, FFT tests in test_spectral_ops.py and test_grid_sample in test_nn.py. In order to keep a passing CI signal, we need to disable these temporarily.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54536\n\nReviewed By: H-Huang\n\nDifferential Revision: D27442974\n\nPulled By: malfet\n\nfbshipit-source-id: 07dffb957757a5fc7afaa5bf78b935a427251ef4", "pr_number": "54536", "files_changed": ["test/test_nn.py", "test/test_spectral_ops.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "d4c37b314e": {"title": "Mark redispatch functions with TORCH_API (#54966)", "body": "Summary:\nSo they can be called from out-of-tree extensions\n\nOtherwise I get linking errors like:\n```\nImportError: /anaconda/envs/mytorch/lib/python3.8/site-packages/torchy-0.1-py3.8-linux-x86_64.egg/_TORCHY.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN2at10redispatch3addEN3c1014DispatchKeySetERKNS_6TensorES5_RKNS1_6ScalarE\n```\n\ncc ezyang bdhirsh\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54966\n\nReviewed By: H-Huang\n\nDifferential Revision: D27439712\n\nPulled By: ezyang\n\nfbshipit-source-id: 4c0b45e87e708c57283758da49c54a767ab7ecbc", "pr_number": "54966", "files_changed": ["aten/src/ATen/templates/RedispatchFunctions.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "1dffbe759b": {"title": "[ROCm] utilize PUBLIC vs PRIVATE linking to avoid incorrect dependencies (#54727)", "body": "Summary:\nFixes the build of projects that depend on torch, such as torchaudio.  Otherwise torchaudio will complain that gloo_hip is missing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54727\n\nReviewed By: H-Huang\n\nDifferential Revision: D27361513\n\nPulled By: ezyang\n\nfbshipit-source-id: 714cc2db23e7adf3e89303e941b78c27625b9460", "pr_number": "54727", "files_changed": ["caffe2/CMakeLists.txt", "caffe2/contrib/CMakeLists.txt", "cmake/Dependencies.cmake"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "854c92078a": {"title": "Fixed the default size of the workspace array for MAGMA's SVD (#54875)", "body": "Summary:\nThe problem was that MAGMA might not set the value for the optimal size of the workspace array leaving it uninitialized. This is fixed by setting the default value for `wkopt` variable.\n\nFixes https://github.com/pytorch/pytorch/issues/54381 and https://github.com/pytorch/pytorch/issues/53976.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54875\n\nReviewed By: H-Huang\n\nDifferential Revision: D27437702\n\nPulled By: mruberry\n\nfbshipit-source-id: bf61555abc4c50e8ef2dae933df24ce4d4fe4527", "pr_number": "54875", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "module: magma", "module: rocm", "open source", "triaged"]}, "2bee09a577": {"title": "[android] fbjni android use prefab dependency, version 0.2.2 (#54792)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54792\n\nTest Plan: Imported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D27370295\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: bde881a8d4edd4636aa4ec7cecbe770b5b65bb1f", "pr_number": "54792", "files_changed": [".circleci/docker/android/build.gradle", "android/build.gradle", "android/pytorch_android/CMakeLists.txt", "android/pytorch_android/build.gradle", "android/test_app/app/build.gradle"], "labels": ["Merged", "Reverted", "cla signed"]}, "4865195cf4": {"title": "[PyTorch] Add DimVector variant of infer_size (#54882)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54882\n\nSometimes we have no reason to think that the output of `infer_size` won't be within the range of typical tensor sizes. In those cases, we can use a DimVector.\nghstack-source-id: 125137792\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D27400387\n\nfbshipit-source-id: 9a11d0f93010540f3aa65c0e208fc8e03f0e8a7f", "pr_number": "54882", "files_changed": ["aten/src/ATen/ExpandUtils.cpp", "aten/src/ATen/ExpandUtils.h", "aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp"], "labels": ["Merged", "cla signed"]}, "444e5f0b60": {"title": "Add Type System (I) (#53244)", "body": "Summary:\n**Summary**\nThis commit adds a new .rst file to update the language specification with the updated content for the Type System section.\n\n**Test Plan**\n\n![image](https://user-images.githubusercontent.com/70345919/109920057-9308b400-7c6e-11eb-8391-83635efbf036.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53244\n\nReviewed By: H-Huang\n\nDifferential Revision: D27445210\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 984c25b06686ba7a72cc03c5c069d819709eedb8", "pr_number": "53244", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "6d87b3667f": {"title": "Added support for TensorList inputs in OpInfo (#54922)", "body": "Summary:\nStack:\n* https://github.com/pytorch/pytorch/issues/54954 Fixed OpInfo jit tests failing for TensorList inputs\n* __#54922 Added support for TensorList inputs in OpInfo__\n\nUpdated OpInfo to accept either a `Tensor` or `TensorList` as `sample.input` and added workarounds to make this work with gradcheck.\n\nNote: JIT testing support for TensorList inputs will be added in a follow up PR.\n\nFixes https://github.com/pytorch/pytorch/issues/51996\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54922\n\nReviewed By: H-Huang\n\nDifferential Revision: D27448952\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 3f24a56f6180eb2d044dcfc89ba59fce8acfe278", "pr_number": "54922", "files_changed": ["test/test_ops.py", "test/test_shape_ops.py", "test/test_sparse.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "449a9514d1": {"title": "Update Kineto submodule (#55011)", "body": "Summary:\nUpdate Kineto submodule rev. Fixes\nhttps://github.com/pytorch/pytorch/issues/54889\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55011\n\nTest Plan: CI\n\nReviewed By: gdankel\n\nDifferential Revision: D27450222\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 0652a5d42182197acc4c9e6f07e71b5b55a557ee", "pr_number": "55011", "files_changed": ["third_party/kineto"], "labels": ["Merged", "cla signed"]}, "a0ae3e520f": {"title": "[Pytorch Mobile] 'fix' filter of named parameters for FL (#54633)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54633\n\nTheres currently no information that could be used to determine what is a parameter during the loading of a mobile module. This prevents named parameters from functioning correctly. This change is a temporary hack to help out federated learning the sole user of this api currently.\nghstack-source-id: 124885201\n\nTest Plan: todo\n\nReviewed By: dhruvbird\n\nDifferential Revision: D27308738\n\nfbshipit-source-id: 0af5d1e8381ab7b7a43b20560941aa070a02e7b8", "pr_number": "54633", "files_changed": ["test/cpp/jit/test_lite_trainer.cpp", "torch/csrc/jit/mobile/module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7fc03dd7c9": {"title": "Back out \"[pytorch][PR] Merge CUDA Streams and Events\" (#54996)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54996\n\nOriginal commit changeset: 45d9fee9a582\n\nTest Plan: CI\n\nReviewed By: jspark1105\n\nDifferential Revision: D27444718\n\nfbshipit-source-id: deb627230817923eaf84ade50ecb14bfbce4e779", "pr_number": "54996", "files_changed": ["test/cpp/jit/tests_setup.py", "test/jit/test_cuda.py", "torch/_utils.py", "torch/csrc/jit/cuda/cuda.h", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/register_cuda_ops.cpp", "torch/cuda/__init__.py", "torch/cuda/_utils.py", "torch/jit/__init__.py", "torch/jit/cuda.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "a74b10def9": {"title": "Keep Markdown ToCs up to date (#54974)", "body": "Summary:\nThis PR uses [markdown-toc](https://github.com/jonschlinkert/markdown-toc#cli) to [automatically update the table of contents for `README.md` and `CONTRIBUTING.md`](https://github.com/pytorch/pytorch/pull/54904#issuecomment-809682134) in CI.\n\nThis keeps the same format already used in `README.md`. While it does slightly change the format for the ToC in `CONTRIBUTING.md`, the new format is actually just the same as the old format that was already being used prior to https://github.com/pytorch/pytorch/issues/51458.\n\nRace condition with https://github.com/pytorch/pytorch/issues/54904.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54974\n\nTest Plan: The new \"Lint / toc\" job in GitHub Actions [succeeds](https://github.com/pytorch/pytorch/pull/54974/checks?check_run_id=2238739005) on this PR, and [fails](https://github.com/pytorch/pytorch/pull/54976/checks?check_run_id=2238784022) on https://github.com/pytorch/pytorch/issues/54976 with an understandable error message.\n\nReviewed By: malfet\n\nDifferential Revision: D27468390\n\nPulled By: samestep\n\nfbshipit-source-id: 14a73f42ed546d4310140b94ded14e099185d0e0", "pr_number": "54974", "files_changed": [".github/workflows/lint.yml", "CONTRIBUTING.md", "README.md"], "labels": ["Merged", "cla signed"]}, "f1f3c8b0fa": {"title": "Adding PyTorch + DNNL + AMD BLIS path (#54953)", "body": "Summary:\nThese changes provide the user with an additional option to choose the DNNL+BLIS path for PyTorch.\n\nThis assumes BLIS is already downloaded or built from source and the necessary library file is available at the location: $BLIS_HOME/lib/libblis.so and include files are available at: $BLIS_HOME/include/blis/blis.h and $BLIS_HOME/include/blis/cblas.h\n\nExport the below variables to build PyTorch with MKLDNN+BLIS and proceed with the regular installation procedure as below:\n$export BLIS_HOME=path-to-BLIS\n$export PATH=$BLIS_HOME/include/blis:$PATH LD_LIBRARY_PATH=$BLIS_HOME/lib:$LD_LIBRARY_PATH\n$export BLAS=BLIS USE_MKLDNN_CBLAS=ON WITH_BLAS=blis\n$python setup.py install\n\nCPU only Dockerfile to build PyTorch with AMD BLIS is available at : docker/cpu-blis/Dockerfile\nExample command line to build using the Dockerfile:\nsudo DOCKER_BUILDKIT=1 docker build . -t docker-image-repo-name\nExample command line to run the built docker container:\nsudo docker run --name container-name -it docker-image-repo-name\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54953\n\nReviewed By: glaringlee\n\nDifferential Revision: D27466799\n\nPulled By: malfet\n\nfbshipit-source-id: e03bae9561be3a67429df3b1be95a79005c63050", "pr_number": "54953", "files_changed": ["cmake/Dependencies.cmake", "cmake/Modules/FindBLAS.cmake", "cmake/Modules/FindBLIS.cmake", "docker/cpu-blis/Dockerfile"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "43d4f3b8d0": {"title": "Implement public API InferenceMode and its error handling (#55008)", "body": "Summary:\nhttps://www.internalfb.com/phabricator/paste/view/P360377337Pull Request resolved: https://github.com/pytorch/pytorch/pull/53343\n\nFor easier review, here's a diff between the version before revert. https://www.internalfb.com/phabricator/paste/view/P360750919\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55008\n\nTest Plan: Imported from OSS\n\nPulled By: ailzhang\n\nReviewed By: bhosmer\n\nDifferential Revision: D27443229\n\nfbshipit-source-id: 01b03446a1f6373f43dd5c7170d26226b50f363c", "pr_number": "53343", "files_changed": ["c10/core/DispatchKeySet.h", "c10/core/InferenceMode.cpp", "c10/core/InferenceMode.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/core/impl/LocalDispatchKeySet.h", "test/cpp/api/CMakeLists.txt", "test/cpp/api/grad_mode.cpp", "test/cpp/api/inference_mode.cpp", "test/cpp/api/support.h", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_type.py", "tools/codegen/model.py", "torch/csrc/autograd/InferenceMode.h", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/functions/utils.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/script.h"], "labels": ["Merged", "Reverted", "cla signed"]}, "cff266544a": {"title": "Fix minor style/typos problems in comment_device_type.py (#54768)", "body": "Summary:\nOne typo, one example correction and capitalization for a couple of comment lines.\n\nailzhang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54768\n\nReviewed By: H-Huang\n\nDifferential Revision: D27362999\n\nPulled By: ezyang\n\nfbshipit-source-id: 91404ac9e9747ef7d7882a5f50b81d7eb570448b", "pr_number": "54768", "files_changed": ["torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed", "open source"]}, "bab8a1a81e": {"title": "[reland] Add annotations to PRs from forks (#54779)", "body": "Summary:\nWe've been using [pytorch/add-annotations-github-action](https://github.com/pytorch/add-annotations-github-action) to add annotations to PRs when they fail Flake8 or clang-tidy. Up until now, though, that functionality has only worked on PRs in pytorch/pytorch itself, not on PRs from forks. This PR fixes that using a technique from [this GitHub blog post](https://securitylab.github.com/research/github-actions-preventing-pwn-requests/) (also linked in a comment in this diff).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54779\n\nTest Plan: janeyx99 and I tested this in the same GitHub repo used to test https://github.com/pytorch/pytorch/issues/54685 and https://github.com/pytorch/pytorch/issues/54693, including with PRs from forks.\n\nReviewed By: seemethere, xuzhao9\n\nDifferential Revision: D27470866\n\nPulled By: samestep\n\nfbshipit-source-id: d165b8e875d412b910592aa897163fb938d23365", "pr_number": "54779", "files_changed": [".github/workflows/add_annotations.yml", ".github/workflows/lint.yml"], "labels": ["Merged", "Reverted", "cla signed"]}, "f71a0daeb7": {"title": "Use faulthandler to dump traceback of timed out processes in unit tests. (#54818)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54818\n\nSeveral flaky tests fail due to some sort of timeout and it isn't\nclear from the error message in CI where exactly each process is stuck. In this\nPR, I've added mechanism to dump the entire python traceback of all python\nthreads when we encounter a timeout.\n\nExample traceback:\n\n```\nProcess 3 timed out with traceback:\nCurrent thread 0x00007ff3363ff700 (most recent call first):\n  File \"torch/testing/_internal/common_distributed.py\", line 373 in _event_listener\n  File \"threading.py\", line 870 in run\n  File \"threading.py\", line 932 in _bootstrap_inner\n  File \"threading.py\", line 890 in _bootstrap\n\nThread 0x00007ff406132180 (most recent call first):\n  File \"torch/distributed/distributed_c10d.py\", line 2477 in barrier\n  File \"torch/testing/_internal/distributed/rpc/rpc_test.py\", line 838 in test_reinit\n  File \"torch/testing/_internal/dist_utils.py\", line 90 in new_test_method\n  File \"torch/testing/_internal/common_distributed.py\", line 292 in wrapper\n  File \"torch/testing/_internal/common_distributed.py\", line 409 in run_test\n  File \"torch/testing/_internal/common_distributed.py\", line 393 in _run\n  File \"multiprocessing/process.py\", line 108 in run\n  File \"multiprocessing/process.py\", line 315 in _bootstrap\n  File \"multiprocessing/popen_fork.py\", line 75 in _launch\n  File \"multiprocessing/popen_fork.py\", line 19 in __init__\n  File \"multiprocessing/context.py\", line 277 in _Popen\n  File \"multiprocessing/process.py\", line 121 in start\n```\nghstack-source-id: 125323810\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27378764\n\nfbshipit-source-id: 661c009a5458c724f004aa83de9347a4bc03b63e", "pr_number": "54818", "files_changed": ["torch/testing/_internal/common_distributed.py"], "labels": ["Merged", "cla signed"]}, "8ad32dbbd7": {"title": "update build tutorial - choose the correct VS version (#54933)", "body": "Summary:\nThere might be regressions in newest VS.\nRemind users to choose the stable VC version as our CI's\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54933\n\nReviewed By: walterddr\n\nDifferential Revision: D27466645\n\nPulled By: malfet\n\nfbshipit-source-id: a6a1ebea4cc1b22e13c7342ee4c061afcef7e2b5", "pr_number": "54933", "files_changed": ["README.md"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b64d775636": {"title": "Adding workflow to auto-label PRs with ROCm (#54989)", "body": "Summary:\nThis PR adds a workflow that automatically adds ROCm label to PRs and issues with ROCm (case insensitive) in their titles.\nNote that this does not remove labels even if the title is changed to no longer contain ROCm, but I can easily add removal functionality if that is desired.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54989\n\nTest Plan: much test in my own repo: https://github.com/janeyx99/gha-experiments/actions (thanks samestep for your help!)\n\nReviewed By: walterddr\n\nDifferential Revision: D27448651\n\nPulled By: janeyx99\n\nfbshipit-source-id: 103f39df0697eb6571c96e88c98d28c8b7adcfd7", "pr_number": "54989", "files_changed": [".github/workflows/auto_label.yml"], "labels": ["Merged", "cla signed"]}, "fb1c193eed": {"title": "Simplify convolution double backward gradInput formulas (#54840)", "body": "Summary:\nCurrently in convolution double backward grad of input is computed as `convT(ggW, gO.T)`. Notice how first argument is, in fact, of the size that convolution weight has, and second is of the size of gradOutput, which is an inverse order compared to how convolutions are regularly called, and sizes are far from what cudnn heuristics is trained for and what cudnn is guaranteed to have efficient kernels for. This takes cudnn 8 to some dark places, calling  kernels that take 20-100 s. But, luckily for us, convT is a commutative operation (unlike conv), so convT(ggW, gO) is actually the same as  convT(gO, ggW), modulo some transposes because of conventions around the weight size, so we  can use convT(gO, ggW). As an added bonus, we don't need a special branch for groups with this formulation.\nFor the following pretty standard convolution,\n - cudnn 7.6+old formulation takes 7.5 ms for double backward,\n - cudnn 8 + old formulation takes ~40 s,\n - cudnn 8 + new formulation is 1.8 ms with benchmark enabled,\n - cudnn 8 + new formulation is 4 ms with benchmark disabled,\n benchmarking script is below:\n```\nimport torch\nimport time\n\n#torch.backends.cudnn.benchmark=True\n\ndef ggI(conv, inp):\n    out = conv(inp)\n    grads = torch.autograd.grad(out, conv.weight, torch.rand_like(out), create_graph=True, retain_graph=True)\n    torch.cuda.synchronize()\n    start = time.time()\n    grads[0].backward(torch.rand_like(grads[0]))\n    torch.cuda.synchronize()\n    print(\"db time: \", time.time()-start)\n    return inp.grad\n\nconv = torch.nn.Conv2d(512,256,kernel_size=3, padding=1, groups=2).cuda()\ninp = torch.randn(1,512,128,128, device=\"cuda\", requires_grad=True)\nfor _ in range(20):\n    ggI(conv, inp)\ntorch.cuda.synchronize()\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54840\n\nReviewed By: mruberry\n\nDifferential Revision: D27384866\n\nPulled By: ngimel\n\nfbshipit-source-id: c6c875776a9801a0a2cd2f34f8ec39d0fcd59df8", "pr_number": "54840", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["Merged", "cla signed"]}, "5c3963373a": {"title": "Handle 1D input for xnnpack::linear (#54986)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54986\n\nIf the input is 1D xnnpack::linear fails while aten::linear makes it (1, D) and continues\n\nTest Plan: buck test //caffe2/test:xnnpack_integration  -- TestXNNPACKOps\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27441966\n\nfbshipit-source-id: dfb2c23b91247632e0e3fd2482056a503c246c39", "pr_number": "54986", "files_changed": ["aten/src/ATen/native/xnnpack/Linear.cpp", "test/test_xnnpack_integration.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "2726de0119": {"title": "[Pytorch] Expose ops present in dispatcher (#54791)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54791\n\nSeveral usecases have a need to want and see what ops are present in a specific pytorch runtime. This diff exposes that information in the dispatcher\nghstack-source-id: 125314247\n\nTest Plan: D26678637 uses this api.\n\nReviewed By: swolchok\n\nDifferential Revision: D27271371\n\nfbshipit-source-id: e572f0c85dcd75d75356e2cd4cfdd77efee17f94", "pr_number": "54791", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["Merged", "cla signed"]}, "63c70ae032": {"title": "various overhead improvements to cuda addmm (#55026)", "body": "Summary:\nAdd fast common case to `prepare_matrix_for_cublas`, use index size instead of size(), move some checks where they belong so they are not triggered where they are guaranteed to be true.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55026\n\nReviewed By: gchanan\n\nDifferential Revision: D27468945\n\nPulled By: ngimel\n\nfbshipit-source-id: 79c9f7b3d61595536f603d6fb0316e6f21630f38", "pr_number": "55026", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["Merged", "cla signed"]}, "28daa6b7dd": {"title": "[Futures] enhance error handling in then() (#54475)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54475\n\nImplements the proposal in https://github.com/pytorch/pytorch/issues/53717#issuecomment-800545655. See that issue for more details, but at a high level:\n\n1. markCompleted() immediately sets completed_ = true\n2. Subclasses of future (such as cuda future) implement a nontrivial `postMarkCompletedHook` which may throw\n3. If above error is caught and we call `setError`, setError itself will error out because completed_ = true.\n\nTo fix this, only call setError if the user-defined cb resulted in an error, otherwise, call `markCompleted` and let postMarkCompletedHook() throw and crash the program (per lw's thoughts this should be a fatal).\nghstack-source-id: 125300388\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27252965\n\nfbshipit-source-id: fda41e8844104774aaf897286512d83ff06632b1", "pr_number": "54475", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/cuda/CUDAFuture.h"], "labels": ["Merged", "cla signed"]}, "a37fbf9b45": {"title": "[Futures] Bump log verbosity when ignoring cb errors in python future. (#54476)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54476\n\nPer title. For `add_done_callback`, we log but swallow exceptions in order to keep consistent with what concurrent.futures python library does, see discussion in https://github.com/pytorch/pytorch/pull/45675.\n\nAlthough, it would be good to improve the verbosity here as this can be a source of confusion if users are setting a different future via `add_done_callback`, and an error is hit resulting in an unexpected hang (see https://github.com/pytorch/pytorch/issues/52132 for more details on how this can happen).\nghstack-source-id: 125300389\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27253004\n\nfbshipit-source-id: 72ed21c8fb6d27de5797c17fc46b762f893e6fea", "pr_number": "54476", "files_changed": ["torch/csrc/jit/python/pybind_utils.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "908a74e8c1": {"title": "[Refactoring] make transformations return whether graph is modified (#54777)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54777\n\nUpdated RemoveListMutation, PeepholeOptimizedListIdoms,\nLoopUnrolling, PeepholeOptimization to return whether graph is\nmodified after transformation, PeepholeAliasSensitivity\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D27412105\n\nfbshipit-source-id: 0c1520bc34f6bd59acd83d98bed58897376eac41", "pr_number": "54777", "files_changed": ["torch/csrc/jit/passes/loop_unrolling.cpp", "torch/csrc/jit/passes/loop_unrolling.h", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/passes/peephole.h", "torch/csrc/jit/passes/peephole_alias_sensitive.cpp", "torch/csrc/jit/passes/peephole_alias_sensitive.h", "torch/csrc/jit/passes/peephole_list_idioms.cpp", "torch/csrc/jit/passes/peephole_list_idioms.h", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/passes/remove_mutation.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "70af5db7ca": {"title": "Remove use_c10_dispatcher option (#54969)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54969\n\nWith all use cases to hacky wrapper removed, all kernels will be\ndispatched with c10 full dispatcher.\nghstack-source-id: 125434790\n\nTest Plan: buck build //caffe2/aten/...\n\nReviewed By: ezyang, walterddr\n\nDifferential Revision: D27436596\n\nfbshipit-source-id: 7a146d1f4a983b4a81f8552be4eec6c482b6bea2", "pr_number": "54969", "files_changed": ["aten/src/ATen/core/op_registration/adaption.h", "aten/src/ATen/core/op_registration/hacky_wrapper_for_legacy_signatures.h", "aten/src/ATen/native/README.md", "aten/src/ATen/templates/Functions.cpp", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/RedispatchFunctions.cpp", "aten/src/ATen/templates/RedispatchFunctions.h", "aten/src/ATen/templates/RegisterDispatchKey.cpp", "aten/src/ATen/templates/RegisterSchema.cpp", "aten/src/ATen/templates/TensorMethods.cpp", "tools/codegen/api/cpp.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/native.py", "tools/codegen/context.py", "tools/codegen/dest/register_dispatch_key.py", "tools/codegen/local.py", "tools/codegen/model.py"], "labels": ["Merged", "cla signed"]}, "f29039677d": {"title": "Refactor get numerical jacobian (#54092)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54092\n\nThis is the first of several refactors to get numerical jacobian:\nThis one just moves some logic around as to try to split the get_numerical_jacobian function into smaller more manageable functions:\n- compute_gradient is now no longer nested, but we have to pass in the parameters instead\n- iter_tensor extracts out the logic of iterating through different types of tensors (the code should be almost the exact same here except for instead of calling into the update jacobian function, we yield the arguments instead)\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D27354268\n\nPulled By: soulitzer\n\nfbshipit-source-id: 73288e3c889ae31bb8bf77a0e3acb3e9020e09a3", "pr_number": "54092", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "0d80f378f6": {"title": "fix boto3 resource not close (#55082)", "body": "Summary:\nTest Plan\nGHA CI\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55082\n\nReviewed By: samestep\n\nDifferential Revision: D27475006\n\nPulled By: walterddr\n\nfbshipit-source-id: ccf50ea0b15ea6840e593a2c056ed2b388a96c52", "pr_number": "55082", "files_changed": ["tools/stats_utils/s3_stat_parser.py"], "labels": ["Merged", "cla signed"]}, "c85d3f501f": {"title": "Move shape prop inside acc_tracer (#55091)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55091\n\nTest Plan: All tests are updated and passing.\n\nReviewed By: 842974287\n\nDifferential Revision: D27471887\n\nfbshipit-source-id: 98969fb1bfc72f6c57835525d82d4a8b78bb19bb", "pr_number": "55091", "files_changed": ["torch/fx/experimental/accelerator_partitioner.py", "torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "790b69e096": {"title": "Language Ref for Statements in Torchscript (#52847)", "body": "Summary:\nAddresses the Statements supported in Torchscript for Language Spec\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52847\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27463142\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: ff3def1b878092b0a2afc7c2f47b7857e6658ecf", "pr_number": "52847", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "8b8c4096ee": {"title": "Added OpInfo gradcheck_wrapper to replace output_func (#54914)", "body": "Summary:\nAdded a field to `OpInfo` to provide a wrapper function for gradcheck. This is useful for functions that need to perform some extra input/output processing to work with gradcheck.\n\nfixes https://github.com/pytorch/pytorch/issues/50837\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54914\n\nReviewed By: H-Huang\n\nDifferential Revision: D27435234\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: fa3e9b61f3d3df221243fd142ddb8b7861dbf669", "pr_number": "54914", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "6c235ef267": {"title": "Allow `std=0` in `torch.normal`, and error if `std<0` (#51317)", "body": "Summary:\nPart of https://github.com/pytorch/pytorch/issues/49998\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51317\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27253939\n\nPulled By: mruberry\n\nfbshipit-source-id: af7a72c3d91549b1a88b73849b6973e7619dc50b", "pr_number": "51317", "files_changed": ["aten/src/ATen/native/DistributionTemplates.h", "test/test_cuda.py", "test/test_tensor_creation_ops.py", "test/test_torch.py", "torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: correctness (silent)", "module: random", "open source", "triaged"]}, "d2aab53dc2": {"title": "[PyTorch] Check is_same instead of data_ptr in addmm_out_cuda_impl (#55111)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55111\n\nI don't see how we could have ended up with !is_same but also identical data_ptr, and is_same is cheaper.\nghstack-source-id: 125438822\n\nTest Plan: Existing CI?\n\nReviewed By: ngimel\n\nDifferential Revision: D27484914\n\nfbshipit-source-id: 22125b29e6e09d312a2b92e893d08c69059e4435", "pr_number": "55111", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["Merged", "cla signed"]}, "058357a439": {"title": "[Gradient Compression] Report compression rate for batched PowerSGD hook (#55103)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55103\n\nPreviously compression rate is only reported in PowerSGD hook. Also report this metric for comprehensive experimentation.\n\nIt is very easy to compute the sizes before and after compression, because there is only one matrix factorization per bucket, and no accumulation within the bucket is needed.\n1) The size before compression is the input tensor size.\n2) The size after compression is the size of P + Q, where each has a size of `square_side_length * state.matrix_approximation_rank`.\nghstack-source-id: 125399028\n\nTest Plan: Tested by running scripts/wayi/torch/power_sgd.py locally.\n\nReviewed By: deadlybulb\n\nDifferential Revision: D27474295\n\nfbshipit-source-id: a2225e85be03ab20238f01014d5ec9ae1787c4fb", "pr_number": "55103", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ba95e08a95": {"title": "[PyTorch] Use DimVector for inputs to as_strided that don't grow dim (#55016)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55016\n\nWhen we call as_strided() and don't add an extra dimension, we should continue to expect that the number of dimensions will fit in a DimVector and thus that using it will save heap allocations.\nghstack-source-id: 125337281\n\nTest Plan: Existing CI\n\nReviewed By: ngimel\n\nDifferential Revision: D27452838\n\nfbshipit-source-id: 8b3d118de322638c0c0e3a4bfcfb3c820c64e6cc", "pr_number": "55016", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed"]}, "5a1191d050": {"title": "Check exception messages in embedding_bag_proxy unit test", "body": "Summary:\nThis replaces the use of `assertRaises` with `assertRaisesRegex` to make sure that we catch the expected exceptions.\nIt also corrects a few unit tests:\n* Test for the case where `input is 2D and offsets is not None` was wrong.\n* Check for `empty offsets` was missing.\n* Check for `offsets length when include_last_offset=True` was wrong.\n\nTest Plan:\n```\nbuck test mode/opt caffe2/torch/fb/training_toolkit/common/proxy_module_thrift/tests:test_embedding_bag_proxy\n    \u2713 ListingSuccess: caffe2/torch/fb/training_toolkit/common/proxy_module_thrift/tests:test_embedding_bag_proxy - main (3.049)\n    \u2713 Pass: caffe2/torch/fb/training_toolkit/common/proxy_module_thrift/tests:test_embedding_bag_proxy - test_module_swapping_py (caffe2.torch.fb.training_toolkit.common.proxy_module_thrift.tests.test_embedding_bag_proxy.EmbeddingBagProxyTest) (1.084)\n    \u2713 Pass: caffe2/torch/fb/training_toolkit/common/proxy_module_thrift/tests:test_embedding_bag_proxy - test_bad_inputs (caffe2.torch.fb.training_toolkit.common.proxy_module_thrift.tests.test_embedding_bag_proxy.EmbeddingBagProxyTest) (1.164)\n    \u2713 Pass: caffe2/torch/fb/training_toolkit/common/proxy_module_thrift/tests:test_embedding_bag_proxy - test_module_swapping_jit (caffe2.torch.fb.training_toolkit.common.proxy_module_thrift.tests.test_embedding_bag_proxy.EmbeddingBagProxyTest) (1.388)\nSummary\n  Pass: 3\n  ListingSuccess: 1\nFinished test run: https://www.internalfb.com/intern/testinfra/testrun/4222124700133860\n\nbuck test caffe2/test:nn\n  Pass: 1086\n  Skip: 1099\n  Timeout: 3\n  Omit: 1\n    {emoji:2702} caffe2/test:nn - test_conv_double_backward (test_nn.TestNN)\n  ListingSuccess: 1\nFinished test run: https://www.internalfb.com/intern/testinfra/testrun/6755399476551597\n\nbuck test caffe2/benchmarks/static_runtime:static_runtime_cpptest\n    \u2713 ListingSuccess: caffe2/benchmarks/static_runtime:static_runtime_cpptest - main (7.985)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.TrivialModel (12.349)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.LongModel (12.805)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.IndividualOps_to (12.890)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.IndividualOps_pow (13.329)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.EmbeddingBag (13.703)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.IndividualOps_Reshape (13.886)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.LeakyReLU (13.964)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.IndividualOps_Binary (13.967)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.DeepWide (14.095)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.KWargsAPI_1 (14.461)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.UnaryOps (14.527)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.CleanUpMemory (14.624)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.FusionPass (14.635)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.KWargsAPI_2 (15.027)\n    \u2713 Pass: caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.IndividualOps_flatten (15.299)\nSummary\n  Pass: 15\n  ListingSuccess: 1\nFinished test run: https://www.internalfb.com/intern/testinfra/testrun/5348024606957775\n```\n\nReviewed By: qizzzh\n\nDifferential Revision: D27415247\n\nfbshipit-source-id: c4915170e89359ea961c1a6df513b29790f147fa", "pr_number": null, "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp"], "labels": []}, "9d6a81d1a6": {"title": "Avoid aggregate initialization for tensorpipe::{Cpu,Cuda}Buffer and tensorpipe::Message::Tensor. (#55136)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55136\n\nThis will ease the transition to the new API where `Buffer` does not\nstore a length anymore.\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27466385\n\nfbshipit-source-id: 9a167f8c501455a3ab49ce75257c69d8b4869925", "pr_number": "55136", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "8b02d1207b": {"title": "Fixed OpInfo jit tests failing for TensorList inputs (#54954)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53906\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54954\n\nReviewed By: glaringlee\n\nDifferential Revision: D27474863\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: cf8c1cac6fd1cceacd6be73a2eb49d28a5cfc20a", "pr_number": "54954", "files_changed": ["test/test_ops.py", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/testing/_internal/common_jit.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/jit_metaprogramming_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "0cfd9e881f": {"title": "[static runtime] fix out variant for 4bit embedding bag (#55096)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55096\n\nThere were issues with D26138322 (https://github.com/pytorch/pytorch/commit/5b0a6482c185428c41d8772a92fa0b77174821fc) that we didn't catch the first time around.\nThis (rebased on top of the to_copy fixes)  fixes the converted remote_ro c2/pt output comparison\n\nTest Plan:\n```\nMKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --c2_model=/data/users/ansha/tmp/adfinder/210494966_0.predictor.disagg.remote_request_only --c2_inputs=/data/users/ansha/tmp/adfinder/models/c2_remote_ro_input_data.pb --pred_net=/data/users/ansha/tmp/adfinder/models/c2_remote_ro_net2.pb --c2_sigrid_transforms_opt=1 --c2_apply_nomnigraph_passes=1 --c2_use_memonger=1 --scripted_model=/data/users/ansha/tmp/adfinder/models_dianshi/210494966_0.predictor.disagg.remote_request_only.pt --pt_inputs=/data/users/ansha/tmp/adfinder/models/remote_ro_wrapped_input_data.pt --pt_enable_static_runtime=1 --pt_cleanup_activations=1 --pt_enable_out_variant=1 --compare_results=1 --iters=1 --warmup_iters=1 --num_threads=1 --do_profile=0 --benchmark_c2_predictor=1 --do_benchmark=1\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27477104\n\nfbshipit-source-id: 5a95dfa7eae23566fadc3fec323ad03a34e6734d", "pr_number": "55096", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag.h", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "f83668b4e5": {"title": "Update release notes scripts following runbook update (#54594)", "body": "Summary:\nThis adds:\n- new categories\n- global commit counter\n- support for new \"Reverted\" label on PRs\n- new export system to multiple files\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54594\n\nReviewed By: H-Huang\n\nDifferential Revision: D27396011\n\nPulled By: albanD\n\nfbshipit-source-id: ca1ec3a1b90221ba26fd8b053dfb10f614f05909", "pr_number": "54594", "files_changed": ["scripts/release_notes/categorize.py", "scripts/release_notes/commitlist.py", "scripts/release_notes/common.py"], "labels": ["Merged", "cla signed"]}, "69c5fd1e00": {"title": "SyncBatchNorm.forward() to handle optional weight (#54568)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54495\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54568\n\nReviewed By: ezyang\n\nDifferential Revision: D27285822\n\nPulled By: malfet\n\nfbshipit-source-id: 4f7b489d80294cb2509eec4f6c4aa22d5c47b35d", "pr_number": "54568", "files_changed": ["torch/nn/modules/_functions.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed"]}, "f0dafeb0cb": {"title": "Trigger azure pipeline for multi gpu tests (#52490)", "body": "Summary:\nThe run on CircleCI:  https://app.circleci.com/pipelines/github/pytorch/pytorch/283891/workflows/e049872e-5327-4f8c-abc3-a72ca6a1a548/jobs/11462671\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52490\n\nReviewed By: glaringlee\n\nDifferential Revision: D27470071\n\nPulled By: malfet\n\nfbshipit-source-id: 9b7615799da5fc8381fef226da003449b2698d35", "pr_number": "52490", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/regenerate.ps1", ".circleci/scripts/trigger_azure_pipeline.py", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", ".jenkins/pytorch/win-test-helpers/test_distributed.bat"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "8d5df95551": {"title": "Make TensorIterator, SparseTensorMath and UnaryOps clang-tidy clean (#55087)", "body": "Summary:\nDisable `cppcoreguidelines-macro-usage` as PyTorch codebase uses a lots\nof macros that violate this rule.\n\nDisable `bugprone-reserved-identifier` and\n`performance-unnecessary-value-param` as those checks are very slow\n\nAdd `NOLINT` to DEFINE_DISPATCH as it introduces non-const global variables\nReplace `for(auto i = 0; i < lim; ++i)` with `for(auto i: c10::irange(lim))` throughout the modified files\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55087\n\nReviewed By: samestep\n\nDifferential Revision: D27475822\n\nPulled By: malfet\n\nfbshipit-source-id: 2651a4b3dc062066a15e69380354414a198fb279", "pr_number": "55087", "files_changed": [".clang-tidy", "aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp"], "labels": ["Merged", "cla signed"]}, "6b20046491": {"title": "Pin ShellCheck version to v0.7.1 (#55109)", "body": "Summary:\nNot sure why I didn't do this in https://github.com/pytorch/pytorch/issues/47786. Version 0.7.1 (the latest `\"stable\"` version of ShellCheck) was released [almost a year ago](https://github.com/koalaman/shellcheck/releases/tag/v0.7.1), but even if releases are infrequent, it's better to just get rid of the nondeterminism that caused https://github.com/pytorch/pytorch/issues/47786 in the first place.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55109\n\nTest Plan: The \"Lint / quick-checks\" job in GitHub Actions.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27483473\n\nPulled By: samestep\n\nfbshipit-source-id: e09f52844db440f2b6ea3cd54340c9e62dea09f4", "pr_number": "55109", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "36c27fd0ac": {"title": "SVD docs improved (#54002)", "body": "Summary:\n- Corrected a few errata in the SVD docs\n- Made the notation more uniform (refer to `Vh` in `linalg.svd`, always use double tilts...)\n- Wrote a better explanation about why the gradients of `U` and `V` are not well-defined when the input is complex or real but has repeated singular values. The previous one pointed to a somewhat obscure post on gauge theory.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54002\n\nReviewed By: malfet\n\nDifferential Revision: D27459502\n\nPulled By: mruberry\n\nfbshipit-source-id: f5c35eca02d35dadd2fc0eeadfacc8824f409400", "pr_number": "54002", "files_changed": ["test/test_torch.py", "torch/_torch_docs.py", "torch/linalg/__init__.py"], "labels": ["Merged", "cla signed", "module: docs", "open source", "triaged"]}, "2798f38c86": {"title": "Added checks for dtype and device of OpInfo's sample_inputs (#54949)", "body": "Summary:\nCurrently, it's not tested whether `op.sample_inputs` actually used the provided dtype and device arguments. This PR fixes that introducing asserts in `test_supported_dtypes`.\nThis will help to detect incorrectly generated inputs in the future.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54949\n\nReviewed By: H-Huang\n\nDifferential Revision: D27435952\n\nPulled By: mruberry\n\nfbshipit-source-id: 8465c459b9b0c007411a9a74340bc2755519624a", "pr_number": "54949", "files_changed": ["test/test_ops.py"], "labels": ["Merged", "cla signed", "module: tests", "open source", "triaged"]}, "a4125876c9": {"title": "Move BackendSelect to default_included_set. (#55117)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55117\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27490571\n\nPulled By: ailzhang\n\nfbshipit-source-id: a0d8a25a8217a754061fbf3b8e31cc1cf2d3bdea", "pr_number": "55117", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "c10/core/DispatchKeySet.h", "c10/core/impl/LocalDispatchKeySet.cpp"], "labels": ["Merged", "cla signed"]}, "53609b4cac": {"title": "fix typo in ReduceMinMaxKernel (#54984)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54984\n\nReviewed By: zou3519\n\nDifferential Revision: D27494418\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 5066df75ba82c15787edbcb0208594aac2bbaf01", "pr_number": "54984", "files_changed": ["aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c64e006fc3": {"title": "Fix security of ROCm labeling workflow (#55157)", "body": "Summary:\nThe current workflow fails when there are backticks in the PR title, because bash tries to evaluate it right away. (example: https://github.com/pytorch/pytorch/runs/2242913870) Moving the variables to the env section away from bash, which removes the security risk.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55157\n\nTest Plan: my repo: https://github.com/janeyx99/gha-experiments/actions/runs/709088679\n\nReviewed By: gchanan\n\nDifferential Revision: D27505033\n\nPulled By: janeyx99\n\nfbshipit-source-id: 1cc7545c18400d63a4490d9b019afe383b272229", "pr_number": "55157", "files_changed": [".github/workflows/auto_label.yml"], "labels": ["Merged", "ROCm", "cla signed"]}, "1b2b3ca86d": {"title": "Language Ref Python Builtin Functions and Values (#52830)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52830\n\nReviewed By: SplitInfinity, nikithamalgifb\n\nDifferential Revision: D27407474\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 06fcafbcc66376c5f1818cb12fca2f2a57843c9d", "pr_number": "52830", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "0eba63ec93": {"title": "Improve testing documentation in `CONTRIBUTING.md` (#54904)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54904\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D27407009\n\nPulled By: ansley\n\nfbshipit-source-id: ae69d8387b55f714fd105efe7c4ecbdd69674f65", "pr_number": "54904", "files_changed": ["CONTRIBUTING.md"], "labels": ["Merged", "cla signed"]}, "f34de6a9f4": {"title": "Modified lstsq_helper to accept rank and singular_values (#54719)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54719\n\nlstsq_helper now takes rank and singular_values that are modified in-place.\nThis is required for adding out= variant.\n\nTODO:\n\n- [ ] Fix CI failures\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27439197\n\nPulled By: mruberry\n\nfbshipit-source-id: f2fe421aa393c2d58f5c50f33e21a9eae57e4f01", "pr_number": "54719", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["Merged", "cla signed", "open source"]}, "b74795c460": {"title": "[Pyper] resize_as_ -> resize_ (#55098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55098\n\nresize_as_ still goes through the dispatcher because it calls tensor.resize_. We can easily call resize_ directly while bypassing the dispatcher.\n\nReviewed By: swolchok\n\nDifferential Revision: D27457894\n\nfbshipit-source-id: 8a5da185d1a6addafbf4915e29613013451b5e43", "pr_number": "55098", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "070169e4d0": {"title": "[ATen] tensor.contiguous() -> tensor.expect_contiguous (#55022)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55022\n\nReplace tensor.contiguous() with tensor.expect_contiguous in aten::narrow_copy\n\nTest Plan: CI\n\nReviewed By: edvgha\n\nDifferential Revision: D27453866\n\nfbshipit-source-id: c5a6e64ccca4cf52cb879dfb02fd4c451fb397cb", "pr_number": "55022", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "dfa2daac1d": {"title": "[PyTorch] Remove outdated C++11 note on C10_DEPRECATED (#55061)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55061\n\nWe're C++14.\nghstack-source-id: 125377571\n\nTest Plan: Review\n\nReviewed By: bhosmer\n\nDifferential Revision: D27467852\n\nfbshipit-source-id: 720cdd02813e84a43357ab5e35dfebe3d773bb0f", "pr_number": "55061", "files_changed": ["c10/util/Deprecated.h"], "labels": ["Merged", "cla signed"]}, "84ad5df8e3": {"title": "Correct the name of the label in auto-label-rocm (#55170)", "body": "Summary:\nThe label name was meant to be \"module: rocm\".\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55170\n\nTest Plan: None.\n\nReviewed By: malfet\n\nDifferential Revision: D27513290\n\nPulled By: samestep\n\nfbshipit-source-id: ef86fcd5f94a76c9e04653995c2ba9369c5ecb34", "pr_number": "55170", "files_changed": [".github/workflows/auto_label.yml"], "labels": ["Merged", "ROCm", "cla signed"]}, "0a329c66bf": {"title": "[PyTorch] Remove stray comments in TensorBody (#54985)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54985\n\nI forgot to fix these before landing D27375016 (https://github.com/pytorch/pytorch/commit/e829754992901cf59d121ece443a5705a2ade13a).\nghstack-source-id: 125291731\n\nTest Plan: Review\n\nReviewed By: bhosmer\n\nDifferential Revision: D27442002\n\nfbshipit-source-id: 0bff8396e90f4e6889bf3320c2e316760491ce2f", "pr_number": "54985", "files_changed": ["aten/src/ATen/templates/TensorBody.h"], "labels": ["Merged", "cla signed"]}, "787854ce41": {"title": "[ZeroRedundancyOptimizer] bounding the multiple gpus unit test to 4 gpus, hardcoded values (#54788)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53322, the test has some hardcoded values to check that the sharding works as expected, and was not used beyond 4 gpus prior\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54788\n\nReviewed By: mrshenli\n\nDifferential Revision: D27483078\n\nPulled By: blefaudeux\n\nfbshipit-source-id: 63fe072c41e1601925af23d8fb1ea3f4729b2044", "pr_number": "54788", "files_changed": ["test/distributed/optim/test_zero_redundancy_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "2962fee99a": {"title": "Fix/suppress a type warning in PyTorch (#55142)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55142\n\nDeclare some functions C10_HOST_DEVICE to fix the NVCC warning.\n\nDuring pytorch compilation, NVCC compiler was emmiting several warnings like this one:\n\n```\ncaffe2/c10/util/TypeCast.h(39): warning: calling a constexpr __host__ function from a __host__ __device__ function is not allowed. The experimental flag '--expt-relaxed-constexpr' can be used to allow this.\n          detected during:\n            instantiation of \"dest_t c10::static_cast_with_inter_type<dest_t, src_t>::apply(src_t) [with dest_t=c10::complex<double>, src_t=__nv_bool]\"\n(158): here\n            instantiation of \"To c10::convert<To,From>(From) [with To=c10::complex<double>, From=__nv_bool]\"\n(170): here\n            instantiation of \"To c10::checked_convert<To,From>(From, const char *) [with To=c10::complex<double>, From=__nv_bool]\"\ncaffe2/c10/core/Scalar.h(63): here\n```\n\nHow to reproduce.\n- Make sure you are on remote/master\n- run:\n  `buck build mode/dev-nosan caffe2/torch/fb/sparsenn:sparsenn_operators_gpu`\n\nTest Plan: - compilation completes without warnings.\n\nReviewed By: r-barnes\n\nDifferential Revision: D27469757\n\nfbshipit-source-id: f8c4eedb637c6d487ac49bb310e48be11db204e2", "pr_number": "55142", "files_changed": ["c10/util/complex.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "faa4da49ff": {"title": "Add code to ensure workflow consistency for autocanceling (#55171)", "body": "Summary:\nCurrently, we only have three GHA workflows that need to be canceled on reruns. To anticipate for future workflows, this PR enables a check that will make sure any new workflow that should be autocanceled on reruns will be included in the cancel_redundant_workflows.yml GHA workflow.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55171\n\nTest Plan: Succeeded quick-checks https://github.com/pytorch/pytorch/runs/2249162035?check_suite_focus=true\n\nReviewed By: samestep\n\nDifferential Revision: D27514294\n\nPulled By: janeyx99\n\nfbshipit-source-id: 27da321f648b97a090052823ec955caffeb6ae97", "pr_number": "55171", "files_changed": [".github/scripts/generate_workflow_names.py", ".github/workflows/cancel_redundant_workflows.yml", ".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "8dc29e8a4a": {"title": "[PyTorch] Allow IValue to construct from Tuple with fewer copies (#54534)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54534\n\nMoving overload of tuple -> IValue constructor was missing.\nghstack-source-id: 124671165\n\nTest Plan:\nCompare assembly for ivalue_test.cpp before/after this\nchange. Newly added snippet stops calling `std::__invoke_impl` with a\nreal function pointer to a by-value variant of\n`c10::ivalue::Tuple::create` and starts directly calling\nby-const-reference variant of `c10::ivalue::Tuple::create` instead.\n\nReviewed By: smessmer\n\nDifferential Revision: D27271895\n\nfbshipit-source-id: 8b0e146a15d66883146b89b93da5e95f903484e6", "pr_number": "54534", "files_changed": ["aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/test/ivalue_test.cpp"], "labels": ["Merged", "cla signed"]}, "22f3b4eaa8": {"title": "Tensor::register_hook: Avoid wrapping hook in two levels of std::function (#53917)", "body": "Summary:\nThe void overload of `register_hook` puts the user's callable into a `std::function` which is used in a lambda, then `_register_hook` wraps that lambda in another `std::function`. This is bad because each call goes through two indirections and also it requires more heap allocations.\n\nInstead, the lambda can capture the original callable without wrapping it in an `std::function` first.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53917\n\nReviewed By: gchanan\n\nDifferential Revision: D27513822\n\nPulled By: swolchok\n\nfbshipit-source-id: 026d40d7e9fb718757b7203737b0662ba36bc021", "pr_number": "53917", "files_changed": ["aten/src/ATen/templates/TensorBody.h"], "labels": ["Merged", "cla signed", "open source"]}, "dded5d72a4": {"title": "[PyTorch] Move Tensor::has_names inline (#54965)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54965\n\nYet another small getter that should be inlineable.\nghstack-source-id: 125338544\n\nTest Plan:\nFramework overhead benchmark w/o arguments\n\nBefore:\n```\nI0329 13:56:41.268244 447880 bench.cpp:186] Mean 0.562635\nI0329 13:56:41.268270 447880 bench.cpp:187] Median 0.562465\nI0329 13:56:41.268276 447880 bench.cpp:188] Min 0.561757\nI0329 13:56:41.268285 447880 bench.cpp:189] stddev 0.000707741\nI0329 13:56:41.268292 447880 bench.cpp:190] stddev / mean 0.0012579\n```\n\nAfter:\n```\nI0329 14:32:34.116181 607857 bench.cpp:186] Mean 0.557326\nI0329 14:32:34.116206 607857 bench.cpp:187] Median 0.557194\nI0329 14:32:34.116212 607857 bench.cpp:188] Min 0.556323\nI0329 14:32:34.116219 607857 bench.cpp:189] stddev 0.000700897\nI0329 14:32:34.116226 607857 bench.cpp:190] stddev / mean 0.00125761\n```\n\nSo roughly 1% faster overall if I've done the mental arithmetic right?\n\nReviewed By: ezyang\n\nDifferential Revision: D27410928\n\nfbshipit-source-id: 4e66d40c71f534f66deb9c64502fb35d0a5997bf", "pr_number": "54965", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.cpp"], "labels": ["Merged", "cla signed"]}, "057ec81b17": {"title": "[PyTorch] OperandInfo ctor should take rvalue reference (#54972)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54972\n\nNo reason to create a temporary.\nghstack-source-id: 125338543\n\nTest Plan: CI\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27437190\n\nfbshipit-source-id: 05eeb3ccd33700d8776b6ce58a120c7697acf49e", "pr_number": "54972", "files_changed": ["aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed"]}, "047a487b07": {"title": "Fix accidental Flake8 excludes (#55178)", "body": "Summary:\n[Currently](https://github.com/pytorch/pytorch/blob/faa4da49ff6fdde8411572fe907e861e95ce2c85/.flake8#L22), our `.flake8` config file has the `exclude` pattern `scripts`. I'm guessing that this is just meant to exclude the top-level `scripts` dir from Flake8, but it also applies to the following (apparently erroneously):\n\n- `.circleci/scripts`\n- `.github/scripts`\n- `test/scripts`\n\nThis PR corrects the problem by making all the `exclude` patterns (except for the wildcard `*.pyi` pattern) relative to the repository root. Also, since this PR already touches all the `exclude` lines, it also sorts them to help reduce merge conflicts when `.flake8` is edited in the future. This sorting happened to reveal that the `build` pattern was previously present twice, so now it has been deduplicated.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55178\n\nTest Plan:\nLocally:\n```\nflake8\n```\nAnd also [in CI](https://github.com/pytorch/pytorch/pull/55178/checks?check_run_id=2249949511).\n\nReviewed By: janeyx99\n\nDifferential Revision: D27520412\n\nPulled By: samestep\n\nfbshipit-source-id: 359275c10ca600ee4ce7906e3a7587ffaa4ae1ed", "pr_number": "55178", "files_changed": [".circleci/scripts/upload_binary_size_to_scuba.py", ".flake8", "test/scripts/run_cuda_memcheck.py"], "labels": ["Merged", "cla signed"]}, "8822c7e052": {"title": "Update TensorPipe submodule. (#55164)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55164\n\nReviewed By: mrshenli\n\nDifferential Revision: D27522063\n\nPulled By: beauby\n\nfbshipit-source-id: 5473ab7a51f5da365bd5931254bc4d9f47b46201", "pr_number": "55164", "files_changed": ["third_party/tensorpipe"], "labels": ["Merged", "cla signed"]}, "5610e8271b": {"title": "Fix skip_if_not_multigpu decorator (#54916)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54916\n\nFixes https://github.com/pytorch/pytorch/issues/54887\n\n`skip_if_not_multigpu` was skipping all the tests that use it.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D27412193\n\nPulled By: H-Huang\n\nfbshipit-source-id: 28d6697bd8cc6b6784cdb038ccb3ff138d0610eb", "pr_number": "54916", "files_changed": ["test/distributed/optim/test_zero_redundancy_optimizer.py", "test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "test/quantization/test_quantize_fx.py", "torch/testing/_internal/common_distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "0d47374c54": {"title": "construct only necessary elements in OffsetCalculator (#55107)", "body": "Summary:\nPer title. Elements beyond `dim` are never accessed because https://github.com/pytorch/pytorch/blob/646510f7028f12e8b1f3a9d3b63b8519ed80e391/aten/src/ATen/cuda/detail/OffsetCalculator.cuh#L49-L51.\nOn `addmm` instruction count per 30 repetitions 1467813 -> 1452261\n`add`  651522 -> 633462\n`add_` 529331 -> 511271\n\nadd benchmarking snippet:\n```\n timer = Timer(\"m1.add_(b);\", setup=\"at::Tensor m1=torch::empty({2,2},device(at::kCUDA) ); at::Tensor b = torch::empty({2}, device(at::kCUDA));\", language=\"c++\", timer=timeit.default_timer)\n stats=timer.collect_callgrind(number=30)\n print(stats.as_standardized().stats(inclusive=False))\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55107\n\nReviewed By: swolchok\n\nDifferential Revision: D27494492\n\nPulled By: ngimel\n\nfbshipit-source-id: 23389a6bc9c9c0096751b95e7f9bf1c9f7bc594f", "pr_number": "55107", "files_changed": ["aten/src/ATen/cuda/detail/OffsetCalculator.cuh"], "labels": ["Merged", "cla signed"]}, "ed4a1d54a7": {"title": "[OpInfo] Enable jit tests for multi_dot (#55147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55147\n\nEnabling this test now that jit supports TensorList inputs\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D27505270\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 05b0d47cb71740309ec5130bf520c576fb90a4d1", "pr_number": "55147", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "b074a24394": {"title": "Port torch.copysign method_tests() to OpInfo (#54945)", "body": "Summary:\nRelated https://github.com/pytorch/pytorch/issues/54261\n\nThis PR ports the method_tests() entries of `torch.copysign` to OpInfo.\n\nWhile porting the tests, the `test_out` cases from `test_ops.py` would fail as the out variant of `torch.copysign` does not support scalar inputs.\n```python\n>>> x = torch.randn(2)\n>>> y = torch.empty_like(x)\n>>> torch.copysign(x, 1.)\ntensor([1.4836, 1.2156])\n>>> torch.copysign(x, 1., out=y)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: copysign(): argument 'other' (position 2) must be Tensor, not float\n```\nThis PR fixes the tests by adding an overload `native_functions` entry and re-dispatching scalar inputs to the existing `copysign_out` function.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54945\n\nReviewed By: gchanan\n\nDifferential Revision: D27505300\n\nPulled By: mruberry\n\nfbshipit-source-id: f68250fa52f8dcfd45426039ec178ca5e883e206", "pr_number": "54945", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "09f1f14569": {"title": "Transition to new tensorpipe::Pipe API. (#55193)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55193\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27466387\n\nfbshipit-source-id: 07b831d699f56874dd45f37e448b8c4244ead5e3", "pr_number": "55193", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "271879fe67": {"title": "[PyTorch Edge] Provide a method ObservedOperators::getUnobservedOperatorList() so that model tracer can empty it out during tracing (#55017)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55017\n\nJacobSzwejbka in D26678637 found the mis-alignment between the operator list that the YAML file claimed and the dispatcher claimed. After some digging and thorough investigation by JacobSzwejbka, we have come to the conclusion that the non-traced operators are more trouble than they are worth since they will result in phantom operators which every user of the capabilities API needs to be aware of (or every language implementation needs to be aware of). Instead, with this change, we can **reliably** trace all operators called via the dispatcher by clearing the list of un-observed operators during model tracing.\n\nAlso another thing to note is that the ignore-list in the observer is a list of base operator names, and not full operator names (with overload), which is whaat tracing based selective build needs. If we use the ignore-list, then we would need to include every overload on un-traced operators.\n\nLatency isn't an issue during model tracing, so this should be generally okay.\n\nRan the following command to re-generate all the YAML files: `buck run caffe2/torch/fb/mobile/cli:cli -- --gen_all_model_configs`\nghstack-source-id: 125337353\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: Sandcastle and wait for unit tests. Also see BSB results in the diff comments.\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D27452855\n\nfbshipit-source-id: 410bafec7ac67503f68623a5e3d4ab258f434cbf", "pr_number": "55017", "files_changed": ["aten/src/ATen/core/dispatch/ObservedOperators.cpp", "aten/src/ATen/core/dispatch/ObservedOperators.h"], "labels": ["Merged", "cla signed"]}, "ec609e7420": {"title": "Adds torch.* API section for TorchScript Lang Ref (#53236)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/53236\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27526584\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: ea931ea63aa4b37a7782935a1760bebffedc5b67", "pr_number": "53236", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "61914cb2fa": {"title": "[ATen][qembeddingbag] Avoid tensor refcount bumps (#55023)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55023\n\nTest Plan: CI\n\nReviewed By: swolchok\n\nDifferential Revision: D27453856\n\nfbshipit-source-id: f2b5ed97d3cc179baba4c158871a0225e3ba9030", "pr_number": "55023", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "3e185253b6": {"title": "Use tensorpipe::Buffer::device() instead of tensorpipe::Buffer::deviceType().", "body": "Summary: The `tensorpipe::Buffer::deviceType()` method is going away.\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27478436\n\nfbshipit-source-id: 3962257bc6237d1dde7e5f4fddae38abe8384c68", "pr_number": null, "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": []}, "f43eb59a68": {"title": "add channels last for MaxPool2d (#48917)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48917\n\nmax_pool2d channels last support forward path\n\nmax_pool2d channels last support backward path\n\nvectorize channels last forward path\n\nrename the header file\n\nfix windows build\n\ncombine PoolingKernel.h into Pool.h\n\nadd data type check\n\nloosen test_max_pool2d_nhwc to cover device CPU\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D25399470\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: b49b9581f1329a8c2b9c75bb10f12e2650e4c65a", "pr_number": "48917", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/native/DilatedMaxPool2d.cpp", "aten/src/ATen/native/Pool.h", "aten/src/ATen/native/cpu/MaxPoolKernel.cpp", "aten/src/ATen/test/vec256_test_all_types.cpp", "test/test_nn.py", "tools/build_variables.bzl"], "labels": ["Merged", "Reverted", "cla signed", "open source"]}, "757e3cbf82": {"title": "ns for fx: add support for shadowing linear fp16 patterns (#54275)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54275\n\nAdds support for NS shadow activations path for the fp16 emulation\npattern such as\n\n```\n... -> dequantize -> linear -> relu -> to(torch.float16) -> ...\n```\n\nThere are a couple of changes necessary here:\n\n1. removing the restriction on the shadowing graph pass that the B\nsubgraph is a single node (since this subgraph is four nodes), and\nmodifying the code to correctly add the relevant inputs versus output\nloggers (input loggers and subgraph copy if we are at start_node,\nand output logger if we are at end_node)\n\n2. modifying the logic for calculating node input and output type\nto work correcty for the `to` and `dequantize` nodes:\n2a. make the function return the first input and output, instead of just\nthe first input\n2b. make the function handle `dequantize` correctly by recursively\nusing the output if its input\n2c. make the function handle `to` correctyl by recursively using the\noutput of its input and the target dtype\n\n3. a bug fix to handle observers in kwargs, while copying subgraphs\n\nNote: input logging for these patterns is not tested yet,\nthis will be in the next PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_linear_fp16\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27172655\n\nfbshipit-source-id: 3bdc86618b2a5782627fcf303d58af7f47fbc30d", "pr_number": "54275", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "cbcde79023": {"title": "ns for fx: refactor test cases (#54280)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54280\n\nSome easy refactors to reduce duplicate logic in test cases\nfor NS for FX. In particular, we start reusing a common model\nwithin this file, and we split the fp16 test cases to be more\nmodular.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27173373\n\nfbshipit-source-id: cf3f21ee8b9b12dff89f1cd2d3ac1749f3f63fe6", "pr_number": "54280", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "b8019cee0e": {"title": "ns for fx: make input logging work for multi-node subgraphs (#54326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54326\n\nFixes unshadowed activation input logging for subgraphs where start_node does\nnot equal end_node. In detail:\n* instead of passing around a single list of nodes, pass around a list\nof nodes to instrument inputs, and a list of nodes to instrument\noutputs. This way we can handle multi-node subgraphs properly, and we\nalso keep the subgraph instance definition out of the public APIs.\n* add a test case\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_linear_fp16_activations\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27190138\n\nfbshipit-source-id: 58e2377c1c128baaf3b760c1ad29098fb21f53d3", "pr_number": "54326", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "5319d17be4": {"title": "ns for fx: make input logging work for multi node subgraphs (#54327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54327\n\nMakes input logging work properly for multi-node subgraphs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_linear_fp16_shadow_activations\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27190137\n\nfbshipit-source-id: 3f39bfd5112d5ee92c1e66c133e970c28db40d46", "pr_number": "54327", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "c6cb99a6c7": {"title": "ns for fx: weight extraction for nni.ConvReLU2d (#54335)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54335\n\nSimple fix to enable weight extraction for nni.ConvReLU2d.\n\nNote: this module only appears if the internal GraphModule APIs are\ncalled, so we add testing for this path.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_mod\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27192844\n\nfbshipit-source-id: 923cf63e29e4638fd77ca42e69aedb15fb20a330", "pr_number": "54335", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "f6b25e758d": {"title": "ns for fx: move it to top level file (#55060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55060\n\nRemoves the previous iteration of Numeric Suite for FX graph mode\nquantization, and moves the current iteration into the top level\nfile.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\npython test/test_quantization.py TestFXGraphMatcher\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27467725\n\nfbshipit-source-id: 4c22b5a3221857231f9f59cf6d2908820e6a7f12", "pr_number": "55060", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/numeric_suite_core_apis_fx.py"], "labels": ["Merged", "cla signed"]}, "a590fa7af4": {"title": "ns for fx: clean up debug print statements (#55077)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55077\n\nDeletes debugging prints from the code, no logic change.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27474700\n\nfbshipit-source-id: 3d9d73da6615ddffdfdb0df270bcdfd2c4b50be3", "pr_number": "55077", "files_changed": ["torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "80b1b7e4b1": {"title": "ns for fx: ensure kwargs are handled when graph matching (#55078)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55078\n\nFixes a TODO, make sure we iterate through kwargs as well as args\nwhen navigating graphs.  We can use `node.all_input_nodes` convenience\nproperty to accomplish this.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27474699\n\nfbshipit-source-id: 8a6e3db5a73328c4f296ac5fce951e81213b6f58", "pr_number": "55078", "files_changed": ["torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "8062545c63": {"title": "ns for fx: weight extaction for conv1d and conv3d (#55079)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55079\n\nExtends weight extraction to conv1d and conv3d.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27474696\n\nfbshipit-source-id: 9d5f892160b1b003aa557cfd099c6834e3f70ded", "pr_number": "55079", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "e406d4e6cb": {"title": "Modified lstsq_helper to accept lapack error codes tensor (#54720)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54720\n\nlstsq_helper takes infos tensor now; it is modified in-place.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27439273\n\nPulled By: mruberry\n\nfbshipit-source-id: b964003982b88be85bf305059a15fb92207e2b6f", "pr_number": "54720", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed", "open source"]}, "09670c7d43": {"title": "Don't globally disable any ShellCheck warnings (#55165)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/47786 updated ShellCheck and fixed the warnings that it was already giving in CI (since it previously didn't cause the job to fail). https://github.com/pytorch/pytorch/issues/54069 enabled two ShellCheck warnings that previously were globally disabled. This PR continues the trend by reenabling the remaining four ShellCheck warnings that previously were globally disabled.\n\nAlso, this PR puts as many remaining ShellCheck arguments as possible into `.shellcheckrc` to make it easier to integrate with editors. For instance, in VS Code, this is now all that is needed (due to https://github.com/koalaman/shellcheck/issues/1818 and the fact that VS Code only runs ShellCheck on one file at a time):\n\n```json\n{\n  \"shellcheck.customArgs\": [\n    \"--external-sources\"\n  ]\n}\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55165\n\nTest Plan:\n[The \"Lint / quick-checks\" job in GitHub Actions](https://github.com/pytorch/pytorch/pull/55165/checks?check_run_id=2250098330), or this command if you want to check locally:\n```\n.jenkins/run-shellcheck.sh\n```\n\nReviewed By: walterddr\n\nDifferential Revision: D27514119\n\nPulled By: samestep\n\nfbshipit-source-id: f00744b2cb90a2ab9aa05957bff32852485a351f", "pr_number": "55165", "files_changed": [".jenkins/pytorch/.shellcheckrc", ".jenkins/pytorch/build-mobile-code-analysis.sh", ".jenkins/pytorch/build-mobile.sh", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/macos-build.sh", ".jenkins/pytorch/macos-common.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/perf_test/common.sh", ".jenkins/pytorch/perf_test/test_cpu_speed_mini_sequence_labeler.sh", ".jenkins/pytorch/perf_test/test_cpu_speed_mnist.sh", ".jenkins/pytorch/perf_test/test_cpu_speed_torch.sh", ".jenkins/pytorch/perf_test/test_cpu_speed_torch_tensor.sh", ".jenkins/pytorch/perf_test/test_gpu_speed_cudnn_lstm.sh", ".jenkins/pytorch/perf_test/test_gpu_speed_lstm.sh", ".jenkins/pytorch/perf_test/test_gpu_speed_mlstm.sh", ".jenkins/pytorch/perf_test/test_gpu_speed_mnist.sh", ".jenkins/pytorch/perf_test/test_gpu_speed_word_language_model.sh", ".jenkins/pytorch/short-perf-test-cpu.sh", ".jenkins/pytorch/short-perf-test-gpu.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-build.sh", ".jenkins/pytorch/win-test.sh", ".jenkins/run-shellcheck.sh"], "labels": ["Merged", "cla signed"]}, "181de40688": {"title": "Split copy_ kernel to InplaceOrView. (#55133)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55133\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27527939\n\nPulled By: ailzhang\n\nfbshipit-source-id: 5ddaac563b5bab38b7091b5b88e00502cb390f1a", "pr_number": "55133", "files_changed": ["torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["Merged", "cla signed"]}, "24c904951c": {"title": "Replace AutoNonVariableTypeMode with InferenceMode in fbcode. (#55114)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55114\n\nTest Plan: CI\n\nReviewed By: ezyang, bhosmer\n\nDifferential Revision: D27472768\n\nfbshipit-source-id: 76f17ef7de40f6e04e2968f8958027b5f93e1c0c", "pr_number": "55114", "files_changed": ["aten/src/ATen/ATen.h", "aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/boxing/KernelFunction.cpp", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm", "aten/src/ATen/test/vulkan_api_test.cpp", "benchmarks/cpp/tensorexpr/bench_fuser_overhead.cpp", "binaries/compare_models_torch.cc", "binaries/lite_interpreter_model_load.cc", "binaries/speed_benchmark_torch.cc", "ios/TestApp/TestApp/Benchmark.mm", "ios/TestApp/TestAppTests/TestAppTests.mm", "test/custom_operator/test_custom_ops.cpp", "test/mobile/custom_build/predictor.cpp", "test/mobile/op_deps/main.cc"], "labels": ["Merged", "cla signed", "fb-exported"]}, "02af4b511d": {"title": "Enhance Pipe docs to explicitly mention RPC initialization. (#55187)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55187\n\nAs described in https://github.com/pytorch/pytorch/issues/54927, Pipe\ndocs didn't explicitly mention initializing RPC. This PR improves the docs and\nalso ensures Pipe throws a more useful error message when RPC is not\ninitialized and not an internal assertion error.\nghstack-source-id: 125563552\n\nTest Plan:\n1) unit test added.\n2) waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27521783\n\nfbshipit-source-id: d1a5c6ca789b9a66c07a794468178c25cfd4b743", "pr_number": "55187", "files_changed": ["test/distributed/pipeline/sync/test_pipe.py", "torch/distributed/pipeline/sync/pipe.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a0bb0968d5": {"title": "[PyTorch] Don't bother with SmallVector in TensorMaker (#55125)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55125\n\nWe can provide an ArrayRef to 1-5 zeros much more efficiently, like this.\nghstack-source-id: 125471024\n\nTest Plan: Existing CI\n\nReviewed By: ezyang\n\nDifferential Revision: D27494800\n\nfbshipit-source-id: 5e2addfabae70960475a4b322925cd0eae71b4c6", "pr_number": "55125", "files_changed": ["aten/src/ATen/templates/Functions.cpp", "aten/src/ATen/templates/Functions.h"], "labels": ["Merged", "cla signed"]}, "7ab53eb960": {"title": "[StaticRuntime] Unbreak benchmarks. (#55199)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55199\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr, hlu1\n\nDifferential Revision: D27526600\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 9318cb5d6adca3e8073f8ec4219afc3cc1c75f7c", "pr_number": "55199", "files_changed": ["benchmarks/static_runtime/deep_wide_pt.h", "torch/csrc/jit/runtime/static/passes.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e593044748": {"title": "[Gradient Compression] Update a warning in ddp_comm_hooks.rst (#55031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55031\n\nIt turns out that PowerSGD hooks can work on PyTorch native AMP package, but not Apex AMP package, which can somehow mutate gradients during the execution of communication hooks.\n\n{F561544045}\nghstack-source-id: 125268206\n\nTest Plan:\nUsed native amp backend for the same pytext model and worked:\nf261564342\nf261561664\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27436484\n\nfbshipit-source-id: 2b63eb683ce373f9da06d4d224ccc5f0a3016c88", "pr_number": "55031", "files_changed": ["docs/source/ddp_comm_hooks.rst"], "labels": ["Merged", "cla signed"]}, "6e2d020037": {"title": "Add interpolation kwarg to torch.quantile (#49267)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49267\n\nThis PR builds upon the PR https://github.com/pytorch/pytorch/pull/48711 by RockingJavaBean. The original PR introduced a BC breaking change by making the interpolation parameter positional. Thus, previous invocations of torch.quantile that did not include the interpolation parameter failed after the PR landed.\n\nTo avoid BC breaking changes, we preserve the original signatures and make the interpolation parameter in the new signatures kwarg only. For now, interpolation cannot have a default value to avoid ambiguity with the deprecated signature. However, due to limitations of codegen and C++, we cannot have a required arg after optional ones. Thus, this PR also makes dim and keepdim requires args. Once we can remove the old signatures, dim, keepdim and interpolation parameters in the new signature will get the default values back.\n\n__TODO__\n ---\n- [ ] Run backward compat tests\n\nThis reverts commit 2f1d1eb7df5e8032392b73751c84025a2aa3d1ee.\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D27337117\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 7fe31f22027645e0d6cb3cab0392d532a4b362c9", "pr_number": "49267", "files_changed": ["aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/Sorting.h", "aten/src/ATen/native/native_functions.yaml", "test/test_reductions.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "38a08a49ea": {"title": "Flip clip_grad_norm default for error_if_nonfinite to false (#55169)", "body": "Summary:\nNon-backwards-compatible change introduced in https://github.com/pytorch/pytorch/pull/53843 is tripping up a lot of code. Better to set it to False initially and then potentially flip to True in the later version to give people time to adapt.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55169\n\nReviewed By: mruberry\n\nDifferential Revision: D27511150\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 1ac018557c0900b31995c29f04aea060a27bc525", "pr_number": "55169", "files_changed": ["test/cpp/api/nn_utils.cpp", "test/test_nn.py", "torch/csrc/api/include/torch/nn/utils/clip_grad.h", "torch/nn/utils/clip_grad.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "fb64caedb5": {"title": "Don't fail \"Add annotations\" if \"Lint\" is canceled (#55242)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/54779 split out the logic from our \"Lint\" workflow into a separate workflow that allows us to annotate PRs from forks. However, as of https://github.com/pytorch/pytorch/issues/54689, it is possible for the \"Lint\" workflow to be canceled, in which case it may not upload the \"flake8-py3\" and \"clang-tidy\" artifacts that the \"Add annotations\" workflow expects. This often results in GitHub pointlessly sending notification emails due to the failure in the \"Add annotations\" workflow. This PR fixes the issue by gracefully handling the case where the expected artifact is absent.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55242\n\nTest Plan: I tested this in the same external sandbox repo used to test https://github.com/pytorch/pytorch/issues/54779.\n\nReviewed By: malfet\n\nDifferential Revision: D27540120\n\nPulled By: samestep\n\nfbshipit-source-id: 47cc02950edbbc6381033bda2fe4570cb3e331cb", "pr_number": "55242", "files_changed": [".github/workflows/add_annotations.yml"], "labels": ["Merged", "cla signed"]}, "29916dbf1e": {"title": "Clang-format _distributed_c10d.pyi (#55220)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55220\n\nghstack-source-id: 125597170\n\nTest Plan: N/A\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D27531346\n\nfbshipit-source-id: c603cadbff682a9361d0e97d164f18b029e396b1", "pr_number": "55220", "files_changed": ["torch/_C/_distributed_c10d.pyi"], "labels": ["Merged", "cla signed"]}, "6e33420436": {"title": "Add embedding bag support to fx_glow (#54909)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54909\n\nPull Request resolved: https://github.com/pytorch/glow/pull/5481\n\nThis diff adds support for embedding bag to fx_glow and a test case to test_fx_glow.\n\nReviewed By: jfix71\n\nDifferential Revision: D27272897\n\nfbshipit-source-id: 9e3be28efee38a01784afceb188a86f6408393dd", "pr_number": "54909", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "6a40339920": {"title": "[SPMD] Error out SPMD mode (#54454)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54454\n\nAccording to the pitch in https://github.com/pytorch/pytorch/issues/47012\n\n1. Let DDP error out if `device_ids` contains multiple devices.\n2. If device_ids is not specified, DDP will use the provided model (module argument in DDP constructor) as-is, regardless if the model is on one GPU or multiple GPUs or on CPU.\n3. Remove the assertion that prevents SPMD in DDP `join()` method, because now SPMD is already forbidden by the constructor. Also remove the relevant unit test `test_ddp_uneven_inputs_replicated_error`.\n\n#Closes: https://github.com/pytorch/pytorch/issues/47012\n\nghstack-source-id: 125644392\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/distributed:distributed_gloo_spawn -- test_cuda\nbuck test mode/dev-nosan caffe2/test/distributed:distributed_gloo_spawn -- test_rnn\n\nbuck test mode/dev-nosan caffe2/test/distributed:c10d -- test_nccl_backend_multi_device_ids_not_allowed\nbuck test mode/dev-nosan caffe2/test/distributed:c10d -- test_nccl_backend_single_device_module_device_ids_None\nbuck test mode/dev-nosan caffe2/test/distributed:c10d -- test_nccl_backend_multi_device_module_device_ids_None\n\nbuck test mode/dev-nosan caffe2/test/distributed:c10d -- test_ddp_multi_device_module_config\n\nwaitforbuildbot\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D27226092\n\nfbshipit-source-id: 3ee1e4bc46e5e362fc82cf7a24b2fafb34fcf1b9", "pr_number": "54454", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e589247a19": {"title": "[SPMD] Change assertions to raising value errors in distributed.py (#54825)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54825\n\nThese assertions are tested in test_c10d.py\n\nContext: https://github.com/pytorch/pytorch/pull/54454#discussion_r602657818\nghstack-source-id: 125602462\n\nTest Plan: buck test mode/dev-nosan caffe2/test/distributed:c10d -- test_ddp_multi_device_module_config\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27381649\n\nfbshipit-source-id: 9b994e9c2acf796770c2f2af2cebdd5561834d14", "pr_number": "54825", "files_changed": ["test/distributed/test_c10d.py", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "2452182e6c": {"title": "[SPMD] Remove test_grad_layout_1devicemodule_2replicaperprocess (#54826)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54826\n\nThis test will no longer work, because we errored out SPMD in #54454.\n\nThis test is already disabled.\nghstack-source-id: 125602473\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27381719\n\nfbshipit-source-id: a3079ff0766f91112cbe58c1f00c1b02d241c8cd", "pr_number": "54826", "files_changed": ["test/distributed/test_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "159fdde9ae": {"title": "Support needsOutputs for RecordFunction and ObserverUtil improvements (#55012)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55012\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54442\n\nAdded needsOutputs support to RecordFunction, improved ObserverUtil functions to handle list data. Minor refactor names to be consistent.\n\nTo get output data from kernel calls, we need to temporarily capture them before passing them to the record function. Then the results are released to function return. We handle two cases, for unboxed and boxed kernels. The boxed version is fairly simple since all outputs are stored in the stack object. For unboxed kernel calls, we added a `ReturnValue` utility class to properly handle the different return values of unboxed kernels.\n\nFor optimization, this intermediate capture is only enabled for observers that request `needsOutputs(true)` and should not affect other observers or when the observer is not enabled.\n\nTest Plan:\n```\n=> buck build //caffe2/test/cpp/jit: --show-output\n=> buck-out/gen/caffe2/test/cpp/jit/jit --gtest_filter=RecordFunctionTest*\nCUDA not available. Disabling CUDA and MultiCUDA tests\nNote: Google Test filter = RecordFunctionTest*-*_CUDA:*_MultiCUDA\n[==========] Running 7 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 7 tests from RecordFunctionTest\n[ RUN      ] RecordFunctionTest.TracedTestInputsOutputs\n[       OK ] RecordFunctionTest.TracedTestInputsOutputs (226 ms)\n[ RUN      ] RecordFunctionTest.SampledCallbacks\n[       OK ] RecordFunctionTest.SampledCallbacks (771 ms)\n[ RUN      ] RecordFunctionTest.RecordFunctionGuard\n[       OK ] RecordFunctionTest.RecordFunctionGuard (0 ms)\n[ RUN      ] RecordFunctionTest.Callbacks\n[       OK ] RecordFunctionTest.Callbacks (2 ms)\n[ RUN      ] RecordFunctionTest.ShouldRun\n[       OK ] RecordFunctionTest.ShouldRun (0 ms)\n[ RUN      ] RecordFunctionTest.Basic\n[       OK ] RecordFunctionTest.Basic (1 ms)\n[ RUN      ] RecordFunctionTest.OperatorNameOverload\n[       OK ] RecordFunctionTest.OperatorNameOverload (1 ms)\n[----------] 7 tests from RecordFunctionTest (1001 ms total)\n\n[----------] Global test environment tear-down\n[==========] 7 tests from 1 test case ran. (1002 ms total)\n[  PASSED  ] 7 tests.\n\n```\n\nReviewed By: ilia-cher\n\nDifferential Revision: D27449877\n\nfbshipit-source-id: 69918b729565f5899471d9db42a587f9af52238d", "pr_number": "55012", "files_changed": ["aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "test/cpp/jit/test_misc.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "6d030c14cf": {"title": "Added pow() on CPU for float16 & bfloat16 (#50999)", "body": "Summary:\nAdded the functionality desired in https://github.com/pytorch/pytorch/issues/50789.\n\n1. Added support for pow() on CPU for `float16` (`Half`) and `bfloat16` types.\nBoth `pow(Tensor, Scalar)` and `pow(Tensor, Tensor)` are now supported for the aforementioned types.\nHowever autograd isn't supported for `Float16` on CPU yet, as `log_vml_cpu` can't be enabled for it.\n2. heitorschueroff added `pow_tensor_scalar_optimized_kernel` to refactor & simplify `PowKernel.cpp`.\nIt provides a common path for all the complex types & floating point types (except Float16, due to lack of complete AVX2 vectorization support for it).  It replaced code that had previously been duplicated for (float, double) and complex types,\nso PowKernel.cpp looks a lot cleaner now.\n3. Enabled (unskipped) some tests for `erf`, `erfc`,`erfinv`, `linalg.norm` and `linalg.vector.norm` which were being skipped earlier due to `pow()` not having been implemented for `float16` & `bfloat16`.\n4. Added an OpInfo for `pow()` & enabled some test cases for `pow()`.\n5. Extended the coverage of existing tests for `pow` in `test_binary_ufuncs.py` in order to enable comparison with `numpy`, even with discontiguous tensors, and added a test to ensure that a runtime error is raised for `pow`'s inplace variant if resizing the base tensor is required during its invocation.\n6. Added `float16` & `bfloat16` to `square`'s dtype lists in its `UnaryUfuncInfo`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50999\n\nReviewed By: zou3519\n\nDifferential Revision: D27478225\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: d309dd98d5a96d0cb9b08281757bb1c65266d011", "pr_number": "50999", "files_changed": ["aten/src/ATen/native/cpu/PowKernel.cpp", "test/test_binary_ufuncs.py", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "4170a6cc24": {"title": "Migrate `mode` from TH to ATen (#52043)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/24731 #24673 https://github.com/pytorch/pytorch/issues/24597 #24526 https://github.com/pytorch/pytorch/issues/46507\nRelated https://github.com/pytorch/pytorch/issues/24507\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52043\n\nReviewed By: mruberry\n\nDifferential Revision: D27468266\n\nPulled By: ngimel\n\nfbshipit-source-id: 35a3229c2a706da9bad4ccd0070161831e5476ba", "pr_number": "52043", "files_changed": ["BUILD.bazel", "aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/TensorModeKernel.cu", "aten/src/ATen/native/cuda/TensorModeKernel.cuh", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCReduceApplyUtils.cuh", "aten/src/THC/THCScanUtils.cuh", "aten/src/THC/THCSortUtils.cuh", "aten/src/THC/THCTensorMath.h", "aten/src/THC/THCTensorMode.cu", "aten/src/THC/THCTensorMode.cuh", "aten/src/THC/generic/THCTensorMode.cu", "aten/src/THC/generic/THCTensorMode.h", "test/backward_compatibility/check_backward_compatibility.py", "test/test_reductions.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "322854d2f0": {"title": "[SPMD] Error out SPMD in C++ Reducer (#55212)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55212\n\nError out SPMD in C++ Reducer.\n\nAdded a new test `test_reducer_no_multi_replicas`, which checks no multiple replicas are allowed at the Reducer constructor.\n\nRemoved 2 tests relevant to reducer in SPMD mode:\n`test_ddp_comm_hook_multiple_replica_check`\n`test_forward_backward_multi_replica`\n\nghstack-source-id: 125602472\n\nTest Plan: waitforbuildbot\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D27497747\n\nfbshipit-source-id: 17ef1bc4d889cbe8076bcb3d504aed4c1aea1562", "pr_number": "55212", "files_changed": ["test/distributed/test_c10d.py", "torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "041b4431b2": {"title": "irange for size_t (#55163)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55163\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D27448156\n\nfbshipit-source-id: 585da57d4de91c692b6360d65f7b8a66deb0f8c1", "pr_number": "55163", "files_changed": ["torch/csrc/api/src/nn/modules/conv.cpp", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/csrc/api/src/optim/adagrad.cpp", "torch/csrc/api/src/optim/adam.cpp", "torch/csrc/api/src/optim/adamw.cpp", "torch/csrc/api/src/optim/lbfgs.cpp", "torch/csrc/api/src/optim/rmsprop.cpp", "torch/csrc/api/src/optim/sgd.cpp", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/autograd.cpp", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/functions/tensor.cpp", "torch/csrc/autograd/python_hook.cpp", "torch/csrc/cuda/comm.cpp", "torch/csrc/cuda/nccl.cpp", "torch/csrc/deploy/test_deploy.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp", "torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/jit/codegen/cuda/arith.cpp", "torch/csrc/jit/codegen/cuda/compute_at.cpp", "torch/csrc/jit/codegen/cuda/executor.cpp", "torch/csrc/jit/codegen/cuda/executor_kernel_arg.cpp", "torch/csrc/jit/codegen/cuda/executor_utils.cpp", "torch/csrc/jit/codegen/cuda/graph_fuser.cpp", "torch/csrc/jit/codegen/cuda/index_compute.cpp", "torch/csrc/jit/codegen/cuda/interface.cpp", "torch/csrc/jit/codegen/cuda/ir_base_nodes.cpp", "torch/csrc/jit/codegen/cuda/ir_iostream.cpp", "torch/csrc/jit/codegen/cuda/ir_nodes.cpp", "torch/csrc/jit/codegen/cuda/kernel_cache.cpp", "torch/csrc/jit/codegen/cuda/lower_thread_predicate.cpp", "torch/csrc/jit/codegen/cuda/predicate_compute.cpp", "torch/csrc/jit/codegen/cuda/scheduler.cpp", "torch/csrc/jit/codegen/cuda/transform_iter.cpp", "torch/csrc/jit/codegen/fuser/compiler.cpp", "torch/csrc/jit/frontend/concrete_module_type.cpp", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/ir/subgraph_matcher.cpp", "torch/csrc/jit/passes/bailout_graph.cpp", "torch/csrc/jit/passes/canonicalize.cpp", "torch/csrc/jit/passes/dead_code_elimination.cpp", "torch/csrc/jit/passes/fixup_trace_scope_blocks.cpp", "torch/csrc/jit/passes/freeze_module.cpp", "torch/csrc/jit/passes/frozen_graph_optimizations.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/lower_tuples.cpp", "torch/csrc/jit/passes/onnx.cpp", "torch/csrc/jit/passes/onnx/eval_peephole.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp", "torch/csrc/jit/passes/onnx/fold_if_node.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/preprocess_for_onnx.cpp", "torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/specialize_autogradzero.cpp", "torch/csrc/jit/passes/subgraph_rewrite.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/passes/utils/subgraph_utils.cpp", "torch/csrc/jit/python/pybind_utils.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp", "torch/csrc/jit/runtime/profiling_record.cpp", "torch/csrc/jit/runtime/register_ops_utils.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/serialization/export.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/import_legacy.cpp", "torch/csrc/jit/serialization/python_print.cpp", "torch/csrc/jit/tensorexpr/bounds_inference.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/eval.cpp", "torch/csrc/jit/tensorexpr/external_functions.cpp", "torch/csrc/jit/tensorexpr/ir.cpp", "torch/csrc/jit/tensorexpr/ir_mutator.cpp", "torch/csrc/jit/tensorexpr/ir_printer.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/llvm_codegen.cpp", "torch/csrc/jit/tensorexpr/loopnest.cpp", "torch/csrc/jit/tensorexpr/mem_dependency_checker.cpp", "torch/csrc/jit/tensorexpr/tensor.cpp", "torch/csrc/jit/testing/file_check.cpp", "torch/csrc/utils.cpp", "torch/csrc/utils/byte_order.cpp", "torch/csrc/utils/invalid_arguments.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/tensor_apply.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/comm.cpp", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["Merged", "Reverted", "cla signed", "fb-exported", "oncall: distributed", "oncall: jit"]}, "8ed20b3f65": {"title": "Leak Caffe2 threadpool in child processes right after fork to prevent segfault (#54895)", "body": "Summary:\n## Problem summary\nFixes https://github.com/pytorch/pytorch/issues/54752 - when the number of threads is more than 3 and at least one `set_num_threads` invocation has taken place before forking child processes by the dataloader, `set_num_threads(1)` in the child process causes a segfault, as during its invocation, the child process is made to handle the data structures of the Caffe2 thread-pool of the parent process, whose data structures it inherits from the parent process (these threads don't exist in the child process, but some of its data structures do, due to the copy-on-write technique used by `fork`).\n\n## Solution\nmalfet [advised](https://github.com/pytorch/pytorch/issues/54752#issuecomment-810315302) & [authored code](https://github.com/pytorch/pytorch/pull/54895#pullrequestreview-625670122) for adding a `pthread_atfork` handler in `pytorch/caffe2/utils/threadpool/pthreadpool-cpp.cc`, that's invoked in the child process right after fork, to leak the Caffe2 thread-pool (the child inherits the thread-pool's data structures from its parent process, but doesn't actually have those threads, since after `fork` , a child process only has one thread).\n\n## Additional changes\nAdded unittest `test_no_segfault` to test for this issue in `test_dataloader.py`\nAlso enabled `test_segfault` (which actually makes sure that segfaults happen in worker processes in a particular case).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54895\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D27542253\n\nPulled By: malfet\n\nfbshipit-source-id: 10f9c67ce1ff1aa37d3efebf405bd93f7f9d2489", "pr_number": "54895", "files_changed": ["caffe2/utils/threadpool/pthreadpool-cpp.cc", "test/test_dataloader.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "e3691be2d9": {"title": "Dump C++ stack traces of all threads for distributed tests. (#55003)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55003\n\nUsing the `caffe2::setPrintStackTracesOnFatalSignal` utility in\ndistributed tests to set a signal handler that dumps the state of all threads\nfor all processes when it receives a FATAL signal. This would help in debugging\ntests further.\n\nI had to revert all the python faulthandler code since only one signal handler\nfunction is supported, so running python faulthandler with\n`setPrintStackTracesOnFatalSignal` doesn't work.\n\nSample output:\n```\nSIGSEGV(11), PID: 3492872, Thread 3492872:\n[0] ???(0x7fa7b2d1d61b) in libcaffe2_caffe2_caffe2_cpu.so\n[1] ???(0x7fa7b2d1d3fb) in libcaffe2_caffe2_caffe2_cpu.so\n[2] ???(0x7fa7b2d1d33d) in libcaffe2_caffe2_caffe2_cpu.so\n[3] ???(0x7fa7b2d1d167) in libcaffe2_caffe2_caffe2_cpu.so\n[4] ???(0x7fa7ce683150) in libpthread.so.0\n[5] ???(0x7fa7be2b233c) in libcaffe2__C_impl_cuda.so\n[6] ???(0x7fa7be2ce80c) in libcaffe2__C_impl_cuda.so\n[7] ???(0x7fa7be2a0512) in libcaffe2__C_impl_cuda.so\n[8] torch::distributed::rpc::TensorPipeAgent::send(torch::distributed::rpc::WorkerInfo const&, torch::distributed::rpc::Message&&, float, std::unordered_map<signed char, signed char, std::hash<signed char>, std::equal_to<signed char>, std::allocator<std::pair<signed char const, signed char> > > const&)+0x24f(0x7fa7be29f71f) in libcaffe2__C_impl_cuda.so\n[9] torch::distributed::autograd::sendMessageWithAutograd(torch::distributed::rpc::RpcAgent&, torch::distributed::rpc::WorkerInfo const&, torch::distributed::rpc::Message&&, bool, float, bool)+0x393(0x7fa7b602b203) in libcaffe2_libtorch.so\n[10] torch::distributed::rpc::pyRpcPythonUdf(torch::distributed::rpc::WorkerInfo const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, float, bool)+0x201(0x7fa7bd844971) in libcaffe2__C_impl_cuda.so\n```\nghstack-source-id: 125630551\n\nTest Plan: waitforbuildbot\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27419714\n\nfbshipit-source-id: 8aca9a14ef688004053d8798124d9c3a3fbe3489", "pr_number": "55003", "files_changed": ["caffe2/python/pybind_state.cc", "caffe2/utils/fatal_signal_asan_no_sig_test.cc", "caffe2/utils/signal_handler.cc", "test/distributed/test_c10d.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e309ab8510": {"title": "OpInfo: `atan2` (#55132)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55132\n\nReviewed By: gchanan\n\nDifferential Revision: D27505153\n\nPulled By: mruberry\n\nfbshipit-source-id: 45430ad0a7efab0b32c945356aa49f45d0175f83", "pr_number": "55132", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "d2a58bfe6f": {"title": "Add mkldnn tanh operator (#54656)", "body": "Summary:\n## :rocket: Feature\nAdd Mkl-Layout kernel for tanh.\n\n## Motivation\nWe want to add a Mkl-Layout kernel for tanh to improve tanh's performance when the input Tensor is Mkl-Layout.\nBecause, PyTorch does not have the Mkl-Layout kernel for tanh, so it cannot execute the tanh input by the Mkl-Layout Tensor.\nOff course you can temporarily avoid this problem by executing to_dense/to_mkldnn, but the performance is significantly reduced due to the copy overhead(1.6-4.3 times slower than CPU kernel).\n\n## Perfomance results\n\n### Environment\n- CPU: Intel(R) Core(TM) i7-8086K CPU @ 4.00GHz\n- OS: 18.04.1 LTS\n- compiler: gcc 7.5.0\n- branch: master\n- commit ID: fe2c126\n- build Environment variable: USE_CUDA=0\n- Python: 3.6.9\n- Intel MKL(Math Kernel Library): 2020.2-254\n- Intel oneDNN: 1.8.1\n\n### Benchmark script\n``` python\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(1)\n\nx = torch.randn(2048, 2048)\nx_mkl = x.to_mkldnn()\n\nprint(\"### CPU tanh\")\nwith torch.autograd.profiler.profile(record_shapes=True) as prof:\n    for i in range(100):\n        output = x.tanh()\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n\nprint(\"\\n### CPU tanh_\")\nwith torch.autograd.profiler.profile(record_shapes=True) as prof:\n    for i in range(100):\n        x.tanh_()\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n\nprint(\"\\n### to_dense/to_mkldnn + tanh\")\nwith torch.autograd.profiler.profile(record_shapes=True) as prof:\n    for i in range(100):\n        output = x_mkl.to_dense().tanh().to_mkldnn()\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n\nprint(\"\\n### to_dense/to_mkldnn + tanh_\")\nwith torch.autograd.profiler.profile(record_shapes=True) as prof:\n    for i in range(100):\n        x_mkl.to_dense().tanh_().to_mkldnn()\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n\nprint(\"\\n### Mkl-Layout tanh\")\nwith torch.autograd.profiler.profile(record_shapes=True) as prof:\n    for i in range(100):\n        output = x_mkl.tanh()\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n\nprint(\"\\n### Mkl-Layout tanh_\")\nwith torch.autograd.profiler.profile(record_shapes=True) as prof:\n    for i in range(100):\n        x_mkl.tanh_()\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n```\n\n### Results\n#### OMP_NUM_THREADS=1 Results(Self CPU time total ms)\n| Operation | CPU kernel | to_dense/to_mkldnn+CPU kernel | Mkl-Layout kernel(This PR) |\n| ---------- | ---------- | ----------------------------- | -------------------------- |\n|tanh | 579.662 | 1658.000 | 617.565 |\n| tanh_ | 554.477 | 881.997 | 589.426 |\n\n#### OMP_NUM_THREADS=6 Results(Self CPU time total ms)\n| Operation | CPU kernel | to_dense/to_mkldnn+CPU kernel | Mkl-Layout kernel(This PR) |\n| ---------- | ---------- | ----------------------------- | -------------------------- |\n|tanh | 182.387 | 421.336 | 136.226 |\n| tanh_ | 94.331 | 404.931 | 99.254 |\n\n## Modification policy for the code\noneDNN is already supported tanh operation.\n\n[oneDNN: Elementwise](https://spec.oneapi.com/versions/latest/elements/oneDNN/source/primitives/eltwise.html)\n\nThere is already exist sigmoid implementation that uses the same Elementwise API as tanh, so we created this PR code with reference to the sigmoid implementation.\n\nhttps://github.com/pytorch/pytorch/blob/527c1e0e37b7c65148bcbc390b65e94fb4624a9d/aten/src/ATen/native/mkldnn/UnaryOps.cpp#L28-L42\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54656\n\nTest Plan:\nA test for sigmoid has already been created as shown below.\nSo, I added a new test of tanh referring to the test of sigmoid.\n\nhttps://github.com/pytorch/pytorch/blob/527c1e0e37b7c65148bcbc390b65e94fb4624a9d/test/test_mkldnn.py#L944-L954\n\n### mkldnn tanh test result\n\n```\n$ python3 test/test_mkldnn.py TestMkldnn.test_tanh\nCouldn't download test skip set, leaving all tests enabled...\n.\n----------------------------------------------------------------------\nRan 1 test in 0.004s\n\nOK\n```\n\nReviewed By: gchanan\n\nDifferential Revision: D27395827\n\nPulled By: ezyang\n\nfbshipit-source-id: d4481332de187e2dea095f9b6aabc73a497960fe", "pr_number": "54656", "files_changed": ["aten/src/ATen/native/mkldnn/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_mkldnn.py"], "labels": ["Merged", "cla signed", "module: mkldnn", "open source", "triaged"]}, "bcdcf347cb": {"title": "Add cusolver potrs and potrsBatched to the backend of torch.cholesky_solve (#54315)", "body": "Summary:\nThis PR adds cusolver potrs and potrsBatched to the backend of torch.cholesky_solve and torch.linalg.cholesky_solve.\n\n`cholesky_solve` heuristics:\n\n- If magma is not installed, or batch_size is 1:\n  - If batch_size > 1 and nrhs == 1, dispatch to `cusolverDn<T>potrsBatched`,\n  - Otherwise, dispatch to `cusolverDnXpotrs` (64 bit) and `cusolverDn<T>potrs` (legacy).\n- Otherwise, use magma.\n\nNote: `cusolverDn<T>potrsBatched` only supports `nrhs == 1`. It is used for `nrhs==1` batched matrix if magma is **not** installed.\n\nSee also https://github.com/pytorch/pytorch/issues/42666 #47953\n\nTodo:\n\n- [x] benchmark and heuristic\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54315\n\nReviewed By: ngimel\n\nDifferential Revision: D27562225\n\nPulled By: mruberry\n\nfbshipit-source-id: 323e5d60610abbbdc8369f5eb112d9fa01da40f6", "pr_number": "54315", "files_changed": ["aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/CUDASolver.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c821b83ab3": {"title": "[typing] make mypy-protobuf output compatible with pyre for caffe2 type stubs (#55294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55294\n\nSome static checkers like pyre have difficulties with types like `builtings.type`, so we strip the `builtins` prefix from autogened proto type stubs.\n\nTest Plan: Let CI run.\n\nReviewed By: d4l3k\n\nDifferential Revision: D27477699\n\nfbshipit-source-id: 45e19835974200a030817d37aec785e3ecb23e8b", "pr_number": "55294", "files_changed": ["caffe2/proto/caffe2_legacy_pb2.pyi", "caffe2/proto/caffe2_pb2.pyi", "caffe2/proto/gen_proto_typestubs.sh", "caffe2/proto/gen_proto_typestubs_helper.py", "caffe2/proto/hsm_pb2.pyi", "caffe2/proto/metanet_pb2.pyi", "caffe2/proto/predictor_consts_pb2.pyi", "caffe2/proto/prof_dag_pb2.pyi", "caffe2/proto/torch_pb2.pyi"], "labels": ["Merged", "cla signed", "fb-exported"]}, "edb919376d": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27563536\n\nfbshipit-source-id: 2323c810b4bcac9934e90675d6291822d463b081", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/static/passes.h"], "labels": []}, "f3969d3db6": {"title": "Fix bug in self.assertExpectedInline (#55149)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55149\n\nI was wondering why no one used this function.  It's because it\ndoesn't work!  Also a small doc improvement for expected inline.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D27523880\n\nPulled By: ezyang\n\nfbshipit-source-id: a1d80c088ebf1c58a2b9b13d28f7f23d08c42e60", "pr_number": "55149", "files_changed": ["torch/testing/_internal/expecttest.py"], "labels": ["Merged", "cla signed"]}, "5e72571df3": {"title": "Fix wrong changes from #54103 (#54610)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54610\n\nThe `.is_view()` method actually only refers to backward mode views\nThis is not a problem right now in master (and thus I didn't revert the other PR) because nothing creates forward AD views.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D27396756\n\nPulled By: albanD\n\nfbshipit-source-id: 64ff11c6f2486c6430714988d1cf6ecf3d80dccb", "pr_number": "54610", "files_changed": ["torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "197f9f0826": {"title": "Merge CUDA Streams and Events (#53902)", "body": "Summary:\n-----------\n- Updates current_stream and default stream API's to take `optional[device]` argument\n- Adds parsing logic to replace `torch.cuda.Stream` and `torch.cuda.Event` -> `torch.classes.cuda.Stream` and `torch.classes.cuda.Event` for JIT\n- Merges StreamContext manager for both Eager and JIT.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53902\n\nTest Plan:\n------\nRun JIT tests:\npython test/test_jit.py -v TestCUDA\n\nRun eager tests:\npython test/test_cuda.py -v TestCuda\n\nReviewed By: glaringlee\n\nDifferential Revision: D27494627\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: b30b0570e38a33fb335c83762eb06ffd46a44b5c", "pr_number": "53902", "files_changed": ["test/cpp/jit/tests_setup.py", "test/jit/test_cuda.py", "torch/_utils.py", "torch/csrc/jit/cuda/cuda.h", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/register_cuda_ops.cpp", "torch/cuda/__init__.py", "torch/cuda/_utils.py", "torch/jit/__init__.py", "torch/jit/cuda.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ebf40e6ed2": {"title": "CI: Run test_lite_interpreter_runtime from built lib directly (#55291)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55291\n\nFrom the script the build happens in cpp-bulid/caffe2. All the executables and dylibs are available there. It may be more straightforward and accurate to use those binaries, instead of copying the test binary to miniconda3 and use dylibs from there.\n\nTest: CI, especially pytorch_macos_10_13_py3_lite_interpreter_build_test.\n\nTest Plan: Imported from OSS\n\nReviewed By: raziel\n\nDifferential Revision: D27566631\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 402b9941ab422979d53243624f67d65752213191", "pr_number": "55291", "files_changed": [".jenkins/pytorch/macos-lite-interpreter-build-test.sh"], "labels": ["Merged", "cla signed"]}, "6c8270ea21": {"title": "fix bc breakage of #52043 (#55303)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55303\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D27567671\n\nPulled By: agolynski\n\nfbshipit-source-id: 771e75b68be52dd5dd31437238d1f9fef481f853", "pr_number": "55303", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["Merged", "cla signed"]}, "7fd3c030ef": {"title": "Write OpInfo for dist (#55092)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53516\ncc anjali411\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55092\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27493577\n\nPulled By: anjali411\n\nfbshipit-source-id: c7e8400a20bbc7138249b249e322b3b23e112336", "pr_number": "55092", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: complex", "open source"]}, "c9b214f9fb": {"title": "Add Python-3.9 PyTorch M1 nightly builds (#55278)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55278\n\nReviewed By: janeyx99\n\nDifferential Revision: D27554985\n\nPulled By: malfet\n\nfbshipit-source-id: 8d2cd0ef1cea7f2c7c586da798f07dde4581d279", "pr_number": "55278", "files_changed": [".circleci/cimodel/data/binary_build_data.py", ".circleci/config.yml"], "labels": ["Merged", "ci/all", "cla signed"]}, "c5a1eb4156": {"title": "extend benchmarks (#54651)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54651\n\nThis PR fleshes out the benchmarks to everything I could come up with. (166 individual cases when all is said and done.) If there's anything you feel warrants a spot in CI that I've missed, by all means let me know.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27537824\n\nPulled By: robieta\n\nfbshipit-source-id: 3819e8fec2131c6b5f29f5099cd41e79131bed90", "pr_number": "54651", "files_changed": ["benchmarks/instruction_counts/core/api.py", "benchmarks/instruction_counts/definitions/setup.py", "benchmarks/instruction_counts/definitions/standard.py"], "labels": ["Merged", "cla signed"]}, "5339d534a3": {"title": "Add runner for instruction count benchmarks. (#54652)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54652\n\nThis PR adds a fairly robust runner for the instruction count microbenchmarks. Key features are:\n\n* Timeout and retry. (In rare cases, Callgrind will hang under heavy load.)\n* Robust error handling and keyboard interrupt support.\n* Benchmarks are pinned to cores. (Wall times still won't be great, but it's something.)\n* Progress printouts, including a rough ETA.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27537823\n\nPulled By: robieta\n\nfbshipit-source-id: 699ac907281d28bf7ffa08594253716ca40204ba", "pr_number": "54652", "files_changed": ["benchmarks/instruction_counts/execution/__init__.py", "benchmarks/instruction_counts/execution/runner.py", "benchmarks/instruction_counts/execution/work.py", "benchmarks/instruction_counts/main.py"], "labels": ["Merged", "cla signed"]}, "fffdc5fa2f": {"title": "docs: Pin docutils to 0.16 (#55309)", "body": "Summary:\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55309\n\nReviewed By: seemethere, samestep\n\nDifferential Revision: D27569585\n\nPulled By: agolynski\n\nfbshipit-source-id: 09f7ee08a0aea9fffd118a290f2295fe9dcab25a", "pr_number": "55309", "files_changed": ["docs/cpp/requirements.txt", "docs/requirements.txt"], "labels": ["Merged", "cla signed"]}, "f4a618bb5a": {"title": "[PyTorch] Don't create intermediate Tensor for at::result_type w/Scalar (#55232)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55232\n\nFixes https://github.com/pytorch/pytorch/issues/55229 .\nghstack-source-id: 125616311\n\nTest Plan: Looks like test/test_type_promotion.py covers this.\n\nReviewed By: ezyang\n\nDifferential Revision: D27536521\n\nfbshipit-source-id: 3e686934f845588da07de9190c9760c8ed453caf", "pr_number": "55232", "files_changed": ["aten/src/ATen/native/TypeProperties.cpp"], "labels": ["Merged", "cla signed"]}, "d0ffada9ee": {"title": ".github: Add scale-config.yml (#55315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55315\n\nTested here: https://github.com/seemethere/test-repo/actions/runs/720143591\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D27572122\n\nPulled By: seemethere\n\nfbshipit-source-id: 0b5a772cebf2a8adb9b8805fd813e9cfbe0249d7", "pr_number": "55315", "files_changed": [".github/scale-config.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "62aa924368": {"title": "[PyTorch] Devirtualize is_contiguous (#54896)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54896\n\nThis should help performance. (For example, it improves total\ntime spent in a C++ benchmark that just adds 2 tensors in place by\nabout 10%.)\nghstack-source-id: 125659451\n\nReviewed By: bhosmer\n\nDifferential Revision: D27404164\n\nfbshipit-source-id: e1dce8c02100ee4ce22510298c7e0d0f192be201", "pr_number": "54896", "files_changed": ["aten/src/ATen/BatchedTensorImpl.cpp", "aten/src/ATen/BatchedTensorImpl.h", "aten/src/ATen/OpaqueTensorImpl.h", "aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/native/metal/MetalTensorImpl.h", "aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["Merged", "Reverted", "cla signed"]}, "45aaaef22c": {"title": "Fix timer overflow on small, fast snippets (#55200)", "body": "Summary:\n- Fixes https://github.com/pytorch/pytorch/issues/54114\n- Capped estimated block size to the largest multiple of ten less than C++ INT_MAX\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55200\n\nTest Plan: unit test doesn't throw exception as expected\n\nReviewed By: robieta\n\nDifferential Revision: D27542652\n\nPulled By: naveedgol\n\nfbshipit-source-id: 3ba68ce84d5fa1d8338cdd5c9f9e5d8c9adda51c", "pr_number": "55200", "files_changed": ["test/benchmark_utils/test_benchmark_utils.py", "torch/utils/benchmark/utils/timer.py"], "labels": ["Merged", "cla signed"]}, "2ee02b30b1": {"title": "Replace rounding_mode=\"true\" with rounding_mode=None (#51988)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51988\n\n* **#51988 Replace rounding_mode=\"true\" with rounding_mode=None**\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27561817\n\nPulled By: mruberry\n\nfbshipit-source-id: 60d1d9c389570f60d599fc1876518717367fb368", "pr_number": "51988", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_autograd.py", "test/test_binary_ufuncs.py", "tools/autograd/derivatives.yaml", "tools/pyi/gen_pyi.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/onnx/symbolic_opset9.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "ef262575dd": {"title": "[pytorch] Fix printing of optional string arguments in schemas (#55196)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55196\n\nThis commit fixes printing of default values for optional string type arguments in schemas. At the moment, these default values are not printed as quoted strings. If a schema with an optional string type parameter with a default value that is not `None` is printed and then parsed, the lack of quotes causes a parsing error.\nghstack-source-id: 125655241\n\nTest Plan: This commit adds a unit test to `test_function_schema.py` to test this case.\n\nDifferential Revision: D27525450\n\nfbshipit-source-id: 23a93169e7599e7b385e59b7cfafb17fd76318b7", "pr_number": "55196", "files_changed": ["aten/src/ATen/core/function_schema.h", "test/test_function_schema.py"], "labels": ["Merged", "cla signed"]}, "d690973295": {"title": "irange on int64_t (#55148)", "body": "Summary:\nConverts loops of the form:\n```\nfor(int64_t VAR=0;VAR<LIMIT;VAR++)\n```\nto the form\n```\nfor(const auto VAR : c10::irange(LIMIT))\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55148\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D27447811\n\nfbshipit-source-id: 6311a094ec4a81a0b57383aaee0ba1b1dc2445c4", "pr_number": "55148", "files_changed": ["torch/csrc/api/src/nn/modules/adaptive.cpp", "torch/csrc/api/src/nn/modules/conv.cpp", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/csrc/api/src/optim/lbfgs.cpp", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/cuda/python_nccl.cpp", "torch/csrc/jit/codegen/cuda/lower_utils.cpp", "torch/csrc/jit/codegen/fuser/compiler.cpp", "torch/csrc/jit/codegen/fuser/executor.cpp", "torch/csrc/jit/passes/onnx/fold_if_node.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/runtime/register_ops_utils.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/tensorexpr/external_functions.cpp", "torch/csrc/utils/tensor_apply.cpp", "torch/csrc/utils/tensor_list.cpp", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "6a2f046504": {"title": "[SPMD] Restrict DDP communication hooks to SPSD mode (#55253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55253\n\nPreviously DDP communication hooks takes a tensor list as the input. Now only takes a single tensor, as the preparation of retiring SPMD and only providing a single model replica for DDP communication hooks.\n\nThe next step is limiting only 1 model replica in Reducer.\nghstack-source-id: 125677637\n\nTest Plan: waitforbuildbot\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27533898\n\nfbshipit-source-id: 5db92549c440f33662cf4edf8e0a0fd024101eae", "pr_number": "55253", "files_changed": ["docs/source/ddp_comm_hooks.rst", "test/distributed/test_c10d.py", "torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py", "torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py", "torch/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py", "torch/lib/c10d/comm.cpp", "torch/lib/c10d/comm.hpp", "torch/lib/c10d/default_comm_hooks.cpp", "torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "b986a76d91": {"title": "Clang-format distributed.py (#55254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55254\n\nghstack-source-id: 125680320\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27542846\n\nfbshipit-source-id: 700c3e59a9df98233fdb27054b472f5cb33eb604", "pr_number": "55254", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "bf37bf7da4": {"title": "Make JSON files more human readable (#55335)", "body": "Summary:\nPrettifies JSON files .pytorch-test-times and .pytorch-slow-tests so that not everything is on one single line.\n\nThis is of slightly more importance as generated  .pytorch-slow-tests ends up getting stored in our test-infra repo ([example](https://github.com/pytorch/test-infra/commit/ad9cd8756589aa28b6f2817caaf074ca93e76cd9)), and it is nice to not have that lil red symbol at the end.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55335\n\nReviewed By: samestep\n\nDifferential Revision: D27576930\n\nPulled By: janeyx99\n\nfbshipit-source-id: be58565b8c8593a9bfcfab383ee19facc79f0572", "pr_number": "55335", "files_changed": ["test/run_test.py", "tools/export_slow_tests.py"], "labels": ["Merged", "cla signed"]}, "fd02fc5d71": {"title": "Port put_ and take from TH to ATen (#53356)", "body": "Summary:\nThe two ports were don together, as they can be implemented with the same kernel. In TH, they were already implemented with the same kernel.\n\nResolves https://github.com/pytorch/pytorch/issues/24751\nResolves https://github.com/pytorch/pytorch/issues/24614\nResolves https://github.com/pytorch/pytorch/issues/24640\nResolves https://github.com/pytorch/pytorch/issues/24772\n\nThis port makes sure that it interacts correctly with the \"deterministic algorithms\" flag, as done in https://github.com/pytorch/pytorch/pull/51388\n\nThis PR also makes these two functions correct in the following aspects (all of them added to the tests as well):\n- Support for complex numbers\n- Correct handling of scalar inputs and zero-dimensional inputs\n- Implementation that does not do any copies nor sorting of any of the input tensors\n- Faster and more correct implementation of the backwards (now it works as it should when `source.shape() != index.shape()`)\n- Now `put_(..., accumulate=True)` is implemented correctly with atomic operations on GPU / CPU (when possible) and is deterministic (modulo the loss of precision that might happen due to the reordering of a sum of floats)\n- Adds the `torch.put` function that was missing, (`index_put` exists, for example)\n- Corrected docs\n\nIt also adds a much more thorough testing to the operations and their gradients.\n\nThere is a BC-breaking change, and that is that now we check that the inputs do not overlap in the `put_` operation. This was handled (some of the cases, other cases were wrong) in the TH implementation by making contiguous copies of the inputs. How should we handle this one?\n\n**Edit.** Benchmarks:\n<details>\n<summary>Script</summary>\n\n```python\nfrom IPython import get_ipython\nimport torch\nfrom itertools import product\n\ntorch.manual_seed(13)\ntorch.set_num_threads(1)\n\nipython = get_ipython()\n\ncpu = torch.device('cpu')\ncuda = torch.device('cuda')\n\ndef run_test(ndims, size, index_len, device, cmd):\n    print(f\"cmd: {cmd}, ndims: {ndims}, tensor_size: {size}, index_len: {index_len}, device: {device}\")\n\n    large_tensor = torch.rand(*([size] * ndims), device=device)\n    small_tensor = torch.rand((index_len,), device=device)\n    index = torch.randint(size * ndims, (index_len,), dtype=torch.long, device=device)\n    if cmd == \"put\":\n        command = \"large_tensor.put_(index, small_tensor, accumulate=False)\"\n        if device == cuda:\n            command += \"; torch.cuda.synchronize()\"\n    elif cmd == \"accumulate\":\n        command = \"large_tensor.put_(index, small_tensor, accumulate=True)\"\n        if device == cuda:\n            command += \"; torch.cuda.synchronize()\"\n    elif cmd == \"take\":\n        command = \"torch.take(large_tensor, index)\"\n        if device == cuda:\n            command += \"; torch.cuda.synchronize()\"\n    ipython.magic(f\"timeit {command}\")\n    print()\n\nfor method, device in product([\"accumulate\", \"put\", \"take\"], [cpu, cuda]):\n    run_test(3, 1000, 10, device, method)\n    run_test(3, 1000, 1000, device, method)\n    run_test(3, 1000, 10000, device, method)\n    run_test(2, 10000, 100000, device, method)\n```\n</details>\n\n```python\nput_(accumulate=False)\n```\n\n<details>\n<summary>ATen CPU (1.5x - 2x speedup)</summary>\n\n```python\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu\n1.05 \u00b5s \u00b1 2.35 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu\n3.15 \u00b5s \u00b1 5.13 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu\n21.6 \u00b5s \u00b1 13.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\ncmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu\n238 \u00b5s \u00b1 781 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\n</details>\n\n<details>\n<summary>TH CPU</summary>\n\n```python\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu\n722 ns \u00b1 2.67 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu\n4.89 \u00b5s \u00b1 18.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu\n42.5 \u00b5s \u00b1 96.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\ncmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu\n428 \u00b5s \u00b1 774 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\n</details>\n<details>\n<summary>ATen GPU (same speed)</summary>\n\n```python\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda\n8.99 \u00b5s \u00b1 16 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda\n10.4 \u00b5s \u00b1 24.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda\n10.4 \u00b5s \u00b1 11.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda\n15.6 \u00b5s \u00b1 1.12 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n</details>\n\n<details>\n<summary>TH GPU</summary>\n\n```python\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda\n8.44 \u00b5s \u00b1 31.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda\n9.09 \u00b5s \u00b1 4.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda\n9.77 \u00b5s \u00b1 0.998 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: put, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda\n15.8 \u00b5s \u00b1 5.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n</details>\n\n```python\nput_(accumulate=True)\n```\n\n<details>\n<summary>ATen CPU (x2 speedup)</summary>\n\n```python\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu\n1.12 \u00b5s \u00b1 2.91 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu\n3.14 \u00b5s \u00b1 2.05 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu\n20.8 \u00b5s \u00b1 25.9 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\ncmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu\n264 \u00b5s \u00b1 263 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\n</details>\n\n<details>\n<summary>TH CPU</summary>\n\n```python\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu\n814 ns \u00b1 1.87 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu\n5.11 \u00b5s \u00b1 6.02 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu\n43.9 \u00b5s \u00b1 49.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\ncmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu\n442 \u00b5s \u00b1 1.07 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\n</details>\n<details>\n<summary>ATen GPU (3x - 11x speedup)</summary>\n\n```python\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda\n9.01 \u00b5s \u00b1 14.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda\n10.4 \u00b5s \u00b1 15.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda\n10.3 \u00b5s \u00b1 44.3 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda\n12.6 \u00b5s \u00b1 19 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n</details>\n\n<details>\n<summary>TH GPU</summary>\n\n```python\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda\n34.7 \u00b5s \u00b1 131 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda\n38.2 \u00b5s \u00b1 116 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\ncmd: accumulate, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda\n61.2 \u00b5s \u00b1 50.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\ncmd: accumulate, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda\n140 \u00b5s \u00b1 24.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\n</details>\n\n```python\ntake()\n```\n\n<details>\n<summary>ATen CPU (1.1x speedup)</summary>\n\n```python\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu\n1.18 \u00b5s \u00b1 2.34 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu\n2.79 \u00b5s \u00b1 2.96 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu\n16.6 \u00b5s \u00b1 10.4 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu\n161 \u00b5s \u00b1 984 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\n</details>\n\n<details>\n<summary>TH CPU</summary>\n\n```python\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cpu\n1.1 \u00b5s \u00b1 3.14 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cpu\n2.93 \u00b5s \u00b1 7.31 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cpu\n18.6 \u00b5s \u00b1 14.5 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cpu\n178 \u00b5s \u00b1 139 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\n</details>\n<details>\n<summary>ATen GPU (same speed)</summary>\n\n```python\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda\n9.38 \u00b5s \u00b1 23.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda\n10.7 \u00b5s \u00b1 9.77 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda\n10.6 \u00b5s \u00b1 107 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda\n11.5 \u00b5s \u00b1 21.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n</details>\n\n<details>\n<summary>TH GPU</summary>\n\n```python\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10, device: cuda\n9.31 \u00b5s \u00b1 7.57 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 1000, device: cuda\n9.52 \u00b5s \u00b1 5.78 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 3, tensor_size: 1000, index_len: 10000, device: cuda\n9.73 \u00b5s \u00b1 17.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\ncmd: take, ndims: 2, tensor_size: 10000, index_len: 100000, device: cuda\n11.7 \u00b5s \u00b1 5.7 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n```\n</details>\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53356\n\nReviewed By: mruberry\n\nDifferential Revision: D27520243\n\nPulled By: ngimel\n\nfbshipit-source-id: e3979349c2c62d2949e09fb05e5fd4883fbc9093", "pr_number": "53356", "files_changed": ["BUILD.bazel", "aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/cuda/detail/OffsetCalculator.cuh", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/test/scalar_tensor_test.cpp", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCTensorIndex.cu", "aten/src/THC/THCTensorMath.h", "aten/src/THC/generic/THCTensorIndex.cu", "aten/src/THC/generic/THCTensorIndex.h", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "tools/code_analyzer/default_op_deps.yaml", "torch/__init__.py", "torch/_tensor_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "module: porting", "open source", "triaged"]}, "e0c5d0ea15": {"title": "Add tutorials to pipeline docs. (#55209)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55209\n\nghstack-source-id: 125588324\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27528715\n\nfbshipit-source-id: e6de3649e7265f34de03d452ffdf66ae45569d58", "pr_number": "55209", "files_changed": ["docs/source/pipeline.rst"], "labels": ["Merged", "cla signed"]}, "0521e420fd": {"title": "[Static Runtime] Temporarily disable fusion tests (#55342)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55342\n\nThe fusion stuff is pretty hard to debug. Given that we're not shipping this part of the stack any time soon, let's temporarily disable them and re-enable them when somebody has the cycles to debug them.\n\nTest Plan: Verified that the tests are now disabled\n\nReviewed By: ajyu\n\nDifferential Revision: D27578573\n\nfbshipit-source-id: cb8d7c9339f7c1700b7653b0231cf570996995ff", "pr_number": "55342", "files_changed": ["test/test_static_runtime.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "697b130374": {"title": "Add some missing types to torch (#55184)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55184\n\nTest Plan: Sandcastle\n\nReviewed By: ezyang\n\nDifferential Revision: D27515470\n\nfbshipit-source-id: 264bc067db8fb430465d14bf9508ac8b1faf0f2f", "pr_number": "55184", "files_changed": ["torch/testing/_internal/dist_utils.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "5584332180": {"title": "Wrap cub in its own namespace (#55292)", "body": "Summary:\nTentative fix for https://github.com/pytorch/pytorch/issues/55027.\nWraps cub import in its name space so that static variables used by cub and thrust don't conflict if they end up in the different libraries when torch is built with BUILD_SPLIT_CUDA. cub variables end up in their own namespace, thrust variables are unwrapped, so they don't clash.\nThis also allows extensions to use cub without wrapping it (thrust will still be problematic). The solution to allowing extensions to use thrust is to stop using thrust in pytorch completely.\nNow importing cub and importing thrust cannot coexist, so I had to move nonzero to its own file, and remove reliance on thrust functions for it. Nonzero now uses cub only.\nAlso, we cannot selectively import just some of cub headers, we are forced to import `cub/cub.cuh`, which is not great.\nCaffe2 ops using cub are not touched (there are too many), so mixing caffe2 and torch will (can) still result in the same bug. We are moving towards disabling c2 ops, so I think this is fine.\nStill, even with that compiler (correctly) warns about redefinition of `CUB_NS_PREFIX` because including `ATen/ATen.h` transitively includes `thrust/complex.h` and that in turn includes original (empty) definition of `CUB_NS_PREFIX`. We probably can just ignore this warning. Here's an example warning:\n```\nIn file included from /data/users/ngimel/pytorch/aten/src/ATen/native/cuda/Nonzero.cu:9:\n/data/users/ngimel/pytorch/aten/src/ATen/cuda/CubUtils.cuh:4: warning: \"CUB_NS_PREFIX\" redefined\n #define CUB_NS_PREFIX namespace at{ namespace native{\n\nIn file included from /home/ngimel/local/cuda/include/thrust/system/cuda/config.h:76,\n                 from /home/ngimel/local/cuda/include/thrust/system/cuda/detail/execution_policy.h:33,\n                 from /home/ngimel/local/cuda/include/thrust/iterator/detail/device_system_tag.h:23,\n                 from /home/ngimel/local/cuda/include/thrust/iterator/iterator_traits.h:111,\n                 from /home/ngimel/local/cuda/include/thrust/detail/type_traits/pointer_traits.h:23,\n                 from /home/ngimel/local/cuda/include/thrust/type_traits/is_contiguous_iterator.h:27,\n                 from /home/ngimel/local/cuda/include/thrust/type_traits/is_trivially_relocatable.h:19,\n                 from /home/ngimel/local/cuda/include/thrust/detail/complex/complex.inl:20,\n                 from /home/ngimel/local/cuda/include/thrust/complex.h:1031,\n                 from /data/users/ngimel/pytorch/c10/util/complex.h:9,\n                 from /data/users/ngimel/pytorch/c10/core/ScalarType.h:4,\n                 from /data/users/ngimel/pytorch/c10/core/Scalar.h:10,\n                 from /data/users/ngimel/pytorch/build/aten/src/ATen/core/TensorBody.h:8,\n                 from /data/users/ngimel/pytorch/aten/src/ATen/Tensor.h:3,\n                 from /data/users/ngimel/pytorch/aten/src/ATen/Context.h:4,\n                 from /data/users/ngimel/pytorch/aten/src/ATen/ATen.h:9,\n                 from /data/users/ngimel/pytorch/aten/src/ATen/native/cuda/Nonzero.cu:1:\n/home/ngimel/local/cuda/include/cub/util_namespace.cuh:43: note: this is the location of the previous definition\n #define CUB_NS_PREFIX\n\n```\nWe will need a lint rule to prevent people from including `cub/cub.cuh`, because this will lead to https://github.com/pytorch/pytorch/issues/55027 reappearing again for some sequence of operations (and will lead to errors with cub code in extensions).\nAlso, for this to work reliably we'll need to make sure that everything calling cub ends up in only one of libtorch_cuda_cu or libtorch_cuda_cpp, otherwise even namespace won't help (there still will be same symbols in 2 libraries).\n\nUpd: libtorch_cuda_cpp and libtorch_cuda_cu still contain the same symbols, which means that there exists a sequence of operations that will cause cache bug to reappear, so this is not a solution, we need to adjust file lists for BUILD_SPLITC_CUDA:\n```\n(pytorch) [ngimel@ ~/local/pytorch/build/lib] nm libtorch_cuda_cu.so | grep PerDeviceAttributeCache | c++filt\n000000000c6bf808 u guard variable for at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache\n000000000c600830 u guard variable for cub::GetPerDeviceAttributeCache<cub::PtxVersionCacheTag>()::cache\n00000000018625e0 t at::native::cub::PerDeviceAttributeCache::DevicePayload at::native::cub::PerDeviceAttributeCache::operator()<at::native::cub::PtxVersion(int&)::{lambda(int&)https://github.com/pytorch/pytorch/issues/1}>(at::native::cub::PtxVersion(int&)::{lambda(int&)https://github.com/pytorch/pytorch/issues/1}&&, int)\n00000000009ce630 t cub::PerDeviceAttributeCache::DevicePayload cub::PerDeviceAttributeCache::operator()<cub::PtxVersion(int&)::{lambda(int&)https://github.com/pytorch/pytorch/issues/1}>(cub::PtxVersion(int&)::{lambda(int&)https://github.com/pytorch/pytorch/issues/1}&&, int)\n000000000c6bf820 u at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache\n000000000c600840 u cub::GetPerDeviceAttributeCache<cub::PtxVersionCacheTag>()::cache\n(pytorch) [ngimel@ ~/local/pytorch/build/lib] nm libtorch_cuda_cpp.so | grep PerDeviceAttributeCache | c++filt\n0000000000ad2d98 u guard variable for at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache\n0000000000ad2da0 u at::native::cub::GetPerDeviceAttributeCache<at::native::cub::PtxVersionCacheTag>()::cache\n```\nUpd2:\nMoved TensorFactories.cu to torch_cuda_cu sources (see a change to caffe2/CMakeLists.txt), so now cub-related symbols are only in libtorch_cuda_cu. We'd need a test for that, any suggestions on how best to test it?\ncc zasdfgbnm malfet\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55292\n\nReviewed By: anjali411\n\nDifferential Revision: D27576442\n\nPulled By: ngimel\n\nfbshipit-source-id: 1ef29503a342bb214794d34a42a47052092a66c1", "pr_number": "55292", "files_changed": [".github/workflows/lint.yml", "aten/src/ATen/cuda/CubUtils.cuh", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/Nonzero.cu", "aten/src/ATen/native/cuda/Randperm.cu", "aten/src/ATen/native/cuda/ScanKernels.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed"]}, "7d9a619796": {"title": "[PyTorch] Fix bin hash comparison failure in clang format script (#55281)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55281\n\n## Summary\n\n` python3 tools/clang_format_all.py` is complaining that binary is not what expected. Find out the reference hash include an extra new line comparing with the actual hash. In this pr,\n1. Use `expr(hash)` to show the raw string, such that it's easier to compare two string.\n2. Remove the extra new line.\n3. Run `python3 tools/clang_format_all.py `, and it formats `torch/csrc/jit/runtime/static/passes.h`.\n\nBefore the change,\n```\n(base) chenlai@chenlai-mp pytorch % python3 tools/clang_format_all.py -v\nFound pre-existing clang-format binary, skipping download\nReference Hash: '5fde7bccf65032da297dfb1f18e4a95e96e278fa397e9dcaf364dfe23ec46353'\nActual Hash: '5fde7bccf65032da297dfb1f18e4a95e96e278fa397e9dcaf364dfe23ec46353'\nThe downloaded binary is not what was expected!\n(base) chenlai@chenlai-mp pytorch %\n```\n\nAfter the change,\n```\n(base) chenlai@chenlai-mp pytorch % python3 tools/clang_format_all.py -v\nFound pre-existing clang-format binary, skipping download\nReference Hash: '5fde7bccf65032da297dfb1f18e4a95e96e278fa397e9dcaf364dfe23ec46353\\n'\nActual Hash: '5fde7bccf65032da297dfb1f18e4a95e96e278fa397e9dcaf364dfe23ec46353'\nThe downloaded binary is not what was expected!\n(base) chenlai@chenlai-mp pytorch %\n```\n\nAfter strip the hash str:\n```\n(base) chenlai@chenlai-mp pytorch % python3 tools/clang_format_all.py -v\nDownloading clang-format to /Users/chenlai/pytorch/.clang-format-bin\n0% |################################################################| 100%\nReference Hash: '5fde7bccf65032da297dfb1f18e4a95e96e278fa397e9dcaf364dfe23ec46353'\nActual Hash: '5fde7bccf65032da297dfb1f18e4a95e96e278fa397e9dcaf364dfe23ec46353'\nUsing clang-format located at /Users/chenlai/pytorch/.clang-format-bin/clang-format\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27556372\n\nPulled By: cccclai\n\nfbshipit-source-id: 2fd1ba220733e767ffab41ab31e162f0bf3f1d62", "pr_number": "55281", "files_changed": ["tools/clang_format_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3551bd31be": {"title": "[PyTorch] Lite interpreter with a backend delegate (#54462)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54462\n\nUnclean files during sync - Sat Mar 20 04:00:02 PDT 2021\n\nUnclean files during sync - Sun Mar 21 04:00:01 PDT 2021\nghstack-source-id: 124585992\n\nTest Plan:\n```\nbuck run xplat/caffe2/fb/test/delegate:interpreter_test -- --model_file_path=/path/to/mobile_model.ptl\n```\n\nReviewed By: raziel\n\nDifferential Revision: D27232309\n\nfbshipit-source-id: 8504a3185339d73bfa6e924485c4745acf269cec", "pr_number": "54462", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_backend_compiler_lib.cpp", "test/cpp/jit/test_backend_compiler_preprocess.cpp", "test/cpp/jit/test_backend_lib.cpp", "test/cpp/lite_interpreter_runtime/CMakeLists.txt", "test/cpp/lite_interpreter_runtime/delegate_test.ptl", "test/cpp/lite_interpreter_runtime/test_lite_interpreter_runtime.cpp", "test/custom_backend/custom_backend.cpp", "test/custom_backend/custom_backend.h", "torch/csrc/jit/backends/backend.h", "torch/csrc/jit/backends/backend_detail.cpp", "torch/csrc/jit/backends/backend_detail.h", "torch/csrc/jit/backends/backend_preprocess.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "cc4036905c": {"title": "[Gradient Compression] Update the default value of start_powerSGD_iter and update the docstring (#55272)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55272\n\n1. Set 1K as the default value of `start_powerSGD_iter` for practicability. The original default value 10 is usually too small for real use cases. The new default value 1K is also consistent with PyTorch Lightning.\n2. Update the docstring of `start_powerSGD_iter` to remind the users to set a value no less than the warm-up steps if any.\n3. Update some unit tests to start PowerSGD early.\n\nghstack-source-id: 125707662\n\nTest Plan: waitforbuildbot\n\nReviewed By: shuyingsunshine21\n\nDifferential Revision: D27553388\n\nfbshipit-source-id: 40076419bc85755c0c0b64b79ba914b241085fcc", "pr_number": "55272", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "1b4bb3691c": {"title": "[Gradient Compression] Update _powerSGD_comm_hook_wrapper to only expose 2 most critical hyperparameters (#55295)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55295\n\nUpdate `_powerSGD_comm_hook_wrapper` to only expose 2 most critical hyperparameters, to make this API more clear to any future user (although the second hyperparameter `start_powerSGD_iter` is not in use yet).\n\nTest Plan: waitforbuildbot\n\nReviewed By: shuyingsunshine21\n\nDifferential Revision: D27561734\n\nfbshipit-source-id: b661981cc033b109f4f2fc92b435567a184a7fb5", "pr_number": "55295", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/__init__.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c3d0607ffa": {"title": "[Static Runtime] Make sure the copy version of the op exist in ReplaceWithCopy (#55337)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55337\n\n`static_runtime::permute_copy` is in fb-only folder. Because `caffe2/test/test_static_runtime.py` is in OSS, we can't load the fb-only operator library. The workaround is to check at runtime whether the op is registered or not.\n\nTest Plan:\nThis fixed two of the broken tests:\n```\n    \u2713 Pass: caffe2/test:static_runtime - test_multihead_attention_layer (test_static_runtime.TestStaticModule) (10.316)\n    \u2713 Pass: caffe2/test:static_runtime - test_mlp (test_static_runtime.TestStaticModule) (16.134)\n```\n\nReviewed By: ajyu\n\nDifferential Revision: D27577066\n\nfbshipit-source-id: ac87dcde71f0d5140ccde448bb49aaebbbb5908a", "pr_number": "55337", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h", "torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "87d55058f1": {"title": "Fix the clang-tidy diff SHA for using PR merge (#55318)", "body": "Summary:\nSince https://github.com/pytorch/pytorch/issues/54967, our clang-tidy CI job has been giving warnings on files that PRs don't touch (see the screenshot below for an example). This PR should fix the issue by comparing against the merge-base of the `merge` commit with `master`, which is just `master` itself.\n\n![clang-tidy](https://user-images.githubusercontent.com/8246041/113618718-eb83f600-960c-11eb-9375-8b88158eb566.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55318\n\nTest Plan: CI.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27572553\n\nPulled By: samestep\n\nfbshipit-source-id: 9a833aaeecc2ab22462b3fa99fa3353490c3de85", "pr_number": "55318", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "e5f66f0059": {"title": "Optimized generic interpolation using TensorIterator (keeps original 2d/3d channels last impl) (#54500)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/10482\n\nA follow-up PR to https://github.com/pytorch/pytorch/pull/51653/\n\nDescription:\n- Replaces nearest/linear/cubic implementations with generic interpolation implementation\n- Retains 2d/3d channels last implementation due to perf slowdown for 1 thread (see below appendix note)\n\nSpeed-ups for cases:\n- upsample_nearest channels first\n- upsample_bicubic channels first/last\n\n### Results for this PR\n\n<details>\n<summary>\n\nBenchmark results between 8518b0e (master) and 73137d8 (this PR)\n\n</summary>\n\n```\nDescription:\n- 20210331-092940_pth_nightly_results_1.9.0a0+git8518b0e.6\n- 20210331-092940_pth_nightly_results_1.9.0a0+git8518b0e.1\n- 20210331-092940_pr_results_1.9.0a0+git73137d8.6\n- 20210331-092940_pr_results_1.9.0a0+git73137d8.1\n\n[---------- upsample_bilinear2d channels_first contiguous torch.float32 ----------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          331.8       |          334.6\n      [1, 3, 320, 320] -> (512, 512)   |         1261.7       |         1271.5\n      [32, 128, 64, 64] -> (32, 32)    |        10164.6       |        10251.4\n      [32, 128, 64, 64] -> (128, 128)  |       195966.1       |       197141.8\n      [1, 3, 500, 500] -> (256, 256)   |          347.7       |          348.3\n      [1, 3, 500, 500] -> (800, 800)   |         3044.9       |         3071.4\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |           76.1       |           77.0\n      [1, 3, 320, 320] -> (512, 512)   |          244.8       |          247.6\n      [32, 128, 64, 64] -> (32, 32)    |         2329.4       |         2315.8\n      [32, 128, 64, 64] -> (128, 128)  |        47855.3       |        49047.7\n      [1, 3, 500, 500] -> (256, 256)   |           78.1       |           78.7\n      [1, 3, 500, 500] -> (800, 800)   |          569.3       |          575.6\n\nTimes are in microseconds (us).\n\n[------- upsample_bilinear2d channels_first non-contiguous torch.float32 --------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |         339.0        |         340.3\n      [1, 3, 320, 320] -> (512, 512)  |        1266.1        |        1277.3\n      [1, 3, 500, 500] -> (256, 256)  |         348.8        |         351.3\n      [1, 3, 500, 500] -> (800, 800)  |        3054.5        |        3077.3\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |          76.6        |          77.4\n      [1, 3, 320, 320] -> (512, 512)  |         246.0        |         248.1\n      [1, 3, 500, 500] -> (256, 256)  |          78.3        |          79.5\n      [1, 3, 500, 500] -> (800, 800)  |         572.2        |         580.0\n\nTimes are in microseconds (us).\n\n[--------- upsample_bilinear2d channels_last non-contiguous torch.float32 --------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         965.4        |         964.9\n      [1, 3, 320, 320] -> (512, 512)   |        3856.2        |        3866.8\n      [32, 128, 64, 64] -> (32, 32)    |        5808.3        |        5812.8\n      [32, 128, 64, 64] -> (128, 128)  |       99575.2        |       97226.2\n      [2, 128, 64, 46] -> (32, 32)     |         110.5        |         109.0\n      [2, 128, 64, 46] -> (128, 128)   |        1662.3        |        1612.0\n      [1, 128, 64, 46] -> (32, 32)     |          55.6        |          55.5\n      [1, 128, 64, 46] -> (128, 128)   |         467.0        |         463.9\n      [1, 3, 500, 500] -> (256, 256)   |         967.7        |         966.7\n      [1, 3, 500, 500] -> (800, 800)   |        9394.7        |        9436.6\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         962.2        |         965.4\n      [1, 3, 320, 320] -> (512, 512)   |        3844.3        |        3844.3\n      [32, 128, 64, 64] -> (32, 32)    |        2270.0        |        2267.6\n      [32, 128, 64, 64] -> (128, 128)  |       31909.7        |       32106.5\n      [2, 128, 64, 46] -> (32, 32)     |          61.3        |          59.9\n      [2, 128, 64, 46] -> (128, 128)   |         912.3        |         893.5\n      [1, 128, 64, 46] -> (32, 32)     |          55.5        |          55.3\n      [1, 128, 64, 46] -> (128, 128)   |         467.0        |         466.4\n      [1, 3, 500, 500] -> (256, 256)   |         967.2        |         971.1\n      [1, 3, 500, 500] -> (800, 800)   |        9383.2        |        9417.4\n\nTimes are in microseconds (us).\n\n[------ upsample_linear1d channels_first contiguous torch.float32 -------]\n                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |        513.5         |         521.8\n      [4, 512, 320] -> [512]  |        999.0         |        1011.8\n6 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |        103.7         |         104.9\n      [4, 512, 320] -> [512]  |        192.2         |         194.9\n\nTimes are in microseconds (us).\n\n[------------- upsample_trilinear3d channels_first contiguous torch.float32 -------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          5.4         |          5.5\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        111.2         |        111.1\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          1.1         |          1.0\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |         23.4         |         23.2\n\nTimes are in milliseconds (ms).\n\n[----------- upsample_trilinear3d channels_last non-contiguous torch.float32 ------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        13521.9       |        12939.9\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       244561.3       |       236595.6\n      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          362.2       |          365.5\n      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        38141.4       |        37957.7\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        12980.4       |        12962.7\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       236256.4       |       236364.5\n      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          367.9       |          393.2\n      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        38222.5       |        38198.3\n\nTimes are in microseconds (us).\n\n[----------- upsample_nearest2d channels_first contiguous torch.float32 ----------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         1205.7       |          107.2\n      [1, 3, 320, 320] -> (512, 512)   |         4793.5       |          357.7\n      [32, 128, 64, 64] -> (32, 32)    |        26550.0       |         6227.1\n      [32, 128, 64, 64] -> (128, 128)  |       341140.3       |       116404.4\n      [1, 3, 500, 500] -> (256, 256)   |         1208.6       |          122.9\n      [1, 3, 500, 500] -> (800, 800)   |        11648.0       |          848.1\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          220.5       |           32.6\n      [1, 3, 320, 320] -> (512, 512)   |          865.4       |           78.1\n      [32, 128, 64, 64] -> (32, 32)    |         4890.9       |         2201.2\n      [32, 128, 64, 64] -> (128, 128)  |        73533.8       |        32315.4\n      [1, 3, 500, 500] -> (256, 256)   |          222.3       |           35.0\n      [1, 3, 500, 500] -> (800, 800)   |         2107.5       |          170.7\n\nTimes are in microseconds (us).\n\n[----------- upsample_nearest2d channels_first contiguous torch.uint8 -----------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        1457.0        |         310.7\n      [1, 3, 320, 320] -> (512, 512)  |        5808.0        |        1196.6\n      [1, 3, 500, 500] -> (256, 256)  |        1460.9        |         312.7\n      [1, 3, 500, 500] -> (800, 800)  |       14094.3        |        2903.5\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |         264.8        |          66.8\n      [1, 3, 320, 320] -> (512, 512)  |        1046.0        |         228.9\n      [1, 3, 500, 500] -> (256, 256)  |         266.0        |          68.0\n      [1, 3, 500, 500] -> (800, 800)  |        2546.6        |         535.8\n\nTimes are in microseconds (us).\n\n[-------- upsample_nearest2d channels_first non-contiguous torch.float32 --------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        1284.3        |        109.9\n      [1, 3, 320, 320] -> (512, 512)  |        4870.0        |        361.6\n      [1, 3, 500, 500] -> (256, 256)  |        1482.8        |        123.3\n      [1, 3, 500, 500] -> (800, 800)  |       12050.3        |        858.8\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |         240.2        |         32.8\n      [1, 3, 320, 320] -> (512, 512)  |         886.1        |         78.4\n      [1, 3, 500, 500] -> (256, 256)  |         274.9        |         34.9\n      [1, 3, 500, 500] -> (800, 800)  |        2188.8        |        174.0\n\nTimes are in microseconds (us).\n\n[--------- upsample_nearest2d channels_first non-contiguous torch.uint8 ---------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        1501.9        |         312.2\n      [1, 3, 320, 320] -> (512, 512)  |        5853.4        |        1202.1\n      [1, 3, 500, 500] -> (256, 256)  |        1574.0        |         313.9\n      [1, 3, 500, 500] -> (800, 800)  |       14210.2        |        2904.5\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |         277.2        |          67.2\n      [1, 3, 320, 320] -> (512, 512)  |        1059.8        |         228.9\n      [1, 3, 500, 500] -> (256, 256)  |         292.2        |          68.1\n      [1, 3, 500, 500] -> (800, 800)  |        2574.4        |         536.2\n\nTimes are in microseconds (us).\n\n[--------- upsample_nearest2d channels_last non-contiguous torch.float32 ---------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         746.0        |         751.1\n      [1, 3, 320, 320] -> (512, 512)   |        2967.6        |        2979.2\n      [32, 128, 64, 64] -> (32, 32)    |        3408.5        |        3379.0\n      [32, 128, 64, 64] -> (128, 128)  |       90166.4        |       90023.0\n      [2, 128, 64, 46] -> (32, 32)     |          74.8        |          74.5\n      [2, 128, 64, 46] -> (128, 128)   |        1591.2        |        1594.3\n      [1, 128, 64, 46] -> (32, 32)     |          39.3        |          39.2\n      [1, 128, 64, 46] -> (128, 128)   |         420.3        |         419.1\n      [1, 3, 500, 500] -> (256, 256)   |         751.6        |         756.3\n      [1, 3, 500, 500] -> (800, 800)   |        7222.2        |        7268.6\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         144.9        |         140.1\n      [1, 3, 320, 320] -> (512, 512)   |         560.7        |         540.6\n      [32, 128, 64, 64] -> (32, 32)    |        1418.1        |        1418.6\n      [32, 128, 64, 64] -> (128, 128)  |       28158.4        |       26411.4\n      [2, 128, 64, 46] -> (32, 32)     |          18.4        |          17.8\n      [2, 128, 64, 46] -> (128, 128)   |         532.3        |         552.0\n      [1, 128, 64, 46] -> (32, 32)     |          13.9        |          13.6\n      [1, 128, 64, 46] -> (128, 128)   |          81.3        |          82.9\n      [1, 3, 500, 500] -> (256, 256)   |         145.9        |         141.6\n      [1, 3, 500, 500] -> (800, 800)   |        1363.4        |        1316.2\n\nTimes are in microseconds (us).\n\n[---------- upsample_nearest2d channels_last non-contiguous torch.uint8 ----------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         795.7        |         824.1\n      [1, 3, 320, 320] -> (512, 512)   |        3163.4        |        3274.8\n      [32, 128, 64, 64] -> (32, 32)    |         798.8        |         812.2\n      [32, 128, 64, 64] -> (128, 128)  |       25259.6        |       25453.1\n      [2, 128, 64, 46] -> (32, 32)     |          39.3        |          39.9\n      [2, 128, 64, 46] -> (128, 128)   |         493.7        |         499.9\n      [1, 128, 64, 46] -> (32, 32)     |          22.6        |          22.9\n      [1, 128, 64, 46] -> (128, 128)   |         249.7        |         254.0\n      [32, 64, 128, 64] -> (32, 32)    |         475.3        |         507.4\n      [32, 64, 128, 64] -> (128, 128)  |       13709.7        |       13767.5\n      [1, 3, 500, 500] -> (256, 256)   |         804.0        |         827.6\n      [1, 3, 500, 500] -> (800, 800)   |        7764.9        |        7982.7\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         150.1        |         151.4\n      [1, 3, 320, 320] -> (512, 512)   |         589.5        |         592.6\n      [32, 128, 64, 64] -> (32, 32)    |         141.3        |         194.5\n      [32, 128, 64, 64] -> (128, 128)  |        6916.5        |        7445.0\n      [2, 128, 64, 46] -> (32, 32)     |          10.0        |          12.5\n      [2, 128, 64, 46] -> (128, 128)   |          95.8        |         141.1\n      [1, 128, 64, 46] -> (32, 32)     |           8.1        |          10.0\n      [1, 128, 64, 46] -> (128, 128)   |          52.5        |          74.3\n      [32, 64, 128, 64] -> (32, 32)    |          79.8        |         123.7\n      [32, 64, 128, 64] -> (128, 128)  |        3639.9        |        4087.9\n      [1, 3, 500, 500] -> (256, 256)   |         150.7        |         152.2\n      [1, 3, 500, 500] -> (800, 800)   |        1430.9        |        1440.7\n\nTimes are in microseconds (us).\n\n[------ upsample_nearest1d channels_first contiguous torch.float32 ------]\n                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |        1601.7        |        241.7\n      [4, 512, 320] -> [512]  |        3188.5        |        435.7\n6 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |         291.9        |         53.3\n      [4, 512, 320] -> [512]  |         577.8        |         88.1\n\nTimes are in microseconds (us).\n\n[------- upsample_nearest1d channels_first contiguous torch.uint8 -------]\n                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |        2010.1        |         532.3\n      [4, 512, 320] -> [512]  |        3999.7        |        1011.4\n6 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |         364.2        |         104.6\n      [4, 512, 320] -> [512]  |         722.8        |         193.5\n\nTimes are in microseconds (us).\n\n[-------------- upsample_nearest3d channels_first contiguous torch.float32 --------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        14801.0       |         977.5\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       217368.5       |       41577.3\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         2670.3       |         210.7\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        42023.6       |       10971.6\n\nTimes are in microseconds (us).\n\n[--------------- upsample_nearest3d channels_first contiguous torch.uint8 ---------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        17151.7       |        3195.8\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       221221.0       |       50524.5\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         3085.3       |         588.6\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        39842.0       |        9141.0\n\nTimes are in microseconds (us).\n\n[------------ upsample_nearest3d channels_last non-contiguous torch.float32 -------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         7694.1       |         7729.0\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       138104.6       |       138158.0\n      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          251.1       |          252.4\n      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        28991.5       |        28882.8\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         1398.3       |         1402.6\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        28056.5       |        28123.2\n      [1, 16, 32, 64, 64] -> [16, 32, 32]     |           50.8       |           51.1\n      [1, 16, 32, 64, 64] -> [64, 128, 128]   |         7595.7       |         7540.7\n\nTimes are in microseconds (us).\n\n[------------- upsample_nearest3d channels_last non-contiguous torch.uint8 --------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         8147.8       |         8176.2\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       114658.1       |       114992.7\n      [1, 16, 32, 64, 64] -> [16, 32, 32]     |          364.3       |          356.0\n      [1, 16, 32, 64, 64] -> [64, 128, 128]   |        17276.0       |        16331.0\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         1469.4       |         1476.1\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        20647.1       |        20722.6\n      [1, 16, 32, 64, 64] -> [16, 32, 32]     |           69.7       |           68.4\n      [1, 16, 32, 64, 64] -> [64, 128, 128]   |         3125.7       |         2948.2\n\nTimes are in microseconds (us).\n\n[----------- upsample_bicubic2d channels_first contiguous torch.float32 ----------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          5961.0      |         1680.2\n      [1, 3, 320, 320] -> (512, 512)   |         23803.7      |         6591.0\n      [32, 128, 64, 64] -> (32, 32)    |        620609.4      |        37981.6\n      [32, 128, 64, 64] -> (128, 128)  |      10120286.1      |       646305.5\n      [1, 3, 500, 500] -> (256, 256)   |          6005.4      |         1694.6\n      [1, 3, 500, 500] -> (800, 800)   |         58271.9      |        16047.6\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          6218.5      |          347.1\n      [1, 3, 320, 320] -> (512, 512)   |         24144.6      |         1253.4\n      [32, 128, 64, 64] -> (32, 32)    |        612762.5      |         6934.8\n      [32, 128, 64, 64] -> (128, 128)  |       9906221.2      |       127411.1\n      [1, 3, 500, 500] -> (256, 256)   |          6241.9      |          350.2\n      [1, 3, 500, 500] -> (800, 800)   |         59052.2      |         2984.8\n\nTimes are in microseconds (us).\n\n[-------- upsample_bicubic2d channels_first non-contiguous torch.float32 --------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        6050.9        |        1694.3\n      [1, 3, 320, 320] -> (512, 512)  |       23897.1        |        6607.9\n      [1, 3, 500, 500] -> (256, 256)  |        6282.8        |        1693.9\n      [1, 3, 500, 500] -> (800, 800)  |       58608.1        |       16061.0\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        6243.7        |         347.6\n      [1, 3, 320, 320] -> (512, 512)  |       24779.9        |        1253.8\n      [1, 3, 500, 500] -> (256, 256)  |        6348.0        |         350.7\n      [1, 3, 500, 500] -> (800, 800)  |       59255.6        |        2983.8\n\nTimes are in microseconds (us).\n\n[--------- upsample_bicubic2d channels_last non-contiguous torch.float32 ---------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+git73137d8\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          6117.0      |         1688.2\n      [1, 3, 320, 320] -> (512, 512)   |         23967.4      |         6644.8\n      [32, 128, 64, 64] -> (32, 32)    |        679574.0      |        78477.4\n      [32, 128, 64, 64] -> (128, 128)  |      10334325.5      |       817649.0\n      [2, 128, 64, 46] -> (32, 32)     |          9828.0      |         4449.2\n      [2, 128, 64, 46] -> (128, 128)   |        134989.3      |        42817.4\n      [1, 128, 64, 46] -> (32, 32)     |          4508.2      |         2228.6\n      [1, 128, 64, 46] -> (128, 128)   |         59404.9      |        21400.4\n      [1, 3, 500, 500] -> (256, 256)   |          6359.0      |         1712.7\n      [1, 3, 500, 500] -> (800, 800)   |         58717.6      |        16086.6\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          6922.0      |          349.5\n      [1, 3, 320, 320] -> (512, 512)   |         24916.5      |         1260.2\n      [32, 128, 64, 64] -> (32, 32)    |        454240.4      |        16491.4\n      [32, 128, 64, 64] -> (128, 128)  |       7198101.5      |       159921.9\n      [2, 128, 64, 46] -> (32, 32)     |         10082.8      |          891.1\n      [2, 128, 64, 46] -> (128, 128)   |        151037.0      |         7704.2\n      [1, 128, 64, 46] -> (32, 32)     |          4325.5      |          633.9\n      [1, 128, 64, 46] -> (128, 128)   |         62400.4      |         3853.5\n      [1, 3, 500, 500] -> (256, 256)   |          6374.9      |          354.9\n      [1, 3, 500, 500] -> (800, 800)   |         58638.8      |         2992.0\n\nTimes are in microseconds (us).\n\nIntermediate benchmark sources:\n\n- results/20210331-092940_pth_nightly_results_1.9.0a0+git8518b0e.log.save\n- results/20210331-092940_pr_results_1.9.0a0+git73137d8.log.save\n```\n\n[Source file](https://raw.githubusercontent.com/vfdev-5/interpolate-tensoriterator/master/step_seven/results/20210326-061238_pr_1.9.0a0%2Bgita17040a_vs_pth_1.9.0a0%2Bgit8518b0e_results.md)\n\n</details>\n\nThis description is based on the benchmarks and the code from [here](https://github.com/vfdev-5/interpolate-tensoriterator/tree/master/step_seven).\n\nJoint work with Francisco Massa (fmassa).\n\n ---\n\nAppendix: Results without original 2d/3d channels last implementation\n\n<details>\n<summary>\n\nQuick benchmark results between 8518b0e (master) and [this branch](https://github.com/pytorch/pytorch/compare/master...Quansight:vfdev-5/generic-upsample-tensor-iterator)\n\n</summary>\n\n```\nDescription:\n- 20212303-061238_pth_nightly_results_1.9.0a0+git8518b0e.opencv.6\n- 20212303-061238_pth_nightly_results_1.9.0a0+git8518b0e.opencv.1\n- 20212303-061238_pr_results_1.9.0a0+gite3a9544.opencv.6\n- 20212303-061238_pr_results_1.9.0a0+gite3a9544.opencv.1\n\n[----------------- upsample_bilinear2d channels_first contiguous -----------------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          348.5       |          331.7\n      [1, 3, 320, 320] -> (512, 512)   |         1254.0       |         1178.1\n      [32, 128, 64, 64] -> (32, 32)    |        10409.4       |        10009.1\n      [32, 128, 64, 64] -> (128, 128)  |       210175.8       |       204542.5\n      [1, 3, 500, 500] -> (256, 256)   |          348.5       |          329.5\n      [1, 3, 500, 500] -> (800, 800)   |         3079.8       |         2890.1\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |           76.4       |           73.4\n      [1, 3, 320, 320] -> (512, 512)   |          247.1       |          232.0\n      [32, 128, 64, 64] -> (32, 32)    |         2371.1       |         2340.5\n      [32, 128, 64, 64] -> (128, 128)  |        62182.6       |        54089.9\n      [1, 3, 500, 500] -> (256, 256)   |           78.2       |           75.8\n      [1, 3, 500, 500] -> (800, 800)   |          569.0       |          541.3\n\nTimes are in microseconds (us).\n\n[-------------- upsample_bilinear2d channels_first non-contiguous ---------------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |         340.5        |         321.9\n      [1, 3, 320, 320] -> (512, 512)  |        1256.1        |        1179.0\n      [1, 3, 500, 500] -> (256, 256)  |         351.4        |         332.0\n      [1, 3, 500, 500] -> (800, 800)  |        3089.1        |        2898.6\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |          77.2        |          75.0\n      [1, 3, 320, 320] -> (512, 512)  |         246.6        |         232.7\n      [1, 3, 500, 500] -> (256, 256)  |          78.6        |          75.4\n      [1, 3, 500, 500] -> (800, 800)  |         576.3        |         539.6\n\nTimes are in microseconds (us).\n\n[------------------------ upsample_bilinear2d channels_last non-contiguous ------------------------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544  |  opencv 4.5.1\n1 threads: -----------------------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          971.9       |         1324.6       |       99.6\n      [1, 3, 320, 320] -> (512, 512)   |         3867.8       |         5329.9       |      271.5\n      [32, 128, 64, 64] -> (32, 32)    |         6010.6       |         6304.3       |\n      [32, 128, 64, 64] -> (128, 128)  |       112299.9       |       116956.8       |\n      [2, 128, 64, 46] -> (32, 32)     |          110.1       |          133.2       |\n      [2, 128, 64, 46] -> (128, 128)   |         1690.1       |         1838.6       |\n      [1, 128, 64, 46] -> (32, 32)     |           55.8       |           73.4       |      185.8\n      [1, 128, 64, 46] -> (128, 128)   |          474.5       |          684.9       |     1445.7\n      [1, 3, 500, 500] -> (256, 256)   |          972.9       |         1343.0       |      149.5\n      [1, 3, 500, 500] -> (800, 800)   |         9460.2       |        12925.8       |      685.1\n6 threads: -----------------------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          956.6       |          260.1       |       27.1\n      [1, 3, 320, 320] -> (512, 512)   |         3867.3       |          967.1       |       63.6\n      [32, 128, 64, 64] -> (32, 32)    |         2489.4       |         2427.0       |\n      [32, 128, 64, 64] -> (128, 128)  |        37462.1       |        41329.8       |\n      [2, 128, 64, 46] -> (32, 32)     |           61.2       |           38.9       |\n      [2, 128, 64, 46] -> (128, 128)   |          904.2       |          652.0       |\n      [1, 128, 64, 46] -> (32, 32)     |           57.1       |           32.0       |      191.1\n      [1, 128, 64, 46] -> (128, 128)   |          491.4       |          138.1       |     1485.8\n      [1, 3, 500, 500] -> (256, 256)   |          977.0       |          257.8       |       36.6\n      [1, 3, 500, 500] -> (800, 800)   |         9470.0       |         2696.0       |      142.8\n\nTimes are in microseconds (us).\n\n[------------- upsample_linear1d channels_first contiguous --------------]\n                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |        516.5         |         524.7\n      [4, 512, 320] -> [512]  |        993.8         |        1008.0\n6 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |        104.3         |         105.4\n      [4, 512, 320] -> [512]  |        193.5         |         195.6\n\nTimes are in microseconds (us).\n\n[-------------------- upsample_trilinear3d channels_first contiguous --------------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          5.5         |         11.5\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        116.3         |        213.1\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |          1.1         |          2.1\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |         36.1         |         47.2\n\nTimes are in milliseconds (ms).\n\n[------------------ upsample_trilinear3d channels_last non-contiguous -------------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         13.1         |         19.9\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        242.3         |        349.4\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         13.1         |          4.4\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        242.4         |         87.2\n\nTimes are in milliseconds (ms).\n\n[------------------ upsample_nearest2d channels_first contiguous -----------------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         1194.5       |          107.8\n      [1, 3, 320, 320] -> (512, 512)   |         4813.8       |          365.5\n      [32, 128, 64, 64] -> (32, 32)    |        26745.6       |         6280.6\n      [32, 128, 64, 64] -> (128, 128)  |       357686.7       |       129032.9\n      [1, 3, 500, 500] -> (256, 256)   |         1205.9       |          123.8\n      [1, 3, 500, 500] -> (800, 800)   |        11770.3       |          879.2\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          220.2       |           32.7\n      [1, 3, 320, 320] -> (512, 512)   |          867.2       |           78.7\n      [32, 128, 64, 64] -> (32, 32)    |         5789.6       |         2241.8\n      [32, 128, 64, 64] -> (128, 128)  |        89125.3       |        41881.3\n      [1, 3, 500, 500] -> (256, 256)   |          224.3       |           34.8\n      [1, 3, 500, 500] -> (800, 800)   |         2182.8       |          176.6\n\nTimes are in microseconds (us).\n\n[--------------- upsample_nearest2d channels_first non-contiguous ---------------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        1279.5        |        110.2\n      [1, 3, 320, 320] -> (512, 512)  |        4908.1        |        367.1\n      [1, 3, 500, 500] -> (256, 256)  |        1488.1        |        123.4\n      [1, 3, 500, 500] -> (800, 800)  |       12186.4        |        879.3\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |         241.8        |         32.6\n      [1, 3, 320, 320] -> (512, 512)  |         889.0        |         79.2\n      [1, 3, 500, 500] -> (256, 256)  |         279.2        |         35.6\n      [1, 3, 500, 500] -> (800, 800)  |        2226.5        |        174.3\n\nTimes are in microseconds (us).\n\n[------------------------ upsample_nearest2d channels_last non-contiguous -------------------------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544  |  opencv 4.5.1\n1 threads: -----------------------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          752.1       |          487.2       |      75.5\n      [1, 3, 320, 320] -> (512, 512)   |         2992.6       |         1880.0       |     251.4\n      [32, 128, 64, 64] -> (32, 32)    |         3458.6       |         3466.5       |\n      [32, 128, 64, 64] -> (128, 128)  |       102350.7       |       103919.4       |\n      [2, 128, 64, 46] -> (32, 32)     |           75.2       |           85.2       |\n      [2, 128, 64, 46] -> (128, 128)   |         1637.0       |         1690.4       |\n      [1, 128, 64, 46] -> (32, 32)     |           39.6       |           47.2       |      37.6\n      [1, 128, 64, 46] -> (128, 128)   |          426.3       |          449.0       |     412.4\n      [1, 3, 500, 500] -> (256, 256)   |          757.5       |          495.5       |      85.0\n      [1, 3, 500, 500] -> (800, 800)   |         7281.4       |         4532.6       |     622.8\n6 threads: -----------------------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |          139.3       |          104.1       |      75.7\n      [1, 3, 320, 320] -> (512, 512)   |          535.5       |          361.2       |      73.0\n      [32, 128, 64, 64] -> (32, 32)    |         1518.6       |         1458.2       |\n      [32, 128, 64, 64] -> (128, 128)  |        37117.7       |        40142.4       |\n      [2, 128, 64, 46] -> (32, 32)     |           17.6       |           26.6       |\n      [2, 128, 64, 46] -> (128, 128)   |          537.6       |          629.4       |\n      [1, 128, 64, 46] -> (32, 32)     |           13.7       |           22.1       |      38.8\n      [1, 128, 64, 46] -> (128, 128)   |           83.6       |           94.5       |     420.2\n      [1, 3, 500, 500] -> (256, 256)   |          140.8       |          104.9       |      87.8\n      [1, 3, 500, 500] -> (800, 800)   |         1317.8       |          853.8       |     139.7\n\nTimes are in microseconds (us).\n\n[------------- upsample_nearest1d channels_first contiguous -------------]\n                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |        1594.3        |        247.4\n      [4, 512, 320] -> [512]  |        3222.6        |        440.4\n6 threads: ---------------------------------------------------------------\n      [4, 512, 320] -> [256]  |         294.4        |         53.7\n      [4, 512, 320] -> [512]  |         575.0        |         88.5\n\nTimes are in microseconds (us).\n\n[--------------------- upsample_nearest3d channels_first contiguous ---------------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |        14952.7       |        1005.7\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       224955.6       |       46228.0\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         2887.2       |         206.2\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        56872.0       |       13566.3\n\nTimes are in microseconds (us).\n\n[------------------- upsample_nearest3d channels_last non-contiguous --------------------]\n                                              |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         7772.3       |         4770.9\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |       144655.1       |       108605.0\n6 threads: -------------------------------------------------------------------------------\n      [1, 3, 16, 320, 320] -> [8, 256, 256]   |         1401.9       |          877.7\n      [1, 3, 16, 320, 320] -> [32, 512, 512]  |        35939.6       |        28621.5\n\nTimes are in microseconds (us).\n\n[------------------ upsample_bicubic2d channels_first contiguous -----------------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         6038.7       |         2340.4\n      [1, 3, 320, 320] -> (512, 512)   |        24040.6       |         9205.9\n      [32, 128, 64, 64] -> (32, 32)    |       471016.3       |        52059.1\n      [32, 128, 64, 64] -> (128, 128)  |      7705594.5       |       884743.9\n      [1, 3, 500, 500] -> (256, 256)   |         6061.5       |         2361.9\n      [1, 3, 500, 500] -> (800, 800)   |        58940.7       |        22401.8\n6 threads: ------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         6594.3       |          466.5\n      [1, 3, 320, 320] -> (512, 512)   |        25361.5       |         1729.1\n      [32, 128, 64, 64] -> (32, 32)    |       487783.5       |        11550.0\n      [32, 128, 64, 64] -> (128, 128)  |      7963636.6       |       196017.3\n      [1, 3, 500, 500] -> (256, 256)   |         6443.8       |          464.1\n      [1, 3, 500, 500] -> (800, 800)   |        61891.9       |         4257.2\n\nTimes are in microseconds (us).\n\n[--------------- upsample_bicubic2d channels_first non-contiguous ---------------]\n                                      |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544\n1 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        6116.7        |        2357.0\n      [1, 3, 320, 320] -> (512, 512)  |       24182.0        |        9213.9\n      [1, 3, 500, 500] -> (256, 256)  |        6349.6        |        2358.5\n      [1, 3, 500, 500] -> (800, 800)  |       59365.2        |       22431.2\n6 threads: -----------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)  |        7155.1        |         464.6\n      [1, 3, 320, 320] -> (512, 512)  |       24566.8        |        1712.4\n      [1, 3, 500, 500] -> (256, 256)  |        7217.5        |         466.6\n      [1, 3, 500, 500] -> (800, 800)  |       59880.2        |        4148.8\n\nTimes are in microseconds (us).\n\n[------------------------ upsample_bicubic2d channels_last non-contiguous -------------------------]\n                                       |  1.9.0a0+git8518b0e  |  1.9.0a0+gite3a9544  |  opencv 4.5.1\n1 threads: -----------------------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         6184.3       |         2360.0       |      215.0\n      [1, 3, 320, 320] -> (512, 512)   |        24499.7       |         9231.1       |      510.7\n      [32, 128, 64, 64] -> (32, 32)    |       548304.5       |        93517.8       |\n      [32, 128, 64, 64] -> (128, 128)  |      7810958.3       |      1086334.6       |\n      [2, 128, 64, 46] -> (32, 32)     |        10883.4       |         5594.9       |\n      [2, 128, 64, 46] -> (128, 128)   |       153253.2       |        57071.2       |\n      [1, 128, 64, 46] -> (32, 32)     |         4519.4       |         2826.5       |      619.7\n      [1, 128, 64, 46] -> (128, 128)   |        61339.7       |        28470.7       |     3654.5\n      [1, 3, 500, 500] -> (256, 256)   |         6444.8       |         2389.9       |      292.9\n      [1, 3, 500, 500] -> (800, 800)   |        59448.0       |        22479.1       |     1316.9\n6 threads: -----------------------------------------------------------------------------------------\n      [1, 3, 320, 320] -> (256, 256)   |         6370.1       |          464.9       |       61.3\n      [1, 3, 320, 320] -> (512, 512)   |        25365.6       |         1767.5       |      145.7\n      [32, 128, 64, 64] -> (32, 32)    |       502888.7       |        22016.3       |\n      [32, 128, 64, 64] -> (128, 128)  |      8072918.9       |       234567.0       |\n      [2, 128, 64, 46] -> (32, 32)     |        11171.4       |         1049.5       |\n      [2, 128, 64, 46] -> (128, 128)   |       152612.5       |        11264.8       |\n      [1, 128, 64, 46] -> (32, 32)     |         4359.3       |          791.4       |      651.1\n      [1, 128, 64, 46] -> (128, 128)   |        61346.5       |         7563.9       |     3765.2\n      [1, 3, 500, 500] -> (256, 256)   |         6644.4       |          469.7       |       77.4\n      [1, 3, 500, 500] -> (800, 800)   |        59947.2       |         4154.3       |      313.2\n\nTimes are in microseconds (us).\n\nIntermediate benchmark sources:\n\n- results/20212303-061238_pth_nightly_results_1.9.0a0+git8518b0e.log.save.opencv\n- results/20212303-061238_pr_results_1.9.0a0+gite3a9544.log.save.opencv\n\n```\n\n[Source file](https://raw.githubusercontent.com/vfdev-5/interpolate-tensoriterator/master/step_seven/results/20212303-061238_pr_1.9.0a0%2Bgite3a9544_vs_pth_1.9.0a0%2Bgit8518b0e_results.opencv.md)\n</details>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54500\n\nReviewed By: glaringlee\n\nDifferential Revision: D27463566\n\nPulled By: fmassa\n\nfbshipit-source-id: ceac3a8cee0eeb1a4ddd9344accffcc65449a49a", "pr_number": "54500", "files_changed": ["aten/src/ATen/native/UpSample.h", "aten/src/ATen/native/UpSampleBicubic2d.cpp", "aten/src/ATen/native/cpu/UpSampleKernel.cpp", "aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "815bfad28c": {"title": "Fix reference cycle in sparse coalesce graph (#52874)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52253\n\nIn the issue reproducer we can replace `torch.sparse.sum(S)` with `S.coalesce()` and get the same memory leak. The reason is that calling `coalesce()` on an already coalesced tensor returns `self`. With autograd, the result gets it's `grad_fn` set to a node that contains a reference to the input tensor, creating a reference cycle. Cloning the tensor fixes this, so `coalesce` always returns a new tensor.\n\nAs an aside, `torch.sparse.sum(S)` doesn't need to coalesce. The result should be the same either way.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52874\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27246997\n\nPulled By: albanD\n\nfbshipit-source-id: 0fe6c11043501a7874a50982afd42964f47470d3", "pr_number": "52874", "files_changed": ["aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_sparse.py", "tools/autograd/derivatives.yaml", "torch/csrc/Module.cpp"], "labels": ["Merged", "Reverted", "cla signed", "module: sparse", "open source", "triaged"]}, "158cdece65": {"title": "Correct many OpInfos \"test_out\" skips. (#55141)", "body": "Summary:\nPartially solves https://github.com/pytorch/pytorch/issues/54061\n\nThis PR solves many of the \"easy to solve\" problems with `out=` not notifying when it resizes a tensor. It also reports the cause of some fails of the `out=` operation in the tests. Hopefully this way we will be able to catch some errors that do not come simply from not using `resize_output`.\ncc mruberry anjali411\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55141\n\nReviewed By: anjali411\n\nDifferential Revision: D27568755\n\nPulled By: mruberry\n\nfbshipit-source-id: a32546555fef8d241de2ef635a99e5615461ed09", "pr_number": "55141", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/Resize.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/ScanKernels.cu", "aten/src/ATen/native/cuda/TriangularOps.cu", "test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: tests", "open source", "triaged"]}, "8243ba7205": {"title": "Add MonkeyType dependency for testing on Linux (#55305)", "body": "Summary:\nInstall Monkey Type as part of our testing on Linux\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55305\n\nReviewed By: ailzhang\n\nDifferential Revision: D27592679\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: c92b786e45fc16288d658228a5f96aca53a3da6b", "pr_number": "55305", "files_changed": [".jenkins/pytorch/common_utils.sh", ".jenkins/pytorch/test.sh"], "labels": ["Merged", "cla signed"]}, "f8788d5188": {"title": "Upgrade onednn to v2.1.2 (#54956)", "body": "Summary:\nThis PR is to upgrade onednn to v2.1.2 which has the following main changes about cpu:\n\n- Improved performance of forward convolution with plain activations for processors with Intel AVX-512 support\n\n- Improved performance of fp32 depthwise convolution with plain activations on CPU.\n\nmore changes can be found in https://github.com/oneapi-src/oneDNN/releases.\n\nIdeep used version is [pytorch-rls-v2.1.2](https://github.com/intel/ideep/tree/pytorch-rls-v2.1.2).\nOneDNN used version is  [v2.1.2](https://github.com/oneapi-src/oneDNN/tree/v2.1.2).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54956\n\nReviewed By: ejguan\n\nDifferential Revision: D27466741\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: ff96e2cbda4b6bf04d299b9978e9125a013ce32f", "pr_number": "54956", "files_changed": ["third_party/ideep", "third_party/mkl-dnn.BUILD"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "8e78a1b084": {"title": "[Resubmit] Fix for incorrect usage of logging in torch/distributed/distributed_c10d.py (#52757)", "body": "Summary:\nResubmit of https://github.com/pytorch/pytorch/pull/51739\nFixes https://github.com/pytorch/pytorch/issues/51428\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52757\n\nReviewed By: cbalioglu\n\nDifferential Revision: D26646843\n\nfbshipit-source-id: df4962ef86ea465307e39878860b9fbbcc958d52", "pr_number": "52757", "files_changed": ["test/distributed/test_c10d.py", "torch/distributed/distributed_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source", "triaged"]}, "a9bcab46ff": {"title": "Revert back changes in test_custom_ops.cpp. (#55350)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55350\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27600413\n\nPulled By: ailzhang\n\nfbshipit-source-id: 5e0d5f13fe3a51fcdccaad8af4d46cbe82795174", "pr_number": "55350", "files_changed": ["test/custom_operator/test_custom_ops.cpp"], "labels": ["Merged", "cla signed"]}, "20d7916a6a": {"title": "[Pytorch Mobile] Fold Conv BatchNorm for functions besides forward (#54619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54619\n\nMinor refactor to conv batchnorm folding to work on other functions besides forward\nghstack-source-id: 125767010\n\nTest Plan: unit test and {P339453712}\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27301452\n\nfbshipit-source-id: 4e0cc544a171a970583979a496b2908935124497", "pr_number": "54619", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/fold_conv_bn.cpp", "torch/csrc/jit/passes/fold_conv_bn.h", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/python/init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "1be909f074": {"title": "[NNAPI] Fix models with no weights (#47517)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47517\n\nWhile we're unlikely to see this in practice, it comes up in unit tests.\nThis type annotation is necessary for `torch.jit.script` to figure out\nthe type of the list if it is empty.\n\nTest Plan: Unit tests in a later diff.\n\nReviewed By: axitkhurana\n\nDifferential Revision: D25317937\n\nPulled By: dreiss\n\nfbshipit-source-id: de8b6665c6fcd3cd2b39e3c696a39336c064e4c1", "pr_number": "47517", "files_changed": ["torch/backends/_nnapi/prepare.py"], "labels": ["Merged", "cla signed"]}, "38a3c28f17": {"title": "[NNAPI] Remove solid weights support (#47518)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47518\n\nThis was left over from an old version of the code.  The idea was that\ninstead of indexing into separate tensors for each weight, you could\nbundle them all into a single file and use different offsets into that\nfile.  With the current design, this is nontrivial to support, so drop\nthe code for now.\n\nTest Plan: CI\n\nReviewed By: axitkhurana\n\nDifferential Revision: D25317935\n\nPulled By: dreiss\n\nfbshipit-source-id: e26ab3a8d437cb1bbb50319209fa56d9c571ce61", "pr_number": "47518", "files_changed": ["torch/backends/_nnapi/serializer.py"], "labels": ["Merged", "cla signed"]}, "beca1fdbec": {"title": "[NNAPI] Fix MUL op (#47519)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47519\n\nThis wasn't updated when _do_add_binary was refactored.\n\nTest Plan: Upcoming unit test.\n\nReviewed By: axitkhurana\n\nDifferential Revision: D25317938\n\nPulled By: dreiss\n\nfbshipit-source-id: 99212404c189481cfa692dd77d8f7c7865b6872b", "pr_number": "47519", "files_changed": ["torch/backends/_nnapi/serializer.py"], "labels": ["Merged", "cla signed"]}, "8d960f7043": {"title": "[NNAPI] Fix hardtanh (#47520)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47520\n\nNNAPI defines \"RELU1\" as clamping from [-1, 1], not [0, 1] as I\npreviously assumed.  Fix our implementation to match.\n\nTest Plan: Upcoming unit test.\n\nReviewed By: axitkhurana\n\nDifferential Revision: D25317934\n\nPulled By: dreiss\n\nfbshipit-source-id: 70efd5bb6092b0628ff6b765ce6f6274ef28d741", "pr_number": "47520", "files_changed": ["torch/backends/_nnapi/serializer.py"], "labels": ["Merged", "cla signed"]}, "8fcf9ca341": {"title": "[NNAPI] Update support for Linear (#54695)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54695\n\nPreviously, torch.nn.Linear was calling aten::addmm internally.  Now\nit's calling aten::linear, so add support for that.\n\nTest Plan: Unit test\n\nReviewed By: axitkhurana\n\nDifferential Revision: D27536795\n\nPulled By: dreiss\n\nfbshipit-source-id: 42c8d2a80b20ac12ed9bba599c5e0e874256bb13", "pr_number": "54695", "files_changed": ["torch/backends/_nnapi/serializer.py"], "labels": ["Merged", "cla signed"]}, "3802edd9ab": {"title": "[NNAPI] Add unit test (#47521)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47521\n\nThis mostly goes op-by-op.  We construct a simple model containing the\nop (in various configurations for complex ops) and verify that it can be\nconverted to NNAPI.  Additionally, if libneuralnetworks is available, we\nalso run both the eager model and NNAPI model and ensure that their\noutputs are equal (allowing for some slight numerical differences).\n\nserializer.py has 94% coverage.  And most of the uncovered lines are\nerror cases, defensive code, or dead code that I might want to use\nlater.  prepare.py has 56% coverage, but probably closer to 75-80% if we\ncould collect coverage from TorchScript.\n\nTest Plan:\nRan tests with NNAPI available.  Made various tweaks to the codebase to\nmake sure tests properly detected bugs.\n\nReviewed By: axitkhurana\n\nDifferential Revision: D25317940\n\nPulled By: dreiss\n\nfbshipit-source-id: 709125af820440bfa7a73bab3304395f115f717f", "pr_number": "47521", "files_changed": ["test/test_nnapi.py"], "labels": ["Merged", "cla signed"]}, "4cf42fc62f": {"title": "[PyTorch] Cache self.size(dim) in TensorShape functions (#55336)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55336\n\nThe compiler cannot optimize this away because it does not know that size() has no side effects and doesn't get changed by anything else that goes on in the function.\nghstack-source-id: 125775704\n\nTest Plan: Spot-check assembly to verify assertion I made in the summary\n\nReviewed By: ngimel\n\nDifferential Revision: D27577299\n\nfbshipit-source-id: 7b7ce1044c4c0b437d95103a5d149acb5d86c1bd", "pr_number": "55336", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed"]}, "933bbbbed6": {"title": "[PyTorch] Fix waste in unfold() (#55339)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55339\n\nUse DimVector. Avoid calling size()/stride() when we know argument is in bounds.\nghstack-source-id: 125839415\n\nTest Plan: Existing CI\n\nReviewed By: hlu1\n\nDifferential Revision: D27577647\n\nfbshipit-source-id: b33057c383037dd0865de3a944ebf225ad8d9169", "pr_number": "55339", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed"]}, "5c402d9026": {"title": "STFT: Clarify output shape in documentation (#54877)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54631\n\nI removed the phrase \"When `onesided` is the default value `True`\". It's not always the default and it's also confusing because it doesn't seem to relate to the bullet points it's introducing. It makes more sense in the sentence before, i.e. these frequencies are included \"when the output is onesided\". So, I've rewritten it as that meaning and included the correct formula for frequencies.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54877\n\nReviewed By: ngimel\n\nDifferential Revision: D27562785\n\nPulled By: mruberry\n\nfbshipit-source-id: d7f36382611e8e176e3370393d1b371d577d46bb", "pr_number": "54877", "files_changed": ["torch/functional.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "79fe5b7897": {"title": "[Doc]fix torch.ceil formula issue(pytorch#54948) (#55039)", "body": "Summary:\nFixes wrong formula https://github.com/pytorch/pytorch/issues/54948\nThe new one is\n<img width=\"157\" alt=\"\u622a\u5c4f2021-03-31 \u4e0b\u53485 25 59\" src=\"https://user-images.githubusercontent.com/32546978/113124411-14407000-9248-11eb-92f6-7b47b4cfd5e4.png\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55039\n\nReviewed By: ngimel\n\nDifferential Revision: D27562484\n\nPulled By: mruberry\n\nfbshipit-source-id: e01d9bfc0cf04558ecff3336a055037e6c3df028", "pr_number": "55039", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "open source"]}, "96655e2b81": {"title": "Re-enable disabled tests workflow with GHA (#55417)", "body": "Summary:\nReplace the old (and disabled) workflow to update disabled tests with a GitHub Action that would gather a list of disabled tests and export them to our test-infra repo.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55417\n\nTest Plan: This [workflow](https://github.com/janeyx99/gha-experiments/runs/2282792158?check_suite_focus=true) has successfully pushed, resulting in this file: https://github.com/pytorch/test-infra/blob/master/stats/.pytorch-disabled-tests\n\nReviewed By: walterddr\n\nDifferential Revision: D27608584\n\nPulled By: janeyx99\n\nfbshipit-source-id: b9dc184712484ef4806f24a34670390f723824bc", "pr_number": "55417", "files_changed": [".github/workflows/update_disabled_tests.yml", "tools/update_disabled_tests.sh"], "labels": ["Merged", "cla signed"]}, "34a7b4aabb": {"title": "[tools] Remove newline from clang-format reference hashes (#55328)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55328\n\n**Summary**\nThe `clang-format` reference hashes committed in #54737 have newlines at\nthe end but the locally computed ones do not. This commit removes these\nnewlines so that the `clang-format` binary verification step doesn't\nfail.\n\n**Test Plan**\n`./tools/clang_format_all.py`, ran successfully.\n\n**Fixes**\nThis commit fixes #54790.\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27577398\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: e30bee58c2eb5ea96ed0a503480dea4f67b86aca", "pr_number": "55328", "files_changed": [".github/workflows/lint.yml", "tools/clang_format_hash/linux64/clang-format-linux64", "tools/clang_format_hash/mac/clang-format-mojave", "tools/clang_format_utils.py"], "labels": ["Merged", "cla signed"]}, "add49e7e4e": {"title": "Enforce PEP263 for PyTorch python codebase (#55346)", "body": "Summary:\nAll python files containing non-ASCII characters should be correctly annotated with `# -*- coding: utf-8 -*-` comment\n\nDelete number of superfluous UTF-8 characters, most commonly UTF-8 opening closing quotation mark U+2019 (\u2019) instead of ascii apostrophe ', for example `Module\u2019s`->`Module's`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55346\n\nReviewed By: samestep\n\nDifferential Revision: D27582044\n\nPulled By: malfet\n\nfbshipit-source-id: c1cd89655915858ff3a41f675cdfffff795a8e44", "pr_number": "55346", "files_changed": [".flake8", "requirements-flake8.txt", "scripts/release_notes/commitlist.py", "test/package/test_misc.py", "test/package/test_resources.py", "test/test_fx.py", "test/test_jit_fuser.py", "test/test_linalg.py", "test/test_torch.py", "tools/print_test_stats.py", "tools/test/test_stats.py", "torch/_torch_docs.py", "torch/distributed/elastic/metrics/__init__.py", "torch/distributed/elastic/rendezvous/__init__.py", "torch/distributed/elastic/rendezvous/api.py", "torch/distributed/elastic/rendezvous/etcd_rendezvous.py", "torch/distributed/nn/api/remote_module.py", "torch/distributed/pipeline/sync/_balance/blockpartition.py", "torch/distributed/pipeline/sync/pipeline.py", "torch/distributed/pipeline/sync/skip/portal.py", "torch/distributed/pipeline/sync/skip/skippable.py", "torch/distributed/rpc/utils.py", "torch/fx/__init__.py", "torch/fx/graph_module.py", "torch/linalg/__init__.py", "torch/nn/modules/conv.py", "torch/nn/modules/fold.py", "torch/onnx/__init__.py", "torch/package/_file_structure_representation.py", "torch/quantization/quantize_jit.py", "torch/utils/benchmark/examples/end_to_end.py"], "labels": ["Merged", "cla signed"]}, "35caae6045": {"title": "Fix boxing/unboxing for Scalar bool values (#53228)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53228\n\nPreviously, if a Scalar value contained a bool and was put into and then out of an IValue, it would magically transform to an int.\nThis PR fixes that and preserves the bool-ness.\nghstack-source-id: 125886985\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: unit tests\n\nReviewed By: ezyang\n\nDifferential Revision: D26800346\n\nfbshipit-source-id: f170a5b8419bde9d3155042f9126e377714ec3ba", "pr_number": "53228", "files_changed": ["aten/src/ATen/core/ivalue.h", "aten/src/ATen/test/ivalue_test.cpp"], "labels": ["Merged", "cla signed"]}, "73aeea648e": {"title": "Fix Scalar formatting (#53229)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53229\n\nScalar formatting was assuming that everything non-float was integral. This would output bools as ints, and even worse, it would crash for complex.\nThis PR fixes that.\nghstack-source-id: 125886979\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: waitforsandcastle\n\nReviewed By: ezyang\n\nDifferential Revision: D26800345\n\nfbshipit-source-id: 1a9efd085276b40d6fb399d255a6bbd7d5f3619f", "pr_number": "53229", "files_changed": ["aten/src/ATen/core/Formatting.h", "aten/src/ATen/test/scalar_test.cpp"], "labels": ["Merged", "cla signed"]}, "db75ebac4a": {"title": "Don't allocate result Tensors in out overloads: Reduction Ops (#53218)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53218\n\nWe have some operators that previously allowed you to pass in an undefined tensor to the out argument,\nand then would go on to allocate that for you. This behavior is broken and doesn't work in JIT when things\nare converted to/from IValues. Because of this, it blocks backend fallbacks because they force going\nthrough IValue.\n\nThis PR removes that behavior and forces out arguments to be defined tensors.\n\nIt only looks at reduction ops for now, there's likely more PRs coming for other ops.\n\nBC Breaking: This breaks BC since those ops previously allowed calling with undefined tensors and that isn't allowed anymore.\nghstack-source-id: 125886980\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: waitforsandcastle\n\nReviewed By: ezyang\n\nDifferential Revision: D26795461\n\nfbshipit-source-id: 158465260fe59deb7d4b2081e810a7434cfba722", "pr_number": "53218", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOpsUtils.h"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "4757d4c077": {"title": "Don't allocate result Tensors in out overloads: at::kron_out() (#53640)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53640\n\nWe have some operators that previously allowed you to pass in an undefined tensor to the out argument,\nand then would go on to allocate that for you. This behavior is broken and doesn't work in JIT when things\nare converted to/from IValues. Because of this, it blocks backend fallbacks because they force going\nthrough IValue.\n\nThis PR is one in a series to remove that behavior and forces out arguments to be defined tensors.\n\nIt only looks at at::kron_out(), but there's more PRs for other ops.\n\nBC Breaking: This breaks BC since those ops previously allowed calling with undefined tensors and that isn't allowed anymore.\nghstack-source-id: 125886981\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: waitforsandcastle\n\nReviewed By: bhosmer, ngimel\n\nDifferential Revision: D26921165\n\nfbshipit-source-id: e61411226c12d33cb196a1e010ff733fe9fa6b7b", "pr_number": "53640", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "34b46359e3": {"title": "Fix forwarding/move bug (#53556)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53556\n\nWhen packing a `Tensor&` (mutable lvalue reference) into an IValue, we accidentally didn't increase the refcount.\nThis wasn't triggered anywhere, until I tried to enable backend fallbacks. Backend fallbacks for ops that\nhave out arguments (i.e. ops that take `Tensor&` arguments and return `Tensor&` arguments) pack those returns\ninto an IValue stack (and accidentally don't increase the refcount), then later that stack gets destructed\n(which decreases the refcount and possibly destroys the Tensor), and the `Tensor&` passed in as an out argument\nis suddenty freed memory.\n\nThis PR fixes that by forwarding instead of moving when wrapping Tensors into IValues.\nghstack-source-id: 125886986\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: waitforsandcastle\n\nReviewed By: swolchok\n\nDifferential Revision: D26896507\n\nfbshipit-source-id: 62102fa89e522699b5174c33279a2b1a775066a4", "pr_number": "53556", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "acfb05ff43": {"title": "Boxing logic forwards arguments to stack (#53624)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53624\n\nPreviously, the boxing logic didn't correctly forward arguments to the stack but called copy constructors.\nThis PR fixes that.\nghstack-source-id: 125886983\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: waitforsandcastle\n\nReviewed By: bhosmer\n\nDifferential Revision: D26852856\n\nfbshipit-source-id: d2463eeca2f3fce1bbe117611be200fda59c880b", "pr_number": "53624", "files_changed": ["aten/src/ATen/core/boxing/impl/boxing.h"], "labels": ["Merged", "cla signed"]}, "8c1a70a7c9": {"title": "[A*][Gen-1.5] Add shape inference func for PredictorCall.", "body": "Summary:\nATT, so that the shape inference works for a model with only distributed parts.\n\nPreviously, we rely on a full_predictor net to do shape inference. For very large models, the full_predictor net won't be generated, so we have to do shape inference based on distributed parts. Surprisingly, the PredictorCall op does tensor name mapping so it has to have shape inference func supported.\n\nTest Plan: Added unittests.\n\nReviewed By: khabinov\n\nDifferential Revision: D27250956\n\nfbshipit-source-id: 3ebd36ba1eb020bb5d00358cffb8f038a6a996e8", "pr_number": null, "files_changed": ["caffe2/utils/proto_utils.cc"], "labels": []}, "82006ba460": {"title": "[PyTorch Edge] Implement fb::jpeg_decode_to_NCHW (#55251)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55251\n\nBased on a discussion with dreiss and JacobSzwejbka, we decided to implement a flexible operator for decoding a JPEG bundled image that allows getting the image in BGR format with scaling, and offsets applied for the MaskRCNN operators without calling `conv2d()` and pulling in a ton of additional operators and kernel functions. Please see the previous diff in the stack for the new operators that the change w/o this diff would have pulled in since Inflatable Arg string is non-trivial.\n\nThis change implements that operator. Please see the comments in the code for detail regarding what the operator does.\nghstack-source-id: 125641068\n\nTest Plan:\nI re-implemented the existing operator in terms of the new operator and used the existing unit test to ensure that the same (or comparable) tensor is produced.\n\n```\ncd fbsource/fbcode/\nbuck test caffe2/test:test_bundled_images\n```\n\nRan this bento notebook https://www.internalfb.com/intern/anp/view/?id=476100 with the new operator `fb::jpeg_decode_to_NCHW` and saw that it is able to generate proposals.\n\nRan the generated hand tracking model with tracer and observed just the 2 new operators and 0 new dtypes copy kernel function, which to me seems like an acceptable set of new ops to pull in since they are relatively simple operators: {P383858691}\n\nReviewed By: dreiss\n\nDifferential Revision: D27531423\n\nfbshipit-source-id: 2dc6c41029236bb71922e51cbfd14a46c5651149", "pr_number": "55251", "files_changed": ["test/test_bundled_images.py"], "labels": ["Merged", "cla signed"]}, "bc05867618": {"title": "Separate TLS for InferenceMode (#55424)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55424\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55238\n\nI tried to avoid creating new TLS, but InferenceMode::is_enabeld()\nis in perf critical path (TensorImpl constructor) so it seems\nworth adding one for it.\nThis PR reduces one sources of instruction count increased by\nhttps://github.com/pytorch/pytorch/pull/55008.\n```\n \u03bb ~ python compare.py\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f59097ef310>\n     100  0x0000000004854750\n    -100  0x0000000004854760\n   -4400  c10::impl::tls_is_dispatch_key_included(...)\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27539230\n\nPulled By: ailzhang\n\nfbshipit-source-id: e040877faef966dca3c2c3d5f9e9a80496c81415", "pr_number": "55424", "files_changed": ["c10/core/InferenceMode.cpp", "c10/core/InferenceMode.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "ece075195d": {"title": "[fix] torch.frac : Handle inf correctly (#52678)", "body": "Summary:\nFixes : https://github.com/pytorch/pytorch/issues/51948\nFixes : https://github.com/pytorch/pytorch/issues/52027\n\nDepends On: https://github.com/pytorch/pytorch/issues/52660\n\nTODO\n* [x] Benchmark\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52678\n\nReviewed By: anjali411\n\nDifferential Revision: D27566407\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 92c7309558ee41f8b9c641f791e8f84819c333e2", "pr_number": "52678", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed", "module: NaNs and Infs", "module: numpy", "open source", "triaged"]}, "c96f076248": {"title": "Fix typo in extending.rst (#55408)", "body": "Summary:\nSmall typo in docs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55408\n\nReviewed By: pbelevich\n\nDifferential Revision: D27611175\n\nPulled By: jamesr66a\n\nfbshipit-source-id: a83a6220054c0411329792c7ac6afceb2b699f44", "pr_number": "55408", "files_changed": ["docs/source/notes/extending.rst"], "labels": ["Merged", "cla signed"]}, "6fd20a8dea": {"title": "Back out \"[pytorch][PR] [fix] torch.frac : Handle inf correctly\"", "body": "Summary: Original commit changeset: 92c7309558ee\n\nTest Plan: reverting D27566407 (https://github.com/pytorch/pytorch/commit/ece075195d49c25213c96b9d53fcf7077215f44a)\n\nDifferential Revision: D27618949\n\nfbshipit-source-id: 7930251f4bc88e7991805d77a617a181d68a4880", "pr_number": null, "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "263d8ef4ef": {"title": "docs: fix formatting for embedding_bag (#54666)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/43499\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54666\n\nReviewed By: H-Huang\n\nDifferential Revision: D27411027\n\nPulled By: jbschlosser\n\nfbshipit-source-id: a84cc174155bd725e108d8f953a21bb8de8d9d23", "pr_number": "54666", "files_changed": ["torch/nn/functional.py", "torch/nn/modules/sparse.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b9a02128bc": {"title": "split nn.functional (#55038)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/52256\n\nSplits torch.nn.functional into a table-of-contents page and many sub-pages, one for each function\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55038\n\nReviewed By: gchanan\n\nDifferential Revision: D27502677\n\nPulled By: zou3519\n\nfbshipit-source-id: 38e450a0fee41c901eb56f94aee8a32f4eefc807", "pr_number": "55038", "files_changed": ["docs/source/nn.functional.rst"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2e9eb5afa2": {"title": "Use slow tests stats in common_utils (#55190)", "body": "Summary:\nThis is a step in adding automatic slowTest detection to our testing infrastructure. This uses stats (updated daily) in https://github.com/pytorch/test-infra/blob/master/stats/.pytorch-slow-tests to determine whether more tests need to be marked as slow as they are run.\n\nMore details in previous PR draft/proposal [here](https://github.com/pytorch/pytorch/pull/54456#issue-598388491), though I no longer think we need the third step as using a raw git file does not require much processing.\n\nUpon looking at [logs](https://circleci.com/api/v1.1/project/github/pytorch/pytorch/12060292/output/107/0?file=true&allocation-id=606660dbd8e5857bcc2b2e0f-0-build%2F60DCA8CD) for the coverage tests as of the first commit [when I had not skipped the tests so we could see their actual times], here are some slow tests that weren't marked as slow before:\n```\ntest_fn_gradgrad_unfold_cpu_complex128 (__main__.TestGradientsCPU) (172.554s)\ntest_matmul_4d_4d_complex_cpu (__main__.TestAutogradDeviceTypeCPU) (180.057s)\ntest_conv1d_basic (__main__.TestXNNPACKConv1dTransformPass) (94.737s)\n```\n\nAnd here is a test that wasn't actually slow but was still marked as slow based on stats:\n```\ntest_trunc_normal (__main__.TestNNInit) ... ok (1.208s)\n```\n\nThe new logs show the above tests as skipped (as they should be):\n[Coverage Test 1](https://app.circleci.com/pipelines/github/pytorch/pytorch/296224/workflows/ba6c2917-51f8-4fb8-be57-90151c2e5c25/jobs/12126156) and [Coverage Test 2](https://app.circleci.com/pipelines/github/pytorch/pytorch/296224/workflows/ba6c2917-51f8-4fb8-be57-90151c2e5c25/jobs/12126155)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55190\n\nReviewed By: samestep\n\nDifferential Revision: D27566663\n\nPulled By: janeyx99\n\nfbshipit-source-id: c13f8c676bb8eb15d9d697d224dbaef7df98aef3", "pr_number": "55190", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "17e5ba44f1": {"title": "[testing] Support input samples where `self` is broadcasted. (#53014)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50747\n\nReference https://github.com/pytorch/pytorch/issues/50006\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53014\n\nReviewed By: SplitInfinity, ngimel\n\nDifferential Revision: D27615320\n\nPulled By: mruberry\n\nfbshipit-source-id: a48bccf06aef1ee8f66a89e6b2bbe79736700b2b", "pr_number": "53014", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "8e6e7dca09": {"title": "[ROCm] if TEST_WITH_ROCM, only instantiate GPU device tests (#55069)", "body": "Summary:\nImproves ROCm CI throughput by instantiating only for device tests that exercise the AMD GPU devices.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55069\n\nReviewed By: mrshenli\n\nDifferential Revision: D27610877\n\nPulled By: malfet\n\nfbshipit-source-id: aa2b42b9f7611dca37cbb922790d7fe0f4be8dbd", "pr_number": "55069", "files_changed": ["torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "3bb1f59a9c": {"title": "avoid CPU std::copysign segfault when compiling on arm64 with gcc 7.5 / 8 for CUDA (#51834)", "body": "Summary:\nIt seems that the std::copysign code introduced in https://github.com/pytorch/pytorch/issues/51706 is too much for gcc 7.5 / 8 when compiled on arm64 (e.g. on Jetson with latest Jetpack) and causes it to produce an internal compiler error with segfault during compilation. This avoids the compiler bug it by not using std::copysign.\n\nA very kind person sent a Jetson Xavier NX {emoji:1f381} thank you {emoji:2764}.\n\nAfter https://github.com/pytorch/pytorch/issues/51900 fixed this for CPU-only arm64 (eg Raspberry), this fixes it for CUDA-using arm64 (e.g. Jetson). CUDA device lambdas must also be present as host functions for technical reasons but they are never used, so we just assert in the CPU variant instead of actually doing the operation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51834\n\nReviewed By: mrshenli\n\nDifferential Revision: D27622277\n\nPulled By: malfet\n\nfbshipit-source-id: a1dc4c3a67f925019782e24b796919e17339749f", "pr_number": "51834", "files_changed": ["aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "c10/cuda/CUDAMathCompat.h"], "labels": ["Merged", "Reverted", "cla signed", "module: arm", "module: jetson", "open source", "triaged"]}, "bfee8d0464": {"title": "[Pytorch Edge] Dont cache inflated bundled inputs (#55181)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55181\n\nThere can be a dramatic model size delta between saving a model after calling generate_bundled_inputs_for_* and saving before. This is due to the caching of the inflated tensor.\n\nThis increases latency when asking for the bundled inputs multiple times. I dont think this matters but it might for something like benchmarking?\nghstack-source-id: 125746773\n\nTest Plan: unit tests.\n\nReviewed By: dreiss\n\nDifferential Revision: D27519487\n\nfbshipit-source-id: 6ba22bff9c4e3a8d86c04627b7cbf47ca2d141b9", "pr_number": "55181", "files_changed": ["torch/utils/bundled_inputs.py"], "labels": ["Merged", "cla signed"]}, "1c78a4a733": {"title": "move list dict and named tuple tests out of py3 and into test_list_dict.py (#55476)", "body": "Summary:\nHackathon: Split test_jit_py3 into jit/ individual tests\n\npart 1: move Dict, List, NamedTuple\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55476\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27625646\n\nPulled By: walterddr\n\nfbshipit-source-id: 2d68f0e24df2c26ea73860b9d36669e2a6e4ff44", "pr_number": "55476", "files_changed": ["test/jit/test_list_dict.py", "test/test_jit_py3.py"], "labels": ["Merged", "Reverted", "cla signed", "hackathon", "oncall: jit"]}, "85fcadc059": {"title": "[lite-interpreter] speed_benchmark_torch support BUILD_LITE_INTERPRETER (#55402)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55402\n\nTest Plan: Imported from OSS\n\nReviewed By: cccclai\n\nDifferential Revision: D27599824\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 3adbb8a16a785d3610404d71ef2d895904b1a8ef", "pr_number": "55402", "files_changed": ["CMakeLists.txt", "binaries/CMakeLists.txt", "binaries/speed_benchmark_torch.cc"], "labels": ["Merged", "cla signed"]}, "797d0c4c68": {"title": "[Hackathon] Add error source range highlighting check in test_recursive_script.py (#55475)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55475\n\nReviewed By: janeyx99\n\nDifferential Revision: D27625464\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 27cf508593f02a26ba63e58b9bbb125b9e90e1ea", "pr_number": "55475", "files_changed": ["test/jit/test_recursive_script.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "b91d48877d": {"title": "Reland Fix reference cycle in sparse coalesce graph (#55404)", "body": "Summary:\nReland of https://github.com/pytorch/pytorch/pull/52874\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55404\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27600438\n\nPulled By: albanD\n\nfbshipit-source-id: f5c286638b324ad59be65657a016028af5e2b303", "pr_number": "55404", "files_changed": ["aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "test/backward_compatibility/check_backward_compatibility.py", "test/test_autograd.py", "test/test_sparse.py", "tools/autograd/derivatives.yaml", "torch/csrc/Module.cpp"], "labels": ["Merged", "cla signed"]}, "10abbb812a": {"title": "Support tensor subclasses in Torchscript (#54817)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54817\n\nTest Plan:\npython test case\n\nImported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27407723\n\nfbshipit-source-id: 459b9067f07908026f94620c1cfa3e00e8b50a4e", "pr_number": "54817", "files_changed": ["test/test_jit.py", "torch/_tensor.py", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/frontend/string_to_type.cpp", "torch/jit/annotations.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "dd2bccafc5": {"title": "nnc hackathon - use new APIs in tests (#55497)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55497\n\nMigrating some of the NNC API's used in testing, from this issue: https://github.com/pytorch/pytorch/issues/55203\n\nI covered the second half of `test_loopnest.cpp`, and migrated (1) and (2) in the above issue: `LoopNest::getLoopStmtsFor`, `splitWithTail`, and `splitWithMask`\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D27628625\n\nPulled By: bdhirsh\n\nfbshipit-source-id: ec15efba45fae0bbb442ac3577fb9ca2f8023c2d", "pr_number": "55497", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp"], "labels": ["Merged", "cla signed", "hackathon"]}, "1e70d217e7": {"title": "Add error message for complex alpha and non-complex inputs (#54964)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54964\n\nPreviously, the following would error out with a strange error message:\n```\nimport torch\nx=torch.randn(2)\ntorch.rsub(x, 1, alpha=2j)\n\nTraceback (most recent call last)\n<ipython-input-2-caf2a1c03d0b> in <module>\n      1 import torch\n      2 x=torch.randn(2)\n----> 3 torch.rsub(x, 1, alpha=2j)\n\nRuntimeError: value cannot be converted to type float without overflow: (-0,-2)\n```\n\nThe reason why this is happening is because the alpha check doesn't check for if `x` is not complex and `alpha` is complex.\nThe error gets thrown further along in the implementation of torch.sub,\nwhen it coerces `alpha` to be the same dtype as the input tensor:\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp#L53\n\nThis PR fixes the bad error message by adding a new check to the alpha check.\n\nTest Plan:\n- pytest test/test_binary_ufuncs.py\n- NB: add, sub, and rsub all share the same alpha check. The test only tests it for torch.add, but that should be sufficient.\n\nReviewed By: gchanan\n\nDifferential Revision: D27504017\n\nPulled By: zou3519\n\nfbshipit-source-id: 70b9aa75a7a4faaaa93f6ba235cae85998a91697", "pr_number": "54964", "files_changed": ["aten/src/ATen/native/BinaryOps.h", "test/test_binary_ufuncs.py"], "labels": ["Merged", "cla signed"]}, "afd549bb8f": {"title": "[Doc] fix profiler doc (#55449)", "body": "Summary:\nfix profiler doc\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55449\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27626301\n\nPulled By: mrshenli\n\nfbshipit-source-id: e9540fa0022c764371c785ca4079797d17532417", "pr_number": "55449", "files_changed": ["torch/profiler/profiler.py"], "labels": ["Merged", "cla signed", "module: docs", "open source", "triaged"]}, "2dd7dafb62": {"title": "[Hackathon][take2] jit py3 move list dict tuple to jit/ (#55515)", "body": "Summary:\nmoved to jit/test_list_dict.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55515\n\nReviewed By: mrshenli\n\nDifferential Revision: D27627615\n\nPulled By: walterddr\n\nfbshipit-source-id: 6b17a4d6535ae2d6d848532a4df2278d3aaefa7b", "pr_number": "55515", "files_changed": ["test/jit/test_list_dict.py", "test/test_jit.py", "test/test_jit_py3.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a20a72d41b": {"title": "Replace assertRaisesRegex w/ assertRaisesRegexWithHighlight test_backends.py (#55493)", "body": "Summary:\nStep to resolving https://github.com/pytorch/pytorch/issues/55072\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55493\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27626192\n\nPulled By: janeyx99\n\nfbshipit-source-id: 047b7b6754e21388f52489160d712858b7aa0288", "pr_number": "55493", "files_changed": ["test/jit/test_backends.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "b1bae01e0c": {"title": "Replace raiseRegex with raiseRegexWithHighlight test_async.py (#55489)", "body": "Summary:\nStep to resolving https://github.com/pytorch/pytorch/issues/55072\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55489\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27625872\n\nPulled By: janeyx99\n\nfbshipit-source-id: 1ee606a30b13d041d8d107e6cc23be16c076d072", "pr_number": "55489", "files_changed": ["test/jit/test_async.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "469734ae54": {"title": "Replace assertRaisesRegex w/ assertRaisesRegexWithHighlight test_builtins (#55496)", "body": "Summary:\nStep to resolving https://github.com/pytorch/pytorch/issues/55072\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55496\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27627271\n\nPulled By: janeyx99\n\nfbshipit-source-id: c59c93018dbb5051e1e49b66298e9caf779b438b", "pr_number": "55496", "files_changed": ["test/jit/test_builtins.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "b665298dc8": {"title": "Use assertRaisesRegexWithHighlight test_custom_operators.py (#55519)", "body": "Summary:\nStep to resolving https://github.com/pytorch/pytorch/issues/55072\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55519\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27627487\n\nPulled By: janeyx99\n\nfbshipit-source-id: 6bd54433617180c56153785b69c2e49faf19ef34", "pr_number": "55519", "files_changed": ["test/jit/test_custom_operators.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "11889a51ed": {"title": "Use assertRaisesRegexWithHighlight test_enum.py (#55521)", "body": "Summary:\nStep to resolving https://github.com/pytorch/pytorch/issues/55072\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55521\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27627616\n\nPulled By: janeyx99\n\nfbshipit-source-id: fabdec52729087b9ae693b14a0bc11c596003035", "pr_number": "55521", "files_changed": ["test/jit/test_enum.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "5d78c4f701": {"title": "Use assertRaisesRegexWithHighlight test_class_type.py (#55510)", "body": "Summary:\nStep to resolving https://github.com/pytorch/pytorch/issues/55072\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55510\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27627208\n\nPulled By: janeyx99\n\nfbshipit-source-id: 6cfbd4523ebd9803496fbdc5128b91110e87e07a", "pr_number": "55510", "files_changed": ["test/jit/test_class_type.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "b9326d418d": {"title": "[Hackathon] Add error source range highlighting check in test_scriptmod_ann (#55482)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55482\n\nReviewed By: janeyx99\n\nDifferential Revision: D27627460\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 099e36f36561c9252b027c8f89b301108133b0a7", "pr_number": "55482", "files_changed": ["test/jit/test_scriptmod_ann.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "5f90ed550c": {"title": "[Hackathon] Add source range highligh check to test_string_formatting (#55491)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55491\n\nReviewed By: janeyx99\n\nDifferential Revision: D27627477\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 4586d7c96eae762be53c1155c6c724c6d65f1e7f", "pr_number": "55491", "files_changed": ["test/jit/test_string_formatting.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "94a3bad343": {"title": "[Hackathon] Add source highlighting check in test_type_sharing (#55498)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55498\n\nReviewed By: janeyx99\n\nDifferential Revision: D27627506\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: abdd2a505099d3976762b4851d1024eb791e9204", "pr_number": "55498", "files_changed": ["test/jit/test_type_sharing.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "0f1350055b": {"title": "[Hackathon] Add source range highlight check to test_with (#55513)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55513\n\nReviewed By: janeyx99\n\nDifferential Revision: D27627520\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 132f4dd2e99d2b5981fdd1522dbf7727b6abf7ab", "pr_number": "55513", "files_changed": ["test/jit/test_with.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "f0ce8593db": {"title": "[Hackathon] Add source highlight check in test_torchbind (#55495)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55495\n\nReviewed By: janeyx99\n\nDifferential Revision: D27627499\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 6749d7f58a98f40d6f301c6f37321ec85707242e", "pr_number": "55495", "files_changed": ["test/jit/test_torchbind.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "524dbe1fa1": {"title": "[Easy] Fix typo in package_exporter.py (#55551)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55551\n\nSimple typo, it should be `OrderedImporter`\n\nTest Plan: ci\n\nDifferential Revision: D27629463\n\nfbshipit-source-id: 745527a8339f03a8fd38d0a4491811b3c9ca9b1e", "pr_number": "55551", "files_changed": ["torch/package/package_exporter.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "b3f1fece1b": {"title": "[Hackathon] add highlight to test_module_interface.py (#55530)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55530\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27628729\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 4d7d2d56f0475c4311fe68ff336c073b564e02fa", "pr_number": "55530", "files_changed": ["test/jit/test_module_interface.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "384daacd1e": {"title": "[Hackathon] Add source range info for tests in test_module_containers (#55500)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55500\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27628752\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 3b0a1a1daae4d701be2358f912cba839844b2a44", "pr_number": "55500", "files_changed": ["test/jit/test_module_containers.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e359842f23": {"title": "Strict typecheck all files in tools/codegen (#55227)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55227\n\nThis seems to increase the number of typechecked files.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D27535373\n\nPulled By: ezyang\n\nfbshipit-source-id: b36f6f8ce52c76848ed600ca9dd6b0c1de5813ff", "pr_number": "55227", "files_changed": ["mypy-strict.ini"], "labels": ["Merged", "cla signed"]}, "d6cbecbbb6": {"title": "[PyTorch] Reapply D27404164: Devirtualize is_contiguous (#55333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55333\n\nReapplying without using enum class in a bitfield. See new\ncomments about gcc bug.\nghstack-source-id: 125776904\n\nTest Plan: Carefully review OSS test failure logs this time\n\nReviewed By: kimishpatel, bhosmer\n\nDifferential Revision: D27576623\n\nfbshipit-source-id: 68fb00e5ff5215e56c8b9bc02717e1e7b2fedf9b", "pr_number": "55333", "files_changed": ["aten/src/ATen/BatchedTensorImpl.cpp", "aten/src/ATen/BatchedTensorImpl.h", "aten/src/ATen/OpaqueTensorImpl.h", "aten/src/ATen/SparseTensorImpl.cpp", "aten/src/ATen/SparseTensorImpl.h", "aten/src/ATen/native/metal/MetalTensorImpl.h", "aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "8ac0619784": {"title": "Avoid infinite recursion in __torch_function__ example (#55391)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55284\n\nThis gets the example to run but probably doesn't help the readability of the example.\n\nThoughts?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55391\n\nReviewed By: mrshenli\n\nDifferential Revision: D27621096\n\nPulled By: ezyang\n\nfbshipit-source-id: d02c4fb0001e54139a167b477fd3b4a229e4dc8c", "pr_number": "55391", "files_changed": ["docs/source/notes/extending.rst"], "labels": ["Merged", "cla signed", "open source"]}, "ec38dda1cc": {"title": "Remove extra close bracket in extending.rst (#55409)", "body": "Summary:\nSmall typo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55409\n\nReviewed By: pbelevich\n\nDifferential Revision: D27611177\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 8a5ff702e4ab8a7eb2403432889f8b7a5a69484b", "pr_number": "55409", "files_changed": ["docs/source/notes/extending.rst"], "labels": ["Merged", "cla signed"]}, "0dff0d1537": {"title": "[ROCM] Disable few tests for Magma (#55534)", "body": "Summary:\nAfter MAGMA has been enabled, around 5k new tests are running now.\nOut of these 5 tests (each having 4 datatypes) are failing on the latest ROCM\nCI with Rocm 4.1.  Disabling these tests for now so the ROCM CI does not fail.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55534\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27630085\n\nPulled By: malfet\n\nfbshipit-source-id: c48d124e6a2b4a4f3c6c4b6ac2bdf6c214f325c7", "pr_number": "55534", "files_changed": ["test/test_linalg.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "3517ee1bcb": {"title": "Fix ordered_dict.h for CUDA on Windows (#55275)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55266\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55275\n\nReviewed By: mrshenli\n\nDifferential Revision: D27623887\n\nPulled By: malfet\n\nfbshipit-source-id: 6dac357e21179a259ac95f0e1b7399b03dacc81d", "pr_number": "55275", "files_changed": ["test/cpp_extensions/setup.py", "test/test_cpp_extensions_aot.py", "torch/csrc/api/include/torch/ordered_dict.h"], "labels": ["Merged", "ci/windows", "cla signed", "open source"]}, "ffe301846b": {"title": "[Hackathon] Add error source range highlighting check in test_hash and test_list_dict (#55490)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55490\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27628697\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 694226f0b083606f665569e6a84d547026c7f19f", "pr_number": "55490", "files_changed": ["test/jit/test_hash.py", "test/jit/test_list_dict.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "f4967d68f5": {"title": "make torch.testing asserts importable (#54769)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54769\n\nFollow-up to #53820. This\n\n- makes the `asserts.py` module private as per suggestion from rgommers in https://github.com/pytorch/pytorch/pull/53820#issuecomment-802661387. With this the functions should only be accessible through `torch.testing`, giving us the option the change the underlying structure later.\n- moves the code from `torch/testing/__init__.py` to `torch/testing/_core.py` (happy to accept other name suggestions). Otherwise we can't import the new `_asserts.py` in `torch/testing/__init__.py` due to circular imports.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D27438451\n\nPulled By: mruberry\n\nfbshipit-source-id: c7292b4d5709185b42b4aac8016648562688040e", "pr_number": "54769", "files_changed": ["torch/jit/_trace.py", "torch/testing/__init__.py", "torch/testing/_asserts.py", "torch/testing/_check_kernel_launches.py", "torch/testing/_core.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py", "torch/testing/asserts.py", "torch/testing/check_kernel_launches.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "902bf0bbbe": {"title": "[special] Alias for sigmoid and logit & follow-up (#54759)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/50345\n\nChages:\n* Alias for sigmoid and logit\n* Adds out variant for C++ API\n* Updates docs to link back to `special` documentation\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54759\n\nReviewed By: mrshenli\n\nDifferential Revision: D27615208\n\nPulled By: mruberry\n\nfbshipit-source-id: 8bba908d1bea246e4aa9dbadb6951339af353556", "pr_number": "54759", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/special.rst", "torch/_torch_docs.py", "torch/csrc/api/include/torch/special.h", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/overrides.py", "torch/special/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "37d1b39413": {"title": "OpInfo: `atan2` (#55132)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55132\n\nReviewed By: mrshenli\n\nDifferential Revision: D27615135\n\nPulled By: mruberry\n\nfbshipit-source-id: 22fa1a225b9a75eb478797316e4462d4af4e8826", "pr_number": "55132", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "f88a3fff65": {"title": "Set requires_gradient to help autodiff to prune unneeded gradients (#54374)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54040\n`prim::RequiresGradCheck` guarantees that requires_grad properties\nof input tensors will match the profiled, otherwise a fallback path\nwill be triggered. This allow us to prune off gradients in backward\ngraph for inputs that don't need gradients. We transfer requires_grad\nproperties from inputs to the `prim::DifferentiableGraph` onto inputs to the\ndifferentiable graph. Autodiff will inspect these properties and prune\noff gradients that aren't required\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54374\n\nReviewed By: H-Huang\n\nDifferential Revision: D27369251\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 2bce7a2d7f2ec091db9bf4c4b91d8b29edd5be11", "pr_number": "54374", "files_changed": ["test/jit/test_autodiff_subgraph_slicing.py", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "f1a0b817f0": {"title": "[pthreadpool] Apply cap for macos builds (#55435)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55435\n\nWe've seen issues from the macos skylight app that PyTorch is super slow due to the lack of cap support in pthreadpools. For mac builds, we set the thread count to `#threads/2`.\nghstack-source-id: 125900852\n\nTest Plan:\n- Sandcastle CI\n- CircleCI\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27578871\n\nfbshipit-source-id: 7b947bc5d6cf289378abf5f479575e112325d02b", "pr_number": "55435", "files_changed": ["caffe2/utils/threadpool/ThreadPool.cc"], "labels": ["Merged", "cla signed"]}, "b5647dd52b": {"title": "Add OpInfo tests for torch.addbmm (#55378)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55378\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27598681\n\nPulled By: anjali411\n\nfbshipit-source-id: 24082f54b12e6346b81c9b6a6e20714e8fd94a9b", "pr_number": "55378", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "473d193966": {"title": "Use mkldnn copy for copy_ when self and src are Mkldnn layout (#54248)", "body": "Summary:\nCurrently, when copy_ is called with Mkldnn layout, a RuntimeError is raised.\n\n**Environment**\n- CPU : Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz\n- PyTorch master(1772e26f6380d1)\n- build with USE_MKLDNN=1\n\n**Sample code to reproduce:**\n```python\nimport torch\n\nx = torch.randn(4, 5, dtype=torch.float32)\nmkldnn_x = x.to_mkldnn()\nmkldnn_y = torch.randn(4, 5, dtype=torch.float32).to_mkldnn()\nmkldnn_y.copy_(mkldnn_x)\n\nprint(x)\nprint(mkldnn_y.to_dense())\n```\n\n**Results:**\nActual:\n```sh\nTraceback (most recent call last):\n  File \"mkldnn_copy.py\", line 6, in <module>\n    mkldnn_y.copy_(mkldnn_x)\nRuntimeError: unsupported tensor layout: Mkldnn\n```\n\nExpected:\n```sh\n# x\ntensor([[ 0.1276, -0.1179,  1.1970,  2.4836,  1.9059],\n        [-1.9647,  0.8613, -0.5060,  0.1555,  0.3661],\n        [-0.1560, -0.2133,  0.3414, -1.7095, -2.3431],\n        [ 1.3291,  0.3083,  0.5523, -2.0577, -0.4740]])\n# mkldnn_y\ntensor([[ 0.1276, -0.1179,  1.1970,  2.4836,  1.9059],\n        [-1.9647,  0.8613, -0.5060,  0.1555,  0.3661],\n        [-0.1560, -0.2133,  0.3414, -1.7095, -2.3431],\n        [ 1.3291,  0.3083,  0.5523, -2.0577, -0.4740]])\n```\n\nThis is because `copy_` does not support Mkldnn layout.\nSo I modified to call `copy_mkldnn_` in `copy_` when both `self` and `src` are Mkldnn layout.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54248\n\nReviewed By: mrshenli\n\nDifferential Revision: D27641352\n\nPulled By: ezyang\n\nfbshipit-source-id: 70a37cdacb4a40b250ca16f2f6ddb6b71ff52d90", "pr_number": "54248", "files_changed": ["aten/src/ATen/native/mkldnn/Copy.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_mkldnn.py", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "module: mkldnn", "open source", "triaged"]}, "d3d7f57c2c": {"title": "Fix a problem when removing parametrizations (#55456)", "body": "Summary:\nThere was an error when removing a parametrization with `leave_parametrized=True`. It had escaped the previous tests. This PR should fix that.\n**Edit.**\nI also took this chance to fix a few mistakes that the documentation had, and to also write the `set_original_` in a more compact way.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55456\n\nReviewed By: mrshenli\n\nDifferential Revision: D27620481\n\nPulled By: albanD\n\nfbshipit-source-id: f1298ddbcf24566ef48850c62a1eb4d8a3576152", "pr_number": "55456", "files_changed": ["test/test_nn.py", "torch/nn/utils/parametrize.py"], "labels": ["Merged", "cla signed", "module: nn", "open source", "triaged"]}, "7d56de1834": {"title": "DOC: use autosummary on tensors.rst (#55042)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/52256\n\nSplits tensors into a table-of-contents page and many sub-pages, one for each function\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55042\n\nReviewed By: mrshenli\n\nDifferential Revision: D27628688\n\nPulled By: zou3519\n\nfbshipit-source-id: 08e87700a8e7d5b3fba3f1949e29e988a42bf2c6", "pr_number": "55042", "files_changed": ["docs/source/autograd.rst", "docs/source/sparse.rst", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "432df40d83": {"title": "[Hackathon] Move python builtins to test_python_builtins.py (#55479)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55479\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27642098\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 8d92a7d0f6db63f3cc3f439cb75a8d809af9106d", "pr_number": "55479", "files_changed": ["test/jit/test_python_builtins.py", "test/test_jit.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "3f9492c8b3": {"title": "[Hackathon] Modernize API used in NNC C++ tests (1/3) (#55512)", "body": "Summary:\nPartially fixes https://github.com/pytorch/pytorch/issues/55203\n\nFixes issues (1) and (2) in the following tests:\ntests in test/cpp/tensorexpr/test_loopnest.cpp from the beginning to LoopNestReorderLongStringFull (including)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55512\n\nReviewed By: mrshenli\n\nDifferential Revision: D27630679\n\nPulled By: soulitzer\n\nfbshipit-source-id: b581aaea4f5f54b3285f0348aa76e99779418f80", "pr_number": "55512", "files_changed": ["test/cpp/tensorexpr/test_loopnest.cpp"], "labels": ["Merged", "cla signed", "hackathon"]}, "8b5da2f48d": {"title": "rename .pytorch-disabled-tests to disabled-tests.json (#55618)", "body": "Summary:\nWe shouldn't store it as a hidden file, and having the json extension is useful.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55618\n\nTest Plan: https://github.com/janeyx99/gha-experiments/runs/2298470065?check_suite_focus=true in my own repo\n\nReviewed By: samestep\n\nDifferential Revision: D27651467\n\nPulled By: janeyx99\n\nfbshipit-source-id: cd9b6c8d065f1ffdcabb0844d375ae8be7177e13", "pr_number": "55618", "files_changed": [".github/workflows/update_disabled_tests.yml"], "labels": ["Merged", "cla signed"]}, "f665a7f8a1": {"title": "[pet] Set error code in reply file when child process is terminated by signals.", "body": "Summary: Fill reply file's error code with ProcessFailure's exitcode. This is necessary when child process terminated by signals (ex. SIGSEGV).\n\nTest Plan:\n- Buck test\n```\nbuck test mode/dev-nosan pytorch/elastic/torchelastic/distributed/fb/test:launch_test\nbuck test mode/dev-nosan caffe2/torch/distributed/elastic/multiprocessing/errors/fb/test:error_handler_fb_test_needed_coverage\n```\n\n- TSM\n```\nfbpkg build -E torchelastic_distributed_sum\n\nbuck run mode/dev-nosan //pytorch/elastic/torchelastic/tsm/fb/cli:tsm -- run_ddp --scheduler mast --fbpkg torchelastic_distributed_sum:ecdf31f --nnodes 2 --nproc_per_node 2 --resource T1  --run_cfg hpcIdentity=oncall_dai_pet,hpcClusterUuid=MastNaoTestCluster main.pa\n```\nhttps://www.internalfb.com/mast/job/tsm_wilsonhong-torchelastic_distributed_sum_ef3fd8d3\n\n- classy_vision\n```\nflow-cli canary  pytorch.elastic.examples.classy_vision.main --entitlement gpu_prod --run-as-secure-group oncall_dai_pet --buck-target //fblearner/flow/projects/pytorch/elastic/examples:workflow\n```\nhttps://our.intern.facebook.com/intern/fblearner/details/263970380/?notif_channel=cli\n\nReviewed By: tierex\n\nDifferential Revision: D27512554\n\nfbshipit-source-id: 903d25d96655085685f874113826d4627d9a79e4", "pr_number": null, "files_changed": ["test/distributed/elastic/multiprocessing/errors/api_test.py", "test/distributed/elastic/multiprocessing/errors/error_handler_test.py", "torch/distributed/elastic/multiprocessing/errors/__init__.py", "torch/distributed/elastic/multiprocessing/errors/error_handler.py"], "labels": []}, "60263e0f5a": {"title": "OpInfo porting for torch.maximum / torch.minimum / torch.fmax / torch.fmin (#55129)", "body": "Summary:\nRelated https://github.com/pytorch/pytorch/issues/54261\n\nThis PR ports the method_tests() entries of following operators to OpInfo.\n- torch.maximum\n- torch.minimum\n- torch.fmax\n- torch.fmin\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55129\n\nReviewed By: ngimel\n\nDifferential Revision: D27562189\n\nPulled By: mruberry\n\nfbshipit-source-id: 9f25aeb09eb353080af43f25ea2e931474510aca", "pr_number": "55129", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "11add8f45f": {"title": "Add --suppress-diagnostics option (#55612)", "body": "Summary:\nAdd option to add //NOLINTNEXTLINE for every detected violation\n\nSeries of automated huge diffs are coming after this one to make large chunks of code clang-tidy\n\nPR generated by new option: https://github.com/pytorch/pytorch/pull/55628\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55612\n\nReviewed By: samestep\n\nDifferential Revision: D27649473\n\nPulled By: malfet\n\nfbshipit-source-id: 251a68fcc50bf0fd69c6566293d4a516c0ab24c8", "pr_number": "55612", "files_changed": ["mypy-strict.ini", "mypy.ini", "tools/clang_tidy.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "2564c0c889": {"title": "avoid CPU std::copysign segfault when compiling on arm64 (take-2) (#55608)", "body": "Summary:\nRe-land of https://github.com/pytorch/pytorch/issues/51834\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55608\n\nReviewed By: ngimel\n\nDifferential Revision: D27649077\n\nPulled By: malfet\n\nfbshipit-source-id: 1a21611fb12106f75fe50e8f9f14796ab6ab9464", "pr_number": "55608", "files_changed": ["aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "c10/cuda/CUDAMathCompat.h"], "labels": ["Merged", "cla signed"]}, "adc65974b2": {"title": "Run ShellCheck on scripts in GitHub Actions workflows (#55486)", "body": "Summary:\nResolves https://github.com/pytorch/pytorch/issues/55314.\n\n- [x] Extract shell scripts from `.github/workflows/*.yml` into `.shellcheck_generated` dir\n- [x] Run ShellCheck on `.shellcheck_generated`\n- [x] Fail if any of the extracted scripts contain [GitHub Actions expressions][]: `${{ <expression> }}`\n- [x] Fix the newly-surfaced warnings\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55486\n\nTest Plan:\nLocally run the \"ShellCheck\" step from \"Lint / quick-checks\".\n\n[github actions expressions]: https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions#about-contexts-and-expressions\n\nReviewed By: malfet\n\nDifferential Revision: D27627590\n\nPulled By: samestep\n\nfbshipit-source-id: 8a22c6743e11b3059506043735f100efdd7c5a26", "pr_number": "55486", "files_changed": [".github/workflows/auto_label.yml", ".github/workflows/build_linux_conda.yml", ".github/workflows/build_linux_libtorch.yml", ".github/workflows/build_linux_wheels.yml", ".github/workflows/clang_format.yml", ".github/workflows/lint.yml", ".gitignore", ".jenkins/run-shellcheck.sh", "mypy-strict.ini", "tools/README.md", "tools/extract_scripts.py", "tools/run_shellcheck.sh"], "labels": ["Merged", "cla signed", "hackathon"]}, "778f9eab6c": {"title": "Don't switch streams when running Caffe2 ops from c10. (#55121)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55121\n\nThis is done by allow -1 as a stream ID, meaning \"don't change\nthe stream\", in SwitchToDevice\n\nFixes #54830\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D27527544\n\nPulled By: ezyang\n\nfbshipit-source-id: c54983d6fc79a8fa1c65a71559a57425e40ba717", "pr_number": "55121", "files_changed": ["caffe2/core/context_gpu.h", "caffe2/core/export_caffe2_op_to_c10.h", "caffe2/core/operator.h"], "labels": ["Merged", "cla signed"]}, "eb5e1fc713": {"title": "[torch] Add cuda support for segment reduction 'max' (#54175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54175\n\nBuilding on top of previous PR. This PR adds cuda support for 1D max reduction.\n\nNext steps:\n- Add support for other major reduction types (e.g. min, sum) for 1D tensor\n- Documentation for the op\n- Perf optimizations and benchmark util\n- Backward support  (not high priority)\n- Support for multi dimensional tensors (on data and lengths) (not high priority)\n- Support for 'indices' (not high priority)\n\nTest Plan: Added unit test\n\nReviewed By: ngimel\n\nDifferential Revision: D27121170\n\nfbshipit-source-id: 1c2565f42e2903e6fc089d56983ce8857efbfa3c", "pr_number": "54175", "files_changed": ["BUILD.bazel", "aten/src/ATen/native/SegmentReduce.cpp", "aten/src/ATen/native/SegmentReduce.h", "aten/src/ATen/native/cuda/SegmentReduce.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_segment_reductions.py"], "labels": ["Merged", "Reverted", "cla signed", "fb-exported"]}, "305abde976": {"title": "Fix nvcc warnings (#55367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55367\n\nDuring compilation, nvcc emits several warnings about unused variables and static funcions:\n\n```\ncaffe2/aten/src/ATen/native/cuda/SpectralOps.cu(231): warning: function \"at::native::_run_cufft\" was declared but never referenced\n\ncaffe2/aten/src/ATen/native/sparse/cuda/SparseMatMul.cu(60): warning: function \"at::native::<unnamed>::confirm_mult_size\" was declared but never referenced\n\ncaffe2/aten/src/ATen/native/cuda/UnaryFractionKernels.cu(112): warning: function \"at::native::nearbyint_wrapper(c10::complex<double>)\" was declared but never referenced\n\ncaffe2/aten/src/ATen/native/cuda/TensorFactories.cu(106): warning: variable \"d_temp_storage\" was declared but never referenced\n\ncaffe2/torch/fb/sparsenn/sparsenn_operators_gpu.cu(2325): warning: variable \"kMaxThreads\" was declared but never referenced\n```\n\nTo reproduce, run the following build command on remote/master:\n```\nbuck build mode/dev-nosan caffe2/torch/fb/sparsenn:sparsenn_operators_gpu\n```\n\nWarnings about unused variables are fixed by removing the variable declaration. However, I don't want to remove the unused static functions. They were probably used before some other part of the code was refactored. They might be useful again in the future. So, I added a #pragma firectives to disable warnings for such functions.\n\nTest Plan: Compilation does not produce warnings any more.\n\nReviewed By: r-barnes\n\nDifferential Revision: D27577342\n\nfbshipit-source-id: e6a6e5ec513996337d904985dd27c60601c74803", "pr_number": "55367", "files_changed": ["aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "aten/src/ATen/native/sparse/cuda/SparseMatMul.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "55db156229": {"title": "remove test_jit_py3.py entirely (#55560)", "body": "Summary:\n1. move module related stuff to test_module_container\n2. created test_types for types and annotation\n3. created test_misc for the rest\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55560\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D27650911\n\nPulled By: walterddr\n\nfbshipit-source-id: d895a7da9e9c3d25a662a37faf4daabc276b9c1a", "pr_number": "55560", "files_changed": ["test/jit/test_misc.py", "test/jit/test_module_containers.py", "test/jit/test_types.py", "test/run_test.py", "test/test_jit.py", "test/test_jit_py3.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "fc45ff8177": {"title": "[skip ci] Document '[skip ci]' (#55418)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55418\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D27609269\n\nPulled By: driazati\n\nfbshipit-source-id: 6ce562950ee35e029f0bfa3d0fffbbcc28265a7a", "pr_number": "55418", "files_changed": ["CONTRIBUTING.md"], "labels": ["Merged", "cla signed"]}, "bf882929f1": {"title": "[skip ci] Add explanation for why we split TORCH_CUDA_API (#55641)", "body": "Summary:\nProvide explanation for why we have (and use) the BUILD_SPLIT_CUDA option as a result of PR https://github.com/pytorch/pytorch/pull/49050.\n\nThis should hopefully clarify why there is both TORCH_CUDA_CU_API and TORCH_CUDA_CPP_API.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55641\n\nReviewed By: samestep\n\nDifferential Revision: D27661729\n\nPulled By: janeyx99\n\nfbshipit-source-id: a68b44df2b45ce10590b9b0229558a1ad40ce485", "pr_number": "55641", "files_changed": ["c10/macros/Export.h"], "labels": ["Merged", "cla signed"]}, "cc11aaaa60": {"title": "Disallow non-breaking spaces (#55465)", "body": "Summary:\nmalfet found a couple of these in https://github.com/pytorch/pytorch/issues/55346; this PR removes the rest and adds a lint that prevents them from being accidentally added again in the future. It also removes the `-o` flag added in https://github.com/pytorch/pytorch/issues/53733 (which was unnecessarily hiding context without reducing the number of lines of output), and updates the lint error messages to reflect that the individual line numbers are shown in the logs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55465\n\nTest Plan:\nThe \"Lint / quick-checks\" job in GitHub Actions should succeed on this PR. To verify that the lint does correctly find and error on non-breaking spaces, checkout ece075195d49c25213c96b9d53fcf7077215f44a and run it locally:\n```sh\n(! git --no-pager grep -In $'\\u00a0' -- . || (echo \"The above lines have non-breaking spaces (U+00A0); please convert them to spaces (U+0020)\"; false))\n```\nIt should print over a hundred lines of output and exit with status 1.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27622136\n\nPulled By: samestep\n\nfbshipit-source-id: e7ffd5a9519093e7a0ffdf55e9291f63e21ce841", "pr_number": "55465", "files_changed": [".github/workflows/lint.yml", "benchmarks/operator_benchmark/README.md", "test/package/test_resources.py", "torch/distributed/elastic/rendezvous/__init__.py", "torch/distributed/elastic/rendezvous/etcd_rendezvous.py", "torch/fx/OVERVIEW.md"], "labels": ["Merged", "cla signed", "fx", "oncall: distributed"]}, "3498fde20e": {"title": "Add AccumulateType in AdaptiveAveragePooling3d.cu (#53607)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/52719\n\n- Changed the type(`scalar_t`) of intermediate results to `at::acc_type<scalar_t, true>`\n\nThis issue occurs by decimal precision of the half precision.\n\nFollows test cases of upper issue, The value range of input tensors are [0, 1] because init by `rand`.\nAnd when the kernel size 1, summations all target values and divide numel of kernel\nhttps://github.com/pytorch/pytorch/blob/34d9278c1913d83db47a25f0cac71b69ab877c84/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu#L94-L95\n\nWhen adding [0, 1] values, if `sum` more than 2048 then not changed values. ( Even if the value is small, the mored exact value is added, but there are still precision issues.)\n(https://en.wikipedia.org/wiki/Half-precision_floating-point_format)\n\nBenchmarks\n- In V100 32GB, Driver : 450.80, cuda 10.1\n- faster than prev\n\n<details><summary>Script</summary><p>\n\n```import torch\nfrom torch.utils.benchmark import Timer\n\ntorch.manual_seed(0)\n\nkernel_sizes = [1, 3, 5, 7, 9, 11, 13]\nshapes = [(12, 12, 12), (16, 16, 16), (16, 32, 32), (16, 56, 56), (16, 112, 112)]\n\ndef run(batch, channel):\n    print(f\"Batch : {batch}, Channel : {channel} / (diff, diff / numel, time)\")\n\n    head = \"\\t\".join(f\"{str(s):30s}\" for s in [\"k \\ shape\"] + shapes)\n    print(head)\n    for kernel_size in kernel_sizes:\n        kernel_size = (kernel_size, kernel_size, kernel_size)\n        pool = torch.nn.AdaptiveAvgPool3d(kernel_size)\n\n        print(f\"{str(kernel_size):30s}\", end=\"\\t\")\n        for shape in shapes:\n            x_half = torch.rand([batch, channel, *shape], dtype=torch.half, device=\"cuda\")\n            x_float = x_half.float()\n\n            y_half = pool(x_half)\n            y_float = pool(x_float)\n\n            timer = Timer(\"pool(x_half)\", globals={\"pool\": pool, \"x_half\": x_half})\n            measurement = timer.blocked_autorange(min_run_time=5)\n\n            diff = (y_float - y_half).abs().sum().item()\n            diff = f\"{diff:.4f}, {diff / y_half.numel():.6f}, {measurement.median * 1e6 :3.2f}us\"\n            print(f\"{diff:30s}\", end=\"\\t\")\n        print(\"\")\n\nrun(1, 1)\nrun(1, 3)\nrun(1, 54)\nrun(1, 16)\n\nrun(8, 1)\nrun(8, 16)\nrun(8, 54)\n\nimport torch\nm = torch.nn.AdaptiveAvgPool3d((1,1,1))\n\ninputs = torch.rand([8,54,16,56,56])\ninputs = inputs.cuda()\ninputs_2 = inputs.half()\n\nprint(\"Float\")\nout = m(inputs).float()\nprint(\"half\")\nout2 = m(inputs_2).float()\n\nprint('Discepancies', torch.sum(torch.abs(out2- out)).item(), torch.sum(torch.abs(out2- out)).item() / out.numel() , out.numel())\n\nprint(\"Sum : \", torch.sum(inputs, dim=(2,3,4))[0, 0], torch.sum(inputs_2, dim=(2,3,4))[0, 0])\n```\n</p>\n</details>\n\n<details><summary>This commit</summary><p>\n\n```\nBatch : 1, Channel : 1 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                         (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.0001, 0.000078, 55.73us       0.0001, 0.000079, 117.51us       0.0000, 0.000003, 379.60us      0.0000, 0.000046, 1046.21us      0.0001, 0.000139, 3897.17us\n(3, 3, 3)                       0.0021, 0.000076, 22.04us       0.0031, 0.000115, 21.47us        0.0022, 0.000080, 41.63us       0.0030, 0.000111, 100.59us       0.0025, 0.000091, 295.04us\n(5, 5, 5)                       0.0103, 0.000083, 21.65us       0.0097, 0.000078, 21.37us        0.0103, 0.000083, 21.60us       0.0114, 0.000091, 25.69us        0.0107, 0.000085, 97.06us\n(7, 7, 7)                       0.0312, 0.000091, 21.52us       0.0290, 0.000084, 21.61us        0.0311, 0.000091, 21.60us       0.0309, 0.000090, 21.44us        0.0334, 0.000097, 33.60us\n(9, 9, 9)                       0.0646, 0.000089, 21.57us       0.0672, 0.000092, 21.89us        0.0662, 0.000091, 21.89us       0.0684, 0.000094, 27.64us        0.0660, 0.000091, 54.85us\n(11, 11, 11)                    0.1251, 0.000094, 21.68us       0.1194, 0.000090, 21.70us        0.1202, 0.000090, 21.72us       0.1233, 0.000093, 22.25us        0.1229, 0.000092, 41.39us\n(13, 13, 13)                    0.2038, 0.000093, 21.57us       0.2047, 0.000093, 21.58us        0.1964, 0.000089, 21.54us       0.2021, 0.000092, 21.94us        0.1989, 0.000091, 40.01us\nBatch : 1, Channel : 3 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                     (16, 32, 32)                    (16, 56, 56)                     (16, 112, 112)\n(1, 1, 1)                       0.0003, 0.000110, 55.74us       0.0003, 0.000093, 118.62us       0.0003, 0.000093, 382.12us      0.0001, 0.000040, 1052.33us      0.0003, 0.000114, 3917.90us\n(3, 3, 3)                       0.0073, 0.000090, 21.84us       0.0075, 0.000093, 22.25us        0.0072, 0.000089, 41.78us       0.0070, 0.000087, 100.27us       0.0069, 0.000086, 293.96us\n(5, 5, 5)                       0.0353, 0.000094, 22.57us       0.0325, 0.000087, 21.64us        0.0343, 0.000092, 22.63us       0.0338, 0.000090, 25.82us        0.0332, 0.000089, 97.16us\n(7, 7, 7)                       0.0937, 0.000091, 22.50us       0.0910, 0.000088, 21.92us        0.0933, 0.000091, 21.99us       0.0948, 0.000092, 21.56us        0.0928, 0.000090, 34.17us\n(9, 9, 9)                       0.1957, 0.000089, 21.68us       0.1984, 0.000091, 21.57us        0.2025, 0.000093, 22.10us       0.1986, 0.000091, 27.66us        0.2020, 0.000092, 55.32us\n(11, 11, 11)                    0.3585, 0.000090, 21.75us       0.3684, 0.000092, 22.70us        0.3706, 0.000093, 21.67us       0.3752, 0.000094, 21.86us        0.3663, 0.000092, 41.22us\n(13, 13, 13)                    0.5931, 0.000090, 21.67us       0.6056, 0.000092, 21.79us        0.6005, 0.000091, 21.79us       0.6112, 0.000093, 21.69us        0.6034, 0.000092, 40.02us\nBatch : 1, Channel : 54 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                     (16, 32, 32)                    (16, 56, 56)                     (16, 112, 112)\n(1, 1, 1)                       0.0051, 0.000095, 55.76us       0.0060, 0.000112, 118.60us       0.0036, 0.000067, 381.50us      0.0054, 0.000100, 1054.03us      0.0048, 0.000089, 4888.68us\n(3, 3, 3)                       0.1332, 0.000091, 21.66us       0.1344, 0.000092, 22.62us        0.1354, 0.000093, 45.72us       0.1364, 0.000094, 106.63us       0.1324, 0.000091, 448.31us\n(5, 5, 5)                       0.6221, 0.000092, 22.48us       0.6220, 0.000092, 21.71us        0.6053, 0.000090, 27.65us       0.6137, 0.000091, 31.40us        0.6209, 0.000092, 172.78us\n(7, 7, 7)                       1.6859, 0.000091, 22.42us       1.6972, 0.000092, 21.96us        1.6849, 0.000091, 23.14us       1.7012, 0.000092, 26.25us        1.6920, 0.000091, 75.58us\n(9, 9, 9)                       3.5811, 0.000091, 21.73us       3.5746, 0.000091, 22.55us        3.6237, 0.000092, 27.66us       3.6046, 0.000092, 59.71us        3.6392, 0.000092, 168.15us\n(11, 11, 11)                    6.5582, 0.000091, 22.05us       6.5746, 0.000091, 21.74us        6.5955, 0.000092, 32.91us       6.5644, 0.000091, 45.57us        6.5697, 0.000091, 114.01us\n(13, 13, 13)                    10.6384, 0.000090, 21.81us      10.8608, 0.000092, 21.79us       10.8375, 0.000091, 37.01us      10.8662, 0.000092, 51.80us       10.8593, 0.000092, 123.19us\nBatch : 1, Channel : 16 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                     (16, 32, 32)                    (16, 56, 56)                     (16, 112, 112)\n(1, 1, 1)                       0.0015, 0.000093, 55.75us       0.0012, 0.000075, 118.10us           0.0013, 0.000079, 379.25us      0.0012, 0.000075, 1047.21us     0.0013, 0.000079, 4451.57us\n(3, 3, 3)                       0.0407, 0.000094, 21.82us       0.0395, 0.000091, 21.69us            0.0385, 0.000089, 42.07us       0.0397, 0.000092, 100.33us      0.0384, 0.000089, 363.31us\n(5, 5, 5)                       0.1858, 0.000093, 21.76us       0.1799, 0.000090, 21.63us            0.1834, 0.000092, 21.76us       0.1890, 0.000095, 26.04us       0.1814, 0.000091, 135.32us\n(7, 7, 7)                       0.4937, 0.000090, 21.65us       0.5076, 0.000092, 21.69us            0.5001, 0.000091, 22.31us       0.4988, 0.000091, 21.59us       0.5123, 0.000093, 50.03us\n(9, 9, 9)                       1.0678, 0.000092, 21.73us       1.0752, 0.000092, 21.75us            1.0673, 0.000091, 21.75us       1.0649, 0.000091, 30.01us       1.0786, 0.000092, 70.92us\n(11, 11, 11)                    1.9591, 0.000092, 21.57us       1.9522, 0.000092, 21.60us            1.9566, 0.000092, 21.73us       1.9475, 0.000091, 23.46us       1.9323, 0.000091, 55.02us\n(13, 13, 13)                    3.1784, 0.000090, 22.02us       3.2165, 0.000092, 21.95us            3.1969, 0.000091, 21.92us       3.2061, 0.000091, 24.40us       3.2578, 0.000093, 56.00us\nBatch : 8, Channel : 1 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                         (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.0010, 0.000122, 55.74us       0.0009, 0.000114, 118.82us           0.0006, 0.000074, 379.80us      0.0009, 0.000107, 1047.31us     0.0008, 0.000102, 3900.36us\n(3, 3, 3)                       0.0219, 0.000101, 21.57us       0.0200, 0.000093, 21.61us            0.0194, 0.000090, 41.74us       0.0208, 0.000096, 99.91us       0.0212, 0.000098, 293.03us\n(5, 5, 5)                       0.0906, 0.000091, 21.46us       0.0911, 0.000091, 21.60us            0.0934, 0.000093, 21.93us       0.0927, 0.000093, 25.74us       0.0913, 0.000091, 96.85us\n(7, 7, 7)                       0.2530, 0.000092, 22.53us       0.2526, 0.000092, 22.46us            0.2558, 0.000093, 22.03us       0.2542, 0.000093, 22.29us       0.2475, 0.000090, 34.44us\n(9, 9, 9)                       0.5305, 0.000091, 22.34us       0.5368, 0.000092, 22.42us            0.5265, 0.000090, 21.74us       0.5370, 0.000092, 27.81us       0.5416, 0.000093, 55.65us\n(11, 11, 11)                    0.9887, 0.000093, 21.80us       0.9660, 0.000091, 21.61us            0.9793, 0.000092, 22.11us       0.9719, 0.000091, 21.80us       0.9650, 0.000091, 43.90us\n(13, 13, 13)                    1.6024, 0.000091, 21.87us       1.6198, 0.000092, 22.65us            1.6242, 0.000092, 21.73us       1.6236, 0.000092, 22.59us       1.6025, 0.000091, 42.77us\nBatch : 8, Channel : 16 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                         (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.0113, 0.000088, 56.66us       0.0117, 0.000091, 119.57us           0.0130, 0.000102, 389.57us      0.0110, 0.000086, 1433.78us     0.0119, 0.000093, 5217.61us\n(3, 3, 3)                       0.3209, 0.000093, 21.54us       0.3184, 0.000092, 22.87us            0.3115, 0.000090, 51.00us       0.3171, 0.000092, 164.17us      0.3182, 0.000092, 500.60us\n(5, 5, 5)                       1.4391, 0.000090, 22.39us       1.4577, 0.000091, 21.69us            1.4601, 0.000091, 53.87us       1.4626, 0.000091, 93.65us       1.4567, 0.000091, 370.11us\n(7, 7, 7)                       4.0501, 0.000092, 22.34us       4.0230, 0.000092, 31.45us            4.0381, 0.000092, 45.19us       4.0171, 0.000091, 65.35us       4.0108, 0.000091, 164.76us\n(9, 9, 9)                       8.5360, 0.000091, 22.80us       8.5456, 0.000092, 27.24us            8.5461, 0.000092, 50.23us       8.5677, 0.000092, 117.63us      8.5645, 0.000092, 270.46us\n(11, 11, 11)                    15.5521, 0.000091, 26.56us      15.5826, 0.000091, 32.81us           15.6014, 0.000092, 63.82us      15.5620, 0.000091, 96.87us      15.5722, 0.000091, 220.24us\n(13, 13, 13)                    25.4146, 0.000090, 32.91us      25.7898, 0.000092, 38.48us           25.6698, 0.000091, 72.02us      25.8193, 0.000092, 121.73us     25.7718, 0.000092, 249.71us\nBatch : 8, Channel : 54 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                         (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.0377, 0.000087, 109.07us      0.0405, 0.000094, 233.17us           0.0392, 0.000091, 998.97us      0.0393, 0.000091, 2960.68us     0.0408, 0.000094, 11879.53us\n(3, 3, 3)                       1.0660, 0.000091, 25.68us       1.0761, 0.000092, 64.12us            1.0725, 0.000092, 182.50us      1.0801, 0.000093, 505.82us      1.0736, 0.000092, 1650.21us\n(5, 5, 5)                       4.9587, 0.000092, 50.84us       4.9336, 0.000091, 47.38us            4.9696, 0.000092, 158.49us      4.9347, 0.000091, 237.39us      4.9303, 0.000091, 965.13us\n(7, 7, 7)                       13.5409, 0.000091, 45.60us      13.5736, 0.000092, 87.45us           13.5012, 0.000091, 141.63us     13.6111, 0.000092, 181.51us     13.5296, 0.000091, 469.77us\n(9, 9, 9)                       28.7817, 0.000091, 58.01us      28.7969, 0.000091, 77.61us           28.8761, 0.000092, 159.33us     28.8786, 0.000092, 334.47us     28.8093, 0.000091, 786.72us\n(11, 11, 11)                    52.4453, 0.000091, 78.19us      52.7265, 0.000092, 95.12us           52.7322, 0.000092, 200.38us     52.6342, 0.000092, 282.41us     52.6467, 0.000092, 652.54us\n(13, 13, 13)                    85.7411, 0.000090, 98.85us      86.7183, 0.000091, 115.28us          86.8545, 0.000092, 232.34us     86.9997, 0.000092, 367.32us     86.9083, 0.000092, 757.73us\nFloat\nhalf\nDiscepancies 0.03963914513587952 9.175728040712852e-05 432\nSum :  tensor(25110.1484, device='cuda:0') tensor(25104., device='cuda:0', dtype=torch.float16)\n```\n</p>\n</details>\n\n<details><summary>1.8.0</summary><p>\n\n```\nBatch : 1, Channel : 1 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                  (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.0023, 0.002275, 74.35us       0.0040, 0.003985, 159.73us        0.3740, 0.374021, 546.59us      0.4587, 0.458663, 1543.16us       0.4906, 0.490637, 5945.97us\n(3, 3, 3)                       0.0100, 0.000370, 20.37us       0.0230, 0.000852, 22.12us         0.0309, 0.001143, 54.75us       0.0520, 0.001926, 129.78us        7.1219, 0.263775, 377.11us\n(5, 5, 5)                       0.0441, 0.000352, 20.06us       0.0394, 0.000316, 20.50us         0.0759, 0.000607, 26.43us       0.1499, 0.001199, 32.01us         0.2707, 0.002166, 128.15us\n(7, 7, 7)                       0.0791, 0.000231, 20.10us       0.1002, 0.000292, 20.56us         0.1812, 0.000528, 20.48us       0.2424, 0.000707, 20.83us         0.4994, 0.001456, 43.97us\n(9, 9, 9)                       0.1122, 0.000154, 20.55us       0.1778, 0.000244, 20.44us         0.2572, 0.000353, 20.15us       0.4149, 0.000569, 35.64us         0.7208, 0.000989, 68.46us\n(11, 11, 11)                    0.2044, 0.000154, 20.47us       0.2647, 0.000199, 20.62us         0.3867, 0.000291, 20.61us       0.6059, 0.000455, 23.54us         1.0902, 0.000819, 53.32us\n(13, 13, 13)                    0.3094, 0.000141, 20.53us       0.3843, 0.000175, 20.60us         0.5756, 0.000262, 20.80us       0.8598, 0.000391, 24.52us         1.4853, 0.000676, 47.70us\nBatch : 1, Channel : 3 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                      (16, 32, 32)                    (16, 56, 56)                      (16, 112, 112)\n(1, 1, 1)                       0.0054, 0.001801, 74.36us       0.0108, 0.003614, 158.94us        1.1183, 0.372768, 547.67us      1.3782, 0.459387, 1545.27us       1.4685, 0.489505, 5949.17us\n(3, 3, 3)                       0.0308, 0.000380, 20.14us       0.0502, 0.000619, 22.11us         0.1210, 0.001493, 54.80us       0.1900, 0.002345, 130.47us        21.3483, 0.263560, 375.68us\n(5, 5, 5)                       0.1179, 0.000314, 20.68us       0.1326, 0.000354, 20.53us         0.2662, 0.000710, 26.51us       0.4116, 0.001098, 31.85us         0.8369, 0.002232, 128.19us\n(7, 7, 7)                       0.2335, 0.000227, 20.40us       0.3057, 0.000297, 20.43us         0.4954, 0.000481, 20.31us       0.7339, 0.000713, 20.74us         1.4208, 0.001381, 44.55us\n(9, 9, 9)                       0.3326, 0.000152, 20.63us       0.5353, 0.000245, 20.42us         0.8025, 0.000367, 20.13us       1.2693, 0.000580, 35.64us         2.2096, 0.001010, 68.88us\n(11, 11, 11)                    0.6121, 0.000153, 20.59us       0.8086, 0.000202, 20.42us         1.1700, 0.000293, 20.71us       1.8170, 0.000455, 23.54us         3.2117, 0.000804, 53.36us\n(13, 13, 13)                    0.9165, 0.000139, 20.51us       1.1395, 0.000173, 20.56us         1.7343, 0.000263, 20.80us       2.5868, 0.000392, 24.59us         4.5823, 0.000695, 47.77us\nBatch : 1, Channel : 54 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                      (16, 32, 32)                    (16, 56, 56)                      (16, 112, 112)\n(1, 1, 1)                       0.1092, 0.002023, 75.45us       0.1709, 0.003165, 160.44us        20.2452, 0.374911, 548.61us     24.7990, 0.459240, 1550.34us      26.4494, 0.489804, 6957.79us\n(3, 3, 3)                       0.5352, 0.000367, 20.58us       1.0281, 0.000705, 24.14us         2.0150, 0.001382, 59.12us       3.3069, 0.002268, 138.23us        384.5216, 0.263732, 529.71us\n(5, 5, 5)                       2.0739, 0.000307, 20.60us       2.5199, 0.000373, 20.44us         4.6916, 0.000695, 33.89us       7.9482, 0.001178, 37.74us         14.2553, 0.002112, 200.54us\n(7, 7, 7)                       4.2236, 0.000228, 20.61us       5.5605, 0.000300, 20.97us         9.0440, 0.000488, 26.40us       12.7847, 0.000690, 30.64us        25.3050, 0.001366, 88.05us\n(9, 9, 9)                       6.0817, 0.000154, 20.63us       9.5416, 0.000242, 20.84us         14.2416, 0.000362, 32.47us      22.8452, 0.000580, 78.57us        40.3246, 0.001024, 194.50us\n(11, 11, 11)                    11.1144, 0.000155, 20.56us      14.5581, 0.000203, 20.91us        20.8263, 0.000290, 38.07us      33.0004, 0.000459, 52.74us        57.3275, 0.000798, 137.19us\n(13, 13, 13)                    16.5176, 0.000139, 21.26us      20.8089, 0.000175, 22.33us        31.3433, 0.000264, 42.93us      45.9733, 0.000388, 59.84us        82.8301, 0.000698, 138.42us\nBatch : 1, Channel : 16 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                      (16, 32, 32)                    (16, 56, 56)                      (16, 112, 112)\n(1, 1, 1)                       0.0274, 0.001715, 74.99us       0.0485, 0.003034, 159.92us    5.9925, 0.374529, 546.35us      7.3389, 0.458679, 1544.53us     7.8354, 0.489714, 6677.00us\n(3, 3, 3)                       0.1560, 0.000361, 20.72us       0.3043, 0.000704, 22.37us     0.5838, 0.001352, 54.97us       1.0455, 0.002420, 130.57us      113.9739, 0.263828, 463.43us\n(5, 5, 5)                       0.6121, 0.000306, 20.12us       0.7247, 0.000362, 20.73us     1.3740, 0.000687, 26.59us       2.3794, 0.001190, 32.12us       4.1929, 0.002096, 165.81us\n(7, 7, 7)                       1.2389, 0.000226, 20.59us       1.6311, 0.000297, 20.53us     2.6732, 0.000487, 20.37us       3.7501, 0.000683, 20.71us       7.4575, 0.001359, 59.16us\n(9, 9, 9)                       1.7983, 0.000154, 20.64us       2.8075, 0.000241, 20.59us     4.2165, 0.000361, 20.38us       6.7153, 0.000576, 38.29us       12.0530, 0.001033, 86.33us\n(11, 11, 11)                    3.3326, 0.000156, 20.56us       4.3061, 0.000202, 20.67us     6.2235, 0.000292, 20.47us       9.8009, 0.000460, 27.41us       16.9994, 0.000798, 68.49us\n(13, 13, 13)                    4.9016, 0.000139, 20.63us       6.1261, 0.000174, 20.65us     9.2106, 0.000262, 20.93us       13.5843, 0.000386, 27.95us      24.6476, 0.000701, 64.88us\nBatch : 8, Channel : 1 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                  (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.0170, 0.002122, 74.99us       0.0316, 0.003946, 160.66us    3.0013, 0.375158, 546.94us      3.6780, 0.459753, 1544.58us     3.9197, 0.489966, 5948.43us\n(3, 3, 3)                       0.0821, 0.000380, 20.27us       0.1559, 0.000722, 22.29us     0.3133, 0.001450, 54.72us       0.5100, 0.002361, 130.12us      57.0481, 0.264111, 376.71us\n(5, 5, 5)                       0.3075, 0.000307, 20.57us       0.3680, 0.000368, 20.69us     0.6786, 0.000679, 26.61us       1.1744, 0.001174, 31.77us       2.0654, 0.002065, 128.31us\n(7, 7, 7)                       0.6512, 0.000237, 20.60us       0.8359, 0.000305, 20.50us     1.3712, 0.000500, 20.75us       1.9472, 0.000710, 20.92us       3.7586, 0.001370, 44.59us\n(9, 9, 9)                       0.9138, 0.000157, 20.43us       1.4198, 0.000243, 20.58us     2.1018, 0.000360, 20.52us       3.3691, 0.000578, 35.90us       5.9491, 0.001020, 69.16us\n(11, 11, 11)                    1.6606, 0.000156, 20.63us       2.1599, 0.000203, 20.57us     3.1240, 0.000293, 20.98us       4.8874, 0.000459, 24.65us       8.4780, 0.000796, 56.47us\n(13, 13, 13)                    2.4987, 0.000142, 20.71us       3.0667, 0.000174, 20.45us     4.6387, 0.000264, 20.76us       6.8187, 0.000388, 25.95us       12.2077, 0.000695, 50.46us\nBatch : 8, Channel : 16 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                  (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.2635, 0.002059, 75.66us       0.4030, 0.003149, 161.78us    48.0296, 0.375231, 550.46us     58.7787, 0.459209, 1902.41us    62.6966, 0.489817, 7817.48us\n(3, 3, 3)                       1.2271, 0.000355, 20.72us       2.4185, 0.000700, 26.44us     4.6933, 0.001358, 64.66us       7.7016, 0.002228, 192.69us      912.0736, 0.263910, 593.69us\n(5, 5, 5)                       4.8716, 0.000304, 24.75us       5.8624, 0.000366, 21.39us     11.0705, 0.000692, 66.94us      18.9280, 0.001183, 104.93us     34.0512, 0.002128, 441.81us\n(7, 7, 7)                       10.1713, 0.000232, 20.98us      13.2273, 0.000301, 36.26us    21.5426, 0.000491, 52.18us      30.1910, 0.000688, 72.94us      59.8381, 0.001363, 191.52us\n(9, 9, 9)                       14.4542, 0.000155, 23.85us      22.6579, 0.000243, 30.59us    33.8839, 0.000363, 57.40us      54.3563, 0.000583, 142.53us     95.8123, 0.001027, 309.24us\n(11, 11, 11)                    26.3348, 0.000155, 30.07us      34.3043, 0.000201, 37.01us    49.8093, 0.000292, 74.04us      78.3720, 0.000460, 110.53us     136.5404, 0.000801, 264.14us\n(13, 13, 13)                    39.3550, 0.000140, 37.38us      49.3207, 0.000175, 43.51us    74.1139, 0.000264, 83.70us      108.7627, 0.000387, 136.09us    196.5412, 0.000699, 280.16us\nBatch : 8, Channel : 54 / (diff, diff / numel, time)\nk \\ shape                       (12, 12, 12)                    (16, 16, 16)                  (16, 32, 32)                    (16, 56, 56)                    (16, 112, 112)\n(1, 1, 1)                       0.8467, 0.001960, 147.36us      1.3993, 0.003239, 314.95us    162.0182, 0.375042, 1327.22us   198.3226, 0.459080, 3921.79us   211.6123, 0.489843, 15646.94us\n(3, 3, 3)                       4.3146, 0.000370, 29.23us       8.1125, 0.000696, 74.94us     15.8886, 0.001362, 223.69us     26.2404, 0.002250, 601.33us     3076.5354, 0.263763, 1974.06us\n(5, 5, 5)                       16.5032, 0.000306, 58.79us      19.6887, 0.000365, 53.79us    37.2731, 0.000690, 192.34us     63.3076, 0.001172, 270.01us     114.8880, 0.002128, 1148.56us\n(7, 7, 7)                       34.0802, 0.000230, 51.12us      44.4087, 0.000300, 100.93us   72.4613, 0.000489, 161.48us     101.9317, 0.000688, 202.91us    201.8955, 0.001363, 545.33us\n(9, 9, 9)                       48.8179, 0.000155, 65.78us      76.3465, 0.000242, 87.48us    114.0228, 0.000362, 179.11us    182.9805, 0.000581, 403.66us    322.7040, 0.001025, 894.86us\n(11, 11, 11)                    88.9993, 0.000155, 88.69us      116.4213, 0.000202, 107.55us  168.3363, 0.000293, 228.71us    264.2232, 0.000460, 322.84us    459.1324, 0.000799, 784.25us\n(13, 13, 13)                    132.7447, 0.000140, 112.91us    165.4525, 0.000174, 131.08us  249.7127, 0.000263, 266.43us    367.0824, 0.000387, 410.17us    663.1367, 0.000699, 847.87us\nFloat\nhalf\nDiscepancies 198.37625122070312 0.4592042852331091 432\nSum :  tensor(25110.1484, device='cuda:0') tensor(25104., device='cuda:0', dtype=torch.float16)\n```\n</p>\n</details>\n\nngimel malfet anjali411\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53607\n\nReviewed By: mruberry\n\nDifferential Revision: D27652337\n\nPulled By: ngimel\n\nfbshipit-source-id: 6439c0cafe6ca3f761a3f5d058050a55e9a0abd8", "pr_number": "53607", "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling3d.cpp", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/csrc/jit/runtime/symbolic_script.cpp"], "labels": ["Merged", "cla signed", "module: cuda", "open source", "triaged"]}, "2ca45cb9e8": {"title": "[hackathon] ci: Only generate cuda tests for cuda configurations (#55522)", "body": "Summary:\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55522\n\nReviewed By: walterddr\n\nDifferential Revision: D27634951\n\nPulled By: seemethere\n\nfbshipit-source-id: 1dccaeb4bc8d0d53d61e467ba676c5c538fd4cf2", "pr_number": "55522", "files_changed": [".jenkins/pytorch/test.sh", "torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed", "hackathon", "module: ci"]}, "c998f3573c": {"title": "[Hackathon]Move tests related to containers in typing to test_typing.py (#55504)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55504\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar, pbelevich\n\nDifferential Revision: D27666760\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: c1a7904f33855efa4f60f8f54c029a95a5fd529c", "pr_number": "55504", "files_changed": ["test/jit/test_typing.py", "test/test_jit.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "90f848572c": {"title": "NNC depthwise conv2d implementation (#54920)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54920\n\nAdd a depthwise convolution implementation and reasonably good\nschedules for 3x3 stride=1,2.\nghstack-source-id: 126076113\n\nTest Plan: new tensorexpr test: Conv.DepthwiseConv2D\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27413745\n\nfbshipit-source-id: 833da6072b655fbe2b679704e9d56a08e1bf7e7e", "pr_number": "54920", "files_changed": ["setup.py", "test/cpp/tensorexpr/test_conv.cpp", "tools/build_variables.bzl", "torch/csrc/jit/tensorexpr/expr.cpp", "torch/csrc/jit/tensorexpr/expr.h", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/operators/conv2d.cpp", "torch/csrc/jit/tensorexpr/operators/conv2d.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "42486963b2": {"title": "Integrate NNC conv2d with fuser (#55213)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55213\n\nAdds the integration of conv2d with the TE fuser.  A few things of interest:\n\n- I'm *super* selective of what convs get lowered.  Only 3x3 depthwise, because\n  I've benchmarked those to death and I'm pretty sure it's a good change.\n\n- I'm allowing single-node \"fusion\" groups for supported convs.  (Maybe this is\n  a sign that conv2d codegen should go through a different path entirely, but\n  it seems to basically work).\n\nI'll shared full benchmarkr results once I clean them up a little.  To\nsummarize, I tested the following torchvision models containing depthwise\nconvolutions.  Results are single-core on a skylake-avx512:\n\nmobilenet_v2: 8% improvement\nmobilenet_v3: 9% improvement\nmnasnet: 10% improvement\nshufflenet: 18% improvement\n\nNote these are comparing against a baseline with a fast-but-buggy grouped\nconvolution implementation in MKLDNN.  So perf results will be better if\ncompared on master, but I'm going to assume the MKLDNN bug will be fixed and\nre-enabled.\n\nPerf results are more complicated when comparing to freezing plus conversion to\nmkldnn layout; mobilenet v2/v3 are still faster, but mnasnet and shufflenet are\nnot.  Landing this doesn't prevent MKLDNN freezing from kicking in though, so\nthere's no harm (although landing mkldnn freezing will regress mobilenet, but\ncest la vie).\nghstack-source-id: 126076112\n\nTest Plan: New unit test, plus torchvision\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27530272\n\nfbshipit-source-id: 92153fad234bc9f1eaa4f7624c543168d1294a87", "pr_number": "55213", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c7312f5271": {"title": "Enabled xla device in CI. (#55658)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55658\n\nFix #55522.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27671867\n\nPulled By: ailzhang\n\nfbshipit-source-id: af8cc5bfe540af6d33d839bf2f2f254290c95da2", "pr_number": "55658", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["Merged", "cla signed"]}, "55d45458bd": {"title": "[cuDNN] Enable Conv3d channels_last_3d (#48430)", "body": "Summary:\nThis PR adds the functionality to use channals_last_3d, aka, NDHWC, in Conv3d. It's only enabled when cuDNN version is greater than or equal to 8.0.5.\n\nTodo:\n\n- [x] add memory_format test\n- [x]  add random shapes functionality test\n\nClose https://github.com/pytorch/pytorch/pull/52547\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48430\n\nReviewed By: mrshenli\n\nDifferential Revision: D27641452\n\nPulled By: ezyang\n\nfbshipit-source-id: 0e98957cf30c50c3390903d307dd43bdafd28880", "pr_number": "48430", "files_changed": ["aten/src/ATen/cudnn/Descriptors.cpp", "aten/src/ATen/cudnn/Descriptors.h", "aten/src/ATen/native/ConvUtils.h", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cudnn/ConvShared.cpp", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "test/test_nn.py", "torch/nn/modules/module.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2a24a2418a": {"title": "common_utils.py use new file names for disabled/slow tests (#55620)", "body": "Summary:\nFollowing these changes in renaming the files:\nhttps://github.com/pytorch/pytorch/pull/55618\nhttps://github.com/pytorch/test-infra/pull/3\n\nWe should update the use sites in common_utils.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55620\n\nReviewed By: samestep\n\nDifferential Revision: D27651884\n\nPulled By: janeyx99\n\nfbshipit-source-id: 298a981e55e0b7c95202294d9bc4b3fcce359590", "pr_number": "55620", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "35a66db774": {"title": "Fix complex mean and reduction tests not being run (#55640)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55640\n\nMean is broken for complex types, since #53218 it's now allocating the result\nas a real tensor which discards the imaginary component. This wasn't picked up\nin testing because `_test_dim_ops` tests are defined as closures inside of\n`_test_dim_ops` instead of as methods on the test class. The result is, they\nnever get run.\n\nFor best results, view diff with \"Hide whitespace changes\".\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27671127\n\nPulled By: mruberry\n\nfbshipit-source-id: 4a1f6fea1048919fda7339c867ee78e88f2d7bd2", "pr_number": "55640", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_reductions.py"], "labels": ["Merged", "cla signed", "open source"]}, "defc649eca": {"title": "Update to short forms of splitWithTail / splitWithMask (#55542)", "body": "Summary:\nSwitched to short forms of `splitWithTail` / `splitWithMask` for all tests in `test/cpp/tensorexpr/test_*.cpp` (except test_loopnest.cpp)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55542\n\nReviewed By: mrshenli\n\nDifferential Revision: D27632033\n\nPulled By: jbschlosser\n\nfbshipit-source-id: dc2ba134f99bff8951ae61e564cd1daea92c41df", "pr_number": "55542", "files_changed": ["test/cpp/tensorexpr/test_memdependency.cpp", "test/cpp/tensorexpr/test_reductions.cpp"], "labels": ["Merged", "cla signed", "hackathon"]}, "159e1100bf": {"title": "[fix][tests] fix logic if env variables not present (#55664)", "body": "Summary:\nFixes: https://github.com/pytorch/pytorch/issues/55670\nReference: https://github.com/pytorch/pytorch/pull/55522\n\n**Cant Run tests locally without setting the ENV variables**\n\n<details>\n\n```\n(pytorch-cuda-dev) kshiteej@qgpu1:~/Pytorch/pytorch_opinfo$ pytest test/test_ops.py\n======================================================================= test session starts ========================================================================\nplatform linux -- Python 3.8.6, pytest-6.1.2, py-1.9.0, pluggy-0.13.1\nrootdir: /home/kshiteej/Pytorch/pytorch_opinfo, configfile: pytest.ini\nplugins: hypothesis-5.38.1\ncollected 0 items\n\n========================================================================= warnings summary =========================================================================\n../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:73\n  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:73: UserWarning: PyTorch was compiled without cuDNN/MIOpen support. To use cuDNN/MIOpen, rebuild PyTorch making sure the library is visible to the build system.\n    warnings.warn(\n\n../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_nn.py:1195\n  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/testing/_internal/common_nn.py:1195: UserWarning: Legacy tensor constructor is deprecated. Use: torch.tensor(...) for creating tensors from tensor-like objects; or torch.empty(...) for creating an uninitialized tensor with specific sizes. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:474.)\n    random_samples = torch.DoubleTensor(1, 3, 2).uniform_()\n\n-- Docs: https://docs.pytest.org/en/stable/warnings.html\n======================================================================= 2 warnings in 2.85s ========================================================================\n```\n\n</details>\n\nhttps://github.com/pytorch/pytorch/blob/c7312f5271b9ce9ac988fffde90818354e5841b8/torch/testing/_internal/common_device_type.py#L479-L486\n\n(When running locally where the environment variable is not set)\n\nThe case when the env variable is not present, `os.getenv` returns `''` which is split and we get `['']` for `only_for` and `except_for`.\n\nhttps://github.com/pytorch/pytorch/blob/c7312f5271b9ce9ac988fffde90818354e5841b8/torch/testing/_internal/common_device_type.py#L496-L497\n\nAt this point, we take the branch and skip all the tests.\n```python\n>>> if [''] and 'cuda' not in ['']:\n...     print(\"TRUE\")\n...\nTRUE\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55664\n\nReviewed By: albanD\n\nDifferential Revision: D27677752\n\nPulled By: malfet\n\nfbshipit-source-id: 071486e3b6b5113c56f0f956b8d99a5ab24068fe", "pr_number": "55664", "files_changed": ["torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed", "open source"]}, "076961e8b5": {"title": "Add tuple add operator (#52292)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/52292\n\nTest Plan: Imported from OSS\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D26792416\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 882325b171c1ff53ec40243d3f9334049c03fe57", "pr_number": "52292", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/ir_emitter.cpp"], "labels": ["Merged", "cla signed"]}, "c0379ac83f": {"title": "Simplify device guard code generation (#55112)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55112\n\nBased on https://github.com/pytorch/pytorch/pull/47765\nghstack-source-id: 126114775\n\nTest Plan: buck build //caffe2/aten/...\n\nReviewed By: ezyang\n\nDifferential Revision: D27487085\n\nfbshipit-source-id: 157fcd19f538ce0c1e053e3e974b48bdb93a0226", "pr_number": "55112", "files_changed": ["tools/codegen/dest/register_dispatch_key.py"], "labels": ["Merged", "cla signed"]}, "11dd6d3dbb": {"title": "Mycontrib Added Example for is_tensor API (#55052)", "body": "Summary:\n[Added  Example for is_tensor API](https://github.com/harishsdev/practice/blob/master/changes_to_opensource/is_tensor_example_added.jpg)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55052\n\nReviewed By: ezyang\n\nDifferential Revision: D27523833\n\nPulled By: gchanan\n\nfbshipit-source-id: 06036342223454856d4cfec46b40a72b311d261f", "pr_number": "55052", "files_changed": ["torch/__init__.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "f3367f917e": {"title": "Translate annotation line numbers from merge to head (#55569)", "body": "Summary:\nThis PR\n\n- adds a `tools/translate_annotations.py` script that\n  - parses annotations into JSON using the regexes that we were previously passing to [`pytorch/add-annotations-github-action`](https://github.com/pytorch/add-annotations-github-action) and\n  - uses `git diff-index` to translate the line numbers for those annotations from the PR `merge` onto the PR `head`, since (as of https://github.com/pytorch/pytorch/issues/54967) we now run CI on the former instead of the latter;\n- modifies the `flake8-py3` and `clang-tidy` jobs to use that script and thus upload JSON in their artifacts instead of raw text; and\n- modifies the \"Add annotations\" workflow to specify `mode: json` to allow it to use those preprocessed annotations.\n\nDepends on https://github.com/pytorch/add-annotations-github-action/pull/18.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55569\n\nTest Plan:\nYou can run the unit tests with this command:\n```\npython tools/test/test_translate_annotations.py\n```\nI also tested the entire system together in my personal sandbox repo.\n\nReviewed By: malfet\n\nDifferential Revision: D27662161\n\nPulled By: samestep\n\nfbshipit-source-id: ecca51b79b9cf00c90fd89f0d41d0c7b89d69c63", "pr_number": "55569", "files_changed": [".github/workflows/add_annotations.yml", ".github/workflows/lint.yml", "mypy-strict.ini", "tools/README.md", "tools/test/test_translate_annotations.py", "tools/translate_annotations.py"], "labels": ["Merged", "cla signed"]}, "53f9fc1802": {"title": "Port hypot method_tests() to OpInfo (#55140)", "body": "Summary:\nRelated https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55140\n\nReviewed By: ngimel\n\nDifferential Revision: D27562164\n\nPulled By: mruberry\n\nfbshipit-source-id: fc698ddc624d2abf5d540aac76baa5d398993f1f", "pr_number": "55140", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "fc1d7a85bb": {"title": "Added an OpInfo for mm & ported its method_tests (#55446)", "body": "Summary:\nAdded an `OpInfo` for `mm` & ported its `method_tests` entry (it only had one).\n\ncc: mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55446\n\nReviewed By: ngimel\n\nDifferential Revision: D27670883\n\nPulled By: mruberry\n\nfbshipit-source-id: 51232f44ab01ad0454113992f80a4cfc730f8800", "pr_number": "55446", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed", "module: tests", "open source", "triaged"]}, "6ee333cdb5": {"title": "modernize test_sparse (#54572)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54572\n\nAdding device generic tests to `test_sparse`.\nFollow-up PR: #54153\n\nI think is ready to review.\nLooking forward your comments cc mruberry.\n\nThanks\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27562663\n\nPulled By: mruberry\n\nfbshipit-source-id: c48973e707f779b529bc7f61b75103194b428987", "pr_number": "54572", "files_changed": ["test/expect/TestSparseCPU.test_print_coalesced_cpu_float64.expect", "test/expect/TestSparseCPU.test_print_uncoalesced_cpu_float64.expect", "test/expect/TestSparseCUDA.test_print_coalesced_cuda_float64.expect", "test/expect/TestSparseCUDA.test_print_uncoalesced_cuda_float64.expect", "test/test_autograd.py", "test/test_sparse.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "open source"]}, "91ab0d9680": {"title": "[hackathon] port addmv to OpInfo (#55545)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55545\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27629053\n\nPulled By: Lilyjjo\n\nfbshipit-source-id: d7a114e21d3b90c2563a26d7103703988114353d", "pr_number": "55545", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "6842da6251": {"title": "[WIP]Relax some limitations of InferenceMode. (#54403)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54403\n\nA few important points about InferenceMode behavior:\n1. All tensors created in InferenceMode are inference tensors except for view ops.\n   - view ops produce output has the same is_inference_tensor property as their input.\n     Namely view of normal tensor inside InferenceMode produce a normal tensor, which is\n     exactly the same as creating a view inside NoGradMode. And view of\n     inference tensor outside InferenceMode produce inference tensor as output.\n2. All ops are allowed inside InferenceMode, faster than normal mode.\n3. Inference tensor cannot be saved for backward.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27316483\n\nPulled By: ailzhang\n\nfbshipit-source-id: e03248a66d42e2d43cfe7ccb61e49cc4afb2923b", "pr_number": "54403", "files_changed": ["aten/src/ATen/native/ComplexHelper.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/quantized/QTensorImpl.cpp", "aten/src/ATen/quantized/QTensorImpl.h", "c10/core/InferenceMode.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "test/cpp/api/grad_mode.cpp", "test/cpp/api/inference_mode.cpp", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_type.py", "tools/codegen/model.py", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/functions/utils.h", "torch/csrc/autograd/saved_variable.cpp"], "labels": ["Merged", "cla signed"]}, "9f519d2d2d": {"title": "Simplify benchmark patterns in mypy-strict.ini (#55700)", "body": "Summary:\nThese two lines were added in https://github.com/pytorch/pytorch/issues/53296, but they are needlessly complicated; this PR consolidates them.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55700\n\nTest Plan:\nRun this command, and verify that the same number of files is given both before and after this PR:\n```\nmypy --config=mypy-strict.ini\n```\n\nReviewed By: robieta\n\nDifferential Revision: D27684278\n\nPulled By: samestep\n\nfbshipit-source-id: a34968cdff29cb8ad83813b277114224b5e37569", "pr_number": "55700", "files_changed": ["mypy-strict.ini"], "labels": ["Merged", "cla signed"]}, "6e4e3a1159": {"title": "Fix annotations in _autograd.pyi (#55706)", "body": "Summary:\n`str` is reserved keyword, besides parameter name in `profiler_kineto.h` is `path`:\nhttps://github.com/pytorch/pytorch/blob/6ee333cdb53d600377f6aa9992123253f2fa5132/torch/csrc/autograd/profiler_kineto.h#L209\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55706\n\nReviewed By: janeyx99\n\nDifferential Revision: D27686286\n\nPulled By: malfet\n\nfbshipit-source-id: b27e8e3812214218054be0e69493177bb728d8d7", "pr_number": "55706", "files_changed": ["torch/_C/_autograd.pyi"], "labels": ["Merged", "cla signed"]}, "7671c15d4f": {"title": "Make VariableVersion::DISABLED the default constructor for VariableVersion. (#55572)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55572\n\nWe used to have VariableVersion default constructor\n`VariableVersion(uint32_t version=0)`. But sometimes\nwe override the version_counter right after it's constructed.\nE.g in SavedVariable/TensorImpl.\nThus we should make DISABLED  the default constructor and else\nwhere using explicit `VariableVersion(uint32_t)` constructor.\nNote this PR effectively changes SavedVariable constructor (which overrides\nversion_counter_ inside) to use the DISABLED constructor and we\ncan see the gains in reduced instruction counts.\n\n```\n// benchmark code\ntimer = Timer(\n    \"y = x * x\",\n    \"\"\"\n    x = torch.rand((3, 3)).requires_grad_()\n    \"\"\",\n    language=Language.PYTHON,\n)\n\n \u03bb ~ python compare.py\nNo CUDA runtime is found, using CUDA_HOME='/public/apps/cuda/10.2'\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts\nobject at 0x7f06c48b3a50>\n     7236  lookdict_unicode_nodummy\n     2600  torch::autograd::VariableType::(...)\n      100  0x0000000017751750\n       -5  unlink_chunk.isra.0\n     -100  0x000000001773e750\n     -402  _int_malloc\n    -1600  operator delete(...)\n    -1600  c10::intrusive_ptr_target::release_resources()\n    -2400  c10::VariableVersion::VersionCounter::~VersionCounter()\n    -3600  torch::autograd::SavedVariable::operator=(...)\n    -4800  operator new(...)\n    -6400  torch::autograd::SavedVariable::SavedVariable(...)\n    -7200  torch::autograd::SavedVariable::SavedVariable()\n    -8400  free\n   -16800  malloc\n   -24400  _int_free\n\nTotal: -67771\n```\nNote there're for other callsites(esp. view related) we just keep it unchanged by\nexplicitly calling `VariableVersion(uint32_t)` but we should be\nable to optimize those in the followup PRs.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D27669074\n\nPulled By: ailzhang\n\nfbshipit-source-id: a4deb297cc89142ae8bd683284516c881ddf3c87", "pr_number": "55572", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "846c8d94c7": {"title": "mark embedding backward non-deterministic for max mode rather than all reducing modes (#55574)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55574\n\nnn.EmbeddingBag backward is non-deterministic when reducing_mode = Max and on GPU, reducing mode Mean and Sum should be deterministic\n\nTest Plan: NA\n\nReviewed By: ngimel\n\nDifferential Revision: D27633832\n\nfbshipit-source-id: 50786ed8522f1aae27442f5f244a65eab8000b06", "pr_number": "55574", "files_changed": ["aten/src/ATen/native/cuda/EmbeddingBag.cu", "torch/testing/_internal/common_nn.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "717d54bc2b": {"title": "[Hackathon] Add source highlighting check to test_unsupported_ops (#55501)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55501\n\nReviewed By: janeyx99\n\nDifferential Revision: D27627517\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 2542473425d10f1e3eb926f9e0fb6cd40679bd82", "pr_number": "55501", "files_changed": ["test/jit/test_unsupported_ops.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "3e9cbe5ef7": {"title": "[SPMD] Remove the code branches only used in SPMD mode from distributed.py (#55353)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55353\n\nRemove all the code branches that will only be executed when `device_ids > 1`.\n\nSome helper functions are also removed:\n1.  `_verify_replicas_within_process` and `verify_replicas_within_process`\n2. `_replicate_modules_within_process`\n3. `parallel_apply`\n\nThe next step is deprecating `_module_copies` field.\nghstack-source-id: 126201121\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27552201\n\nfbshipit-source-id: 128d0216a202f5b1ba4279517d68c3badba92a6c", "pr_number": "55353", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/__init__.py", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "fc349cbcde": {"title": "OpInfo for kron (#55546)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55546\n\nTest Plan: pytest test/test_ops.py -v -k \"_kron_\"\n\nReviewed By: albanD\n\nDifferential Revision: D27681131\n\nPulled By: asuhan\n\nfbshipit-source-id: e480d8f163d73b9ca5353b2320ccb0631a5f06c5", "pr_number": "55546", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "d33829f844": {"title": "Fix type annotations for state_dict() override (#55704)", "body": "Summary:\nChange annotation to OrderedDict, but stringify it to stay compatible with Python-3.6\n\nFixes https://github.com/pytorch/pytorch/issues/55302\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55704\n\nReviewed By: walterddr\n\nDifferential Revision: D27686011\n\nPulled By: malfet\n\nfbshipit-source-id: 3a8dedf33f38d86767ebd4e8a1a8abfe850b375a", "pr_number": "55704", "files_changed": ["torch/nn/modules/module.py"], "labels": ["Merged", "cla signed"]}, "d695ba94f6": {"title": "Replace AutoNonVariableTypeMode with InferenceMode in static runtime.", "body": "Test Plan:\nhttps://www.internalfb.com/intern/aibench/details/3752129704\nhttps://www.internalfb.com/intern/aibench/details/1306815519\n\nReviewed By: hlu1\n\nDifferential Revision: D27691509\n\nfbshipit-source-id: d43db028a399bb02166a539577f6922237145f83", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp"], "labels": []}, "2496a09314": {"title": "[Gradient Compression] Fix PowerSGD docstring by removing an extra whitespace (#55666)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55666\n\n{F590513307}\n\nSome code is not properly displayed due to an extra whitespace ahead of `(num_rows + num_cols)`.\nghstack-source-id: 126148569\n\nTest Plan: Locally viewed\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27673663\n\nfbshipit-source-id: 603ae4ddbe86ceaefc311885b82b0f6b48b57b27", "pr_number": "55666", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "fa19b6dd4d": {"title": "[PyTorch] New expand_inplace API with MaybeOwned<Tensor> and no unary tuples (#55065)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55065\n\nexpand_inplace may give you the same Tensor(s) back, and it unnecessarily wrapped single-Tensor results in a tuple. Further diffs will deprecate and replace the rest of the similar APIs in ExpandUtils.\nghstack-source-id: 126170049\n\nTest Plan: beyonce_test\n\nReviewed By: ezyang\n\nDifferential Revision: D27469297\n\nfbshipit-source-id: 56cf14bc5603355f399fef2e5b02b97afa504428", "pr_number": "55065", "files_changed": ["aten/src/ATen/ExpandUtils.h", "aten/src/ATen/TensorIndexing.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cpu/DistributionTemplates.h", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "torch/csrc/utils/tensor_apply.cpp"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "e8dd65102b": {"title": "[PyTorch] Use infer_size_dimvector in ExpandUtils (#55180)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55180\n\nEven if we're expanding a Tensor's dimensions, DimVector's size is still a good guess at the rank of a Tensor in general. None of these sites actually seem to need a std::vector.\nghstack-source-id: 126170045\n\nTest Plan: Existing CI\n\nReviewed By: ezyang\n\nDifferential Revision: D27520127\n\nfbshipit-source-id: 4064764fad1b3782b379f04627b48331c3ee011f", "pr_number": "55180", "files_changed": ["aten/src/ATen/ExpandUtils.h"], "labels": ["Merged", "cla signed"]}, "6fd875923e": {"title": "[PyTorch] Add MaybeOwned::operator*() && (#55244)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55244\n\nAdd the ability to move from the underlying object in a `MaybeOwned`.\n\nFWIW, `MaybeOwned` is new territory for me personally and this move-and-dereference operation is even more so, but I think it makes sense and the tests pass.\nghstack-source-id: 126170046\n\nTest Plan: Added automated tests.\n\nReviewed By: bhosmer\n\nDifferential Revision: D27522809\n\nfbshipit-source-id: 82b180031e93d725209b6328f656315c232e5237", "pr_number": "55244", "files_changed": ["c10/test/util/MaybeOwned_test.cpp", "c10/util/MaybeOwned.h"], "labels": ["Merged", "cla signed"]}, "16a9141e2c": {"title": "[PyTorch] Update expand_outplace API to match expand_inplace (#55245)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55245\n\nLike `expand_inplace`, `expand_outplace` now returns\n`MaybeOwned<Tensor>` in most cases. I wasn't confident around the\nownership semantics of the `TensorList` -> `std::vector<Tensor>` case, so I\nleft that one alone.\nghstack-source-id: 126170052\n\nTest Plan: Existing CI.\n\nReviewed By: ezyang\n\nDifferential Revision: D27522811\n\nfbshipit-source-id: 28c5a626b65681e361f4006a0aaa7dc23ba9612a", "pr_number": "55245", "files_changed": ["aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Lerp.cu"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "12c19c398c": {"title": "[PyTorch] Update expand_size API to match expand_inplace (#55246)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55246\n\nc10::MaybeOwned<Tensor> and no more unary tuples.\nghstack-source-id: 126170051\n\nTest Plan: Existing CI\n\nReviewed By: ngimel\n\nDifferential Revision: D27523682\n\nfbshipit-source-id: 2590993cfc62136e65fd9a791e4ab68b2c366556", "pr_number": "55246", "files_changed": ["aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "151869aca6": {"title": "[PyTorch][easy] Use sizes()[x] instead of size(x) in addr (#55247)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55247\n\nWhen x is known to be in-bounds, sizes() is faster.\nghstack-source-id: 126170048\n\nTest Plan: CI\n\nReviewed By: hlu1\n\nDifferential Revision: D27523681\n\nfbshipit-source-id: 021c82a8a6b770802f4cd51cf6ff77046d71c938", "pr_number": "55247", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp"], "labels": ["Merged", "cla signed"]}, "548765d9a5": {"title": "[PyTorch] Add & use inferExpandGeometry_dimvector (#55316)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55316\n\nNo need for heap allocations in the common case here.\nghstack-source-id: 126170054\n\nTest Plan: Existing CI\n\nReviewed By: hlu1\n\nDifferential Revision: D27571942\n\nfbshipit-source-id: 11fbf077c583c80ea63e024d2b9e1599785fff71", "pr_number": "55316", "files_changed": ["aten/src/ATen/ExpandUtils.cpp", "aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/TensorShape.cpp", "torch/csrc/jit/passes/shape_analysis.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c9b94a85e9": {"title": "change torch.testing helper asserts to checks (#54780)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54780\n\n- In #53152 we opted to use `tb=native`. Thus, regardless if we use `pytest` to run the tests `__tracebackhide__` is not honored. and additional layers of helper functions make the traceback harder to parse. To overcome this, we change the internal helpers to return `ok: bool, msg: Optional[str]` and only raise the error in the top level function. We do that already in the current implementation that we are trying to replace:\n    https://github.com/pytorch/pytorch/blob/36ce673f16269724f629f2a1897b8720ba6c8f18/torch/testing/__init__.py#L92-L93\n    https://github.com/pytorch/pytorch/blob/36ce673f16269724f629f2a1897b8720ba6c8f18/torch/testing/__init__.py#L112\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27438849\n\nPulled By: mruberry\n\nfbshipit-source-id: 3e7a33dabb45463c29e8b9736fad09efb523f18d", "pr_number": "54780", "files_changed": ["torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "255494c2aa": {"title": "torch.testing allclose -> close (#54781)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54781\n\nRight now the functions have divergent names with one postfixed `_equal` and the other `_allclose`. I've opted to use `_(equal|close)` over `_all(equal|close)` think it is a reasonable assumption that all values need to be equal or close for this pass even without explicitly naming the function this way.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27438957\n\nPulled By: mruberry\n\nfbshipit-source-id: 2951dac06d1430e15119ae94eafa234f3eb02f09", "pr_number": "54781", "files_changed": ["torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "84a7ab250b": {"title": "Optimize constructing tensors from external data (#55705)", "body": "Summary:\nThis PR optimizes the way tensors are constructed from external data. It avoids allocating an empty tensor beforehand and directly constructs the target tensor by passing the newly-initialized `DataPtr`. Running some Facebook-internal benchmarks showed that combined with https://github.com/pytorch/pytorch/issues/54530 this PR achieves performance parity with Caffe2 tensor construction. (Overall ~2x speed improvement over the original `at::from_blob()` implementation.)\n\nTesting is done with the existing unit and integration tests as there is no user-observable API change.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55705\n\nReviewed By: ezyang\n\nDifferential Revision: D27686043\n\nPulled By: cbalioglu\n\nfbshipit-source-id: b365c614476bcf0567797dfaf2add1b76fb6c272", "pr_number": "55705", "files_changed": ["aten/src/ATen/templates/Functions.cpp", "aten/src/ATen/templates/Functions.h"], "labels": ["Merged", "cla signed"]}, "facbcec298": {"title": "Make leak_corrupted_threadpool non-atomic (#55341)", "body": "Summary:\nFollowing up on https://github.com/pytorch/pytorch/pull/54895#discussion_r606402656.\n\nA race-condition wouldn't arise because `leak_corrupted_threadpool` can be set to true only after fork via the `pthread_atfork` handler, when a (child) process would be single-threaded. It's set to false also when the process is still single-threaded (`pthreadpool` is called during an invocation to `set_num_threads`, prior to which a child process would remain single-threaded). All threads (if & when multiple threads would be created) would always see `leak_corrupted_threadpool` as false if it would be accessed concurrently.\n\nSince no reader threads can exist while a writer thread changes its value (false->true and true->false), `leak_corrupted_threadpool` might as well be a non-atomic bool.\n\n### Pros\n1. No thread-synchronization is required for `leak_corrupted_threadpool`, as it's a non-atomic bool.\n2. The call to `compare_exchange_strong` has been be removed.\n\ncc: malfet VitalyFedyunin ezyang\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55341\n\nReviewed By: albanD\n\nDifferential Revision: D27669442\n\nPulled By: ezyang\n\nfbshipit-source-id: 926cb5c1b0a537c1c2ab164b0d51d37c1f1b67f0", "pr_number": "55341", "files_changed": ["caffe2/utils/threadpool/pthreadpool-cpp.cc"], "labels": ["Merged", "cla signed", "open source"]}, "b80c6f863f": {"title": "Disambiguate error message for working with not fully refined tuple types (#55745)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55745\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27698691\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 7855042d37290f19d53adfc0b4da606430501663", "pr_number": "55745", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/sugared_value.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "93bf0ae6fc": {"title": "Remove legacy constructor calls from pytorch codebase. (#54142)", "body": "Summary:\nFollow up from https://github.com/pytorch/pytorch/issues/53889\nRelated to https://github.com/pytorch/pytorch/issues/47112\n\nRemoving every occurrence of the legacy constructor call present in PyTorch at:\n- _docs_\n- _benchmarks_\n- _test_\n- _caffe2_\n- _CONTRIBUTING.md_\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54142\n\nReviewed By: ngimel\n\nDifferential Revision: D27699450\n\nPulled By: mruberry\n\nfbshipit-source-id: 530aa3f5746cc8bc1407d5d51b2bbd8075e30546", "pr_number": "54142", "files_changed": ["CONTRIBUTING.md", "benchmarks/overrides_benchmark/bench.py", "benchmarks/overrides_benchmark/common.py", "benchmarks/overrides_benchmark/pyspybench.py", "caffe2/python/operator_test/heatmap_max_keypoint_op_test.py", "caffe2/python/operator_test/layer_norm_op_test.py", "caffe2/python/operator_test/torch_integration_test.py", "docs/source/jit_language_reference_v2.rst", "docs/source/notes/extending.rst", "docs/source/tensor_attributes.rst", "test/benchmark_utils/test_benchmark_utils.py", "test/cpp/api/optim_baseline.py", "test/jit/test_isinstance.py", "test/jit/test_tracer.py", "test/onnx/test_models.py", "test/onnx/test_pytorch_onnx_caffe2.py", "test/quantization/test_numeric_suite_fx.py", "test/quantization/test_qat_module.py", "test/quantization/test_workflow_module.py", "test/test_autograd.py", "test/test_cuda.py", "test/test_dataloader.py", "test/test_fx.py", "test/test_fx_experimental.py", "test/test_indexing.py", "test/test_jit.py", "test/test_linalg.py", "test/test_metal.py", "test/test_mobile_optimizer.py", "test/test_multiprocessing.py", "test/test_nn.py", "test/test_numpy_interop.py", "test/test_optim.py", "test/test_shape_ops.py", "test/test_tensor_creation_ops.py", "test/test_tensorboard.py", "test/test_torch.py", "test/test_view_ops.py", "test/test_vulkan.py", "test/test_xnnpack_integration.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "a3c062d4f5": {"title": "docs: improve torch.matrix_exp() (#55626)", "body": "Summary:\nAdd a signature and make the mathematical expression related to the signature\n\nFixes https://github.com/pytorch/pytorch/issues/55599\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55626\n\nReviewed By: ngimel\n\nDifferential Revision: D27699518\n\nPulled By: mruberry\n\nfbshipit-source-id: e61d76e99eb8fc36114c1c2ee90990740d78beea", "pr_number": "55626", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "open source"]}, "19f15317a0": {"title": "[BE][Docs] Improve dist.new_group doc (#55660)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55660\n\nNoticed this doc was missing clarification on nccl env vars that\ninit_process_group docs have. Also, specify default behavior when backend=None\nis passed in.\nghstack-source-id: 126251116\n\nTest Plan: Ci\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27672208\n\nfbshipit-source-id: 2e79d297174e135173bceb059450ea267367bde4", "pr_number": "55660", "files_changed": ["torch/distributed/distributed_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "66289673f7": {"title": "patching requires_grad on DifferentiableGraph (#55701)", "body": "Summary:\nThe retrieval of profile node is much easier prior to inserting guard node.\ntest cases updated to reflect the patch on a previously failing cases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55701\n\nReviewed By: pbelevich\n\nDifferential Revision: D27701216\n\nPulled By: Krovatkin\n\nfbshipit-source-id: e2e6b64b682377e622b75c762e85ff7967e45118", "pr_number": "55701", "files_changed": ["test/jit/test_autodiff_subgraph_slicing.py", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "399b66c813": {"title": "Ports logdet from method_tests() to op_db (#55743)", "body": "Summary:\nPer title. Also updates some tensor construction helpers.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55743\n\nReviewed By: ngimel\n\nDifferential Revision: D27702060\n\nPulled By: mruberry\n\nfbshipit-source-id: f64b7bee855733ad1f4fd182819ceec5831d9878", "pr_number": "55743", "files_changed": ["test/test_indexing.py", "test/test_linalg.py", "test/test_ops.py", "test/test_testing.py", "test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "3f8d476857": {"title": "Split out CUDA RPC tests (#55695)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55695\n\nIn order to be able to run CUDA tests on their own (e.g., to avoid running CPU tests on GPU machines).\n\nDone by moving test methods to a separate class (and sometimes introducing a \"common\" base class for utils), and then providing new entry points inside a `cuda/` subdirectory.\n\nTest Plan: Checked they are run on Sandcastle.\n\nReviewed By: mrshenli\n\nDifferential Revision: D27618198\n\nfbshipit-source-id: 8f671657f79c8ae115748ab7752fe0066705893b", "pr_number": "55695", "files_changed": [".jenkins/pytorch/multigpu-test.sh", "test/distributed/rpc/cuda/test_process_group_agent.py", "test/distributed/rpc/cuda/test_tensorpipe_agent.py", "test/run_test.py", "torch/distributed/CONTRIBUTING.md", "torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py", "torch/testing/_internal/distributed/rpc_utils.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3c6b52ae62": {"title": "Cache slow/disabled test files (#55682)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55682\n\nFixes #55648\n\nFor now it downloads and writes the relevant files to the system's temp dir and marks it as valid for 3 hours.\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet, nikithamalgifb\n\nDifferential Revision: D27685616\n\nPulled By: driazati\n\nfbshipit-source-id: 27469b85fe4b6b4addde6b22bf795bca3d4990ef", "pr_number": "55682", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "13153924cc": {"title": "OpInfo porting for msort operator (#55488)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55488\n\nReviewed By: ngimel\n\nDifferential Revision: D27708648\n\nPulled By: iramazanli\n\nfbshipit-source-id: 62b6bc5bd6e54c593b9afac56cb2511411683416", "pr_number": "55488", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "e05ca753bf": {"title": "Fix nightly tool for python 3.6 (#55776)", "body": "Summary:\nGiven that the minimal required Python version for using PyTorch is 3.6, the development tools should also be able to handle it. `./tools/nightly.py` currently uses the parameters `capture_output` and `text` of `subprocess.run` that were only added for [Python 3.7](https://docs.python.org/3/library/subprocess.html#subprocess.run).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55776\n\nReviewed By: ngimel\n\nDifferential Revision: D27709124\n\nPulled By: ezyang\n\nfbshipit-source-id: aeea15a891ba792f3cd5fa602f0d7b746007e30c", "pr_number": "55776", "files_changed": ["tools/nightly.py"], "labels": ["Merged", "cla signed", "open source"]}, "211d31afc9": {"title": "symeig supports complex backward (#55085)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53651\nI did not put much effort in improving the docs, as I will go over all these docs in future PRs\ncc anjali411\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55085\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27493604\n\nPulled By: anjali411\n\nfbshipit-source-id: 413363013e188bc869c404b2d54ce1f87eef4425", "pr_number": "55085", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_variable_type.py", "torch/_torch_docs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "complex_autograd", "module: complex", "open source"]}, "c6d9ca0c2b": {"title": "[reland]Replace AutoNonVariableTypeMode with InferenceMode in static runtime. (#55731)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55731\n\nForgot to export the diff in my last one. Retry...\n\nTest Plan:\nhttps://www.internalfb.com/intern/aibench/details/3752129704\nhttps://www.internalfb.com/intern/aibench/details/1306815519\n\nReviewed By: hlu1\n\nDifferential Revision: D27694660\n\nfbshipit-source-id: b351338fa789b9e9c7337df9b1bc1bc0fc387f5d", "pr_number": "55731", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "5fb1142702": {"title": "Add CSR (compressed sparse row) layout for sparse tensors (#50937)", "body": "Summary:\nImplement compressed sparse row format. Derived from the GCS implementation at https://github.com/pytorch/pytorch/pull/44190\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50937\n\nReviewed By: mrshenli\n\nDifferential Revision: D27439865\n\nPulled By: ezyang\n\nfbshipit-source-id: 3ba3dcb9679505b980ff6a5f513e913bbae2fb1d", "pr_number": "50937", "files_changed": ["BUILD.bazel", "aten/src/ATen/SparseCsrTensorImpl.cpp", "aten/src/ATen/SparseCsrTensorImpl.h", "aten/src/ATen/SparseCsrTensorUtils.h", "aten/src/ATen/core/DeprecatedTypeProperties.h", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/mkl/SparseCsrLinearAlgebra.cpp", "aten/src/ATen/native/mkl/SparseCsrLinearAlgebra.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/templates/TensorBody.h", "benchmarks/README.md", "benchmarks/sparse/README.md", "benchmarks/sparse/__init__.py", "benchmarks/sparse/spmm.py", "benchmarks/sparse/spmv.py", "benchmarks/sparse/test_csr.sh", "benchmarks/sparse/utils.py", "c10/core/Backend.h", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.h", "c10/core/Layout.h", "c10/core/TensorImpl.h", "c10/core/TensorOptions.h", "docs/source/name_inference.rst", "docs/source/sparse.rst", "test/test_jit.py", "test/test_sparse_csr.py", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_torch_functions.cpp", "tools/build_variables.bzl", "tools/codegen/gen.py", "tools/codegen/model.py", "tools/pyi/gen_pyi.py", "torch/_tensor.py", "torch/_tensor_docs.py", "torch/_tensor_str.py", "torch/_torch_docs.py", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/tensor/python_tensor.cpp", "torch/csrc/utils/tensor_layouts.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_new.h", "torch/overrides.py"], "labels": ["Merged", "cla signed", "module: sparse", "open source", "triaged"]}, "80d04f910c": {"title": "fix typo in argmax docstring (#55239)", "body": "Summary:\nargmax docstring previously said that it returns indexes of the first 'minimal' value, fixed typo in that line to 'maximal'\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55239\n\nReviewed By: albanD\n\nDifferential Revision: D27641562\n\nPulled By: mrshenli\n\nfbshipit-source-id: f8b5c579400088b5210c83a05da6c4c106fbf95d", "pr_number": "55239", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: docs", "open source", "triaged"]}, "684589e8e0": {"title": "[codemod][fbcode][1/n] Apply buildifier", "body": "Test Plan: Manual inspection. Sandcastle.\n\nReviewed By: karlodwyer, zsol\n\nDifferential Revision: D27702434\n\nfbshipit-source-id: ee7498331c51daf44a29f2de452e3b02488b9af3", "pr_number": null, "files_changed": ["aten.bzl", "tools/rules/cu.bzl", "tools/rules/workspace.bzl"], "labels": []}, "01441af763": {"title": "Use mypy internals instead of fnmatch for mypy wrapper (#55702)", "body": "Summary:\nI noticed that https://github.com/pytorch/pytorch/issues/53296 added these two lines to the `files` list in `mypy-strict.ini`:\n```\n    benchmarks/instruction_counts/*.py,\n    benchmarks/instruction_counts/*/*.py,\n```\nI opened https://github.com/pytorch/pytorch/issues/55700 to simplify them into one line, but I was also curious whether `tools/mypy_wrapper.py` correctly handles those patterns, so I added the `test_glob_wildcards_dont_expand_or_collapse` case shown in this PR. Turns out, it doesn't!\n\nI believe this is because [`mypy` uses `glob`](https://github.com/python/mypy/blob/v0.770/mypy/config_parser.py#L45-L63) to parse these patterns, and for some reason, [`fnmatch`](https://docs.python.org/3/library/fnmatch.html) and [`glob`](https://docs.python.org/3/library/glob.html) don't agree with each other on what `*` means:\n\n- according to `fnmatch`, `*` seems to mean `.*`\n- according to `glob`, `*` seems to mean `[^/]*`\n\n[This SO answer](https://stackoverflow.com/a/60174071) suggests using the [`glob.globmatch` function from the `wcmatch` library](https://facelessuser.github.io/wcmatch/glob/#globmatch) to solve the issue, but [we didn't want to add another external dependency](https://github.com/pytorch/pytorch/pull/55702#discussion_r610868623), so instead I simply modified our matching function to just directly call `mypy`'s own internal function that does the globbing (linked above).\n\nOne possible downside of this approach is that now the tests in `tools/test/test_mypy_wrapper.py` could break if the directory structure of PyTorch is changed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55702\n\nTest Plan:\n```\npython tools/test/test_mypy_wrapper.py\n```\n\nReviewed By: malfet, seemethere\n\nDifferential Revision: D27684499\n\nPulled By: samestep\n\nfbshipit-source-id: d99387a579c21eee73d1714e3e815ab7155f9646", "pr_number": "55702", "files_changed": ["mypy-strict.ini", "tools/mypy_wrapper.py", "tools/test/test_mypy_wrapper.py"], "labels": ["Merged", "cla signed"]}, "008ec544f4": {"title": "[p2c2][operators] Self binning histogram op error msg", "body": "Summary: Change error msg to include the min max values when failing.\n\nTest Plan:\nExisting unit tests:\n```\nbuck test //caffe2/caffe2/python/operator_test:self_binning_histogram_test\n```\nFailing wf with error msg:\nf264505545\n\nReviewed By: TailofJune\n\nDifferential Revision: D27630820\n\nfbshipit-source-id: c490ce8c8c40414403634979c9beaf9c08569a96", "pr_number": null, "files_changed": ["caffe2/operators/self_binning_histogram_op.h"], "labels": []}, "f7a51b2ab9": {"title": "Don't set version_counter on inference tensor for unsafe_ ops. (#55819)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55819\n\nTest Plan:\nOn devserver: `buck run //xplat/langtech/tuna/cli:tuclix -- --model-dir ~/workspace/portal_en_US/ --audio-file ~/fbsource/fbcode/shortwave/test/data/audio_unittest.wav.to.raw` on top of Rittzz's D27691649\nOn device:\n\nReviewed By: Rittzz\n\nDifferential Revision: D27716745\n\nfbshipit-source-id: 1921f18ee6b06990f71b86b9c4b3e1f3ce531001", "pr_number": "55819", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "c00b9dc599": {"title": "Small typo in comment (#55485)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55485\n\nReviewed By: albanD\n\nDifferential Revision: D27641537\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1dc0d2d77c47a66dcf10866801a1e0f495422149", "pr_number": "55485", "files_changed": ["aten/src/ATen/core/ivalue.h"], "labels": ["Merged", "cla signed", "open source"]}, "af1a772876": {"title": "Disable overloading of std::max & std::min for inputs of distinct types (#55638)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55613\n\n### Problem\nBy default, `std::max` and `std::min` only operate on inputs of the same type.\nIn [`c10/util/BFloat16-math.h`](https://github.com/pytorch/pytorch/blob/master/c10/util/BFloat16-math.h), `std::max` & `std::min` have been overloaded:\nhttps://github.com/pytorch/pytorch/blob/305abde976d232494c6b5252111ce15523511ffe/c10/util/BFloat16-math.h#L32-L33\nezyang [observed](https://github.com/pytorch/pytorch/pull/55586#issuecomment-815862373) &  [illustrated](https://godbolt.org/z/bjTjPMMco) that calls to `std::max` & `std::min` for distinct input types (eg. `std::max(int, float)`) are being handled via `BFloat16`'s aforementioned overloads via implicit conversion to `BFloat16`. (I haven't looked into why yet).\n\n### Solution implemented\n1. Disabled overloading of `std::max` & `std::min` for inputs of distinct types by removing these overloads for `BFloat16`.\n2. Instead, `<` and `>` operators are now being overloaded for `BFloat16` now (for comparison with another `BFloat16`), since `std::max` and `std::min` use these operators.\n3. Calls to `std::max` and `std::min` with inputs of distinct types are only present at 3 places in the codebase, where they can either be handled by a `static_cast`, or by changing the type:\na. [`aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp#L111)\nb. [`aten/src/ATen/native/cpu/BinaryOpsKernel.cpp`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp#L74)\nc. [`aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp#L2998-L2999)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55638\n\nReviewed By: albanD\n\nDifferential Revision: D27669702\n\nPulled By: ezyang\n\nfbshipit-source-id: 790a67b76f86c25fad2c7ed0345b7f35ab5eca68", "pr_number": "55638", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "c10/util/BFloat16-inl.h", "c10/util/BFloat16-math.h"], "labels": ["Merged", "cla signed", "open source"]}, "566e06eb9b": {"title": "Use _WeakTensorRef over weakref in test_autograd.py (#55726)", "body": "Summary:\nThere are a few autograd tests checking for tensors leaked by reference cycles. This changes them to  use `_WeakTensorRef` over `weakref`. `_WeakTensorRef`, added in https://github.com/pytorch/pytorch/issues/52874, accesses the C++ level `TensorImpl` reference count, compared to `weakref` which accesses python refcounts and so can only tell if the python wrapper object gets deallocated. Not only is this less code, it's also more accurately detecting that the Tensor itself is deallocated.\n\nI didn't touch `weakref` usage in [test_anomaly_assign_parent_cleanup](https://github.com/pytorch/pytorch/blob/fc349cbcde10a5d9e8c1fc9a47bbba025a8e9018/test/test_autograd.py#L3733) and [test_nested_anomaly_printstack_cleanup](https://github.com/pytorch/pytorch/blob/fc349cbcde10a5d9e8c1fc9a47bbba025a8e9018/test/test_autograd.py#L3772) because these are intentionally testing for python object cleanup.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55726\n\nReviewed By: ngimel\n\nDifferential Revision: D27718526\n\nPulled By: albanD\n\nfbshipit-source-id: 37a4914360e35dd4ae8db06b29525cebec4d4b84", "pr_number": "55726", "files_changed": ["test/test_autograd.py"], "labels": ["Merged", "cla signed", "open source"]}, "561b507843": {"title": "Eliminate device guard in generic dispatch key kernel wrappers (#55131)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55131\n\nBenchmark `zeros_out`:\n\n```python\nfrom torch.utils.benchmark import Timer\ncounts = Timer(\n    stmt=\"\"\"at::zeros_out(t, {1});\"\"\",\n    setup=\"auto t = at::empty({1});\",\n    language=\"cpp\",\n).collect_callgrind(number=1_000)\nprint(counts)\n```\n\nWith device guard:\n```\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7f834f095ca0>\nat::zeros_out(t, {1});\nsetup: auto t = at::empty({1});\n                           All          Noisy symbols removed\n    Instructions:      1396022                    1396022\n    Baseline:                0                          0\n1000 runs per measurement, 1 thread\n```\n\nWithout device guard:\n```\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7f25e48927c0>\nat::zeros_out(t, {1});\nsetup: auto t = at::empty({1});\n                           All          Noisy symbols removed\n    Instructions:      1296022                    1296022\n    Baseline:                0                          0\n1000 runs per measurement, 1 thread\n```\n\nWe see about `7.7%` improvement.\n\nghstack-source-id: 126295368\n\nTest Plan:\n```\nbuck build //caffe2/aten/...\nbuck test mode/dev mode/no-gpu //caffe2/test:torch  -- 'caffe2/test:torch - test_msnpu_error (test_torch.TestTorch)'\n```\n\nReviewed By: ezyang\n\nDifferential Revision: D27496584\n\nfbshipit-source-id: 97f783a809b77b28f77a93096d69b3da9ee69df7", "pr_number": "55131", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp", "test/test_torch.py", "tools/codegen/dest/register_dispatch_key.py"], "labels": ["Merged", "cla signed"]}, "d0cd16899f": {"title": "rework device type filter rule (#55753)", "body": "Summary:\nCurrently common_device_type generates device-specific test based on vague rules. see https://github.com/pytorch/pytorch/issues/55707.\nThis should fix https://github.com/pytorch/pytorch/issues/55707\n\n# Changes included\nThis PR changes the rule:\n1. First user provided args (`except_for` and `only_for`) are processed to filter out desired device type from a ALL_AVAILABLE_LIST\n2. Then environment variables are processed the exact same way.\n\ntests are generated based on the final filtered list.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55753\n\nTest Plan: CI\n\nReviewed By: seemethere, ngimel\n\nDifferential Revision: D27709192\n\nPulled By: walterddr\n\nfbshipit-source-id: 1d5378ef013b22a7fb9fdae24b486730b2e67401", "pr_number": "55753", "files_changed": ["torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed"]}, "48ddc9762b": {"title": "Upgrade mypy to version 0.812 (#55712)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54211\n\nThis was a little more annoying than expected, because the `exclude = ` key in `mypy.ini` is weird. I'll file an upstream issue about that.\n\nI ignored one file, `torch/distributed/elastic/agent/server/api.py` that had ~8 errors that were hard to figure out. This can be done in a follow-up.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55712\n\nReviewed By: walterddr\n\nDifferential Revision: D27694976\n\nPulled By: malfet\n\nfbshipit-source-id: 228d8be6af040343ce46595dabaca212e69ccc68", "pr_number": "55712", "files_changed": [".circleci/docker/common/install_conda.sh", ".github/workflows/lint.yml", ".github/workflows/test_tools.yml", "caffe2/contrib/fakelowp/test/test_chunking.py", "caffe2/contrib/fakelowp/test/test_fc_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_int8_ops_nnpi.py", "caffe2/contrib/fakelowp/test/test_layernorm_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_4bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp16.py", "caffe2/contrib/fakelowp/test/test_sls_8bit_nnpi_fp32.py", "mypy.ini", "tools/print_test_stats.py", "torch/distributed/benchmarks/benchmark_ddp_rpc.py", "torch/distributed/elastic/agent/__init__.py", "torch/distributed/elastic/agent/server/api.py", "torch/distributed/elastic/agent/server/local_elastic_agent.py", "torch/jit/_recursive.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/modules/adaptive.py", "torch/nn/quantized/modules/conv.py", "torch/nn/utils/rnn.pyi", "torch/testing/_asserts.py", "torch/testing/_internal/common_device_type.py", "torch/utils/benchmark/examples/sparse/fuzzer.py", "torch/utils/data/_typing.py"], "labels": ["Merged", "cla signed", "module: typing", "oncall: jit", "open source"]}, "e7bb00cb49": {"title": "Add a warning message to retire ProcessGroup RPC backend (#55616)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55616\n\nTest Plan: Imported from OSS\n\nReviewed By: beauby\n\nDifferential Revision: D27650627\n\nPulled By: mrshenli\n\nfbshipit-source-id: ecf06f3b77c7e66b32822dfabf2ef88864b0e5bd", "pr_number": "55616", "files_changed": ["torch/distributed/rpc/__init__.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5ba4cfb7bf": {"title": "Minor typo fixes in `_script.py` (#55818)", "body": "Summary:\nI was reading through this file to get a better understanding of torch.jit.script and just fixed these along the way.\n\nThe only functional change is [here](https://github.com/pytorch/pytorch/compare/master...janeyx99:minor-jit-nits?expand=1#diff-c05f6af41a2d9c7ec7a2b15088259fb74763f7d1406da70f324fc6b20af47427R824). Everything else is documentation only.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55818\n\nReviewed By: walterddr\n\nDifferential Revision: D27718853\n\nPulled By: janeyx99\n\nfbshipit-source-id: a08f5451a904ef7a440be418f11ec083dd14766d", "pr_number": "55818", "files_changed": ["torch/jit/_script.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a756a9e553": {"title": "Add device id to ConvolutionParams (#50892)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50844\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50892\n\nReviewed By: mruberry\n\nDifferential Revision: D27703874\n\nPulled By: ngimel\n\nfbshipit-source-id: aefa4f44ca3387c2f7aa06136e5c62d66a4ac6ab", "pr_number": "50892", "files_changed": ["aten/src/ATen/native/cudnn/ConvShared.cpp", "aten/src/ATen/native/cudnn/ConvShared.h"], "labels": ["Merged", "cla signed", "module: cuda", "open source", "triaged"]}, "bbcb12614e": {"title": "Sort slow tests json by test name (#55862)", "body": "Summary:\nThis will make https://github.com/pytorch/test-infra/commits/master more readable in the future\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55862\n\nReviewed By: ngimel\n\nDifferential Revision: D27728462\n\nPulled By: malfet\n\nfbshipit-source-id: 2f10dd7ace49f343c4b91fc02be9d955fdbf67cc", "pr_number": "55862", "files_changed": ["tools/export_slow_tests.py"], "labels": ["Merged", "cla signed"]}, "5cd73df8f8": {"title": "[Hackathon]Move complex tests to test_complex.py (#55514)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55514\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27679881\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 8a4f4ab8f375187b72ede6feaea37ab546da6d3e", "pr_number": "55514", "files_changed": ["test/jit/test_complex.py", "test/test_jit.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "8fc16da649": {"title": "[Hackathon]Move tests for slice to test_slice.py (#55524)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55524\n\nTest Plan: Imported from OSS\n\nReviewed By: driazati\n\nDifferential Revision: D27686738\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: f1896d739c3a3a7ece987f6eea4072477626231b", "pr_number": "55524", "files_changed": ["test/jit/test_slice.py", "test/test_jit.py"], "labels": ["Merged", "cla signed", "hackathon", "oncall: jit"]}, "da01f4398b": {"title": "Add InferenceMode TLS to ThreadLocalState. (#55822)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55822\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27721285\n\nPulled By: ailzhang\n\nfbshipit-source-id: c978927f8cb3a91de45635b8279e166a3d5652ab", "pr_number": "55822", "files_changed": ["aten/src/ATen/ThreadLocalState.cpp", "aten/src/ATen/ThreadLocalState.h", "c10/core/InferenceMode.h"], "labels": ["Merged", "cla signed"]}, "5a4e5db9ad": {"title": "docs: fix profiler docstring (#55750)", "body": "Summary:\nDescription:\n- change the docstrings for profiler module as per google docstring\n- add link to `torch.autograd` module\n- document `ProfilerAction` and `ProfilerActivity`\n\nhttps://12292060-65600975-gh.circle-artifacts.com/0/docs/profiler.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55750\n\nReviewed By: yinghai\n\nDifferential Revision: D27725494\n\nPulled By: ngimel\n\nfbshipit-source-id: 32d0a18e274a871ac712b28b61ba63eb08299a03", "pr_number": "55750", "files_changed": ["docs/source/profiler.rst", "torch/profiler/__init__.py", "torch/profiler/profiler.py"], "labels": ["Merged", "cla signed", "open source"]}, "4cfbb2401f": {"title": "[ROCM] Re-enable 3 previously faling tests in test_cuda.py (#55813)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53190\nThe following tests are passing in ROCM 4.1. Hence re-enabling them.\ntest_grad_scaling_multigpu\ntest_streaming_backwards_device_transfer\ntest_streaming_backwards_multiple_streams\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55813\n\nReviewed By: yinghai\n\nDifferential Revision: D27725547\n\nPulled By: ngimel\n\nfbshipit-source-id: d8b3ed69fa44c2086f0666b4db0fabb30ad59439", "pr_number": "55813", "files_changed": ["test/test_cuda.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "b4cb020c0f": {"title": "[Gradient Compression] Make orthogonalization_epsilon configurable in PowerSGDState (#55738)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55738\n\nPer title, and use 0 as the default value.\n\nIt turns out that setting this epsilon as 0 can accelerate convergence and improve accuracy for some use cases.\n\nTest Plan:\nunit tests\nf264687105\nf264675194\n\nReviewed By: shuyingsunshine21\n\nDifferential Revision: D27694971\n\nfbshipit-source-id: b61528c6c817127974acdc4635bccf607532287f", "pr_number": "55738", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "37ac271089": {"title": "[AutoAccept][Codemod][FBSourceGoogleJavaFormatLinter] Daily `arc lint --take GOOGLEJAVAFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27731676\n\nfbshipit-source-id: 9402fa9f19b9186a2f38e56c110800254a8e8d91", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/java/org/pytorch/LiteModuleLoader.java", "android/pytorch_android/src/main/java/org/pytorch/LiteNativePeer.java", "android/pytorch_android/src/main/java/org/pytorch/Module.java", "android/pytorch_android/src/main/java/org/pytorch/NativePeer.java"], "labels": []}, "56212daf7e": {"title": "allow tests to run locally without setting environment variables (#55880)", "body": "Summary:\nFixes breakage caused by https://github.com/pytorch/pytorch/issues/55753\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55880\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27735299\n\nPulled By: mruberry\n\nfbshipit-source-id: f8f927f95e4f7fe5f00448ed25d23dac3b7104a4", "pr_number": "55880", "files_changed": ["torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed"]}, "4b09756d26": {"title": "[SPMD] Move a comment (#55877)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55877\n\nAddress a comment in: https://github.com/pytorch/pytorch/pull/55353/files/10bc1dae408260e34d678e02a56c4d231ad8787b#r610930244\nghstack-source-id: 126369525\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27729567\n\nfbshipit-source-id: 5509ebfba2b741cd3532c69044227e5af0fb54fc", "pr_number": "55877", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "00737efdb2": {"title": "[shape inference] Add shape inference func for Bucketize", "body": "Summary: ATT, to ensure output has the same dim type with the input. We need to find a more generic way though...\n\nTest Plan: unit test\n\nReviewed By: ipiszy, khabinov\n\nDifferential Revision: D27690748\n\nfbshipit-source-id: e53832c67b8ac86973c288d2d6b76ef8e5db14b9", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h", "caffe2/opt/shape_info.cc"], "labels": []}, "9ccae89102": {"title": "port addcmul to OpInfo (#55517)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55517\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27649413\n\nPulled By: albanD\n\nfbshipit-source-id: e1faf25cf7f9c3636f62db1512aee78fd7c4f9b6", "pr_number": "55517", "files_changed": ["test/test_autograd.py", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "505f6f325f": {"title": "port addcdiv to opinfo (#55518)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55518\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27649411\n\nPulled By: albanD\n\nfbshipit-source-id: cfb0a235d94ef62589acbeb9bf11d2ea17248484", "pr_number": "55518", "files_changed": ["test/test_autograd.py", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "192df16a4d": {"title": "move logaddexp{2} to opinfo (#55535)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55535\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27649410\n\nPulled By: albanD\n\nfbshipit-source-id: 4453da3853e2ac8e2e625ae9bdb9f717336bb3ec", "pr_number": "55535", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "5dba4ff786": {"title": "move topk to use OpInfo (#55547)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55547\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin, mruberry\n\nDifferential Revision: D27649412\n\nPulled By: albanD\n\nfbshipit-source-id: e36a5bb5703681b7f7647ca30d6f4a72faf5ed0e", "pr_number": "55547", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "d7d7556f17": {"title": "Move tensor implicit conversions to test_builtins.py (#55532)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55532\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27729682\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: d2517ee68b83e59cde87b8fb7d5bf7203f02cbc6", "pr_number": "55532", "files_changed": ["test/jit/test_builtins.py", "test/test_jit.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "24f9a446c9": {"title": "Fix wrong detection of depthwise conv on neon (#55794)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54136\n\ntldr: dephwise conv require that the nb of output channel is 1.\n\nThe code here only handles this case and previously, all but the first output channel were containing uninitialized memory. The nans from the issue were random due to the allocation of a torch.empty() that was sometimes returning non-nan memory.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55794\n\nReviewed By: ngimel\n\nDifferential Revision: D27711717\n\nPulled By: albanD\n\nfbshipit-source-id: 00eac3fd59db1d09fe7bab89427b105a019e7a5d", "pr_number": "55794", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["Merged", "cla signed"]}, "75eb026e07": {"title": "migrate matrix_exp to opInfo tests (#55533)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55533\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27628966\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 87dd1858a1ebe22dcca9bd90b8cdca8c3d67d0e9", "pr_number": "55533", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "4753100a3b": {"title": "Un-ignore F403 in .flake8 (#55838)", "body": "Summary:\nGenerally wildcard imports are bad for the reasons described here: https://www.flake8rules.com/rules/F403.html\n\nThis PR replaces wildcard imports with an explicit list of imported items where possible, and adds a `# noqa: F403` comment in the other cases (mostly re-exports in `__init__.py` files).\n\nThis is a prerequisite for https://github.com/pytorch/pytorch/issues/55816, because currently [`tools/codegen/dest/register_dispatch_key.py` simply fails if you sort its imports](https://github.com/pytorch/pytorch/actions/runs/742505908).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55838\n\nTest Plan: CI. You can also run `flake8` locally.\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27724232\n\nPulled By: samestep\n\nfbshipit-source-id: 269fb09cb4168f8a51fd65bfaacc6cda7fb87c34", "pr_number": "55838", "files_changed": [".flake8", "benchmarks/fastrnns/__init__.py", "benchmarks/fastrnns/runner.py", "test/jit/test_hooks.py", "test/jit_hooks/model.py", "test/mobile/test_lite_script_module.py", "test/onnx/model_defs/__init__.py", "test/onnx/test_pytorch_common.py", "test/test_jit_fuser_legacy.py", "test/test_jit_legacy.py", "test/test_jit_profiling.py", "test/test_jit_simple.py", "test/test_mobile_optimizer.py", "tools/autograd/gen_annotated_fn_args.py", "tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_python_functions.py", "tools/autograd/gen_trace_type.py", "tools/autograd/gen_variable_factories.py", "tools/autograd/gen_variable_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/autograd.py", "tools/codegen/api/cpp.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/meta.py", "tools/codegen/api/native.py", "tools/codegen/api/python.py", "tools/codegen/api/structured.py", "tools/codegen/api/translate.py", "tools/codegen/api/types.py", "tools/codegen/context.py", "tools/codegen/dest/native_functions.py", "tools/codegen/dest/register_dispatch_key.py", "tools/codegen/gen.py", "tools/codegen/model.py", "tools/codegen/selective_build/selector.py", "tools/lite_interpreter/gen_selected_mobile_ops_header.py", "tools/pyi/gen_pyi.py", "torch/__init__.py", "torch/autograd/_functions/__init__.py", "torch/cuda/__init__.py", "torch/distributed/__init__.py", "torch/distributed/elastic/rendezvous/__init__.py", "torch/distributed/nn/__init__.py", "torch/distributed/rpc/__init__.py", "torch/distributions/__init__.py", "torch/for_onnx/__init__.py", "torch/multiprocessing/__init__.py", "torch/nn/__init__.py", "torch/nn/intrinsic/__init__.py", "torch/nn/intrinsic/qat/__init__.py", "torch/nn/intrinsic/quantized/__init__.py", "torch/nn/intrinsic/quantized/_reference/__init__.py", "torch/nn/qat/__init__.py", "torch/nn/quantizable/__init__.py", "torch/nn/quantized/__init__.py", "torch/nn/quantized/_reference/__init__.py", "torch/nn/quantized/dynamic/__init__.py", "torch/quantization/__init__.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/quantize.py", "torch/quantization/qconfig.py", "torch/testing/__init__.py", "torch/utils/benchmark/__init__.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed"]}, "5e625906e9": {"title": "Fix lint for redundant-workflows list (#55916)", "body": "Summary:\nCurrently this lint is [passing](https://github.com/pytorch/pytorch/runs/2335195975) on https://github.com/pytorch/pytorch/issues/55176 when it should be failing, because it is using [`l.sort()` instead of `sorted(l)`](https://docs.python.org/3/howto/sorting.html).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55916\n\nTest Plan: Check out https://github.com/pytorch/pytorch/pull/55176/commits/0c29aa1679403df6227d9e4d0b59267457ec309f, start a `python3` shell, and run the steps from this lint. The final `assert` from before this PR should succeed, and the `assert` from this PR should fail. The lint should succeed on this PR's CI, though, since the list of workflows is correct on `master`.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27739792\n\nPulled By: samestep\n\nfbshipit-source-id: 068fa846569eb83b98088215d8a1b63d12560633", "pr_number": "55916", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "381b3d8f4b": {"title": "Refactor get numerical jacobian to calculate wrt all outputs at once (#54378)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54378\n\n### For release notes\n`torch.autograd.gradcheck.get_numerical_jacobian` (not part of the public api) is being deprecated.\n\nIn the future, user code relying on this function will break because, among other changes, `get_numerical_jacobian` now returns `List[Tuple[torch.Tensor]]` instead of `List[torch.Tensor]`.\n\n(more details if necessary)\nFor a `fn` that takes in M inputs and N outputs we now return a list of M N-tuples of jacobians where `output[i][j]` would represent the numerical jacobian w.r.t. to the ith input and the jth output. Previously `get_numerical_jacobian` returned a list of tensors where each tensor represents the jacobian w.r.t. to each of the M inputs and a specific output. Finally, the function passed in as the parameter `fn` should expect to handle individual parameters, where previously `fn` is required to expect its parameters wrapped in a tuple.\n\n --- end --\n\nThis PR addresses the comment here https://github.com/pytorch/pytorch/pull/53857#discussion_r595429639, to reduce the run-time of old gradcheck's get numerical jacobian by a factor of num_outputs. However, because very few ops actually return multiple outputs, there is not too much real speed up here.\n\nThe main benefit of doing this change as part of the refactor is that it helps us isolate the possible bugs that are specific to switching `get numerical jacobian` to run in a per output way vs all outputs at once. Much of the logic implemented here will be the same for the fast gradcheck case, so knowing for certain that everything should pass after this stage will make the next step much simpler.\n\nThe get_numerical_jacobian api is also being used in common_nn. So we update the callsite there as well.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27728720\n\nPulled By: soulitzer\n\nfbshipit-source-id: ee0f90b4f26ddc5fdbe949c4965eaa91c9ed0bb8", "pr_number": "54378", "files_changed": ["test/test_autograd.py", "test/test_overrides.py", "torch/autograd/gradcheck.py", "torch/testing/_internal/common_nn.py"], "labels": ["Merged", "cla signed", "module: deprecation"]}, "8c8f8829f0": {"title": "Factor out numerical logic (#54479)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54479\n\nThis change is similar to #54049 in that it helps us factor out some code that can be used in both fast and slow versions of gradcheck.\n - `compute_gradient` and `compute_numerical_jacobian_cols` have  fewer responsibilities:\n   - compute_numerical_jacobian_cols essentially only handles the complexity of complex derivatives\n   - compute_gradient handles only finite differencing (and doesn't worry about different layouts and indexing into the input tensor)\n  - we have two stages again where we first compute the columns separately, then combine them\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27728727\n\nPulled By: soulitzer\n\nfbshipit-source-id: fad3d5c1a91882621039beae3d0ecf633c19c28c", "pr_number": "54479", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "2bb58a06ef": {"title": "move logic to skip a redispatch directly inside of resize_output (#55162)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55162\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27506253\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 02fddb1926de49cd8c915c549eb99d92e58e75e1", "pr_number": "55162", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/Resize.h", "tools/codegen/dest/register_dispatch_key.py"], "labels": ["Merged", "cla signed"]}, "70a09d97d1": {"title": "Use nodes instead of node", "body": "Summary: `networkx 2.4+` replaced `node` attribute to `nodes` in graph object. This caused failures in `caffe2`'s' `topological_sort_traversal_longest_path` function which uses networkx library for topological sort.\n\nDifferential Revision: D27718857\n\nfbshipit-source-id: 812fbb613946565d089cc84a20f3cdf7df046e19", "pr_number": null, "files_changed": ["caffe2/python/memonger.py"], "labels": []}, "18662d4321": {"title": "[Static runtime] refactor MemoryPlanner codes to prepare for output tensor memory planning (#55809)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55809\n\n[Static runtime] refactor MemoryPlanner codes to prepare for output tensor memory planning\n\nTest Plan: buck test mode/dev //caffe2/caffe2/fb/predictor:pytorch_predictor_test -- --exact 'caffe2/caffe2/fb/predictor:pytorch_predictor_test - PyTorchPredictor.StaticRuntime'\n\nReviewed By: bwasti\n\nDifferential Revision: D27411416\n\nfbshipit-source-id: 7dae7c2586ce3b4ebacf6169017140166c30e99c", "pr_number": "55809", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "c3a49cb30c": {"title": "Better types in fbcode/caffe2/torch/jit/_script.py (#55856)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55856\n\nTest Plan: Sandcastle\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27715495\n\nfbshipit-source-id: 9804e2d432fda302117f05a0d21cbb7f0dd3ae38", "pr_number": "55856", "files_changed": ["torch/jit/_script.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "657b66e87d": {"title": "[NCCL] Log when barrier guesses device to use (#54991)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54991\n\nActual proposed fix is in\nhttps://github.com/pytorch/pytorch/pull/53934, in the meantime, would be useful\nto include this LOG when barrier does not know what devices to use, and suggest\nthe workaround of passing in device_ids into barrier().\nghstack-source-id: 126351889\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27444917\n\nfbshipit-source-id: 0f269c5a7732e5be6e51adfca7ef70d04ffd71d3", "pr_number": "54991", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8f953ef544": {"title": "Increase token count threshold for calling thrust sort in embedding backward (#49913)", "body": "Summary:\nIncreases the token count threshold to expand the span of custom CUDA kernel implementation of embedding backward. Here is the speedup for embedding backward implementation for DGXV100-128GB and DGXA100-640GB given below. I picked 6144 as the new threshold since anything below it mostly results in faster execution with custom CUDA kernel. One important advantage of the custom CUDA kernel is that it allows CUDA graph capture, whereas thrust code path results in CPU syncs, prohibiting graph capture (times below are collected without graph capture). For reference, MLPerf BERT benchmark uses num_features=1024.\n\n  | num_tokens | num_features | thrust path(ms) | custom kernel(ms) | speedup\n-- | -- | -- | -- | -- | --\nDGXV100 |   |   |   |   |\n  | 1024 | 64 | 0.36 | 0.18 | 2.04\n  | 1024 | 256 | 0.43 | 0.30 | 1.46\n  | 1024 | 1024 | 0.89 | 0.74 | 1.20\n  | 1024 | 2048 | 1.50 | 1.33 | 1.12\n  | 1024 | 4096 | 2.71 | 2.50 | 1.08\n  | 1024 | 8192 | 5.07 | 4.89 | 1.04\n  | 2048 | 64 | 0.33 | 0.23 | 1.46\n  | 2048 | 256 | 0.41 | 0.33 | 1.26\n  | 2048 | 1024 | 0.92 | 0.79 | 1.17\n  | 2048 | 2048 | 1.54 | 1.38 | 1.11\n  | 2048 | 4096 | 2.80 | 2.54 | 1.10\n  | 2048 | 8192 | 5.29 | 4.98 | 1.06\n  | 4096 | 64 | 0.46 | 0.32 | 1.43\n  | 4096 | 256 | 0.50 | 0.47 | 1.07\n  | 4096 | 1024 | 1.02 | 0.88 | 1.15\n  | 4096 | 2048 | 1.70 | 1.59 | 1.07\n  | 4096 | 4096 | 3.06 | 2.68 | 1.14\n  | 4096 | 8192 | 5.79 | 5.28 | 1.10\n  | 5120 | 64 | 0.42 | 0.33 | 1.28\n  | 5120 | 256 | 0.51 | 0.46 | 1.11\n  | 5120 | 1024 | 1.06 | 0.93 | 1.14\n  | 5120 | 2048 | 1.77 | 1.55 | 1.14\n  | 5120 | 4096 | 3.18 | 2.76 | 1.15\n  | 5120 | 8192 | 6.24 | 5.46 | 1.14\n  | 6144 | 64 | 0.42 | 0.36 | 1.17\n  | 6144 | 256 | 0.52 | 0.50 | 1.05\n  | 6144 | 1024 | 1.10 | 0.98 | 1.13\n  | 6144 | 2048 | 1.85 | 1.61 | 1.15\n  | 6144 | 4096 | 3.34 | 2.84 | 1.17\n  | 6144 | 8192 | 6.19 | 5.69 | 1.09\n  | 8192 | 64 | 0.42 | 0.48 | 0.88\n  | 8192 | 256 | 0.51 | 0.65 | 0.78\n  | 8192 | 1024 | 1.14 | 1.12 | 1.01\n  | 8192 | 2048 | 1.92 | 1.77 | 1.09\n  | 8192 | 4096 | 3.49 | 3.03 | 1.15\n  | 8192 | 8192 | 6.59 | 5.96 | 1.11\n  | 16384 | 64 | 0.46 | 0.82 | 0.56\n  | 16384 | 256 | 0.59 | 0.99 | 0.60\n  | 16384 | 1024 | 1.35 | 1.54 | 0.88\n  | 16384 | 2048 | 2.31 | 2.24 | 1.03\n  | 16384 | 4096 | 4.20 | 3.63 | 1.16\n  | 16384 | 8192 | 8.26 | 7.51 | 1.10\n  | 32768 | 64 | 0.47 | 1.48 | 0.32\n  | 32768 | 256 | 0.68 | 1.70 | 0.40\n  | 32768 | 1024 | 1.63 | 2.35 | 0.69\n  | 32768 | 2048 | 2.87 | 3.19 | 0.90\n  | 32768 | 4096 | 5.26 | 4.86 | 1.08\n  | 32768 | 8192 | 10.17 | 9.92 | 1.03\n  | 65536 | 64 | 0.50 | 2.81 | 0.18\n  | 65536 | 256 | 0.78 | 3.12 | 0.25\n  | 65536 | 1024 | 2.02 | 3.99 | 0.51\n  | 65536 | 2048 | 3.58 | 5.06 | 0.71\n  | 65536 | 4096 | 6.68 | 7.40 | 0.90\n  | 65536 | 8192 | 13.08 | 15.35 | 0.85\nDGXA100 |   |   |   |   |\n  | 1024 | 64 | 0.28 | 0.09 | 3.05\n  | 1024 | 256 | 0.30 | 0.17 | 1.71\n  | 1024 | 1024 | 0.51 | 0.39 | 1.31\n  | 1024 | 2048 | 0.81 | 0.68 | 1.20\n  | 1024 | 4096 | 1.43 | 1.24 | 1.16\n  | 1024 | 8192 | 2.63 | 2.42 | 1.09\n  | 2048 | 64 | 0.25 | 0.12 | 2.15\n  | 2048 | 256 | 0.29 | 0.22 | 1.36\n  | 2048 | 1024 | 0.53 | 0.44 | 1.20\n  | 2048 | 2048 | 0.86 | 0.73 | 1.18\n  | 2048 | 4096 | 1.51 | 1.30 | 1.16\n  | 2048 | 8192 | 2.81 | 2.55 | 1.10\n  | 4096 | 64 | 0.31 | 0.20 | 1.57\n  | 4096 | 256 | 0.35 | 0.33 | 1.08\n  | 4096 | 1024 | 0.63 | 0.57 | 1.10\n  | 4096 | 2048 | 1.08 | 0.86 | 1.26\n  | 4096 | 4096 | 2.11 | 1.44 | 1.46\n  | 4096 | 8192 | 3.33 | 2.81 | 1.19\n  | 5120 | 64 | 0.36 | 0.22 | 1.63\n  | 5120 | 256 | 0.37 | 0.37 | 0.98\n  | 5120 | 1024 | 0.66 | 0.62 | 1.07\n  | 5120 | 2048 | 1.05 | 0.92 | 1.15\n  | 5120 | 4096 | 1.83 | 1.51 | 1.21\n  | 5120 | 8192 | 3.35 | 2.94 | 1.14\n  | 6144 | 64 | 0.29 | 0.25 | 1.18\n  | 6144 | 256 | 0.37 | 0.43 | 0.86\n  | 6144 | 1024 | 0.70 | 0.68 | 1.03\n  | 6144 | 2048 | 1.08 | 0.98 | 1.11\n  | 6144 | 4096 | 1.89 | 1.57 | 1.20\n  | 6144 | 8192 | 3.49 | 3.07 | 1.14\n  | 8192 | 64 | 0.29 | 0.31 | 0.95\n  | 8192 | 256 | 0.37 | 0.52 | 0.70\n  | 8192 | 1024 | 0.71 | 0.79 | 0.90\n  | 8192 | 2048 | 1.16 | 1.10 | 1.06\n  | 8192 | 4096 | 2.04 | 1.70 | 1.20\n  | 8192 | 8192 | 3.86 | 3.32 | 1.16\n  | 16384 | 64 | 0.31 | 0.55 | 0.56\n  | 16384 | 256 | 0.42 | 0.93 | 0.45\n  | 16384 | 1024 | 0.87 | 1.24 | 0.70\n  | 16384 | 2048 | 1.46 | 1.57 | 0.93\n  | 16384 | 4096 | 2.60 | 2.23 | 1.17\n  | 16384 | 8192 | 5.15 | 4.69 | 1.10\n  | 32768 | 64 | 0.33 | 1.03 | 0.32\n  | 32768 | 256 | 0.49 | 1.78 | 0.28\n  | 32768 | 1024 | 1.11 | 2.18 | 0.51\n  | 32768 | 2048 | 1.90 | 2.54 | 0.75\n  | 32768 | 4096 | 3.45 | 3.31 | 1.04\n  | 32768 | 8192 | 6.46 | 6.43 | 1.00\n  | 65536 | 64 | 0.36 | 2.19 | 0.16\n  | 65536 | 256 | 0.56 | 3.41 | 0.17\n  | 65536 | 1024 | 1.39 | 4.01 | 0.35\n  | 65536 | 2048 | 2.48 | 4.45 | 0.56\n  | 65536 | 4096 | 4.50 | 5.44 | 0.83\n  | 65536 | 8192 | 8.49 | 10.55 | 0.80\n\nHere is the script used to generate the times (30522 is used in BERT MLPerf benchmark as vocabulary size, hence is used in this example):\n\n```\nimport torch\nimport torch.nn as nn\nimport time\n\nvocabulary_size = 30522\nfor num_tokens in [512,1024,2048,4096,5120,6144,8192,16384,32768,65536]:\n    for hidden_dim in [64,256,1024,2048,4096,8192]:\n        fprop_time_avg = 0.0\n        bprop_time_avg = 0.0\n        emb = nn.Embedding(vocabulary_size, hidden_dim).cuda()\n        for trial in range(0,10):\n            inds = torch.round(torch.rand(num_tokens) * (vocabulary_size-1)).to(dtype=torch.int64).cuda()\n            y = emb(inds)\n            dy = torch.randn_like(y)\n            torch.cuda.synchronize()\n            t_start_bwd = time.time()\n            y.backward(dy)\n            torch.cuda.synchronize()\n            t_stop_bwd = time.time()\n            bprop_time_avg += t_stop_bwd - t_start_bwd\n        bprop_time_avg /= 10.0\n        print(\"bprop num_tokens %5d, num_features %5d, time %2.6f\" %(num_tokens, hidden_dim, bprop_time_avg))\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49913\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27727738\n\nPulled By: ngimel\n\nfbshipit-source-id: fa497b6745b6d20bb11352579ed9eb5b66a8b1e2", "pr_number": "49913", "files_changed": ["aten/src/ATen/native/cuda/Embedding.cu"], "labels": ["Merged", "cla signed", "module: cuda", "module: embedding", "module: nn", "open source", "triaged"]}, "ea446ed600": {"title": "[PyTorch] Allow copy operations on MaybeOwned (#55419)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55419\n\nTurns out it's useful to have these. I chose to implement them in the straightforward safe way, rather than always borrowing.\nghstack-source-id: 126369328\n\nTest Plan: Added more automated tests.\n\nReviewed By: hlu1\n\nDifferential Revision: D27545805\n\nfbshipit-source-id: 84bb4458b86672ad340cc1f0aa18b80ca7ee13f1", "pr_number": "55419", "files_changed": ["c10/test/util/MaybeOwned_test.cpp", "c10/util/MaybeOwned.h"], "labels": ["Merged", "cla signed"]}, "de53de39d7": {"title": "[PyTorch] Mark borrowed case as C10_LIKELY in MaybeOwned (#55553)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55553\n\nIf this case isn't likely, user code would have been better off with a regular T.\nghstack-source-id: 126369326\n\nTest Plan: Existing CI\n\nReviewed By: ezyang\n\nDifferential Revision: D27630287\n\nfbshipit-source-id: b074af3a65c61dfe9e0246df046cc8c49e8efb03", "pr_number": "55553", "files_changed": ["c10/util/MaybeOwned.h"], "labels": ["Merged", "cla signed"]}, "aceceb3d5c": {"title": "Reland #50999 (Added pow() on CPU for float16 & bfloat16) (#55280)", "body": "Summary:\n#### Reason for relanding\nLine 1607 of `torch/testing/_internal/common_methods_invocations.py` of https://github.com/pytorch/pytorch/issues/50999  had `dtype` instead of `dtype=torch.bool`, so 4 of the 9 sample inputs for `bool` had incorrect dtype. This bug was caught by https://github.com/pytorch/pytorch/issues/54949.\n\n1. Added support for pow() on CPU for `float16` (`Half`) and `bfloat16` types.\nBoth `pow(Tensor, Scalar)` and `pow(Tensor, Tensor)` are now supported for the aforementioned types.\nHowever autograd isn't supported for `Float16` on CPU yet, as `log_vml_cpu` can't be enabled for it.\n2. heitorschueroff added `pow_tensor_scalar_optimized_kernel` to refactor & simplify `PowKernel.cpp`.\nIt provides a common path for all the complex types & floating point types (except Float16, due to lack of complete AVX2 vectorization support for it).  It replaced code that had previously been duplicated for (float, double) and complex types,\nso PowKernel.cpp looks a lot cleaner now.\n3. Enabled (unskipped) some tests for `erf`, `erfc`,`erfinv`, `tan` and `linalg.vector.norm` which were being skipped earlier due to `pow()` not having been implemented for `float16` & `bfloat16`.\n4. Added an OpInfo for `pow()` & enabled some test cases for `pow()`.\n5. Extended the coverage of existing tests for `pow` in `test_binary_ufuncs.py` in order to enable comparison with `numpy`, even with discontiguous tensors, and added a test to ensure that a runtime error is raised for `pow`'s inplace variant if resizing the base tensor is required during its invocation.\n6. Added `float16` & `bfloat16` to `square`'s dtype lists in its `UnaryUfuncInfo`.\n7. Removed redundant `dtypesIfCPU` and `dtypesIfCUDA` from `OpInfo`s where they are equal to `dtypes`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55280\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27591772\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: c7420811b32595bb3353149a61e54a73f2eb352b", "pr_number": "55280", "files_changed": ["aten/src/ATen/native/cpu/PowKernel.cpp", "test/test_binary_ufuncs.py", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "d1fac54f13": {"title": "[Pytorch] Only print gradient of a tensor if it requires_grad (#54446)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54446\n\nSeveral people now have run into an issue with printing tensors using lite interpreter in xplat. https://fb.workplace.com/groups/2148543255442743/?multi_permalinks=2620088118288252&notif_id=1616432955971055&notif_t=work_group_activity&ref=notif This is due to the fallback of _fw_grad also requiring autograd to exist. Introduce a new function that can be used to guard against calling _fw_grad if autograd isnt built.\n\nghstack-source-id: 126334787\n\nTest Plan: ci, tested the guard in by printing in a tensor in a situation where autograd isnt built.\n\nReviewed By: albanD\n\nDifferential Revision: D27239164\n\nfbshipit-source-id: 4b98b4b7770b153bc2c13c95f7d256425e09ef39", "pr_number": "54446", "files_changed": ["aten/src/ATen/core/Formatting.cpp"], "labels": ["Merged", "cla signed"]}, "6dd1978d4b": {"title": "print average duration for caffe2 benchmark", "body": "Summary: print average duration for caffe2 benchmark\n\nTest Plan:\nbuck run //xplat/caffe2:caffe2_benchmarkAppleMac -- --init_net ~/track_init_net.pb --net ~/track_predict_net.pb --warmup 10 --input 'data' --input_dims '1,4,128,256' --input_type float --iter 20\nUsing additional configuration options from .buckconfig.local\nBuilding: finished in 0.6 sec (100%) 247/2137 jobs, 0 updated\n  Total time: 0.6 sec\nAverage Duration: 18111 us\n\nReviewed By: larryliu0820\n\nDifferential Revision: D27745416\n\nfbshipit-source-id: a5d20b8ef0ba4a9547d396738d5ddd1aca57684d", "pr_number": null, "files_changed": ["binaries/benchmark_helper.cc"], "labels": []}, "5ffc4e3b0f": {"title": "refactor prepare_for_backward (#54977)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54977\n\nput part of codes in prepare_for_backward into functions, so that those functions can be used in static graph training and delay all reduce later on.\nghstack-source-id: 126366714\n\nTest Plan: unit tests\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27439195\n\nfbshipit-source-id: 8899eda621260232d774cb145f9c6d683c47e188", "pr_number": "54977", "files_changed": ["torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5a45b1b2f2": {"title": "Add nondeterministic alert for `index_put_` when `accumulate=False` (#55827)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55516\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55827\n\nReviewed By: yinghai\n\nDifferential Revision: D27725794\n\nPulled By: ngimel\n\nfbshipit-source-id: f6b5b3e635170524fdb5a0141ebd27925c37e8d9", "pr_number": "55827", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "module: determinism", "open source"]}, "bf8b790ba7": {"title": ".github: Bump disk size for auto-scaled workers (#55955)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55955\n\nWas experiencing build failures related to disk size issues, let's bump\nto 150 to see if that resolves these issues\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D27747958\n\nPulled By: seemethere\n\nfbshipit-source-id: 9222475d2298cf942479650567616489387bf552", "pr_number": "55955", "files_changed": [".github/scale-config.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "c47cc30bf5": {"title": "Skip testing torch.float16 in test_isnan (#55906)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/issues/55905\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55906\n\nReviewed By: walterddr\n\nDifferential Revision: D27737356\n\nPulled By: malfet\n\nfbshipit-source-id: 39571cfe6f078af8bb7387ed459a5d0f2410bad1", "pr_number": "55906", "files_changed": ["test/test_jit_fuser_te.py"], "labels": ["Merged", "cla signed"]}, "b98f011cd4": {"title": "cmake: Enable (s)ccache for nccl builds (#55814)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55814\n\nI don't really know if the original issue is resolved but let's just\ncheck and see if this passes CI so that we can potentially get some\nspeed up on our builds\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D27715734\n\nPulled By: seemethere\n\nfbshipit-source-id: a8f90774dfd25b0abf8e57283fe3591a8d8f3c4b", "pr_number": "55814", "files_changed": ["cmake/External/nccl.cmake"], "labels": ["Merged", "cla signed", "module: build", "module: nccl"]}, "2237754b13": {"title": "Update a `batch_first` arg for transformers like GRU and LSTM. (#55285)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/25100 #43112\n\nEDIT: pardon my inexperience since this is my first PR here, that I did not realize the doc should not have any trailing white spaces, and `[E712] comparison to False should be 'if cond is False:' or 'if not cond:'`, now both fixed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55285\n\nReviewed By: ngimel\n\nDifferential Revision: D27710107\n\nPulled By: jbschlosser\n\nfbshipit-source-id: c4363a4604548c0d84628c4997dd23d6b3afb4d9", "pr_number": "55285", "files_changed": ["test/test_nn.py", "third_party/ideep", "torch/nn/modules/activation.py", "torch/nn/modules/transformer.py", "torch/nn/quantizable/modules/activation.py"], "labels": ["Merged", "Reverted", "cla signed", "open source"]}, "8596ac186b": {"title": "deterministic code path for gather_backward for dim = 1 (#55573)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55573\n\nprovide a deterministic code path for gather_backward when dim = 1\n\nTest Plan:\nbuck test //caffe2/test:torch -- test_gather_backward\n    \u2713 Pass: caffe2/test:torch - test_gather_backward_one_dim (test_torch.TestTorch) (1.099)\n    \u2713 Pass: caffe2/test:torch - test_gather_backward_deterministic_path (test_torch.TestTorch) (1.166)\n\ntest on GPU\n\nbuck test mode/opt //caffe2/test:torch_cuda -- test_gather_backward_deterministic\n\nStarted reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/1407375070421778\n    \u2713 ListingSuccess: caffe2/test:torch_cuda - main (7.484)\n    \u2713 Pass: caffe2/test:torch_cuda - test_gather_backward_deterministic_path_cuda (test_torch.TestTorchDeviceTypeCUDA) (26.145)\n    \u2713 Pass: caffe2/test:torch_cuda - main (26.145)\nSummary\n  Pass: 2\n  ListingSuccess: 1\n\nReviewed By: ngimel\n\nDifferential Revision: D27632008\n\nfbshipit-source-id: ec27475332a3b36360cc014193256c21cba77d63", "pr_number": "55573", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "de5e3b5eb0": {"title": "Fix OSS flaky test_destroy_full_group on MPI backend in pytorch_linux_xenial_cuda10_2_cudnn7_py3_multigpu_test environment by adding a barrier and retrying MPI_Comm_create 3 times (#55921)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55921\n\nFix this flaky test by adding a barrier and retrying the flaky function call `MPI_Comm_create` 3 times.\n\nCouldn't figure out the root cause why `createProcessGroupMPI` can be flaky when just creating a subgroup communicator by mainly invoking `MPI_Comm_create`. Here `createProcessGroupMPI` does not involve any p2p or collective communication at all. Cannot further dig into `MPI_Comm_create`, which is in MPI codebase.\n\nAlso checked the commit history, and no commit on `ProcessGroupMPI.cpp` can be found within a few days before Mar 10th.\n\nFirst failure (on Mar 10th):\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/283704/workflows/d84ac4a0-42e3-4925-b1cf-32d3c3d1022a/jobs/11456129\n\nNote that the test failure cannot be reproduced locally.\n\nVerified the fix on CI:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/300586/workflows/a5c16db4-3ae2-44c7-a9c8-b0885dad2a64/jobs/12356852\ntest_destroy_full_group has rerun 100 times and pass.\n\n#Closes: https://github.com/pytorch/pytorch/issues/53899\nghstack-source-id: 126414937\n\nTest Plan:\n```\nexport BACKEND=mpi\nexport WORLD_SIZE=2\npytest -k test_destroy_full_group test/distributed/test_distributed_fork.py -vs\n```\n\n```\n#!/bin/bash\nfor i in {1..100}\ndo\npytest -k test_destroy_full_group test/distributed/test_distributed_fork.py\ndone\n```\n\nThe CI tests triggered by a new branch:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch?branch=ci-all%2Fwayi_mpi\n\nReviewed By: mrshenli\n\nDifferential Revision: D27245421\n\nfbshipit-source-id: 86e7fe208e34eda8a33885e385d56ec6b60eca27", "pr_number": "55921", "files_changed": ["torch/lib/c10d/ProcessGroupMPI.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "132f5c1f36": {"title": "Clang-format ProcessGroupMPI.cpp (#55969)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55969\n\nPer title\nghstack-source-id: 126453717\n\nTest Plan: N/A\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27752173\n\nfbshipit-source-id: e5069b91d699b9d02b12e5dab5e62007dbcee9f0", "pr_number": "55969", "files_changed": ["torch/lib/c10d/ProcessGroupMPI.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d398a705c6": {"title": "Clang-format batchnorm.py and distributed.py (#55971)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55971\n\nPer title\nghstack-source-id: 126454339\n\nTest Plan: N/A\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27752315\n\nfbshipit-source-id: 64ca5dea7b2689037594a6bd9a75641a9bb817c1", "pr_number": "55971", "files_changed": ["torch/nn/modules/batchnorm.py", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "cf7c5dcae3": {"title": "[PyTorch] Avoid double indirection in MaybeOwned's borrowed state (#55685)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55685\n\nThis diff introduces a traits class that tells `MaybeOwned` how\nto borrow a specific type. While it is still capable of handling a\ngeneric `T` by storing `const T*` and how to do so is shown in a\ncomment, it is not committed in live code because it is not needed.\n\nInstead, we have specific traits implementations for\n`c10::intrusive_ptr<T>` and `Tensor` that implement the borrowed state\nas just a plain old `c10::intrusive_ptr<T>` or `Tensor` (respectively)\nthat we manipulate to avoid reference counting operations. We do this\nentirely with public API to `c10::intrusive_ptr<T>` and could do\nlikewise with `Tensor`, but (as comments in the code explain) adding a\nprivate constructor to `MaybeOwnedTraits<Tensor>` allowed additional\nperformance optimization.\n\nThis representation of `MaybeOwned` seems to be more efficient than\nthe generic `T-or-pointer-to-const-T` representation. Intuitively, we\navoid a double indirection at minimal cost vs the previous\nimplementation. It *also* seems to be more efficient than the pointer\ntagging representation I sent out as #55555; apparently, having the\nextra word for a flag is much cheaper than the masking operands for\npointer tagging and the same double indirection as the generic\nrepresentation.\n\nIn particular, this seems to have the same *effect* as the\n`TensorHandle` idea we've discussed internally (a hypothetical class\nlike `Tensor` that wraps a raw `TensorImpl*` and shares the generated\nmethods of `Tensor` so that everything still works), but you have to\nbe explicit about borrowing and use pointer syntax to get the\neffect. Unlike `TensorHandle`, you can use it as internal state in a\nclass and \"upgrade\" from a borrow to an owned `Tensor` derived from\nyour original borrow if necessary.\n\nNote that this is just a representational change and it still has the\nsame semantics: you need to keep the T you borrowed from around!\nghstack-source-id: 126405920\n\nTest Plan:\nPrevious diff changes the MaybeOwned tests to cover\nboth `intrusive_ptr` and `Tensor`, which we need in order to ensure\nthat our trait implementations are correct.\n\nFurther diffs in this stack will use this type to hold operand tensors\nin `TensorIteratorBase` to allow borrowing at relatively small cost\n(very roughly, a 6% win in the successful borrowing case for our\nadd-in-place benchmark at the cost of a 2.5% regression in the\nlegacy non-borrowing case, and we know that we will be able to borrow\nin structured kernels and probably most unstructured operands as\nwell).\n\nReviewed By: ezyang\n\nDifferential Revision: D27679723\n\nfbshipit-source-id: 57104f4edabc545ff83657233fde9eb40b969826", "pr_number": "55685", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "c10/util/MaybeOwned.h", "c10/util/intrusive_ptr.h"], "labels": ["Merged", "cla signed"]}, "86368700e8": {"title": "[PyTorch] Change MaybeOwned tests to use intrusive_ptr and Tensor (#55684)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55684\n\nUpcoming changes to `MaybeOwned<T>` will require that T is\none of these two types and will have custom code for both.\n\nThis diff updates the tests to continue to build under these new\nrequirements; it is being sent separately to demonstrate that the\ntests continue to work on the current implementation.\nghstack-source-id: 126405918\n\nTest Plan: CI will run the rewritten tests.\n\nReviewed By: bhosmer\n\nDifferential Revision: D27630289\n\nfbshipit-source-id: e38097d9ca04f3337cfa543ebcc8fb5d6916fcf3", "pr_number": "55684", "files_changed": ["aten/src/ATen/test/MaybeOwned_test.cpp", "c10/test/util/MaybeOwned_test.cpp"], "labels": ["Merged", "cla signed"]}, "3646fa3621": {"title": "Fix tensorpipe test (#55979)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55979\n\nFix name used for this test\nghstack-source-id: 126465107\n\nTest Plan: CI\n\nReviewed By: pbelevich, H-Huang\n\nDifferential Revision: D27755320\n\nfbshipit-source-id: fead989041d703d473b6847ee0cee1deebe12571", "pr_number": "55979", "files_changed": ["torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "9f89b53d7d": {"title": "Synchronize RRef.to_here() CUDA Streams properly (#54932)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54932\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D27684022\n\nPulled By: pbelevich\n\nfbshipit-source-id: 2bae51ab6649258d0219ca4e9dbbf45ac6a76c28", "pr_number": "54932", "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/request_callback.cpp", "torch/csrc/distributed/rpc/request_callback.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_impl.h", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.h", "torch/csrc/distributed/rpc/utils.h", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "67dcd62310": {"title": "Don't split oversize cached blocks (#44742)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/35901\n\nThis change is designed to prevent fragmentation in the Caching Allocator.  Permissive block splitting in the allocator allows very large blocks to be split into many pieces.  Once split too finely it is unlikely all pieces will be 'free' at that same time so the original allocation can never be returned.   Anecdotally, we've seen a model run out of memory failing to alloc a 50 MB block on a 32 GB card while the caching allocator is holding 13 GB of 'split free blocks'\n\nApproach:\n\n- Large blocks above a certain size are designated \"oversize\".  This limit is currently set 1 decade above large, 200 MB\n- Oversize blocks can not be split\n- Oversize blocks must closely match the requested size (e.g. a 200 MB request will match an existing 205 MB block, but not a 300 MB block)\n- In lieu of splitting oversize blocks there is a mechanism to quickly free a single oversize block (to the system allocator) to allow an appropriate size block to be allocated.  This will be activated under memory pressure and will prevent _release_cached_blocks()_ from triggering\n\nInitial performance tests show this is similar or quicker than the original strategy.  Additional tests are ongoing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44742\n\nReviewed By: ngimel\n\nDifferential Revision: D23752058\n\nPulled By: ezyang\n\nfbshipit-source-id: ccb7c13e3cf8ef2707706726ac9aaac3a5e3d5c8", "pr_number": "44742", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "docs/source/notes/cuda.rst", "torch/csrc/cuda/Module.cpp", "torch/cuda/memory.py", "torch/utils/collect_env.py"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "b1d17bc55f": {"title": "Added OpInfo for torch.sum (#55406)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55406\n\nReviewed By: mrshenli\n\nDifferential Revision: D27620593\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 73f0a1890d3a92c5374470610dce086a868763b3", "pr_number": "55406", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "2587a28bbd": {"title": "Improve the instructions on how to build the docs (#56018)", "body": "Summary:\nThis PR includes:\n\n- A formatting change to make katex installation instructions more visible for Facebook employees.\n- A short tip about how to start a lightweight HTTP server on a remote machine to browse the doc build artifacts.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56018\n\nReviewed By: H-Huang\n\nDifferential Revision: D27765157\n\nPulled By: cbalioglu\n\nfbshipit-source-id: 67663de0ba7b742e0deb5358d1e45eea9edd840f", "pr_number": "56018", "files_changed": ["CONTRIBUTING.md"], "labels": ["Merged", "cla signed"]}, "444b318a90": {"title": "ns for fx: add linear-relu mod weight extraction (#55080)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55080\n\nAdds support for extracting weights of linear-relu module pattern.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D27474701\n\nfbshipit-source-id: 69ceaadc28d7fdcebd16d519367274d348b0dd29", "pr_number": "55080", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "37a404610f": {"title": "ns for fx: add allowlist for ops with same signature across dtypes (#55154)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55154\n\nAdds functionality to NS to allow matching nodes which have the\nsame signature across dtypes.  For now, only the skeleton is added,\nwe can fill out the rest of the ops later.  This is to unblock\nthe work to change `cat` to have the same signature for fp32 and int8,\nand keep the testing we have for `cat` in NS.\n\nFor context, the main reason we are not matching nodes with equal types,\nfor now, is user defined types for which we do not know the signature.\nFor now, the design is strictly allowlist of everything. In the future,\nwe may adjust the design to safely match user defined types.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_ops_with_same_fp32_and_int8_signature\npython test/test_quantization.py TestFXGraphMatcher.test_nodes_with_equal_types_do_not_get_matched\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D27504624\n\nfbshipit-source-id: 4f8eb4f3258caf6f99aa373ca7ba516ebbcf4779", "pr_number": "55154", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "1fb2abc7ad": {"title": "ns for fx: rename SugraphTypeRelationship to SubgraphTypeRelationship (#55155)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55155\n\nFixes typo in enum name, no logic change\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27504625\n\nfbshipit-source-id: 21605dadb48225987f1da5ad5f6c30b0183278f2", "pr_number": "55155", "files_changed": ["torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "13d7b40ea0": {"title": "ns for fx: add F.conv2d and F.conv3d weight extraction (#55287)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55287\n\nAdds support for extracting weights from F.conv2d and F.conv3d.\nF.conv1d and the fused variants are saved for future PRs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_conv_fun\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27575424\n\nfbshipit-source-id: e945912d7d0ab320f47cab30d00d60ddb7497158", "pr_number": "55287", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "457fac0a33": {"title": "ns for fx: move more weight matching logic to weight_utils.py (#55288)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55288\n\nNo logic change, just moving util-like code to the utils file.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27575423\n\nfbshipit-source-id: cd5188a0940bb664be7d0275faa7df8ea18401a8", "pr_number": "55288", "files_changed": ["torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/ns_types.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "8b992ab0e4": {"title": "ns for fx: add conv1d weight extraction (#55327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55327\n\nAdds NS functionality for extracting weights from `F.conv1d` nodes.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_conv_fun\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27575425\n\nfbshipit-source-id: 65fa194802ac7a9fb75b7616d962c5c2e71321ff", "pr_number": "55327", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/weight_utils.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "784ae23d43": {"title": "ns for fx: fix bug in weight extraction testing (#55431)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55431\n\nFixes a bug in the test cases, returning early resulted\nin some tests not being run.  Adds logic for `nni.LinearReLU`,\nwhich was unmasked by making the tests run\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_mod\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27622415\n\nfbshipit-source-id: 79d9e3125e5d881d9d13645abbe4bd007a5e1d44", "pr_number": "55431", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "1ea95fa5b2": {"title": "ns for fx: add test case for linear dynamic (#55432)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55432\n\nAs titled.\n\nTest Plan:\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_dynamic\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27622416\n\nfbshipit-source-id: 319cfc0401e843006cafe4c6a272cb4d7462db18", "pr_number": "55432", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "8188d18f8d": {"title": "ns for fx: add functional conv-relu fusion support (#55433)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55433\n\nMakes `F.conv{n}d -> F.relu` patterns work for NS weight\nextraction.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_conv_fun\n```\n\nImported from OSS\n\nReviewed By: hx89\n\nDifferential Revision: D27622417\n\nfbshipit-source-id: d3ee08bd19865874cff3776c3b69e232fdfc5912", "pr_number": "55433", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "1688a5d31a": {"title": "Cleanup since FEATURE_TORCH_MOBILE is always true. (#55835)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55835\n\nNow that https://github.com/pytorch/pytorch/pull/55238 is landed for a\nweek and no complains. It seems safe to say FEATURE_TORCH_MOBILE is\nalways true and we can do some cleanup.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang, walterddr\n\nDifferential Revision: D27721284\n\nPulled By: ailzhang\n\nfbshipit-source-id: 4896bc5f736373d0922cfbe8eed0d16df62f0fa1", "pr_number": "55835", "files_changed": ["BUILD.bazel", "CMakeLists.txt", "aten/src/ATen/VmapMode.cpp", "aten/src/ATen/core/grad_mode.cpp", "aten/src/ATen/core/interned_strings.h", "c10/macros/cmake_macros.h.in"], "labels": ["Merged", "cla signed"]}, "88c06d9dfc": {"title": "Add cuda device synchronization support in JIT (#55469)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55469\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27749077\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: bce3d331ab781cf3232b47b4f02ef504b9eadc7e", "pr_number": "55469", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_cuda.py", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/python/python_sugared_value.cpp", "torch/csrc/jit/runtime/register_cuda_ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3fe4718d16": {"title": "Add `padding_idx` argument to EmbeddingBag (#49237)", "body": "Summary:\nThis PR adds a `padding_idx` parameter to `nn.EmbeddingBag` and `nn.functional.embedding_bag`. As with `nn.Embedding`'s `padding_idx` argument, if an embedding's index is equal to `padding_idx` it is ignored, so it is not included in the reduction.\n\nThis PR does not add support for `padding_idx` for quantized or ONNX `EmbeddingBag` for opset10/11 (opset9 is supported). In these cases, an error is thrown if `padding_idx` is provided.\n\nFixes https://github.com/pytorch/pytorch/issues/3194\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49237\n\nReviewed By: walterddr, VitalyFedyunin\n\nDifferential Revision: D26948258\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 3ca672f7e768941f3261ab405fc7597c97ce3dfc", "pr_number": "49237", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/EmbeddingBag.h", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cuh", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/api/functional.cpp", "test/cpp/api/modules.cpp", "test/quantization/test_quantize_jit.py", "test/test_nn.py", "tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/nn/functional/embedding.h", "torch/csrc/api/include/torch/nn/modules/embedding.h", "torch/csrc/api/include/torch/nn/options/embedding.h", "torch/csrc/api/src/nn/modules/embedding.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/nn/functional.py", "torch/nn/functional.pyi.in", "torch/nn/modules/sparse.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py", "torch/overrides.py", "torch/testing/_internal/common_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "1127bab828": {"title": "Make GHA for consistency cancel_redundant_workflow return useful err msg (#55961)", "body": "Summary:\nThis way, the user gets more useful actionable results from the GHA.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55961\n\nTest Plan: CI\n\nReviewed By: samestep\n\nDifferential Revision: D27749013\n\nPulled By: janeyx99\n\nfbshipit-source-id: bb0edbcdab29ba8ef99005f6fcf52de6782b468d", "pr_number": "55961", "files_changed": [".github/scripts/ensure_consistent_workflows.py", ".github/scripts/generate_workflow_names.py", ".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "ba320cec6b": {"title": "Prepare for Azure Pipeline for multi-gpu tests (#55600)", "body": "Summary:\nPrevious PR: https://github.com/pytorch/pytorch/issues/52490\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55600\n\nReviewed By: albanD, seemethere\n\nDifferential Revision: D27667544\n\nPulled By: malfet\n\nfbshipit-source-id: f5843379807d8c95f3791d19ac0ab2d1973fa087", "pr_number": "55600", "files_changed": [".circleci/config.yml", ".circleci/regenerate.ps1", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".jenkins/pytorch/win-test-helpers/test_distributed.bat"], "labels": ["Merged", "cla signed", "open source"]}, "416c18b7c9": {"title": "Add a batch_first arg to Transformer / MHA modules (#55285)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/25100 #43112\n\nEDIT: pardon my inexperience since this is my first PR here, that I did not realize the doc should not have any trailing white spaces, and `[E712] comparison to False should be 'if cond is False:' or 'if not cond:'`, now both fixed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55285\n\nReviewed By: mruberry\n\nDifferential Revision: D27765694\n\nPulled By: jbschlosser\n\nfbshipit-source-id: c34774fa065d67c0ac130de20a54e66e608bdbf4", "pr_number": "55285", "files_changed": ["test/test_nn.py", "torch/nn/modules/activation.py", "torch/nn/modules/transformer.py", "torch/nn/quantizable/modules/activation.py"], "labels": ["Merged", "Reverted", "cla signed", "open source"]}, "087049000b": {"title": "Make c10 clang-tidy clean (#55870)", "body": "Summary:\nThis change was autogenerated by running:\n```\n% find c10 -iname \"*.cpp\" -exec python3 tools/clang_tidy.py -c build -x {} -s \\;\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55870\n\nReviewed By: janeyx99\n\nDifferential Revision: D27728617\n\nPulled By: malfet\n\nfbshipit-source-id: bede4d7f0c106d51394d1e9efddf01bf894421c5", "pr_number": "55870", "files_changed": ["c10/benchmark/intrusive_ptr_benchmark.cpp", "c10/core/Allocator.cpp", "c10/core/CPUAllocator.cpp", "c10/core/CopyBytes.cpp", "c10/core/DefaultDtype.cpp", "c10/core/GeneratorImpl.cpp", "c10/core/InferenceMode.cpp", "c10/core/TensorImpl.cpp", "c10/core/UndefinedTensorImpl.cpp", "c10/core/impl/DeviceGuardImplInterface.cpp", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/core/impl/SizesAndStrides.cpp", "c10/core/thread_pool.cpp", "c10/mobile/CPUCachingAllocator.cpp", "c10/mobile/CPUProfilingAllocator.cpp", "c10/test/core/CompileTimeFunctionPointer_test.cpp", "c10/test/core/DeviceGuard_test.cpp", "c10/test/core/DispatchKeySet_test.cpp", "c10/test/core/impl/InlineDeviceGuard_test.cpp", "c10/test/core/impl/InlineStreamGuard_test.cpp", "c10/test/core/impl/SizesAndStrides_test.cpp", "c10/test/util/Array_test.cpp", "c10/test/util/Bitset_test.cpp", "c10/test/util/ConstexprCrc_test.cpp", "c10/test/util/Half_test.cpp", "c10/test/util/LeftRight_test.cpp", "c10/test/util/Metaprogramming_test.cpp", "c10/test/util/TypeIndex_test.cpp", "c10/test/util/TypeList_test.cpp", "c10/test/util/TypeTraits_test.cpp", "c10/test/util/accumulate_test.cpp", "c10/test/util/bfloat16_test.cpp", "c10/test/util/either_test.cpp", "c10/test/util/exception_test.cpp", "c10/test/util/flags_test.cpp", "c10/test/util/intrusive_ptr_test.cpp", "c10/test/util/irange_test.cpp", "c10/test/util/logging_test.cpp", "c10/test/util/optional_test.cpp", "c10/test/util/ordered_preserving_dict_test.cpp", "c10/test/util/registry_test.cpp", "c10/test/util/string_view_test.cpp", "c10/test/util/tempfile_test.cpp", "c10/test/util/typeid_test.cpp", "c10/util/Exception.cpp", "c10/util/Logging.cpp", "c10/util/MathConstants.cpp", "c10/util/SmallVector.cpp", "c10/util/ThreadLocalDebugInfo.cpp", "c10/util/Type.cpp", "c10/util/flags_use_no_gflags.cpp", "c10/util/intrusive_ptr.h", "c10/util/numa.cpp", "c10/util/typeid.cpp"], "labels": ["Merged", "cla signed"]}, "c8cf9114bf": {"title": "Include short test suites ln total_seconds stat (#56040)", "body": "Summary:\nUp until this PR, the top-level `total_seconds` stat we've been uploading to S3 has only included suites longer than one second. This PR corrects that issue, and also clarifies the script's textual output for \"longest tests of entire run\".\n\n(Note that the `total_time` local variable is passed as the `total_seconds` parameter in the call to `assemble_s3_object`.)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56040\n\nTest Plan:\nCreate a simple test file (call it `test_quick_maths.py`) with these contents:\n\n```py\nfrom torch.testing._internal.common_utils import TestCase, run_tests\n\nclass TestQuickMaths(TestCase):\n    def test_two_plus_two(self):\n        self.assertEqual(2 + 2, 4)\n\nif __name__ == '__main__':\n    run_tests()\n```\n\nRun it and save the test results:\n\n```sh\nrm -r /tmp/reports ; python3 test_quick_maths.py --save-xml=/tmp/reports\n```\n\nThen display them using the script:\n\n```sh\ntools/print_test_stats.py /tmp/reports\n```\n\n- Before this PR:\n\n  ```\n  No scribe access token provided, skip sending report!\n  Total runtime is 0:00:00\n  0 longest tests of entire run:\n  ```\n\n- With this PR:\n\n  ```\n  No scribe access token provided, skip sending report!\n  Total runtime is 0:00:00.108000\n  0 longest tests of entire run (ignoring suites totaling less than 1.0 seconds):\n  ```\n\nIf you were to upload this to S3 (see https://github.com/pytorch/pytorch/issues/49190 for an example of how to do this manually), the top-level `total_seconds` field should also change from `0` to `0.108`.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27770666\n\nPulled By: samestep\n\nfbshipit-source-id: 8255a4726ab3a692bbeff4c48974fbb3c6375142", "pr_number": "56040", "files_changed": ["tools/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "fd15557ccc": {"title": "breakup autograd documentation (#55672)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/52256\n\nUse autosummary instead of autofunction to create subpages for autograd functions. I left the autoclass parts intact but manually laid out their members.\n\nAlso the Latex formatting of the spcecial page emitted a warning (solved by adding `\\begin{align}...\\end{align}`) and fixed alignment of equations (by using `&=` instead of `=`).\n\nzou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55672\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27736855\n\nPulled By: zou3519\n\nfbshipit-source-id: addb56f4f81c82d8537884e0ff243c1e34969a6e", "pr_number": "55672", "files_changed": ["docs/source/autograd.rst", "torch/autograd/__init__.py", "torch/special/__init__.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "40d74e6f71": {"title": "breakup optim, cuda documentation (#55673)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/52256\n\nUse autosummary instead of autofunction to create subpages for optim and cuda functions/classes.\n\nAlso fix some minor formatting issues in optim.LBFGS and cuda.stream docstings\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55673\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27747741\n\nPulled By: zou3519\n\nfbshipit-source-id: 070681f840cdf4433a44af75be3483f16e5acf7d", "pr_number": "55673", "files_changed": ["docs/source/cuda.rst", "docs/source/optim.rst", "torch/cuda/__init__.py", "torch/optim/lbfgs.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "48a7d69946": {"title": "Catch and ignore tracebacks for compilation errors (#55986)", "body": "Summary:\nThe Python traceback on a cmake invocation is meaningless to most developers, so this PR wraps it in a `try..catch` so we can ignore it and save scrolling through the 20-or-so lines.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55986\n\nPulled By: driazati\n\nReviewed By: wanchaol\n\nDifferential Revision: D27769304\n\nfbshipit-source-id: 5889eea03db098d10576290abeeb4600029fb3f2", "pr_number": "55986", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["Merged", "cla signed"]}, "bc86358cf5": {"title": "Make run_test.py work even if s3_stat_parser fails to import (#56039)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56039\n\nPython will try to eagerly resolve the name references even if\nthe import failed.  Quote them so that it doesn't.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D27770536\n\nPulled By: ezyang\n\nfbshipit-source-id: b111739289498f9bab856fb9424f3080efee4ee0", "pr_number": "56039", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed"]}, "0b8bd22614": {"title": "Fix bug with rebuilding extensions every import (#56015)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56015\n\nReviewed By: mruberry\n\nDifferential Revision: D27765934\n\nPulled By: ezyang\n\nfbshipit-source-id: 65cace951fce5f2284ab91d8bd687ac89a2311fb", "pr_number": "56015", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed", "open source"]}, "cc7fab6e9c": {"title": "Update pthreadpool (#55950)", "body": "Summary:\nThis updates pthreadpool to include [this commit](https://github.com/Maratyszcza/pthreadpool/commit/a134dd5d4cee80cce15db81a72e7f929d71dd413), which removes a bunch of deprecation warnings in the build.\n\nFixes https://github.com/pytorch/pytorch/issues/33760\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55950\n\nReviewed By: mruberry\n\nDifferential Revision: D27773417\n\nPulled By: driazati\n\nfbshipit-source-id: b4397787d882228bae47ddd8ccf628047466b904", "pr_number": "55950", "files_changed": ["third_party/pthreadpool"], "labels": ["Merged", "cla signed"]}, "1e225a5187": {"title": "Add a few InferenceMode test cases to the wall. (#55993)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55993\n\nTest Plan: Imported from OSS\n\nReviewed By: robieta\n\nDifferential Revision: D27769478\n\nPulled By: ailzhang\n\nfbshipit-source-id: 009592d64ef24e1cf7e977d02acf662eb841ca58", "pr_number": "55993", "files_changed": ["benchmarks/instruction_counts/definitions/standard.py"], "labels": ["Merged", "cla signed"]}, "70f5905565": {"title": "Add formulas and basic tests (#49098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49098\n\nRFC: https://github.com/pytorch/rfcs/pull/11\n\nThis PR adds:\n- Codegen support to define forward grad formulas and few manual formulas\n- Codegen support to automatically generate formulas as well as few usage\n- Tests for basic forward grad components\n\nCodegen generated examples.\nFor each of them, the only part that is changed is the if statement before the return checking for fw grad defined.\n\n- For manual entry:\n```yaml\n- name: max(Tensor self) -> Tensor\n  self: evenly_distribute_backward(grad, self, result)\n  result: max_forward(self_fw_grad, self, result)\n```\n\n```cpp\nTensor max(const Tensor & self) {\n  auto& self_ = unpack(self, \"self\", 0);\n  auto _any_requires_grad = compute_requires_grad( self );\n  std::shared_ptr<MaxBackward1> grad_fn;\n  if (_any_requires_grad) {\n    grad_fn = std::shared_ptr<MaxBackward1>(new MaxBackward1(), deleteNode);\n    grad_fn->set_next_edges(collect_next_edges( self ));\n    grad_fn->self_ = SavedVariable(self, false);\n  }\n  #ifndef NDEBUG\n  c10::optional<Storage> self__storage_saved =\n    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;\n  c10::intrusive_ptr<TensorImpl> self__impl_saved;\n  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();\n  #endif\n  auto tmp = ([&]() {\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n    return at::max(self_);\n  })();\n  auto result = std::move(tmp);\n  #ifndef NDEBUG\n  if (self__storage_saved.has_value())\n    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));\n  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());\n  #endif\n  if (grad_fn) {\n      set_history(flatten_tensor_args( result ), grad_fn);\n  }\n  throw_error_for_complex_autograd(result, \"max\");\n  if (isFwGradDefined(self)) {\n      auto self_fw_grad = toLegacyFwGrad(self);\n      auto self_primal = toLegacyPrimal(self);\n      auto result_new_fw_grad = max_forward(self_fw_grad, self_primal, result);\n      if (result_new_fw_grad.defined()) {\n        result.set_fw_grad(result_new_fw_grad, /* level */ 0, /* is_inplace_op */ false);\n      }\n  }\n  if (grad_fn) {\n    grad_fn->result_ = SavedVariable(result, true);\n  }\n  return result;\n}\n```\n\n- For element wise entry:\n```yaml\n- name: abs(Tensor self) -> Tensor\n  self: grad * self.sgn()\n  result: auto_element_wise\n```\n\n```cpp\nTensor abs(const Tensor & self) {\n  auto& self_ = unpack(self, \"self\", 0);\n  auto _any_requires_grad = compute_requires_grad( self );\n  std::shared_ptr<AbsBackward> grad_fn;\n  if (_any_requires_grad) {\n    grad_fn = std::shared_ptr<AbsBackward>(new AbsBackward(), deleteNode);\n    grad_fn->set_next_edges(collect_next_edges( self ));\n    grad_fn->self_ = SavedVariable(self, false);\n  }\n  #ifndef NDEBUG\n  c10::optional<Storage> self__storage_saved =\n    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;\n  c10::intrusive_ptr<TensorImpl> self__impl_saved;\n  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();\n  #endif\n  auto tmp = ([&]() {\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n    return at::abs(self_);\n  })();\n  auto result = std::move(tmp);\n  #ifndef NDEBUG\n  if (self__storage_saved.has_value())\n    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));\n  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());\n  #endif\n  if (grad_fn) {\n      set_history(flatten_tensor_args( result ), grad_fn);\n  }\n  throw_error_for_complex_autograd(result, \"abs\");\n  if (isFwGradDefined(self)) {\n      auto self_fw_grad = toLegacyFwGrad(self);\n      auto self_primal = toLegacyPrimal(self);\n      auto result_new_fw_grad = self_fw_grad * self_primal.sgn();\n      if (result_new_fw_grad.defined()) {\n        result.set_fw_grad(result_new_fw_grad, /* level */ 0, /* is_inplace_op */ false);\n      }\n  }\n  return result;\n}\n```\n- For linear entry:\n```yaml\n- name: clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\n  self: grad\n  result: auto_linear\n```\n\n```cpp\nTensor clone(const Tensor & self, c10::optional<MemoryFormat> memory_format) {\n  auto& self_ = unpack(self, \"self\", 0);\n  auto _any_requires_grad = compute_requires_grad( self );\n  std::shared_ptr<CloneBackward> grad_fn;\n  if (_any_requires_grad) {\n    grad_fn = std::shared_ptr<CloneBackward>(new CloneBackward(), deleteNode);\n    grad_fn->set_next_edges(collect_next_edges( self ));\n  }\n  #ifndef NDEBUG\n  c10::optional<Storage> self__storage_saved =\n    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;\n  c10::intrusive_ptr<TensorImpl> self__impl_saved;\n  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();\n  #endif\n  auto tmp = ([&]() {\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n    return at::clone(self_, memory_format);\n  })();\n  auto result = std::move(tmp);\n  #ifndef NDEBUG\n  if (self__storage_saved.has_value())\n    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));\n  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());\n  #endif\n  if (grad_fn) {\n      set_history(flatten_tensor_args( result ), grad_fn);\n  }\n  if (isFwGradDefined(self)) {\n      auto self_fw_grad = toLegacyFwGrad(self);\n      auto result_new_fw_grad = at::clone(self_fw_grad, memory_format);\n      if (result_new_fw_grad.defined()) {\n        result.set_fw_grad(result_new_fw_grad, /* level */ 0, /* is_inplace_op */ false);\n      }\n  }\n  return result;\n}\n```\n\n- For no entry:\n```yaml\n- name: angle(Tensor self) -> Tensor\n  self: angle_backward(grad, self)\n```\n\n```cpp\nTensor angle(const Tensor & self) {\n  auto& self_ = unpack(self, \"self\", 0);\n  auto _any_requires_grad = compute_requires_grad( self );\n  std::shared_ptr<AngleBackward> grad_fn;\n  if (_any_requires_grad) {\n    grad_fn = std::shared_ptr<AngleBackward>(new AngleBackward(), deleteNode);\n    grad_fn->set_next_edges(collect_next_edges( self ));\n    grad_fn->self_ = SavedVariable(self, false);\n  }\n  #ifndef NDEBUG\n  c10::optional<Storage> self__storage_saved =\n    self_.has_storage() ? c10::optional<Storage>(self_.storage()) : c10::nullopt;\n  c10::intrusive_ptr<TensorImpl> self__impl_saved;\n  if (self_.defined()) self__impl_saved = self_.getIntrusivePtr();\n  #endif\n  auto tmp = ([&]() {\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n    return at::angle(self_);\n  })();\n  auto result = std::move(tmp);\n  #ifndef NDEBUG\n  if (self__storage_saved.has_value())\n    AT_ASSERT(self__storage_saved.value().is_alias_of(self_.storage()));\n  if (self__impl_saved) AT_ASSERT(self__impl_saved == self_.getIntrusivePtr());\n  #endif\n  if (grad_fn) {\n      set_history(flatten_tensor_args( result ), grad_fn);\n  }\n  throw_error_for_complex_autograd(result, \"angle\");\n  TORCH_CHECK(!(isFwGradDefined(self)), \"Trying to use forward prop with angle that does not support it.\");\n  return result;\n}\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D25607505\n\nPulled By: albanD\n\nfbshipit-source-id: fe2315d587689af1cd5968536fa26c680b8b8829", "pr_number": "49098", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/autograd.py", "torch/csrc/autograd/FunctionsManual.h"], "labels": ["Merged", "Reverted", "cla signed"]}, "2e7e4d0795": {"title": "ci: Add job to ensure python2 setup.py compat (#56057)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56057\n\nJust to make sure we don't add anything there that'd break python 2 users from receiving the correct error message\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D27774120\n\nPulled By: seemethere\n\nfbshipit-source-id: e40a1a2672a69eed3b6e834b1acbb7a04c0adec1", "pr_number": "56057", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "50bd6a3640": {"title": "ci: Remove CUDA 10.1 builds (#56056)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56056\n\nSince internal systems as well as colab have updated beyond CUDA 10.1\nI'd say it's safe to remove CUDA 10.1 builds entirely\n\nAs mentioned in https://github.com/pytorch/pytorch/issues/55829#issuecomment-818236019\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D27772826\n\nPulled By: seemethere\n\nfbshipit-source-id: 1599bba26b73b909b2575130219e2708ade5654c", "pr_number": "56056", "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/config.yml", ".github/scripts/generate_binary_build_matrix.py"], "labels": ["Merged", "cla signed", "module: ci"]}, "ed03a0791e": {"title": "Change MessageType values from decimals to hexadecimals for readability (#55985)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55985\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27758101\n\nPulled By: pbelevich\n\nfbshipit-source-id: 45a7c4d1c4fea874bca7b96e7f2b699ce3a199e5", "pr_number": "55985", "files_changed": ["torch/csrc/distributed/rpc/message.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "6b8696172f": {"title": "Fixed some Clang-Tidy checks in Aten Context class (#55942)", "body": "Summary:\nClang-Tidy displayed that it's possible to make some methods static and const in Context class. So I made.\nIt also shows that it has some unused headers from standard libraries included, which i will fix with a next PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55942\n\nReviewed By: mruberry\n\nDifferential Revision: D27766213\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 4bd9b92c0b8e5c540ac94fbd2bdace64949946e3", "pr_number": "55942", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "44e2c2cdfb": {"title": "Add a lint for native_functions.yaml (#56059)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56059\n\nThe lint doesn't do very much, mostly it enforces that indentation\nis consistent.  The real point of the lint is to just make sure\nthat we can still do surgery on codemod with tools like ruamel,\nby reusing the configuration in this script.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D27774590\n\nPulled By: ezyang\n\nfbshipit-source-id: c26bc6c95a478bd9b86387b18de7e906e7d13193", "pr_number": "56059", "files_changed": [".github/scripts/lint_native_functions.py", ".github/workflows/lint.yml", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "1a116a9332": {"title": "[Static runtime] Add optimize_graph_output_memory flag (#55811)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55811\n\n- Added manage_graph_output_memory flag to opts (default false)\n- Added checking for flag dependency between enable_out_variant and optimize_graph_output_memory and optimize_memory\n- Minor refactoring for readability\n\nTest Plan: buck test mode/dev //caffe2/caffe2/fb/predictor:pytorch_predictor_test -- --exact 'caffe2/caffe2/fb/predictor:pytorch_predictor_test - PyTorchPredictor.StaticRuntime\n\nReviewed By: hlu1\n\nDifferential Revision: D27573780\n\nfbshipit-source-id: 28698657f686f27b8ad60e1276cdf17402d2cf91", "pr_number": "55811", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "55432982d2": {"title": "[OpInfo][take2] move matmul to OpInfo (#55947)", "body": "Summary:\nThis is a reland of https://github.com/pytorch/pytorch/issues/55543 after fixing bfloat16 issues.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55947\n\nReviewed By: mruberry\n\nDifferential Revision: D27765035\n\nPulled By: walterddr\n\nfbshipit-source-id: b27a769de7686777012194ebbc1f38fc5d4acb67", "pr_number": "55947", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "ci/all", "cla signed"]}, "f8d331b33b": {"title": "PyTorch Execution Graph Observers (#55957)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55957\n\nThis diff adds an execution graph observer that tracks all operators (dispatcher autograd, jit, user defined, etc.) and their inputs and outputs. The results are written to a temp JSON file which can be used for further analysis. This support various use cases, such as dependency analysis, performance optimizations, etc.\n\nSome minor refactoring of existing code for clarity and completeness.\n\nTest Plan:\nExample output:\n\n{F603167736}\n\n```\n=> buck build caffe2/torch/fb/observers:execution_graph_observer_runner --show-output\n\n=> buck-out/gen/caffe2/torch/fb/observers/execution_graph_observer_runner --pytorch_enable_execution_graph_observer=true --pytorch_execution_graph_observer_iter_label=\"## START ##\" --pytorch_execution_graph_observer_iter_target=3\nI0414 01:26:55.834039 1038798 ExecutionGraphObserver.cpp:408] Enabled PyTorch execution graph observer\nI0414 01:26:55.834717 1038798 ExecutionGraphObserver.cpp:411] Matching iteration start label: \"## START ##\"\nI0414 01:26:55.834940 1038798 ExecutionGraphObserver.cpp:423] Target iteration: 3\nI0414 01:26:55.835962 1038798 ExecutionGraphObserverRunner.cpp:50] Running test execution graph observer runner.\nI0414 01:26:55.836180 1038798 ExecutionGraphObserverRunner.cpp:51] iterations: 10\nI0414 01:26:55.836419 1038798 ExecutionGraphObserverRunner.cpp:52] output file name: /tmp/pytorch_execution_graph_1618388815_1038798_3.json\nI0414 01:26:56.246432 1038798 ExecutionGraphObserver.cpp:137] Writing PyTorch execution graph to: /tmp/pytorch_execution_graph_1618388815_1038798_3.json\nI0414 01:26:56.278715 1038798 ExecutionGraphObserver.cpp:314] PyTorch execution graph is written to file: /tmp/pytorch_execution_graph_1618388815_1038798_3.json\n```\n\nsee `/tmp/pytorch_execution_graph_[timestamp]_[process_id]_[iter_target].json`\n\nReviewed By: albanD\n\nDifferential Revision: D27238906\n\nfbshipit-source-id: 3eb717d7d512e2d51d3162e9995b1ccd18e5a725", "pr_number": "55957", "files_changed": ["torch/csrc/autograd/function.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "92a09fb87a": {"title": "Manual revert of D27369251 (#56080)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56080\n\nReviewed By: hansonw\n\nDifferential Revision: D27777498\n\nPulled By: Krovatkin\n\nfbshipit-source-id: f72ca725ceba3c1fbd54c30014ac001d4b35b9eb", "pr_number": "56080", "files_changed": ["test/jit/test_autodiff_subgraph_slicing.py", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c5f9e043e9": {"title": "Collect instruction counts (and wall times) for CI (#55428)", "body": "Summary:\nThis PR add a `--mode` flag and a script to collect microbenchmarks in a single JSON file. I also added a version check since benchmarks are expected to evolve; this also turned up a determinism bug in `init_from_variants`. (`set` is not ordered, unlike `dict`)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55428\n\nTest Plan:\nRun in CI\n\nCC: ngimel wconstab ezyang bhosmer\n\nReviewed By: mruberry\n\nDifferential Revision: D27775284\n\nPulled By: robieta\n\nfbshipit-source-id: c8c338fedbfb2860df207fe204212a0121ecb006", "pr_number": "55428", "files_changed": ["benchmarks/instruction_counts/applications/__init__.py", "benchmarks/instruction_counts/applications/ci.py", "benchmarks/instruction_counts/core/api.py", "benchmarks/instruction_counts/execution/work.py", "benchmarks/instruction_counts/main.py"], "labels": ["Merged", "cla signed"]}, "f5a7b2e641": {"title": "Put llvmMathExtras in c10 namespace (#55886)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55886\n\nWe've imported llvm's MathExtras header, but now that we want to also\ninclude LLVM (which includes its own MathExtras), we need to guard the c10\nversion appropriately (or interwine llvm more deeply with our build than just\nthe CPU fuser, which I'm not super excited about doing just yet).\nghstack-source-id: 126375067\n\nTest Plan: build\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27731038\n\nfbshipit-source-id: 7c136341d6b433b3876ee983820016df75c14dec", "pr_number": "55886", "files_changed": ["c10/util/llvmMathExtras.h"], "labels": ["Merged", "cla signed"]}, "1995640d86": {"title": "Fix compiler warnings in mkldnn Pooling (#56095)", "body": "Summary:\nAlso, add `-Werror` flag to prevent this regressions from happening in\nthe future\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56095\n\nReviewed By: walterddr\n\nDifferential Revision: D27781603\n\nPulled By: malfet\n\nfbshipit-source-id: 2a404788a965c380ff9feb72d0b2d967b131371f", "pr_number": "56095", "files_changed": ["aten/src/ATen/native/mkldnn/Pooling.cpp", "caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "81f181567a": {"title": "Add `USE_MAGMA` build flag (#55994)", "body": "Summary:\nMany model pipelines/workflows don't use MAGMA even though it is included in the build by default. Leaving MAGMA kernels out of the build can save 60+MB of GPU memory when loading `libtorch_cuda.so` (tested on V100, current upstream master).\n\nA current sharp corner of this flag is that toggling it when rebuilding requires `torch/include/THC/THCGeneral.h` to be *manually* deleted by the user, as even running `make clean` or `setup.py` with `--cmake` does not properly regenerate it with the appropriate substitution for `#cmakedefine USE_MAGMA`. Is there a way to force the regeneration of the header during a rebuild?\n\nCC malfet ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55994\n\nReviewed By: mruberry\n\nDifferential Revision: D27766287\n\nPulled By: malfet\n\nfbshipit-source-id: 93deca57befa0febb9c5b7875ecf0015c547d421", "pr_number": "55994", "files_changed": ["CMakeLists.txt", "cmake/Dependencies.cmake"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "75b6644a4c": {"title": "Add USE_NUMPY define only if PyTorch is compiled with Numpy (#56102)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55849\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56102\n\nReviewed By: driazati, mruberry\n\nDifferential Revision: D27784057\n\nPulled By: malfet\n\nfbshipit-source-id: 636a005e7f74a58d47188f18b74f7deb4afe5fcb", "pr_number": "56102", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "1e9c7ad4cb": {"title": "Add a test to measure `import torch` time (#56041)", "body": "Summary:\nThis PR adds a couple very simple tests which (as the code comment says) measure the time it takes to `import torch` and ask for the CUDA device count.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56041\n\nTest Plan:\n```\n$ rm -r /tmp/reports ; python3 test/test_import_time.py --save-xml=/tmp/reports\n\nRunning tests...\n----------------------------------------------------------------------\n..\n----------------------------------------------------------------------\nRan 2 tests in 1.855s\n\nOK\n\nGenerating XML reports...\n```\n```\n$ tools/print_test_stats.py /tmp/reports\nNo scribe access token provided, skip sending report!\nclass TestImportTime:\n    tests: 2 failed: 0 skipped: 0 errored: 0\n    run_time: 1.85 seconds\n    avg_time: 0.93 seconds\n    median_time: 0.93 seconds\n    2 longest tests:\n        test_time_cuda_device_count time: 1.10 seconds\n        test_time_import_torch time: 0.75 seconds\n\nTotal runtime is 0:00:01\n2 longest tests of entire run:\n    TestImportTime.test_time_cuda_device_count  time: 1.10 seconds\n    TestImportTime.test_time_import_torch  time: 0.75 seconds\n```\n\nReviewed By: driazati\n\nDifferential Revision: D27770908\n\nPulled By: samestep\n\nfbshipit-source-id: 01bbf5a339f41d3a1f493e6fa8c946ff7567daec", "pr_number": "56041", "files_changed": ["test/run_test.py", "test/test_import_time.py"], "labels": ["Merged", "cla signed"]}, "6eeffc64f1": {"title": "Port NumPy typing testing style to PyTorch (#54234)", "body": "Summary:\nThis is a follow-up PR of https://github.com/pytorch/pytorch/issues/52408 and includes the `pass/` and `fail/` directories.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54234\n\nReviewed By: walterddr\n\nDifferential Revision: D27681410\n\nPulled By: malfet\n\nfbshipit-source-id: e6817df77c758f4c1295ea62613106c71cfd3fc3", "pr_number": "54234", "files_changed": ["test/test_typing.py", "test/typing/fail/bitwise_ops.py", "test/typing/fail/creation_ops.py", "test/typing/fail/random.py", "test/typing/pass/creation_ops.py", "test/typing/pass/math_ops.py", "test/typing/reveal/tensor_constructors.py", "test/typing/reveal/tensor_sampling.py", "torch/quasirandom.py", "torch/random.py"], "labels": ["Merged", "cla signed", "module: typing", "open source"]}, "047164437e": {"title": "[TensorPipe] Prepare for new Pipe API. (#55820)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55820\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27648291\n\nfbshipit-source-id: e08db6e8c1f5f333ec355de29e25fbe552904b25", "pr_number": "55820", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "3802e577fb": {"title": "[TensorPipe] Use Descriptor::Tensor::sourceDevice in tensorpipe_agent. (#55821)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55821\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27661608\n\nfbshipit-source-id: fd241f073d8928528a749758c7d0f570dfeb677b", "pr_number": "55821", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "699b47cd2c": {"title": "Update use_deterministic_algorithms docs (#55413)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55086\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55413\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27759069\n\nPulled By: mruberry\n\nfbshipit-source-id: 16c0dc1dc6f80ddd4f131e5e91729bbda8850878", "pr_number": "55413", "files_changed": ["torch/__init__.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "84e6580b5f": {"title": "Use cusolver potrs as the backend of cholesky_inverse for batch_size == 1 on CUDA (#54676)", "body": "Summary:\nThis PR adds the functionality to use cusolver potrs as the backend of cholesky_inverse for batch_size == 1 on CUDA.\n\nCusolver `potri` is **not** used, because\n\n- it only returns the upper or lower triangular matrix as a result. Although the other half is zero, we may still need extra kernels to get the full Hermitian matrix\n- it's no faster than cusolver potrs in most cases\n- it doesn't have a batched version or 64-bit version\n\n`cholesky_inverse` dispatch heuristics:\n\n- If magma is not installed, or batch_size is 1, dispatch to `cusolverDnXpotrs` (64 bit) and `cusolverDn<T>potrs` (legacy).\n- Otherwise, use magma.\n\nSee also https://github.com/pytorch/pytorch/issues/42666 #47953\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54676\n\nReviewed By: ngimel\n\nDifferential Revision: D27723805\n\nPulled By: mruberry\n\nfbshipit-source-id: f65122812c9e56a781aabe4d87ed28b309abf93f", "pr_number": "54676", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "2f895f790a": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27789747\n\nfbshipit-source-id: ef4882e92d7755669083573c43ae6c5088bf01ab", "pr_number": null, "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": []}, "50057e560b": {"title": "[special] Add `i0e` (#54409)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/50345\n\nChanges:\n* Add `i0e`\n* Move some kernels from `UnaryOpsKernel.cu` to `UnarySpecialOpsKernel.cu` to decrease compilation time per file.\n\nTime taken by i0e_vs_scipy tests: around 6.33.s\n\n<details>\n\n<summary>Test Run Log</summary>\n\n```\n(pytorch-cuda-dev) kshiteej@qgpu1:~/Pytorch/pytorch_module_special$ pytest test/test_unary_ufuncs.py -k _i0e_vs\n======================================================================= test session starts ========================================================================\nplatform linux -- Python 3.8.6, pytest-6.1.2, py-1.9.0, pluggy-0.13.1\nrootdir: /home/kshiteej/Pytorch/pytorch_module_special, configfile: pytest.ini\nplugins: hypothesis-5.38.1\ncollected 8843 items / 8833 deselected / 10 selected\n\ntest/test_unary_ufuncs.py ...sss....                                                                                                                         [100%]\n\n========================================================================= warnings summary =========================================================================\n../../.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:73\ntest/test_unary_ufuncs.py::TestUnaryUfuncsCUDA::test_special_i0e_vs_scipy_cuda_bfloat16\n  /home/kshiteej/.conda/envs/pytorch-cuda-dev/lib/python3.8/site-packages/torch/backends/cudnn/__init__.py:73: UserWarning: PyTorch was compiled without cuDNN/MIOpen support. To use cuDNN/MIOpen, rebuild PyTorch making sure the library is visible to the build system.\n    warnings.warn(\n\n-- Docs: https://docs.pytest.org/en/stable/warnings.html\n===================================================================== short test summary info ======================================================================\nSKIPPED [3] test/test_unary_ufuncs.py:1182: not implemented: Could not run 'aten::_copy_from' with arguments from the 'Meta' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_copy_from' is only available for these backends: [BackendSelect, Named, InplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:56 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_4.cpp:8761 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_4.cpp:9348 [kernel]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:250 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n==================================================== 7 passed, 3 skipped, 8833 deselected, 2 warnings in 6.33s =====================================================\n```\n\n</details>\n\nTODO:\n* [x] Check rendered docs (https://11743402-65600975-gh.circle-artifacts.com/0/docs/special.html)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54409\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27760472\n\nPulled By: mruberry\n\nfbshipit-source-id: bdfbcaa798b00c51dc9513c34626246c8fc10548", "pr_number": "54409", "files_changed": ["BUILD.bazel", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/cpu/vec256/vsx/vec256_double_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_float_vsx.h", "aten/src/ATen/cpu/vml.h", "aten/src/ATen/native/Math.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/special.rst", "test/test_unary_ufuncs.py", "torch/csrc/api/include/torch/special.h", "torch/overrides.py", "torch/special/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "6350fcef83": {"title": "[testing] add `broadcasts_input` and verifies the behaviour for inplace_variant. (#55771)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55595\n\n* Add `broadcasts_input` attribute to SampleInput\n* Update test_variant_consistency_eager to verify that sample with `broadcasts_input==True` and inplace variant raises an error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55771\n\nReviewed By: jbschlosser, ngimel\n\nDifferential Revision: D27760530\n\nPulled By: mruberry\n\nfbshipit-source-id: feb0658730d4cff483848a5ade9512837a65c24c", "pr_number": "55771", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "b383b63550": {"title": "[ROCm] Updating ROCM_HOME handling for >ROCm 4.0 (#55968)", "body": "Summary:\n- This change is required to handle the case when hipcc is\n  updated to the latest using update-alternatives.\n- Update-alternatives support for few ROCm binaries is available\n  from ROCm 4.1 onwards.\n- This change doesnt not affect any previous versions of ROCm.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55968\n\nReviewed By: mruberry\n\nDifferential Revision: D27785123\n\nPulled By: ezyang\n\nfbshipit-source-id: 8467e468d8d51277fab9b0c8cbd57e80bbcfc7f7", "pr_number": "55968", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "1d49fd31c4": {"title": "[reland] Add formulas and basic tests (#56083)", "body": "Summary:\nReland of https://github.com/pytorch/pytorch/pull/49098\nSee original issue for details.\n\nThe only difference with previous PR is the fix of the _embedding_bag_dense_backward formula to stop declaring a backward formula for an argument that does not exists.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56083\n\nReviewed By: samestep\n\nDifferential Revision: D27778221\n\nPulled By: albanD\n\nfbshipit-source-id: 159ef91ca931ef2ccfbc3d1c46c7880c32919dc9", "pr_number": "56083", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/autograd.py", "torch/csrc/autograd/FunctionsManual.h"], "labels": ["Merged", "cla signed"]}, "e0f9a5fed8": {"title": "[BE] add test selector to test_testing (#55931)", "body": "Summary:\nThis is a reflection of recent failures in https://github.com/pytorch/pytorch/issues/55753 and https://github.com/pytorch/pytorch/issues/55522.\nWe are lacking a test to safeguard these test env var.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55931\n\nTest Plan:\n1. CI\n2. Run locally using `python test/test_testing.py -k test_filtering_env_var -v`\n  - gives failure on 2ca45cb9e8 and d0cd16899f\n  - passes on 159e1100bf and current master\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27747537\n\nPulled By: walterddr\n\nfbshipit-source-id: c88e1c818199c7838866037d702d4012cacf510e", "pr_number": "55931", "files_changed": ["test/test_testing.py", "torch/testing/_internal/common_device_type.py"], "labels": ["Merged", "cla signed"]}, "6daa1760d7": {"title": "Skip geqrf test if compiled without LAPACK (#56105)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55929\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56105\n\nReviewed By: walterddr\n\nDifferential Revision: D27785443\n\nPulled By: malfet\n\nfbshipit-source-id: 9701f693a71f77259c0a6371106e7185cc49a803", "pr_number": "56105", "files_changed": ["test/test_torch.py"], "labels": ["Merged", "cla signed"]}, "61725f15c0": {"title": "cleanup unused implicit argument of expand function (#56101)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56101\n\nReviewed By: mruberry\n\nDifferential Revision: D27783771\n\nPulled By: ngimel\n\nfbshipit-source-id: 73044461fc2d7bfab5e84eef87ff381f40a46bad", "pr_number": "56101", "files_changed": ["aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed"]}, "71a5314591": {"title": "Fix ScriptMethod dispatch on __torch_function__ (#56103)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56103\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27784142\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 555dcb7c3a98b8fb9e9ca9b499cafad54e819aa7", "pr_number": "56103", "files_changed": ["test/test_jit.py", "torch/csrc/jit/python/pybind_utils.h", "torch/fx/proxy.py"], "labels": ["Merged", "cla signed", "fx", "oncall: jit"]}, "9d3d169d2d": {"title": "Implement hardswish/hardsigmoid on MKLDNN tensors (#55218)", "body": "Summary:\nAdding hardwish and hardsigmoid improves mobilenetv3 by ~13%\n\n  | hardswish | base |\n-- | -- | -- | --\nrun 1 | 1305.032 | 1486.013 |\nrun 2 | 1290.142 | 1491.001 |\nrun 3 | 1305.51 | 1491.66 |\nrun 4 | 1308.788 | 1495.577 |\navg | 1302.368 | 1491.063 | 0.873449\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55218\n\nReviewed By: albanD\n\nDifferential Revision: D27701276\n\nPulled By: Krovatkin\n\nfbshipit-source-id: cde78da71d327e65461e80fbb6c3bb3429505410", "pr_number": "55218", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e8faf69739": {"title": "fix torch.pow type promotion issue (#54085)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54085\n\nFixes https://github.com/pytorch/pytorch/issues/50121.\n\nThis fixes two similar issues pointed out with the dtype that `torch.pow` performs its computation. Thanks ngimel for spotting the issues originally (comments [here](https://github.com/pytorch/pytorch/pull/53669#discussion_r594624355) and [here](https://github.com/pytorch/pytorch/pull/53669#discussion_r594719704))!\n\nBefore:\n```\n>>> torch.pow(2, torch.tensor([17], dtype=torch.uint8), out=torch.tensor([0]))\ntensor([0])\n>>> torch.pow(2, torch.tensor(17, dtype=torch.uint8), out=torch.tensor(0))\ntensor(131072)\n>>> torch.pow(2, torch.tensor([17], dtype=torch.uint8, device='cuda'), out=torch.tensor([0], device='cuda'))\ntensor([131072], device='cuda:0')\n>>> torch.pow(2, torch.tensor(17, dtype=torch.uint8, device='cuda'), out=torch.tensor(0, device='cuda'))\ntensor(131072, device='cuda:0')\n```\n\nAfter:\n```\n>>> torch.pow(2, torch.tensor([17], dtype=torch.uint8), out=torch.tensor([0]))\ntensor([0])\n>>> torch.pow(2, torch.tensor(17, dtype=torch.uint8), out=torch.tensor(0))\ntensor(0)\n>>> torch.pow(2, torch.tensor([17], dtype=torch.uint8, device='cuda'), out=torch.tensor([0], device='cuda'))\ntensor([0], device='cuda:0')\n>>> torch.pow(2, torch.tensor(17, dtype=torch.uint8, device='cuda'), out=torch.tensor(0, device='cuda'))\ntensor(0, device='cuda:0')\n```\n\nIn all four cases above, `tensor(0, ...)` is the correct value because the computed \"common dtype\" among the inputs is expected to be `uint8`. Computing `2 ** 7` in uint8 will then overflow to zero. Finally, we cast the computed output to the output tensor's dtype, which is `int32`.\n\nThere were two separate issues fixed in this PR: one for cpu and one for cuda:\n* For CPU, The `pow(Scalar, Tensor)` overload wasn't calling `set_wrapped_number(true)` after wrapping the scalar in a Tensor, which caused the \"promoted\" scalar to incorrectly participate in type promotion (see the documented behavior [here](https://github.com/pytorch/pytorch/blob/aa8714dfedc73c67524e2394fe04d115f0783a09/c10/core/TensorImpl.h#L590))\n* For CUDA, the cuda kernels defined in `PowKernel.cu` were using the output's dtype to run the computation, instead of the common dtype.\n\nAs an aside: The CPU and CUDA kernels actually both use `iter.dtype()` instead of `iter.common_dtype()` to run the computation, which I fixed. The reason that only manifested here for CUDA is because TensorIterator has cpu-specific logic to create temporary outputs with the intermediate dtype (shown [here](https://github.com/pytorch/pytorch/blob/aa8714dfedc73c67524e2394fe04d115f0783a09/aten/src/ATen/TensorIterator.cpp#L349)). I'm not sure what the end state is there- I can imagine that being something we're more okay doing for cpu than for cuda, but it also leads to hard-to-track-down inconsistencies between the two like in this case.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27096330\n\nPulled By: bdhirsh\n\nfbshipit-source-id: a7e2909243851625cb3056d1e7abb2383bfe95f2", "pr_number": "54085", "files_changed": ["aten/src/ATen/ScalarOps.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/PowKernel.cu", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/test_binary_ufuncs.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed"]}, "82a7fff3cd": {"title": "Modify a few APIs to take/return const Tensor& instead of Tensor& (#55797)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55797\n\nIn all of these cases, the inside of the function didn't make use\nof the fact that the tensor was a mutable reference\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27712132\n\nPulled By: ezyang\n\nfbshipit-source-id: 99e0bb1d783f63d2d42ab53d3d406b2064405ef4", "pr_number": "55797", "files_changed": ["aten/src/ATen/TensorIndexing.cpp", "aten/src/ATen/TensorIndexing.h", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/NamedTensor.h", "aten/src/ATen/native/NamedTensor.cpp", "torch/csrc/utils/tensor_apply.cpp", "torch/csrc/utils/tensor_apply.h"], "labels": ["Merged", "cla signed"]}, "61418aa069": {"title": "Make THPVariable_Unpack work on THPVariable too (#55798)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55798\n\nI'm going to change how cdata is implemented internally, so I want to\nmake all callsites call through THPVariable_Unpack even if they\nactually have a THPVariable in hand\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27712131\n\nPulled By: ezyang\n\nfbshipit-source-id: bd2eb1e43c52c6b7a776ff3a45350a23934e643c", "pr_number": "55798", "files_changed": ["torch/csrc/autograd/python_variable.h"], "labels": ["Merged", "cla signed"]}, "6ec71ed4f9": {"title": "Replace all direct cdata access with THPVariable_Unpack (#55799)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55799\n\nI'm going to change the implementation of cdata soon so I need to\nabstract over cdata access with a function.  Additionally, many\nusers are casting manually casting to THPVariable to access\nthe member so I can remove these unsafe casts in the client code\n(the implementation, of course, is still doing an unsafe cast.)\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27712130\n\nPulled By: ezyang\n\nfbshipit-source-id: 95fcc013bf3913d67f2c634068eb5b3aab144cb3", "pr_number": "55799", "files_changed": ["aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/core/VariableHooksInterface.h", "aten/src/ATen/native/VariableMethodStubs.cpp", "aten/src/ATen/templates/TensorBody.h", "tools/autograd/gen_python_functions.py", "tools/autograd/templates/python_variable_methods.cpp", "tools/codegen/api/python.py", "torch/csrc/Generator.cpp", "torch/csrc/Size.cpp", "torch/csrc/autograd/python_cpp_function.cpp", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_hook.cpp", "torch/csrc/autograd/python_legacy_variable.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/python/python_arg_flatten.cpp", "torch/csrc/tensor/python_tensor.cpp", "torch/csrc/utils/pybind.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6c65ce8ee1": {"title": "Use THPVariable_Unpack in python_nccl (#56016)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56016\n\nMissed these because I don't build on CUDA\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27765124\n\nPulled By: ezyang\n\nfbshipit-source-id: aa202f594659d53c903b88c9d4a4cbb0e1c0b40a", "pr_number": "56016", "files_changed": ["torch/csrc/cuda/python_nccl.cpp"], "labels": ["Merged", "cla signed"]}, "1934725875": {"title": "Use cascade summation in nll_loss on CPU (#55841)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55657\n\nThis also avoids summing `total_weight_val` when weights aren't supplied. Avoiding accumulated error completely.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55841\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27751492\n\nPulled By: ngimel\n\nfbshipit-source-id: 2c2dc48f31c25dfa9db48693e3f765b179771a3c", "pr_number": "55841", "files_changed": ["aten/src/ATen/native/LossNLL.cpp", "aten/src/ATen/native/LossNLL2d.cpp", "aten/src/ATen/native/cpu/utils.h", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "a60dca8e80": {"title": "Make the script generate cancel_redundant_workflows.yml (#56092)", "body": "Summary:\nThis way, the user would just have to run the `regenerate_cancel_redundant_workflow.py` script to fix the inconsistency (instead of manual stuff).\n\nLots of the indentation changes were caused by regenerating the file, which I don't think is terrible, and ruamel.yaml did great at preserving comments and order and such!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56092\n\nReviewed By: samestep\n\nDifferential Revision: D27780877\n\nPulled By: janeyx99\n\nfbshipit-source-id: dd2996a88cd70a83d8daac33ba6659f93add8b92", "pr_number": "56092", "files_changed": [".github/scripts/ensure_consistent_workflows.py", ".github/scripts/regenerate_cancel_redundant_workflow.py", ".github/workflows/cancel_redundant_workflows.yml", ".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "49e5e284ea": {"title": "Additional annotations in fbcode/caffe2/torch/_jit_internal.py (#55855)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55855\n\nTest Plan: Sandcastle\n\nReviewed By: ezyang\n\nDifferential Revision: D27715202\n\nfbshipit-source-id: 99d59345a1915030f12441de91a6b7d4250a1f43", "pr_number": "55855", "files_changed": ["torch/_jit_internal.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "728d2e4e0f": {"title": "[BE] Speed up runtime of test_ddp_model_diff_across_ranks (#55659)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55659\n\nAs per https://github.com/pytorch/pytorch/issues/55583, this is the most expensive distributed test.\n\nInstead of waiting for process 0 in this test to be taken down by\nnccl_async_error_handling, just remove the barrier and let the process exit\nwhen the backend is NCCL.\n\nA slight downside here is that the test no longer verifies that the process\nwould be brought down by nccl_async_error_handling, but\nnccl_async_error_handling is already well tested in other tests. If we feel we\nneed to ensure this for this test, then we can pass in a process group with a\nsmaller timeout as an alternative solution.\n\nThe test now runs in 4-6s as opposed to 70. Ran the test 1000 times to verify\nno flakiness\nghstack-source-id: 126590904\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D27672161\n\nfbshipit-source-id: 38fb518606daac9b0390ca4c3ce1a72dc2da36fc", "pr_number": "55659", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "0a06d054d0": {"title": "Revert \"Only allow hub.load() from original repo. (#54451)\" (#56048)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56048\n\nThis reverts commit c411017a41988e9c5184279c1ec7dd7ef4e1a6fe.\n\nThis implementation broke CI in pytorch/vision and it's not handling\ntags properly. So I want to revert it first to unblock vision CI and\nsend out a proper fix later.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D27771701\n\nPulled By: ailzhang\n\nfbshipit-source-id: 932f9be72a1ae1816f4032643b3c2dde0cb7ae4c", "pr_number": "56048", "files_changed": ["test/test_utils.py", "torch/hub.py"], "labels": ["Merged", "cla signed"]}, "857d8264a7": {"title": "Skip RPC's CPU-only tests on CircleCI GPU jobs (#55778)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55778\n\nThe RPC suite takes very long to run, and most of it is CPU-only. As long as we run the CPU-only part on some CPU worker on CircleCI, we can skip it on the GPU workers (which are expensive and we should waste their time).\n\nghstack-source-id: 126270873\n\nTest Plan: Exported to CircleCI and checked that the CPU-only part still runs on the CPU workers but doesn't on the GPU workers.\n\nReviewed By: mrshenli\n\nDifferential Revision: D27705941\n\nfbshipit-source-id: a0a509d6e72cf69e417f4b48336df534b070a66d", "pr_number": "55778", "files_changed": ["test/distributed/rpc/test_faulty_agent.py", "test/distributed/rpc/test_process_group_agent.py", "test/distributed/rpc/test_tensorpipe_agent.py"], "labels": ["Merged", "ci/all", "cla signed", "oncall: distributed"]}, "6366658fbf": {"title": "Add OpInfo for torch.nansum (#55523)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55523\n\nReviewed By: agolynski\n\nDifferential Revision: D27796660\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: fea4d9dcccb7f4b9ba1b00079fb3899a8d20ba4b", "pr_number": "55523", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "6c327ef9d4": {"title": "matches_jit_signatures is dead (#53637)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53637\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26920687\n\nPulled By: ezyang\n\nfbshipit-source-id: 288bd9dca63da04ccc633d939833066a3305a68a", "pr_number": "53637", "files_changed": ["aten/src/ATen/native/README.md", "tools/codegen/gen.py", "tools/shared/cwrap_common.py"], "labels": ["Merged", "cla signed"]}, "5ed3be799d": {"title": "skip test_filtering_env_var for rocm (#56178)", "body": "Summary:\nROCM doesn't report the correct number of expected test device type. Skipping for now.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56178\n\nReviewed By: seemethere\n\nDifferential Revision: D27802139\n\nPulled By: walterddr\n\nfbshipit-source-id: 2e58df1a3ba2411e690be52babf946e284c4efcc", "pr_number": "56178", "files_changed": ["test/test_testing.py"], "labels": ["Merged", "cla signed", "module: rocm"]}, "63f83edcfb": {"title": "OpInfo porting for torch.real & torch.imag (#55134)", "body": "Summary:\nRelated https://github.com/pytorch/pytorch/issues/54298\n\nThis PR ports the method_tests() entries of torch.real & torch.imag to OpInfo.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55134\n\nReviewed By: agolynski\n\nDifferential Revision: D27793242\n\nPulled By: anjali411\n\nfbshipit-source-id: 0e9a987bfef16e78a1cda81ce14970993a59e467", "pr_number": "55134", "files_changed": ["test/test_autograd.py", "torch/testing/_core.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: complex", "open source", "triaged"]}, "06ea73942a": {"title": "[easy] Rename fb::jpeg_decode_to_NCHW to fb::image_decode_to_NCHW (#55857)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55857\n\nSince OpenCV supports more than just the JPEG file format.\n\nghstack-source-id: 126528422\n\nTest Plan: Build\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D27722865\n\nfbshipit-source-id: 6cf83bf187bb1fb3a28e3aa2a011959ef8925449", "pr_number": "55857", "files_changed": ["test/test_bundled_images.py"], "labels": ["Merged", "cla signed"]}, "16820bba5a": {"title": "[nnc][trivial] Trailing underscore style for llvmCode, asmCode members (#56118)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56118\n\nthat's it\nghstack-source-id: 126592595\n\nTest Plan: compile\n\nReviewed By: huiguoo\n\nDifferential Revision: D27781682\n\nfbshipit-source-id: 12728c279d0e02eb007093e18d9fc989456bea77", "pr_number": "56118", "files_changed": ["torch/csrc/jit/tensorexpr/llvm_codegen.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "33159b68a3": {"title": "Revert \"Deprecate legacy constructor `torch.Tensor()` (#54414)\" (#55831)", "body": "Summary:\nThis PR reverts https://github.com/pytorch/pytorch/pull/54414 because of https://github.com/pytorch/pytorch/issues/55780\n\ncc ysiraichi\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55831\n\nReviewed By: agolynski\n\nDifferential Revision: D27762264\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 8079a660cc440cafb9d22aa031d36dde121e13b3", "pr_number": "55831", "files_changed": ["docs/source/tensors.rst", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed"]}, "14d529a368": {"title": "Add support for refinement for torch.jit.Future (#56148)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56148\n\nFixes issue: #55787\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D27796830\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: b7a60218010793a54eb52d6b7602d333dc5a1c9e", "pr_number": "56148", "files_changed": ["test/test_jit.py", "torch/csrc/jit/frontend/script_type_parser.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "9bfe16a308": {"title": "should_check_autodiff is now should_autodiff_node (#56013)", "body": "Summary:\nThe name `should_check_autodiff` became `should_autodiff_node` but documentation did not change. The identifier is used in `test/test_jit.py`. It seems the file is too big for github to link to the line, but it is the return value from `normalize_check_ad`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56013\n\nReviewed By: agolynski\n\nDifferential Revision: D27800008\n\nPulled By: Lilyjjo\n\nfbshipit-source-id: 88a43c14c0f48fb3f94792e3fd6de2bd6a59a1a2", "pr_number": "56013", "files_changed": ["torch/csrc/jit/OVERVIEW.md", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "a3a75bd35e": {"title": "Add complex autograd support for `torch.cross` (#55854)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53512\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55854\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27737571\n\nPulled By: anjali411\n\nfbshipit-source-id: 38165b952cc4c9213d61c7d98b549b984c154927", "pr_number": "55854", "files_changed": ["test/test_linalg.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py"], "labels": ["Merged", "cla signed", "module: complex", "open source", "triaged"]}, "bb245b6444": {"title": "[optim] refactor adamax to use functional API (#55830)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55830\n\nghstack-source-id: 126325537\n\nReviewed By: driazati\n\nDifferential Revision: D26561017\n\nfbshipit-source-id: 41273d200e546d4ac08d39b57865d63c624f143a", "pr_number": "55830", "files_changed": ["torch/optim/_functional.py", "torch/optim/adamax.py"], "labels": ["Merged", "cla signed"]}, "8ef13cf976": {"title": "[optim] refactor rprop to use functional API (#55832)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55832\n\nghstack-source-id: 126325541\n\nReviewed By: driazati\n\nDifferential Revision: D27703877\n\nfbshipit-source-id: 34d4ce7b7d124c0cd75e2f6d0bc8f836713b7301", "pr_number": "55832", "files_changed": ["torch/optim/_functional.py", "torch/optim/rprop.py"], "labels": ["Merged", "cla signed"]}, "4e9e7200f2": {"title": "[dist_optim] Add distributed functional Adamax optimizer (#55833)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55833\n\nAdd distributed functional Adamax optimizer, to support in TorchScript\nghstack-source-id: 126325538\n\nReviewed By: rohan-varma\n\nDifferential Revision: D26696540\n\nfbshipit-source-id: 6242faebd2476847831a05df7f8b0d616f2b5355", "pr_number": "55833", "files_changed": ["torch/distributed/optim/functional_adamax.py", "torch/distributed/optim/optimizer.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "dd090e72b2": {"title": "[dist_optim] add distributed functional rprop optimizer (#55834)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55834\n\nghstack-source-id: 126325536\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27703878\n\nfbshipit-source-id: 5c8ec9a4ccb4442b2b51d48d75ea5cd506179f14", "pr_number": "55834", "files_changed": ["torch/distributed/optim/functional_rprop.py", "torch/distributed/optim/optimizer.py", "torch/testing/_internal/distributed/rpc/dist_optimizer_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "f02454f957": {"title": "Fix ChanelShuffle named tensor warnings (#55911)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54846\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55911\n\nReviewed By: agolynski\n\nDifferential Revision: D27798078\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 1ebd325ac8a21f82c395d2eafac7ef2ecd1f32b1", "pr_number": "55911", "files_changed": ["aten/src/ATen/native/ChanelShuffle.cpp", "test/test_nn.py", "torch/nn/modules/channelshuffle.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "e1752ffa04": {"title": "[reland][ROCm] use hiprtc precompiled header (#55965)", "body": "Summary:\nRevert \"Revert D27449031 (https://github.com/pytorch/pytorch/commit/2a7df657feef7534bfe4bce14da06dca29d38b0f): [pytorch][PR] [ROCm] use hiprtc precompiled header\".  Reland PR https://github.com/pytorch/pytorch/issues/54350.\n\nThis reverts commit 204ac21bf1457022caab197001788239720b96d6.\n\nThe original PR was reverted under suspicion that it was causing CI instability, but it was instead due to a hardware failure.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55965\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27755907\n\nPulled By: malfet\n\nfbshipit-source-id: 75bf0b9d888df3dee62f00a366b1123757e0474e", "pr_number": "55965", "files_changed": ["cmake/Dependencies.cmake", "cmake/public/LoadHIP.cmake", "torch/csrc/jit/codegen/cuda/executor.cpp", "torch/csrc/jit/codegen/cuda/executor_utils.cpp", "torch/csrc/jit/codegen/fuser/codegen.cpp", "torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp", "torch/csrc/jit/codegen/fuser/cuda/resource_strings.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp"], "labels": ["Merged", "cla signed", "module: rocm", "oncall: jit", "open source"]}, "8e82e932f3": {"title": "Reland: D27652485: [nnc] Enable CPU fusion only when num_threads == 1\" (#56120)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56120\n\nThis reverts commit ad17fadbfc786dc1ccb42e822208ff03c2a2b72c (D27786457).\n\nThe big annoyance here is that depending on the threading mode you may not be\nable to toggle num_threads at will, so the fusion tests won't fail.\n\nI hate this solution, but I'm adding a secondary override for the TE fuser.\nNow you need to both turn on fusion (_jit_override_can_fuse_on_cpu), and you're\nOK if you're running with 1 thread, or you can add\n`_jit_set_texpr_parallel_cpu_enabled` to enable it anyways.\n\nThis is (a) mainly for tests, since a real user probably won't fiddle aimlessly\nwith the thread count, and (b) will go away once NNC's threading support is\nfully baked.\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D27788199\n\nPulled By: bertmaher\n\nfbshipit-source-id: 070d04474f15e9689dbdf8cc1fde43050c6506b1", "pr_number": "56120", "files_changed": ["test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/jit/test_profiler.py", "test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.h", "torch/csrc/jit/python/init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ff1498e668": {"title": "Add cost inference for MulGradient operator", "body": "Summary: Add cost inference for MulGradient operator; also whitelist MulGradient in COMPUTE_OP_TYPES in dense_perf_estimation\n\nTest Plan: buck run //caffe2/caffe2/python/operator_test:elementwise_ops_test\n\nReviewed By: CrazySherman\n\nDifferential Revision: D27614003\n\nfbshipit-source-id: 30901e5e2b6ce7e2183c2362d1bf9f895046cf55", "pr_number": null, "files_changed": ["caffe2/operators/elementwise_ops_schema.cc"], "labels": []}, "3c4e1cd141": {"title": "remove annoying warnings from common_nn.py (#55982)", "body": "Summary:\n^^\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55982\n\nReviewed By: mruberry\n\nDifferential Revision: D27776380\n\nPulled By: Chillee\n\nfbshipit-source-id: 22b3a8de73416821bed56b75b68dca1c33a21250", "pr_number": "55982", "files_changed": ["torch/testing/_internal/common_nn.py"], "labels": ["Merged", "cla signed"]}, "cfc9716246": {"title": "Change all unary functions stubs to use TensorIteratorBase& (#56078)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56078\n\nThis is in preparation for making all unary functions structured.\nI don't actually have to make them structured yet as TensorIterator&\ncasts to TensorIteratorBase&\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27777768\n\nPulled By: ezyang\n\nfbshipit-source-id: 05a3a95f200698eef72c5c74fff85fe881e1c4a3", "pr_number": "56078", "files_changed": ["aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorFactories.h", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/DistributionTemplates.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/AbsKernel.cu", "aten/src/ATen/native/cuda/DistributionCauchyKernel.cu", "aten/src/ATen/native/cuda/DistributionExponentialKernel.cu", "aten/src/ATen/native/cuda/DistributionGeometricKernel.cu", "aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu", "aten/src/ATen/native/cuda/DistributionRandomKernel.cu", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/DistributionUniform.cu", "aten/src/ATen/native/cuda/UnaryComplexKernels.cu", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "aten/src/ATen/native/cuda/UnaryGeometricKernels.cu", "aten/src/ATen/native/cuda/UnaryLogKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu"], "labels": ["Merged", "cla signed"]}, "52f1a07b63": {"title": "Python API for Vitals (#53238)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53238\n\nThere is a tension for the Vitals design: (1) we want a macro based logging API for C++ and (2) we want a clean python API. Furthermore, we want to this to work with \"print on destruction\" semantics.\n\nThe unfortunate resolution is that there are (2) ways to define vitals:\n(1) Use the macros for local use only within C++ - this keeps the semantics people enjoy\n(2) For vitals to be used through either C++ or Python, we use a global VitalsAPI object.\n\nBoth these go to the same place for the user: printing to stdout as the globals are destructed.\n\nThe long history on this diff shows many different ways to try to avoid having 2 different paths... we tried weak pointers & shared pointers, verbose switch cases, etc. Ultimately each ran into an ugly trade-off and this cuts the difference better the alternatives.\n\nTest Plan:\nbuck test mode/dev caffe2/test:torch -- --regex vital\nbuck test //caffe2/aten:vitals\n\nReviewed By: orionr\n\nDifferential Revision: D26736443\n\nfbshipit-source-id: ccab464224913edd07c1e8532093f673cdcb789f", "pr_number": "53238", "files_changed": ["aten/src/ATen/core/Vitals.cpp", "aten/src/ATen/core/Vitals.h", "aten/src/ATen/test/vitals.cpp", "test/test_torch.py", "tools/build_variables.bzl", "torch/csrc/Module.cpp", "torch/overrides.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "1ca51f0fba": {"title": "[kineto] deprecate metdata args from ClientTraceActivity (#55988)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55988\n\nPull Request resolved: https://github.com/pytorch/kineto/pull/165\n\nas part of the ClientTraceActivity -> GenericTraceActivity migration, move all the metadata fields to JSON encoded string\n\nTest Plan:\n- `buck build`\n- tested with subsequent diffs\n\nReviewed By: gdankel\n\nDifferential Revision: D27340314\n\nfbshipit-source-id: f55b77a779e4bda1fb8667cb4e0f4252b93af5ea", "pr_number": "55988", "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "5ad3bc715c": {"title": "ns for fx: change node I/O determination to strict allowlist (#55434)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55434\n\nBefore this PR, there was some hacky logic which determined\nthe input and output types of nodes based on heuristics such\nas inspecting `__module__`, or assuming that an op has an\nI/O dtype of `torch.float` when the heuristics did not find\nany matches.  This is problematic because the heuristics were not exact,\nand this could result in non-sensical shadow graphs when the heuristics\nwould return an incorrect dtype.\n\nThis PR switches the dtype determination to an allowlist system,\nwhere we specify exactly what the dtypes are for the nodes or modules\nwhich are in an allowlist, and we add an `UNKNOWN` type for everything\nelse.  The shadow logic is changed to skip inserting shadows on any\nfunction or module where the I/O dtype is unknown.\n\nThe current allowlist only contains functions necessary for the\ncurrently existing tests.  Filling out the allowlist with all necessary\ntorch functions is left for a future PR.\n\nAs a result of this, we can do the following (also implemented in this PR):\n1. enable graph matching on nodes with equal types (for example,\nF.linear and F.linear). The restriction that only nodes with equal types\nwas in the code as a placeholder, it's better to allow comparisons of\nnodes of equal types. One case where this is useful is unshadowed\nactivations.\n2. enable models with user defined modules to be passed to Numeric Suite\nAPIs without errors.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\npython test/test_quantization.py TestFXGraphMatcherModels\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27622418\n\nfbshipit-source-id: 40dcba0222c01154c141467640c1eb89725f33a7", "pr_number": "55434", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "1cbc4023e9": {"title": "ns for fx: add qat handling for weight extraction (#55506)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55506\n\nMakes the NS weight extraction tests also test QAT, and fixes\nthe mappings where necessary to cover all the fusions and make\nthe tests pass.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_mod_ptq\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_extract_weights_mod_qat\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27650409\n\nfbshipit-source-id: c5bd9268d1bc559afc27d4c5109effd77bf1538a", "pr_number": "55506", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "f6a3936ab3": {"title": "ns for fx: extend functional weight extraction testing to QAT (#55507)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55507\n\nAs titled, extends the test cases for weight extraction from\nfunctionals to cover QAT.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27650408\n\nfbshipit-source-id: 8ce87d56bbc0da7c2330ece71a897d6d8c5110a0", "pr_number": "55507", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "37fbc069f1": {"title": "ns for fx: qat test cases for unshadowed activations (#55508)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55508\n\nAdds QAT test cases for unshadowed activation APIs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27650406\n\nfbshipit-source-id: bcbbdf1d32b8f8627c30d6aaf22607f34d1e2e08", "pr_number": "55508", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "84b5f67d9b": {"title": "ns for fx: add qat tests cases for shadowed activations (#55614)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55614\n\nAdds testing for shadowed activations APIs and QAT.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_add_shadow_loggers_mod_ptq\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_add_shadow_loggers_mod_qat\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_add_shadow_loggers_fun_ptq\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_add_shadow_loggers_qat_qat\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27650405\n\nfbshipit-source-id: c5138d98aa072e2927a54329c87e755413adeb5d", "pr_number": "55614", "files_changed": ["test/quantization/test_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "c8209a7336": {"title": "ns for fx: move pattern utils to separate file (#55805)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55805\n\nNo logic change, just moving util functions to separate file.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27719982\n\nfbshipit-source-id: c80d5397c1efeb9fc83eacaa532ecbde557cca3f", "pr_number": "55805", "files_changed": ["torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/pattern_utils.py"], "labels": ["Merged", "cla signed"]}, "f59244ec16": {"title": "ns for fx: add test for op relationship coverage (#55837)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55837\n\nAdds a test that checks that all of the relevant op pairs defined in\n`quantization_mappings.py` are also defined as related by Numerical\nSuite.\n\nNote: this does not cover all the ops, just the ones in\n`quantization_mappings.py`.  A future PR will fill out the remainder.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher.test_op_relationship_mapping\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27719979\n\nfbshipit-source-id: 9e852ef94da5f7a653ea15ba52c68a89c8e30208", "pr_number": "55837", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/pattern_utils.py", "torch/quantization/quantization_mappings.py"], "labels": ["Merged", "cla signed"]}, "400398006f": {"title": "[PARAM] Param comms debug info (#55976)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55976\n\n- Define a concrete `DebugInfo` to collect Param comms.\n- Add a macro to easily log `DebugInfo`\n\nTest Plan:\nTested on `ads:simplified_launcher` with `dyno gputrace`\nlocally tested in libkinetoObserver that it can collect the debug Infobase\n\nReviewed By: kingchc, ilia-cher\n\nDifferential Revision: D26773447\n\nfbshipit-source-id: a8eeede2d6dbf34d7a1b3614843b4a1baba94448", "pr_number": "55976", "files_changed": ["c10/util/ThreadLocalDebugInfo.h", "torch/lib/c10d/CMakeLists.txt", "torch/lib/c10d/ParamCommsUtils.cpp", "torch/lib/c10d/ParamCommsUtils.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "7d410bc3c8": {"title": ".github: Add initial linux CI workflow (#55176)", "body": "Summary:\nThis is a commandeer of https://github.com/pytorch/pytorch/issues/54091.\n\nTODO:\n\n- [x] understand why the build is [failing](https://github.com/pytorch/pytorch/pull/55176/checks?check_run_id=2254742265) here when it was [succeeding](https://github.com/pytorch/pytorch/pull/54091/checks?check_run_id=2177844748) on https://github.com/pytorch/pytorch/issues/54091\n- [x] fix the build failure\n- [x] fix the test failure(s)\n- [x] add CI check to generate YAML workflows from templates, similar to https://github.com/pytorch/pytorch/issues/55171\n- [ ] uncomment the rest of the matrix\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55176\n\nReviewed By: walterddr\n\nDifferential Revision: D27803529\n\nPulled By: seemethere\n\nfbshipit-source-id: 52a65ec8f7a83b929fed47f0bbdca544210ec9c2", "pr_number": "55176", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/scripts/install_nvidia_utils_linux.sh", ".github/scripts/report_git_status.sh", ".github/templates/linux_ci_workflow.yml.in", ".github/workflows/lint.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml", ".gitignore", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/common.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test.sh", "Makefile"], "labels": ["Merged", "Reverted", "cla signed"]}, "bd3c63aeeb": {"title": "[PyTorch Edge] Move torch::jit::mobile::_export_operator_list() from serialization/export_module.cpp to mobile/import.cpp (#56044)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56044\n\nWe want to be able to drop the dependence of full-jit deps in the auto-generated unit tests for 2 reasons:\n\n1. Running bloaty on the auto-generated unit tests should be somewhat representative of the actual size.\n2. The runtime environment of the auto-generated unit tests should be as close to the production environment as possible to ensure that we are running the tests in a production-like runtime.\n\nDue to the dependece on full-jit, we aren't there yet. For the auto-generated tests, we probably don't need to depend on `_export_operator_list()` evetually, but for now we do since it is used to decide whether the model being run is a Metal GPU model or a CPU model, and gates whether the test runs that model or not.\n\nEventually, we can stop doing this in the test and do it in the codegen from PTM-CLI instead (by fetching the operators from that tool, and writing out to the BUCK file which backend(s) this model is targeting). However, that will take some time to land, so in the spirit of expediency, this change is being proposed.\n\nDiscussed this offline with iseeyuan\nghstack-source-id: 126656877\n\nTest Plan: Build + BSB.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27694781\n\nfbshipit-source-id: f31a2dfd40803c02f4fd19c45a3cc6fb9bdf9697", "pr_number": "56044", "files_changed": ["torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import.h", "torch/csrc/jit/serialization/export.h", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4611387608": {"title": "[optim] take kw-only argument for functional optim APIs (#56185)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56185\n\nghstack-source-id: 126670123\n\nReviewed By: albanD\n\nDifferential Revision: D27802169\n\nfbshipit-source-id: f5e1cb2046dcdeecf5f6b0f70892828bf0adb22f", "pr_number": "56185", "files_changed": ["torch/distributed/optim/functional_adadelta.py", "torch/distributed/optim/functional_adagrad.py", "torch/distributed/optim/functional_adam.py", "torch/distributed/optim/functional_adamax.py", "torch/distributed/optim/functional_adamw.py", "torch/distributed/optim/functional_rmsprop.py", "torch/distributed/optim/functional_rprop.py", "torch/distributed/optim/functional_sgd.py", "torch/optim/_functional.py", "torch/optim/adadelta.py", "torch/optim/adagrad.py", "torch/optim/adam.py", "torch/optim/adamax.py", "torch/optim/adamw.py", "torch/optim/rmsprop.py", "torch/optim/rprop.py", "torch/optim/sgd.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d2d1112513": {"title": "Set ThreadLocalState correctly in the autograd engine (#56174)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56174\n\nevaluate_function:\n1. calls the autograd function (call_function)\n2. accumulates gradients into buffers\n\nPreviously, ThreadLocalStateGuard only covered part of `call_function`.\nHowever, it should cover all Tensor operations in `evaluate_function`,\nso this PR moves it to do so.\n\nOne alternative would have been to move ThreadLocalStateGuard to here:\nhttps://github.com/pytorch/pytorch/blob/71f9e99e293d0eff8da665b69543d044a6a4454d/torch/csrc/autograd/engine.cpp#L394\n\nUnfortunately that adds 2% additional instructions according to the\ninstruction count benchmark in the next section. This is because\n`evaluate_function` does an early return:\nhttps://github.com/pytorch/pytorch/blob/71f9e99e293d0eff8da665b69543d044a6a4454d/torch/csrc/autograd/engine.cpp#L732-L735\nIf this is preferred, please let me know.\n\nTest Plan:\n- run existing tests. It's hard to actually come up with a test case for\nthis.\n\nBenchmark plan:\n\nTL;DR: Instruction count decreases by a little after this PR.\n```\nimport torch\nfrom torch.utils.benchmark import Timer\n\ntimer = Timer(\n    stmt=\"\"\"\\\ntorch::autograd::grad({y}, {x}, {}, /*retain_grad=*/true);\"\"\",\n    setup=\"\"\"\\\nauto x = torch::ones({}, torch::requires_grad());\nauto y = x * 2;\"\"\",\n    language=\"cpp\")\n\nstats = timer.collect_callgrind()\nprint(stats)\n```\nThis gave the following:\n```\nBefore:\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7f4b28ce6a90>\ntorch::autograd::grad({y}, {x}, {}, /*retain_grad=*/true);\nsetup:\n  auto x = torch::ones({}, torch::requires_grad());\n  auto y = x * 2;\n\n                           All          Noisy symbols removed\n    Instructions:      3514184                    3514184\n    Baseline:                0                          0\n100 runs per measurement, 1 thread\n\nAfter:\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fdbc9d187d0>\ntorch::autograd::grad({y}, {x}, {}, /*retain_grad=*/true);\nsetup:\n  auto x = torch::ones({}, torch::requires_grad());\n  auto y = x * 2;\n\n                           All          Noisy symbols removed\n    Instructions:      3513884                    3513884\n    Baseline:                0                          0\n100 runs per measurement, 1 thread\n```\n\nReviewed By: albanD\n\nDifferential Revision: D27799283\n\nPulled By: zou3519\n\nfbshipit-source-id: 0a8213824e08c04748d38e66604c73f395285d63", "pr_number": "56174", "files_changed": ["torch/csrc/autograd/engine.cpp"], "labels": ["Merged", "cla signed"]}, "b405e2ce12": {"title": "Implicit conversion from null tensor to NoneType (#55823)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55823\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27717324\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: a071b90bcea9e8f2b5da633a8dadd11772fb5101", "pr_number": "55823", "files_changed": ["test/test_jit.py", "torch/jit/annotations.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "0a541e23e1": {"title": "[nn] Add allow_duplicate option for named_modules (#54812)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54812\n\nNeeded for quantization since different attribute might refer to the same module instance\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D27408376\n\nfbshipit-source-id: cada85c4a1772d3dd9502c3f6f9a56d690d527e7", "pr_number": "54812", "files_changed": ["test/test_nn.py", "torch/csrc/api/include/torch/python.h", "torch/distributed/nn/api/remote_module.py", "torch/nn/modules/module.py"], "labels": ["Merged", "cla signed"]}, "1a1b23f00c": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D27818592\n\nfbshipit-source-id: dc9d12a747464bb3c3d88bead606de6e9233b80c", "pr_number": null, "files_changed": ["torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": []}, "f236c27819": {"title": "Update Gloo submodule (#56189)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56189\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27814124\n\nPulled By: pbelevich\n\nfbshipit-source-id: cdea2db24634d9d171cac60709ef5135c099aabe", "pr_number": "56189", "files_changed": ["third_party/gloo"], "labels": ["Merged", "cla signed"]}, "e387bd780e": {"title": "Ignore envrc files (#56199)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56199\n\nReviewed By: ejguan\n\nDifferential Revision: D27821439\n\nPulled By: agolynski\n\nfbshipit-source-id: 4be7158d723c58f82b6ec56b3817932899e1b196", "pr_number": "56199", "files_changed": [".gitignore"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c5e80d30bf": {"title": "Harden \"Add annotations\" workflow (#56071)", "body": "Summary:\nResolves https://github.com/pytorch/pytorch/issues/55810 by closing some possible security holes due to using [GitHub Actions `${{ <expressions> }}`](https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions#about-contexts-and-expressions) in `.github/workflows/add_annotations.yml` and also patching a few other possible scenarios that could cause the workflow to fail by a PR passing a malformed artifact.\n\n- [x] flag and remove GitHub Actions expressions in JS scripts\n- [x] don't fail the workflow if the artifact doesn't look as expected\n- [x] write unit tests for `tools/extract_scripts.py`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56071\n\nTest Plan:\nI tested the end-to-end \"Lint\" and \"Add annotations\" system in a separate sandbox repo, including the following cases:\n\n- well-formed artifact\n- missing artifact\n- artifact containing a file named `linter-output.zip` (name clash)\n- artifact whose `commit-sha.txt` doesn't contain a 40-digit hex string\n- artifact whose `commit-sha.txt` contains a 40-digit hex string that isn't a valid Git hash for the current repo\n  - in this last case, the workflow does fail, but handling that is the responsibility of [pytorch/add-annotations-github-action](https://github.com/pytorch/add-annotations-github-action), not pytorch/pytorch\n\nTo run the new unit tests added in this PR:\n```\npython tools/test/test_extract_scripts.py\n```\n\nReviewed By: seemethere\n\nDifferential Revision: D27807074\n\nPulled By: samestep\n\nfbshipit-source-id: e2d3cc5437fe80ff03d46237ebba289901bc567c", "pr_number": "56071", "files_changed": [".github/workflows/add_annotations.yml", ".github/workflows/lint.yml", ".gitignore", "mypy-strict.ini", "tools/README.md", "tools/extract_scripts.py", "tools/test/test_extract_scripts.py"], "labels": ["Merged", "cla signed"]}, "f9b3dcba0d": {"title": "Store coverage.xml as artifact for windows test jobs (#56179)", "body": "Summary:\nCurrently, coverage stats is getting covered for sharded windows tests. This PR attempts to store the coverage.xml file as an artifact.\n\nI wonder what CircleCI will do when the artifacts don't exist (for nonsharded tests), and if we could conditionally store artifacts.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56179\n\nReviewed By: samestep\n\nDifferential Revision: D27800628\n\nPulled By: janeyx99\n\nfbshipit-source-id: 919f5696c0d7b4ee0d99969f35797f5be644c364", "pr_number": "56179", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed"]}, "119b3eccda": {"title": "Revert \"Revert D27598681: Add OpInfo tests for torch.addbmm\" (#55908)", "body": "Summary:\nThis reverts commit fd450ff1b93e4c498e7326cb35c7c26760c5ddbf.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55908\n\nReviewed By: agolynski\n\nDifferential Revision: D27800571\n\nPulled By: anjali411\n\nfbshipit-source-id: f04144afe7768872acb3fc2f5f242bb0093abc5e", "pr_number": "55908", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "48c6f0c25e": {"title": "Add OpInfo for torch.mean (#55525)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55525\n\nReviewed By: agolynski\n\nDifferential Revision: D27796651\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 6473d854f090ff62c856b404870f226f46569449", "pr_number": "55525", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "fe18144618": {"title": "Generalize HIP-specific launch bounds to apply to CUDA as well (#56143)", "body": "Summary:\nLaunch bounds for HIP were added along the way, but the smaller CUDA devices (like Jetson) also benefit from them.\nSo here I go over the HIP-specific launch bounds and try to generalize them to cover CUDA, too.\n\nThe long term goal is to eventually not need to resort to somewhat ad-hoc adaptations like the reduction of block size discussed in https://github.com/pytorch/pytorch/issues/8103, but have good coverage of our kernels with launch bound annotations.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56143\n\nReviewed By: agolynski\n\nDifferential Revision: D27804640\n\nPulled By: ngimel\n\nfbshipit-source-id: d4c345f9f7503e050a46361bfe2625865d0a42ba", "pr_number": "56143", "files_changed": ["aten/src/ATen/cuda/CUDAApplyUtils.cuh", "aten/src/ATen/native/cuda/Dropout.cu", "aten/src/ATen/native/cuda/MultinomialKernel.cu", "aten/src/ATen/native/cuda/SortingCommon.cuh", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/TensorTransformations.cu", "aten/src/ATen/native/cuda/TriangularOps.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDAApplyUtils.cuh", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/THC/THCApply.cuh", "aten/src/THC/THCReduceAll.cuh", "aten/src/THC/THCTensorScatterGather.cu", "aten/src/THCUNN/MultiLabelMarginCriterion.cu", "aten/src/THCUNN/SpatialClassNLLCriterion.cu"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "5ec6434945": {"title": "ns for fx: move op dtype category mapping to separate file (#55858)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55858\n\nMoves the mappings of input and output dtypes of various ops\ninto its own file, and makes the variable names more clear. No logic\nchange.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D27740662\n\nfbshipit-source-id: d384e7e542d9cc868d9cee9c53c2ac2f74a15a48", "pr_number": "55858", "files_changed": ["torch/quantization/ns/mappings.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "430fc03e3f": {"title": "ns for fx: add category for ops which accept fp32 or int8 input (#55859)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55859\n\nAdds mappings for ops which can accept either fp32 or int8 input,\nsuch as `F.relu`.  A future PR will fill out the op coverage.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_op_with_either_fp32_or_int8_input\n```\n\nImported from OSS\n\nReviewed By: raghuramank100\n\nDifferential Revision: D27740659\n\nfbshipit-source-id: cfc3dd58319b7161ca7f1fe05cd22d9a3ff11141", "pr_number": "55859", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "2380cc7d65": {"title": "ns for fx: fill out coverage for node I/O types (#55918)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55918\n\nAdds coverage for determining I/O dtype for various ops. This will\nenable shadowing of these ops.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_op_io_dtype_coverage\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27740661\n\nfbshipit-source-id: c5ce873ec56bffa50ca46d2fe134c70ed677e37e", "pr_number": "55918", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/mappings.py"], "labels": ["Merged", "cla signed"]}, "0fbc2be234": {"title": "ns for fx: enable `call_method` nodes in graph matching (#56194)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56194\n\nEnables the NS graph matcher to also match `call_method` nodes.\nThese are useful for ops such as `torch.sigmoid`.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher.test_methods\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27805333\n\nfbshipit-source-id: 509ae283db6b245671f11e3eb6b7fcb3a5735ef5", "pr_number": "56194", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/ns_types.py", "torch/quantization/ns/pattern_utils.py", "torch/quantization/ns/weight_utils.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "07f3eaa716": {"title": "ns for fx: remove deprecated code (#56195)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56195\n\nThis is outdated, removing (forgot to clean up in a previous PR).\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27805334\n\nfbshipit-source-id: 3b035945b4928a3c727e96e0f7fe0efe201f42c0", "pr_number": "56195", "files_changed": ["torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "6de5d13e0f": {"title": "ns for fx: make `call_method` nodes work in NS APIs (#56196)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56196\n\nEnables `call_method` nodes to work in NS APIs for unshadowed\nand shadowed activations.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_op_io_dtype_coverage\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_match_activations_meth_ptq\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_add_shadow_loggers_meth_ptq\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27805335\n\nfbshipit-source-id: 39b9c02c5c5faf098f2dd4f36d1ea8296d51a63c", "pr_number": "56196", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "ae0af8bb51": {"title": "ns for fx: move unmatchable mod/fun/meth mapping to mappings file (#56197)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56197\n\nNo logic change, just moving code around.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_op_io_dtype_coverage\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27805332\n\nfbshipit-source-id: 0a63cf6ef7e5c4f655cdd5a18d54cc988424ac80", "pr_number": "56197", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/mappings.py"], "labels": ["Merged", "cla signed"]}, "9f216b9499": {"title": "ns for fx: enable shadowing int8 to int8 (#56205)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56205\n\nAllows for int8 modules to shadow int8 modules. This is useful when\ncomparing quantized models with different qconfigs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_int8_shadows_int8\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27807405\n\nfbshipit-source-id: 10c3bc7ab9bb1e6808aa1af23a34c7cf380465fd", "pr_number": "56205", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py"], "labels": ["Merged", "cla signed"]}, "dd8bfe2b93": {"title": "Finish deprecation cycle for inplace view error checks (#56093)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50617\n\nAlso updates the relevant tests to expect errors instead of warnings\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56093\n\nReviewed By: agolynski\n\nDifferential Revision: D27806795\n\nPulled By: soulitzer\n\nfbshipit-source-id: 93c5c28edb1f97fa4457332c2ef4711f050ac81f", "pr_number": "56093", "files_changed": ["test/test_autograd.py", "test/test_nn.py", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "03cc9fabd4": {"title": "Added complex datatype support to sigmoid on cuda (#55975)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55359\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55975\n\nReviewed By: ezyang\n\nDifferential Revision: D27770438\n\nPulled By: prabhat00155\n\nfbshipit-source-id: 730193950805ce28d8672104fe446a647194e8cb", "pr_number": "55975", "files_changed": ["aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "bb35b066af": {"title": "Put `env` before `run` or `with` in GHA workflows (#56268)", "body": "Summary:\nAddresses seemethere's [comment](https://github.com/pytorch/pytorch/pull/56071#discussion_r614469633) on https://github.com/pytorch/pytorch/issues/56071.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56268\n\nTest Plan: CI.\n\nReviewed By: seemethere, ejguan\n\nDifferential Revision: D27823149\n\nPulled By: samestep\n\nfbshipit-source-id: 44d816abd85372b58c70bd81b189a0659a4079a4", "pr_number": "56268", "files_changed": [".github/workflows/add_annotations.yml", ".github/workflows/auto_label.yml", ".github/workflows/clang_format.yml", ".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "3e0744a1ae": {"title": "[sparsity] Moving only the C++ files from internal to OSS", "body": "Summary:\nThis splits the previous diff into multiple parts. This introduces only the c++ files.\n\nThe unittests pass as part of the internal build. Will be put in the OSS in the later PRs\n\nTest Plan:\n`buck test mode/opt //caffe2/torch/fb/model_optimization:sparsity_test`\n\n```\nParsing buck files: finished in 2.0 sec\nCreating action graph: finished in 16.4 sec\nBuilding: finished in 55.0 sec (100%) 20264/20264 jobs, 16 updated\n  Total time: 01:13.6 min\nMore details at https://www.internalfb.com/intern/buck/build/c9c5e69e-ce00-4560-adce-58b68bc43e47\nTpx test run coordinator for Facebook. See https://fburl.com/tpx for details.\nRunning with tpx session id: 1e678a07-0689-45b4-96f3-54d0a3181996\nTrace available for this run at /tmp/tpx-20210415-161113.966600/trace.log\nStarted reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/3096224795029304\n    \u2713 ListingSuccess: caffe2/torch/fb/model_optimization:sparsity_test - main (4.186)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (1.752)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseKernels) (1.884)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear_serdes (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (2.013)\nSummary\n  Pass: 3\n  ListingSuccess: 1\n```\n\nReviewed By: raghuramank100\n\nDifferential Revision: D27812204\n\nfbshipit-source-id: 6becaba3ab9cd054caf8b9bbae53af6d01347809", "pr_number": null, "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/ao_sparse/README", "aten/src/ATen/native/ao_sparse/library.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/ao_sparse/quantized/cpu/packed_params.h", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_unpack.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qnnpack_utils.h"], "labels": []}, "7629477ff7": {"title": "Filter out more expected errors from sccache log (#56281)", "body": "Summary:\nThis PR extends `.jenkins/pytorch/print_sccache_log.py` to filter out a distracting \"error\" message that walterddr came across while debugging failures in https://github.com/pytorch/pytorch/issues/55176:\n\n```\n=================== sccache compilation log ===================\nERROR 2021-04-05T15:44:18Z: sccache::server: Compilation failed: Output { status: ExitStatus(ExitStatus(256)), stdout: \"\", stderr: \"/var/lib/jenkins/.cache/torch_extensions/test_compilation_error_formatting/main.cpp: In function \u2018int main()\u2019:\\n/var/lib/jenkins/.cache/torch_extensions/test_compilation_error_formatting/main.cpp:2:23: error: expected \u2018;\u2019 before \u2018}\u2019 token\\n int main() { return 0 }\\n                       ^\\n\" }\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56281\n\nTest Plan: TODO (reviewers: is there an easy way to test this?)\n\nReviewed By: walterddr\n\nDifferential Revision: D27826064\n\nPulled By: samestep\n\nfbshipit-source-id: 7322a830c1246820a5b2b7bbeaa4697ebd13b617", "pr_number": "56281", "files_changed": [".jenkins/pytorch/print_sccache_log.py"], "labels": ["Merged", "cla signed"]}, "164bee1d09": {"title": "Return a CType instead of a string for returns, beef up CType (#55046)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55046\n\nUpdating `returns` in the codegen to return a CType instead of a raw string.\n\nThis has benefit of putting all stringifying logic through CType, which is useful in the followup PR when I add namespaces.\n\nI also added new CTypes for other templated C++ types: array, vector and tuple. Mostly because it makes the namespacing logic in the next PR significantly easier. It also seems more natural to me that `BaseCType` shouldn't represent specializations of templated types.\n\nThere's a little bit of weirdness, types that are currently *only* used for returns, i.e. `TupleCType`. Returns aren't named, so I opted not to give it one- so we can add it in later if we discover that we need it.\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27708348\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 230b210c3e53be1bd362105fbea8451055dc59a8", "pr_number": "55046", "files_changed": ["tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_trace_type.py", "tools/autograd/gen_variable_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/autograd.py", "tools/codegen/api/cpp.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/native.py", "tools/codegen/api/python.py", "tools/codegen/api/structured.py", "tools/codegen/api/types.py", "tools/codegen/dest/native_functions.py", "tools/codegen/dest/register_dispatch_key.py", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "947c7a8215": {"title": "add C++ namespacing logic to ctypes (#55047)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55047\n\nAdded namespaces to all of the `CTypes` printed in the codegen. This is pretty much required if we want to use codegen externally, since we can no longer assume that we're inside of the `at::` namespace.\n\nImportant changes are in `types.py`.\n\nHow do we add the notion of namespaces to C++ types without people having to write \"at::Tensor\" everywhere? Before this PR, `CType` held a raw string representing the type, i.e. `BaseCType(\"Tensor\", binds)`. This PR introduces a set of singleton base C++ types in `types.py`, that know how to print their namespace. Instead, we'd write `BaseCType(tensorT, binds)`, where printing `tensorT` will properly print out \"at::Tensor\".\n\nThis also means that you can't create arbitrary `CTypes`. If we need a new C++ type in the codegen, we need to add it to the list in `types.py`.\n\nOne blip in the design: we don't want to change `RegistrationDeclarations.yaml`, since that'll break external backends that ingest it. I added separate functions to display types without the namespace that are used to create RegistrationDeclarations.yaml`. With an external codegen API though, we can eventually kill it :)\n\nI also didn't realize until this PR that `Declarations.yaml` is still directly in use, by some python/autograd codegen. Rather than keep that yaml byte-for-byte compatible, I just updated the callsites in the autograd codegen to work with namespaces. In the NEXT pr, I try to clean up some of the autograd codegen to stop using raw strings to match against C++ types.\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27708349\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 56a4f81fc101795bcb9ee1f722121480fb2356ad", "pr_number": "55047", "files_changed": ["caffe2/contrib/aten/gen_op.py", "tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/cpp.py", "tools/codegen/api/native.py", "tools/codegen/api/python.py", "tools/codegen/api/structured.py", "tools/codegen/api/translate.py", "tools/codegen/api/types.py", "tools/codegen/dest/register_dispatch_key.py", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "eca98fedb5": {"title": "split out NamedCType from CType. Remove direct string comparison from autograd codegen (#55334)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55334\n\nThe goal of this PR is to clean up some of the autograd codegen to compare C++ types using `CType` objects instead of raw strings. My last PR in the stack made that string comparison a little more fragile, since the raw C++ strings needed to be namespace-aware.\n\nI confirmed byte-for-byte no codegen changes vs. the last PR (which added namespaces to the codegen) by running `diff -qr ../pytorch-common_test/torch/csrc/autograd/generated/ ../pytorch-callgrind_test_after2/torch/csrc/autograd/generated/` and `diff -qr ../pytorch-common_test/build/aten/src/ATen/ ../pytorch-callgrind_test_after2/build/aten/src/ATen/`\n\nNote that a better end-state for the autograd codegen would be to do all of its type pattern matching directly off of JIT types, instead of off of CType\u2019s (which are really just generated from JIT types, incorporating C++ specific semantics). That looks like it\u2019ll require a pretty substantial change though, so I\u2019m not doing it in this PR.\n\nAs part of this change (and after talking with ezyang), I split off the `CType` data class into a separate `NamedCType` class, which holds a name and a `CType`. This way, `CType` only knows about actual C++ types, making it easier to compare CType\u2019s to each other in the codegen when we only care about the type. The core change is in `types.py`, but it required a bunch of downstream changes to update all of the places where we create `CType`s to create `NamedCType`s instead.\n\nThe main change in the autograd codegen was that I updated `SavedAttribute` to store a `NamedCType`. The other autograd changes all pretty much came from that change.\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27708347\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 3e07c80569c7b229c638f389e76e319bff6315f9", "pr_number": "55334", "files_changed": ["tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/autograd.py", "tools/codegen/api/cpp.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/native.py", "tools/codegen/api/structured.py", "tools/codegen/api/translate.py", "tools/codegen/api/types.py", "tools/codegen/dest/register_dispatch_key.py"], "labels": ["Merged", "cla signed"]}, "0dc6e7ae38": {"title": "Move grad_mode.h/cpp to c10. (#56204)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56204\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27807139\n\nPulled By: ailzhang\n\nfbshipit-source-id: 2b693eb0a1034138d8bd68836528078ea5f38145", "pr_number": "56204", "files_changed": ["aten/src/ATen/core/grad_mode.cpp", "aten/src/ATen/core/grad_mode.h", "c10/core/GradMode.cpp", "c10/core/GradMode.h", "caffe2/c2_aten_srcs.bzl", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed"]}, "8d7faa2af8": {"title": "Update _torch_docs.py to close #56240. (#56242)", "body": "Summary:\nUpdate _torch_docs.py to close https://github.com/pytorch/pytorch/issues/56240.\nAdded the \"generator\" argument to the docs of torch.rand and torch.randn.\n\nFixes https://github.com/pytorch/pytorch/issues/56240\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56242\n\nReviewed By: ejguan\n\nDifferential Revision: D27821513\n\nPulled By: agolynski\n\nfbshipit-source-id: e42c431eddc7a83bd1c1ea368a2effbe3f10e92e", "pr_number": "56242", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "1ec12fd491": {"title": "Add minidump collection via breakpad (#55647)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55647\n\nThis adds [breakpad](https://github.com/google/breakpad) which comes with out-of-the-box utilities to register a signal handler that writes out a minidump on an unhandled exception. Right now this is gated behind a flag in `torch.utils`, but in the future it could be on by default. Sizewise this adds aboute 500k to `libtorch_cpu.so` (187275968 B to 187810016 B).\n\n```bash\n$ cat <<EOF > test.py\nimport torch\n\ntorch.utils.enable_minidump_collection()\n\n# temporary util that just segfaults\ntorch._C._crash()\nEOF\n\n$ python test.py\nWrote minidump to /tmp/pytorch_crashes/6a829041-50e9-4247-ea992f99-a74cf47a.dmp\nfish: \u201cpython test.py\u201d terminated by signal SIGSEGV (Address boundary error)\n$ minidump-2-core /tmp/pytorch_crashes/6a829041-50e9-4247-ea992f99-a74cf47a.dmp -o core.dmp\n$ gdb python core.dmp\n... commence debugging ...\n```\n\nRight now all exceptions that get passed up to Python don't trigger the signal handler (which by default only\nhandles [these](https://github.com/google/breakpad/blob/main/src/client/linux/handler/exception_handler.cc#L115)). It would be possible for PyTorch exceptions to explicitly write a minidump when passed up to Python (maybe only when the exception is unhandled or something).\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27679767\n\nPulled By: driazati\n\nfbshipit-source-id: 1ab3b5160b6dc405f5097eb25acc644d533358d7", "pr_number": "55647", "files_changed": ["caffe2/CMakeLists.txt", "test/test_cpp_extensions_jit.py", "tools/build_variables.bzl", "torch/CMakeLists.txt", "torch/csrc/Module.cpp", "torch/csrc/api/include/torch/utils.h", "torch/csrc/utils/crash_handler.cpp", "torch/csrc/utils/crash_handler.h", "torch/csrc/utils/init.cpp", "torch/csrc/utils/init.h", "torch/utils/__init__.py", "torch/utils/_crash_handler.py", "torch/utils/collect_env.py"], "labels": ["Merged", "cla signed"]}, "164de39a11": {"title": "Fix build failure due to namespace change for log_out and tanh_out (#56278)", "body": "Summary:\nThere is a build failure in `bench_approx.cpp` due to namespace change for log_out and tanh_out.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56278\n\nReviewed By: bertmaher, nikithamalgifb\n\nDifferential Revision: D27825621\n\nPulled By: navahgar\n\nfbshipit-source-id: 0bccd324af92a3460610bf475514449f0223de2b", "pr_number": "56278", "files_changed": ["benchmarks/cpp/tensorexpr/bench_approx.cpp"], "labels": ["Merged", "cla signed"]}, "5a9b1ddf3b": {"title": "fix the readme link (#56269)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56269\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D27824048\n\nPulled By: ejguan\n\nfbshipit-source-id: 8d5ecbdf502ae8bf8e807b55f6daeb3ff234aa62", "pr_number": "56269", "files_changed": ["c10/core/impl/README.md"], "labels": ["Merged", "cla signed", "open source"]}, "a6940aae37": {"title": "[19/n][torch/elastic][upstream] Replace pytorch.distributed.launch with torchelastic launcher (#56214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56214\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56037\n\nThe diff introduces new  `torch.distributed.elastic_launch` and removes internals of `torch.distributed.launch` keeping backwards compatibility.\n\nSince torchelastic and torch.launch are not fully compatible due to `--use_env` arg, the `torch.distributed.launch` deprecation is going to be iterative: as part of pytorch 1.9 we are going to deprecate it, and in the following releases we will remove `torch.distributed.launch`\n\nThe diff leaves `torchelastic.distributed.launch` module, and the follow up diffs will migrate the users form `torchelastic.distributed.launch` to `torch.distributed.elastic_launch`\n\nTest Plan: buck test mode/dev-nosan //pytorch/elastic/torchelastic/distributed/...\n\nReviewed By: H-Huang\n\nDifferential Revision: D27805799\n\nfbshipit-source-id: 599a4c0592fbc7a1bc1953040626dd6b72bac907", "pr_number": "56214", "files_changed": ["test/distributed/argparse_util_test.py", "torch/distributed/argparse_util.py", "torch/distributed/elastic/rendezvous/etcd_server.py", "torch/distributed/elastic_launch.py", "torch/distributed/launch.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "d4fad109e8": {"title": "Add OpInfo tests for torch.inner (#55536)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55536\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27650199\n\nPulled By: ejguan\n\nfbshipit-source-id: 5805f1ca25019fc57971e31659fac345646368b6", "pr_number": "55536", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "ci/all", "cla signed", "hackathon"]}, "1360980659": {"title": "Remove duplicate test due to rebasing mistake (#56287)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56287\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27828430\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: a5d846871ce78399409113fd5dbf2c43a4e46296", "pr_number": "56287", "files_changed": ["test/test_jit.py"], "labels": ["Merged", "cla signed"]}, "d30e31cfe6": {"title": "[20/n][torch/elastic][upstream] Move torchelastic.distributed.tests to pytorch.distributed (#56215)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56215\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56077\n\nMove torchelastic.distributed.tests to pytorch.distributed\n\nTest Plan: buck test mode/dev-nosan //pytorch/elastic/torchelastic/distributed/...\n\nReviewed By: H-Huang\n\nDifferential Revision: D27808887\n\nfbshipit-source-id: 6c9e2cba0bb202d8a5497697773d48e215e555f8", "pr_number": "56215", "files_changed": ["test/distributed/launcher/bin/test_script_local_rank.py", "test/distributed/launcher/elastic_launch_test.py", "test/distributed/launcher/launch_test.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "ce1380f9b5": {"title": "fixing Optional[Tensor] type in autodiff (#55565)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54783\n\nWe need to be extra careful with the pattern to legitimately use `unchecked_unwrap_optional` in autodiff.\nThis would at least allow us to start support `Optional[Tensor]` in autodiff, which is quite common in composite layers.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55565\n\nReviewed By: ejguan\n\nDifferential Revision: D27825336\n\nPulled By: Krovatkin\n\nfbshipit-source-id: a8562eb10ea741effff430d7417d313b1eb53dfe", "pr_number": "55565", "files_changed": ["test/jit/test_autodiff_subgraph_slicing.py", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/runtime/symbolic_script.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "643dd26389": {"title": "Fix formatting for the new language reference (#56042)", "body": "Summary:\nThis PR fixes the formatting issues in the new language reference\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56042\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27830179\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: bce3397d4de3f1536a1a8f0a16f10a703e7d4406", "pr_number": "56042", "files_changed": ["docs/source/jit.rst", "docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "83cfaf1a12": {"title": "[kineto] deprecate pthreadid (#56209)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56209\n\nPull Request resolved: https://github.com/pytorch/kineto/pull/172\n\nin this diff of the stack, we remove the threadId field from the ClientTraceActivity as our step towards the deprecation\n\nTest Plan: sandcastle builds to cover all the dependent targets\n\nReviewed By: ilia-cher\n\nDifferential Revision: D27662747\n\nfbshipit-source-id: 040ba040390680a0fc63ddc8149c6fad940439fc", "pr_number": "56209", "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "d312aeb6ac": {"title": "Implement faster gradcheck but not enabled for most things (#54480)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54480\n\nThis PR shouldn't really change the behavior of gradcheck for most ops. However, the changes in test_autograd allow us to run basic checks for both fast and slow (instead of previously just slow). All it should be doing is wrapping the preexisting tests we introduced in prior PRs in a function which takes `fast_mode` as a param. We then call this function twice, once with `fast_mode=True` and once with `fast_mode=False`.\n\nPlan for rollout:\n - This PR should only land the code (and runs some basic checks as described above).\n   - This should help us verify that a) slow is still working as expected b) basic functionality of fast works\n   - After we land this, but before we run the next PR in the stack, we should land https://github.com/pytorch/pytorch/pull/55182. This is to ensure that there is no gap where the slow tests aren't running.\n - The next PR is responsible for enabling the fast_mode=True flag on all tests (where the function has real inputs/outputs), and selectively disabling for the cases the fail.\n - Finally in a later PR, we reenable fast-gradcheck for functions w/ complex inputs/outputs\n\nTODOs and open questions (not necessarily blocking this PR):\n - ~How do we think about atol/rtol~ (scale atol, keep rtol as-is)\n - ~reenable fast-gradcheck for complex numbers~\n - ~when inputs are uncoalesced we don't truly test this case because we coalesce the inputs before calling function. Revisit this when https://github.com/pytorch/pytorch/pull/52874/files is landed~\n\n### Developer Experience\nSample output when jacobian mismatch occurs:\n```\nTraceback (most recent call last):\n  File \"/home/s/local/pytorch4/test/test_autograd.py\", line 4220, in test_gradcheck_jacobian_mismatch\n    check(fast_mode=True)\n  File \"/home/s/local/pytorch4/test/test_autograd.py\", line 4196, in check\n    gradcheck(fn, (x,), fast_mode=fast_mode)\n  File \"/home/s/local/pytorch4/torch/testing/_internal/common_utils.py\", line 2067, in gradcheck\n    return torch.autograd.gradcheck(fn, inputs, **kwargs)\n  File \"/home/s/local/pytorch4/torch/autograd/gradcheck.py\", line 1020, in gradcheck\n    if not fast_gradcheck(fail_test, seeded_func, func_out, tupled_inputs, outputs, eps, rtol,\n  File \"/home/s/local/pytorch4/torch/autograd/gradcheck.py\", line 915, in fast_gradcheck\n    return fail_test(get_notallclose_msg(a, n, i, j, prefix) + jacobians_str)\n  File \"/home/s/local/pytorch4/torch/autograd/gradcheck.py\", line 996, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor(0.9195)\nanalytical:tensor(0.9389)\n\nThe above quantities relating the numerical and analytical jacobians are computed\nin fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background\nabout fast mode. Below, we recompute numerical and analytical jacobians in slow mode:\n\nNumerical:\n tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 1.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 1.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 1.0000]])\nAnalytical:\ntensor([[1.0100, 0.0100, 0.0100, 0.0100],\n        [0.0100, 1.0100, 0.0100, 0.0100],\n        [0.0100, 0.0100, 1.0100, 0.0100],\n        [0.0100, 0.0100, 0.0100, 1.0100]])\n\nThe max per-element difference (slow mode) is: 0.010000000000054632.\n```\nAdditionally, if the per-element difference is small i.e., `allclose(analytical_slow, numerical_slow, rtol, atol) is True` we follow up with this message:\n```\nFast gradcheck failed but element-wise differences are small. This means that the\ntest might've passed in slow_mode!\n\nIf you are adding a new operator, please file an issue and then use one of the\nworkarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.\n\nIf the test\n- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck\n  with `fast_mode=False` as a keyword argument.\n- is OpInfo-based (e.g., in test_ops.py), then modify the OpInfo for the test\n  to have `gradcheck_fast_mode=False`\n- is a Module test (e.g., in common_nn.py), then modify the corresponding\n  module_test entry to have `gradcheck_fast_mode=False`\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr, ejguan\n\nDifferential Revision: D27825160\n\nPulled By: soulitzer\n\nfbshipit-source-id: 1fe60569d8b697c213b0d262a832622a4e9cf0c7", "pr_number": "54480", "files_changed": ["test/test_autograd.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "cd780e1c6e": {"title": "Move graph iterator to seperate utility file (#56211)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56211\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27828150\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: f91747fabde9caf864a62e4028fdc7bbbab7ee66", "pr_number": "56211", "files_changed": ["torch/csrc/jit/frontend/exit_transforms.cpp", "torch/csrc/jit/runtime/graph_iterator.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "5eadc243f3": {"title": "Preserve node meta info in split_module (#56212)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56212\n\nThe current design doesn't make it easy to use `node.copy()`. Explicitly copy over the node's meta.\n\nTest Plan: Updated `test_subgraph_creation` in `test_fx_experimental`\n\nReviewed By: jamesr66a\n\nDifferential Revision: D27808477\n\nfbshipit-source-id: 7fe7b6428c830307dbd1e395f16fa2774936d3b3", "pr_number": "56212", "files_changed": ["test/test_fx_experimental.py", "torch/fx/passes/split_module.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "72a93a6337": {"title": "Fix warnings in ivalue test (#56303)", "body": "Summary:\nSimple conversion of `std::unordered_map` -> `c10::Dict`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56303\n\nPulled By: driazati\n\nReviewed By: swolchok\n\nDifferential Revision: D27833970\n\nfbshipit-source-id: e08b852a20b1cabc1cef890cdcbacbd0d40a3a8a", "pr_number": "56303", "files_changed": ["aten/src/ATen/test/ivalue_test.cpp"], "labels": ["Merged", "cla signed"]}, "a43483586d": {"title": "A heuristic to avoid perf incompatible MKLDNN formats for binary ops (#56089)", "body": "Summary:\nAfter adding new ops to a set of fusible ops, mobilenetv3 slows down to **9000ms from 1200ms** without this fix.\n\nThis happens because one of the inputs was expanded and converted to nchw/nhwc\nwe might end up in a very bad spot if the second argument\nis in a blocked format. In this case, MKLDNN uses its\nreference implementation for a binary operation that follows\nthese broadcasts and it could be up to ~100x slower.\nWe use a very simple heuristic to convert an arg in nchw\nto the blocked format of the other argument.\n\n* MKLDNN_VERBOSE without the issue:\n[test_mobilenet_nopool.txt](https://github.com/pytorch/pytorch/files/6319528/test_mobilenet_nopool.txt)\n* MKLDNN_VERBOSE with the issue (Note the times for `ref` operations)\n[test_mobilenet_pool.txt](https://github.com/pytorch/pytorch/files/6319529/test_mobilenet_pool.txt)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56089\n\nReviewed By: eellison\n\nDifferential Revision: D27796688\n\nPulled By: Krovatkin\n\nfbshipit-source-id: fc34d76358ce899e3b1f2b69efb9b5c38f5af1ad", "pr_number": "56089", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7636cb6bab": {"title": "clean up unused reduction functions in THC (#56293)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56293\n\nReviewed By: mruberry\n\nDifferential Revision: D27833949\n\nPulled By: ngimel\n\nfbshipit-source-id: b9bf03c783b41c35890249902ea9bf1c34c9c13d", "pr_number": "56293", "files_changed": ["aten/src/THC/CMakeLists.txt", "aten/src/THC/THCReduceAll.cuh", "aten/src/THC/THCTensorMathReduce.cuh", "aten/src/THC/generic/THCTensorMathReduce.cu"], "labels": ["Merged", "cla signed"]}, "04e7891aab": {"title": "Add adaptive_avgpool2d to the set of fusible ops (#56180)", "body": "Summary:\nThis improves mobilenetv3 perf from ~1300msto 1147ms (~12%)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56180\n\nReviewed By: Chillee\n\nDifferential Revision: D27840860\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 6ce38e93fd2f55e68a69c34b45271743f84a13b8", "pr_number": "56180", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "be2a0805d2": {"title": "[TensorPipe] Update tensorpipe subodule + remove TP_NEW_API switch. (#56260)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56260\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27693102\n\nfbshipit-source-id: b682e88f818657065a478b5a90ca1a4ca8c52018", "pr_number": "56260", "files_changed": ["third_party/tensorpipe", "torch/csrc/distributed/rpc/tensorpipe_utils.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "50d4c63f46": {"title": "Allow inlining of more Tensor methods (#53905)", "body": "Summary:\nThis `is_meta` call in `TensorIterator` shows up in profiling as around 4-5% of fast setup time:\nhttps://github.com/pytorch/pytorch/blob/49a5f99440bde6a2f214e9c0b64c3ae0fdfb5a59/aten/src/ATen/TensorIterator.cpp#L886\n\nAfter inlining, `is_meta()` compiles to a single `test` instruction. Saving 20-30 ns per operator call. The functions I'm moving into the header here are all similar, in that they inline away to almost nothing.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53905\n\nReviewed By: gchanan\n\nDifferential Revision: D27513232\n\nPulled By: swolchok\n\nfbshipit-source-id: 33ec9eefecd0ddebc285e1d830edb558818dc391", "pr_number": "53905", "files_changed": ["aten/src/ATen/templates/TensorBody.h", "aten/src/ATen/templates/TensorMethods.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "36b476ccdd": {"title": "Added OpInfos for eq, ne, ge, gt, le, and lt (#55709)", "body": "Summary:\nA https://github.com/pytorch/pytorch/issues/54261 task\nAdded OpInfos for `eq`, `ne`, `ge`, `gt`, `le`, and `lt`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55709\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27760382\n\nPulled By: mruberry\n\nfbshipit-source-id: 30d8c9633c69a097c1e4a9daf4178c617c0a9093", "pr_number": "55709", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "8c74e1b840": {"title": "Vectorize copysign on CPU (#51792)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51792\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski\n\nDifferential Revision: D27769007\n\nPulled By: mruberry\n\nfbshipit-source-id: 65fceb9f59ed6afee4452278992340da104ed5fe", "pr_number": "51792", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_bfloat16.h", "aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/cpu/vec256/vec256_int.h", "aten/src/ATen/cpu/vec256/vec256_qint.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "c10/util/copysign.h"], "labels": ["Merged", "cla signed", "open source"]}, "b0e0841f98": {"title": "OpInfo porting for logsumexp operator (#55520)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55520\n\nReviewed By: mruberry\n\nDifferential Revision: D27844357\n\nPulled By: iramazanli\n\nfbshipit-source-id: 6228041be9edc0a148fa34e965d2ff6423649b05", "pr_number": "55520", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "a24b17248f": {"title": "Short circuits DistributedDataParallel._recursive_to's copy and stream syncs if input is already on the right device (#55624)", "body": "Summary:\n^\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55624\n\nReviewed By: pbelevich, agolynski\n\nDifferential Revision: D27836170\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 954bf336d70f9e80c045a6a96c1d8843c7f1cf2c", "pr_number": "55624", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "open source"]}, "ce05b7a324": {"title": "[c10d] Remove deprecated use of torch.LongTensor, torch.ByteTensor (#55861)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55861\n\nAPIs such as torch.LongTensor and torch.ByteTensor are deprecated and\nthe recommended API is torch.tensor(args, dtype=...). Use this API in\ndistributed_c10d.\nghstack-source-id: 126777875\n\nTest Plan: CI\n\nReviewed By: pbelevich\n\nDifferential Revision: D27726600\n\nfbshipit-source-id: 07eb8168d93697593589002c93c3903ce29431ef", "pr_number": "55861", "files_changed": ["torch/distributed/distributed_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3e42da09df": {"title": "Porting logcumsumexp tests to OpInfo (#56135)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56135\n\nReviewed By: mruberry\n\nDifferential Revision: D27844398\n\nPulled By: iramazanli\n\nfbshipit-source-id: e0191314dc4e248501ad25170da0b77c0b799781", "pr_number": "56135", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "ffdecc1ac4": {"title": "[CUDA graphs] Allows DeviceCachingAllocator to capture cross-stream memory use (#55860)", "body": "Summary:\nSafely deallocating and repurposing memory used across streams relies on recording end-of-life events in all an allocation's usage streams beyond its original allocation stream. The events are later queried to see if all GPU work in those extra streams that could have used the allocation is done (from the CPU's perspective) before repurposing the allocation for use in its original stream.\n\nThe trouble is, calling EventQuery on an ordinary event recorded in a capturing stream is illegal. Calling EventQuery while capture is underway is also illegal. So when we call `tensor.record_stream` (or `c10::cuda::cudaCachingAllocator::recordStream`) on any tensor that's used or deleted in or around a capture, we often end up with a confusing error thrown from the cudaEventQuery in DeviceCachingAllocator::process_events().\n\nThis PR enables hopefully-safe deletion of tensors used across streams in or around capture with a conservative but simple approach: don't record or process end of life events for such tensors until the allocator's sure no captures are underway. You could whiteboard cases where this causes cross-stream-used allocations to be unavailable for reuse longer than absolutely necessary, but cross-stream-used allocations are uncommon, so for practical purposes this approach's impact on the memory footprint of captured sequences should be small.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55860\n\nReviewed By: ejguan\n\nDifferential Revision: D27822557\n\nPulled By: ezyang\n\nfbshipit-source-id: b2e18a19d83ed05bad67a8157a14a606ed14d04e", "pr_number": "55860", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp", "test/test_cuda.py"], "labels": ["Merged", "cla signed", "module: cuda", "module: cuda graphs", "open source", "triaged"]}, "2219286de4": {"title": "Updated internal code for orgqr function (#56247)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56247\n\nMoved `apply_orgqr` to `BatchLinearAlgebraKernel.cpp`.\n\nRemoved `infos` tensor parameter. We don't need to expose\nlapack/cusolver error codes because they do not contain any useful\ninformation about the input. Its value is checked only in debug mode now\nremoving the device syncronization from the cuSOLVER path of\n`torch.linalg.householder_product` or `torch.orgqr`.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27844339\n\nPulled By: mruberry\n\nfbshipit-source-id: 47aa20dfe2c116951b968362ad55e837caece042", "pr_number": "56247", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "0e106fce9c": {"title": "add tests for torch.testing (#54784)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54784\n\n* #54769 make torch.testing asserts importable\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27717422\n\nPulled By: mruberry\n\nfbshipit-source-id: 7526af4f17d8ffcc4ea5e5a5d98f07ceac89df40", "pr_number": "54784", "files_changed": ["test/test_testing.py"], "labels": ["Merged", "cla signed", "open source"]}, "7513455c74": {"title": "Make tensordot resize output tensor's size if out= argument is specified & make it safely cast & copy output (#56286)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/56022.\nFixes https://github.com/pytorch/pytorch/issues/56316\n\nFor `torch.tensordot`,\n1. `tensordot`'s out variant now resizes the output tensor provided as the `out` argument if necessary.\n2. Added a check to verify if the output tensor provided as the argument for `out` is on the same device as the input tensors.\n3. Added a check to verify if the dtype of the result is castable to the dtype of the output tensor provided as an argument for `out`.\n4. Because of (2) & (3), `tensordot`'s out variant now [safely casts & copies output](https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch).\n5. `test_tensordot` in `test_linalg.py` had a bug - the output tensor wasn't being defined to be on the same device as the input tensors. It was fixed by simply using a `device` argument in its definition.\n6. Added an `OpInfo` for `tensordot` and modified the `OpInfo` for `inner`.\n\ncc heitorschueroff mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56286\n\nReviewed By: ngimel\n\nDifferential Revision: D27845980\n\nPulled By: mruberry\n\nfbshipit-source-id: 134ab163f05c31a6900dd65aefc745803019e037", "pr_number": "56286", "files_changed": ["aten/src/ATen/native/Linear.cpp", "test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "1077f87269": {"title": "Support factory kwargs in torch.nn modules (#54508)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/pull/53144\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54508\n\nReviewed By: mrshenli\n\nDifferential Revision: D27600457\n\nPulled By: jbschlosser\n\nfbshipit-source-id: b58bfee61c3917524b4622f63ef216c27a588eb1", "pr_number": "54508", "files_changed": ["test/run_test.py", "test/test_module_init.py", "torch/nn/__init__.py", "torch/nn/modules/activation.py", "torch/nn/modules/adaptive.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/instancenorm.py", "torch/nn/modules/linear.py", "torch/nn/modules/normalization.py", "torch/nn/modules/rnn.py", "torch/nn/modules/sparse.py", "torch/nn/modules/transformer.py", "torch/nn/parameter.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/nn/quantizable/modules/activation.py", "torch/nn/quantizable/modules/rnn.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/activation.py", "torch/nn/quantized/modules/batchnorm.py", "torch/nn/quantized/modules/conv.py", "torch/nn/quantized/modules/normalization.py", "torch/quantization/observer.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "7d17559152": {"title": "[special] OpInfo `i0e`: fix missing check (#56232)", "body": "Summary:\nFixes: https://github.com/pytorch/pytorch/issues/56274\n\nMissed to check if scipy is installed or not (for reference tests).\n\nThanks mruberry !!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56232\n\nReviewed By: ejguan\n\nDifferential Revision: D27832397\n\nPulled By: ezyang\n\nfbshipit-source-id: abc40ce7bf14d3c0f20877030880663ccb7fe375", "pr_number": "56232", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "92991d9533": {"title": "Add OpInfo for (nan)quantile (#55548)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55548\n\nReviewed By: mruberry\n\nDifferential Revision: D27796638\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: d7f09c4ffa7d726cc8228a16f818b74fb9e1a93a", "pr_number": "55548", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "b6b2fc7e3f": {"title": "Added OpInfos of add & mm (#55915)", "body": "Summary:\nAdded `OpInfo`s of `add` & `mm`.\n\ncc anjali411\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55915\n\nReviewed By: agolynski\n\nDifferential Revision: D27800077\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 84be4b0930f6ef472622e6721a516cc182ac76d1", "pr_number": "55915", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d806b06167": {"title": "Support int32 indices in torch.repeat_interleave (#55102)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55102\n\nTo avoid casting a tensor to `.long()`, we introduce support for int32 in `torch.repeat_interleave`.\n\nReviewed By: ezyang\n\nDifferential Revision: D27478235\n\nfbshipit-source-id: 08b4cce65fe94ff10535ddc07e1ba2bacea6a2cf", "pr_number": "55102", "files_changed": ["aten/src/ATen/native/Repeat.cpp", "aten/src/ATen/native/Repeat.h", "aten/src/ATen/native/cuda/Repeat.cu", "test/test_torch.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "98ac6f7cbc": {"title": "Increase default rendezvous timeout to 15 minutes", "body": "Summary: Increase default rendezvous timeout to 15 minutes to address slow static initialization.\n\nTest Plan: n/a\n\nReviewed By: wilson100hong\n\nDifferential Revision: D27725655\n\nfbshipit-source-id: a1b8c49b225b61be0d13ff5e52bf6677bf72f792", "pr_number": null, "files_changed": ["torch/distributed/launcher/api.py"], "labels": []}, "838d3079ad": {"title": "Lazily initialize alias db in remove_mutation opt (#55949)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55949\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D27793881\n\nfbshipit-source-id: eebde5b5142d8fecfee4756604d313b0da809882", "pr_number": "55949", "files_changed": ["torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/alias_analysis.h", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/passes/remove_mutation.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6409d34482": {"title": "Sort glob of files to ensure it is deterministic (#55850)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55850\n\nghstack-source-id: 126339587\n\nTest Plan: diff on top builds successfully on Sandcastle\n\nReviewed By: wconstab\n\nDifferential Revision: D27722254\n\nfbshipit-source-id: 181ae1a874dbfc73688dcc5b7e9264d79abd44d3", "pr_number": "55850", "files_changed": ["torch/csrc/deploy/interpreter/freeze.py"], "labels": ["Merged", "cla signed"]}, "8881f504f1": {"title": "Remove the unused maximum and minimum functions in vec256_base (#56313)", "body": "Summary:\nThey are unused, unrelated to vectorization, and confusing for code\nreaders (each of them have 2 overloads that are actually used).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56313\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27854290\n\nPulled By: ezyang\n\nfbshipit-source-id: 14945ceac39a3f19e5d0f8d762b17f8c2172b966", "pr_number": "56313", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h"], "labels": ["Merged", "cla signed", "open source"]}, "98162cb0bb": {"title": "Enable AutoGradMode in InferenceMode. (#56107)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56107\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich, driazati\n\nDifferential Revision: D27807137\n\nPulled By: ailzhang\n\nfbshipit-source-id: bfacf11ec5a431589cec73d6371cac81b425a115", "pr_number": "56107", "files_changed": ["c10/core/InferenceMode.h", "test/cpp/api/inference_mode.cpp"], "labels": ["Merged", "cla signed"]}, "5748cc0d11": {"title": "[Mobile GPU] Ban mutations in JIT passes (#56070)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56070\n\n**Summary**\n\nCurrently, we're returning copies instead of alias on mobile GPU (Metal/Vulkan). As suggested by ailzhang , we could use the JIT pass - `RemoveTensorMutation` to ban mutations ahead of time. I've tested two scenarios as shown below. They both work fine on mobile.\n\n- view\n\n```\nclass Model (torch.nn.Module):\n    def forward(self, x):\n        y = x.view(-1)\n        z = torch.tensor(2.0).float()\n        y.add_(z)\n        return x\n\nm = Model()\nx = torch.rand(2, 3)\ny = m(x)\n```\n- transpose\n\n```\nclass Model (torch.nn.Module):\n    def forward(self, x):\n        y = x.transpose(1, 2)\n        z = torch.tensor(2.0).float()\n        x.add_(z)\n        return y\n\nm = Model()\nx = torch.rand(1, 2, 3)\ny = m(x)\n```\n\nAs we're adding more ops, we should add more tests to cover all the alias ops - https://github.com/pytorch/pytorch/blob/master/tools/autograd/gen_inplace_or_view_type.py#L31-L80\n\n**Next step**\n\nSynced offline with eellison. Since mutation removal is also being used in ONNX, Static runtime, some jit optimizations, Torch -> TVM, etc, instead of inventing something new, we would continue to make it better in cases where it fails.\n\nAlthough this JIT pass could work for most of the mobile models, there are cases that it can't cover. What we're going to do next is to implement stub ops for GPU models to let them run on server side, such that users can compare results to see if there is any discrepancy.\n\nghstack-source-id: 126802123\n\nTest Plan:\n- Sandcastle\n- CircleCI\n\nReviewed By: raziel\n\nDifferential Revision: D27692683\n\nfbshipit-source-id: 9d1be8a6c0a276032b1907807a54fbe2afd882f9", "pr_number": "56070", "files_changed": ["torch/csrc/jit/passes/metal_rewrite.cpp", "torch/csrc/jit/passes/vulkan_rewrite.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "48aaea3359": {"title": "unified GlooStore and c10d store API (#56222)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56222\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55719\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27785267\n\nPulled By: msaroufim\n\nfbshipit-source-id: ce247f9226ecc971af8e1f08adeb835f64973e12", "pr_number": "56222", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "40483acc51": {"title": "Support factory kwargs in torch.nn modules (#54508)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/pull/53144\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54508\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27855386\n\nPulled By: jbschlosser\n\nfbshipit-source-id: dabd505d2a04208e74b158570fb2859c736eea2c", "pr_number": "54508", "files_changed": ["test/run_test.py", "test/test_module_init.py", "torch/nn/__init__.py", "torch/nn/modules/activation.py", "torch/nn/modules/adaptive.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/instancenorm.py", "torch/nn/modules/linear.py", "torch/nn/modules/normalization.py", "torch/nn/modules/rnn.py", "torch/nn/modules/sparse.py", "torch/nn/modules/transformer.py", "torch/nn/parameter.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/nn/quantizable/modules/activation.py", "torch/nn/quantizable/modules/rnn.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/activation.py", "torch/nn/quantized/modules/batchnorm.py", "torch/nn/quantized/modules/conv.py", "torch/nn/quantized/modules/normalization.py", "torch/quantization/observer.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "e3900d2ba5": {"title": "Add lint for unqualified `noqa` (#56272)", "body": "Summary:\nAs this diff shows, currently there are a couple hundred instances of raw `noqa` in the codebase, which just ignore all errors on a given line. That isn't great, so this PR changes all existing instances of that antipattern to qualify the `noqa` with respect to a specific error code, and adds a lint to prevent more of this from happening in the future.\n\nInterestingly, some of the examples the `noqa` lint catches are genuine attempts to qualify the `noqa` with a specific error code, such as these two:\n```\ntest/jit/test_misc.py:27:            print(f\"{hello + ' ' + test}, I'm a {test}\") # noqa E999\ntest/jit/test_misc.py:28:            print(f\"format blank\") # noqa F541\n```\nHowever, those are still wrong because they are [missing a colon](https://flake8.pycqa.org/en/3.9.1/user/violations.html#in-line-ignoring-errors), which actually causes the error code to be completely ignored:\n\n- If you change them to anything else, the warnings will still be suppressed.\n- If you add the necessary colons then it is revealed that `E261` was also being suppressed, unintentionally:\n  ```\n  test/jit/test_misc.py:27:57: E261 at least two spaces before inline comment\n  test/jit/test_misc.py:28:35: E261 at least two spaces before inline comment\n  ```\n\nI did try using [flake8-noqa](https://pypi.org/project/flake8-noqa/) instead of a custom `git grep` lint, but it didn't seem to work. This PR is definitely missing some of the functionality that flake8-noqa is supposed to provide, though, so if someone can figure out how to use it, we should do that instead.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56272\n\nTest Plan:\nCI should pass on the tip of this PR, and we know that the lint works because the following CI run (before this PR was finished) failed:\n\n- https://github.com/pytorch/pytorch/runs/2365189927\n\nReviewed By: janeyx99\n\nDifferential Revision: D27830127\n\nPulled By: samestep\n\nfbshipit-source-id: d6dcf4f945ebd18cd76c46a07f3b408296864fcb", "pr_number": "56272", "files_changed": [".github/workflows/lint.yml", "benchmarks/fastrnns/bench.py", "benchmarks/fastrnns/factory.py", "benchmarks/operator_benchmark/benchmark_all_other_test.py", "benchmarks/operator_benchmark/benchmark_all_quantized_test.py", "benchmarks/operator_benchmark/benchmark_all_test.py", "benchmarks/operator_benchmark/benchmark_core.py", "benchmarks/operator_benchmark/benchmark_pytorch.py", "benchmarks/operator_benchmark/c2/add_test.py", "benchmarks/operator_benchmark/c2/batch_box_cox_test.py", "benchmarks/operator_benchmark/c2/batch_gather_test.py", "benchmarks/operator_benchmark/c2/clip_ranges_test.py", "benchmarks/operator_benchmark/c2/concat_test.py", "benchmarks/operator_benchmark/c2/matmul_test.py", "benchmarks/operator_benchmark/c2/quantile_op_test.py", "benchmarks/operator_benchmark/c2/replace_nan_test.py", "benchmarks/operator_benchmark/operator_benchmark.py", "benchmarks/operator_benchmark/pt/cat_test.py", "benchmarks/operator_benchmark/pt/qactivation_test.py", "benchmarks/operator_benchmark/pt/qobserver_test.py", "benchmarks/operator_benchmark/pt/qpool_test.py", "benchmarks/operator_benchmark/pt/stack_test.py", "benchmarks/operator_benchmark/pt_extension/cpp_extension_test.py", "test/benchmark_utils/test_benchmark_utils.py", "test/distributed/pipeline/sync/skip/test_api.py", "test/distributed/pipeline/sync/skip/test_gpipe.py", "test/distributed/pipeline/sync/skip/test_inspect_skip_layout.py", "test/distributed/pipeline/sync/skip/test_leak.py", "test/distributed/pipeline/sync/skip/test_stash_pop.py", "test/distributed/pipeline/sync/skip/test_tracker.py", "test/distributed/pipeline/sync/test_pipeline.py", "test/jit/test_class_type.py", "test/jit/test_complex.py", "test/jit/test_list_dict.py", "test/jit/test_misc.py", "test/jit/test_recursive_script.py", "test/jit/test_save_load.py", "test/onnx/test_models_onnxruntime.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/onnx/test_pytorch_onnx_onnxruntime_cuda.py", "test/package/package_a/fake_script_class.py", "test/test_dispatch.py", "test/test_jit.py", "test/test_nnapi.py", "test/test_quantization.py", "test/test_tensorboard.py", "test/test_utils.py", "torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py", "torch/distributed/elastic/agent/server/__init__.py", "torch/distributed/elastic/events/__init__.py", "torch/distributed/elastic/metrics/__init__.py", "torch/distributed/elastic/multiprocessing/__init__.py", "torch/distributed/elastic/multiprocessing/errors/__init__.py", "torch/distributed/elastic/rendezvous/etcd_rendezvous.py", "torch/distributed/elastic/timer/__init__.py", "torch/distributed/elastic/utils/__init__.py", "torch/distributed/elastic/utils/data/__init__.py", "torch/distributed/launcher/__init__.py", "torch/distributed/nn/jit/templates/remote_module_template.py", "torch/functional.py", "torch/fx/graph.py", "torch/fx/symbolic_trace.py", "torch/jit/annotations.py", "torch/jit/quantized.py", "torch/jit/supported_ops.py", "torch/nn/functional.py", "torch/nn/quantized/dynamic/modules/rnn.py", "torch/nn/utils/rnn.py", "torch/quantization/fuse_modules.py", "torch/testing/_internal/common_utils.py", "torch/testing/_internal/distributed/distributed_test.py", "torch/testing/_internal/distributed/rpc/faulty_rpc_agent_test_fixture.py", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/utils/benchmark/utils/common.py", "torch/utils/tensorboard/__init__.py"], "labels": ["Merged", "cla signed"]}, "04607a58f1": {"title": "[pytorch] Fix compiler warnings from conv.h (#56181)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56181\n\nNeed to change to size_t vs size_t:\n\nReviewed By: ezyang\n\nDifferential Revision: D27800849\n\nfbshipit-source-id: 25f744128eb8750c382dc967a99af3c9f16247d9", "pr_number": "56181", "files_changed": ["torch/csrc/api/include/torch/nn/modules/conv.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "a14178ed5c": {"title": "Remove useless code (#56230)", "body": "Summary:\nSince we're using specific VS, we don't need to specify VC version.\nIn fact, the VC version is not used in CI now.\n\nWhy I make this change now?\nI'm writing a robot to update the vs_install.ps1 (https://github.com/pytorch/pytorch/pull/56261/) every 2 weeks.\nIt will submit a PR to check if the latest VS is compatible with PyTorch automatically.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56230\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27856647\n\nPulled By: ezyang\n\nfbshipit-source-id: b46f2bdf35ab5841fded470e23bbf7a01d5f60f4", "pr_number": "56230", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": ["Merged", "ci/all", "cla signed", "open source"]}, "638617f9f8": {"title": "Write mini dump on pybind exceptions (#55652)", "body": "Summary:\nWe register an [error handler](https://pybind11.readthedocs.io/en/stable/advanced/exceptions.html#registering-custom-translators) with pybind so that C++ exceptions are passed to Python and raised as runtime errors that can be `try...except`ed etc. Since these don't terminate the program (until Python does), they never fire the signal handler to write a minidump out with the crash information. This PR adds some logic in the exception translator to write out a minidump if enabled.\n](https://our.intern.facebook.com/intern/diff/27830952/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55652\n\nPulled By: driazati\n\nReviewed By: bertmaher\n\nDifferential Revision: D27830952\n\nfbshipit-source-id: 26e8f913e99dff971a4eb09eb87221c66f759763", "pr_number": "55652", "files_changed": ["test/test_utils.py", "torch/csrc/Module.cpp", "torch/csrc/utils/crash_handler.cpp", "torch/csrc/utils/crash_handler.h", "torch/csrc/utils/init.cpp", "torch/utils/__init__.py", "torch/utils/_crash_handler.py"], "labels": ["Merged", "cla signed"]}, "2f5c352162": {"title": "Fix protobuf warnings in caffe2 (#56186)", "body": "Summary:\nThis guards some deprecated usages of the Protobuf API behind an `#ifdef` (this is how onnx does it as well)\n](https://our.intern.facebook.com/intern/diff/27803121/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56186\n\nPulled By: driazati\n\nReviewed By: bertmaher, dzhulgakov\n\nDifferential Revision: D27803121\n\nfbshipit-source-id: 2d3a348ec1ab9879a0d8f2dff17c5444fd4baf2c", "pr_number": "56186", "files_changed": ["caffe2/utils/proto_utils.cc"], "labels": ["Merged", "cla signed"]}, "34d0bd5b1d": {"title": "Fix TestTypeHints.test_doc_examples (#56388)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/54268 removed `test_run_mypy` since now we're running `mypy` as its own job in GitHub Actions, but previously we used this `set_cwd` context manager in that test to ensure that we picked up the `mypy` config correctly. However, for some reason, we have not been doing that in `test_doc_examples`, which has been succeeding in CI for a while despite being broken.\n\nSpecifically, [`run_test.py` changes the working directory to `test/` before running test files](https://github.com/pytorch/pytorch/blob/48aaea3359cffe7982ce24b8742c7a1f4b456a92/test/run_test.py#L534-L535), which is contrary to [what `CONTRIBUTING.md` instructs developers to do](https://github.com/pytorch/pytorch/blob/48aaea3359cffe7982ce24b8742c7a1f4b456a92/CONTRIBUTING.md#python-unit-testing). As a result, in CI, `test/test_type_hints.py` has been passing in CI, but if you run it locally from the root of the repo, this you get this error:\n```\nF\n======================================================================\nFAIL: test_doc_examples (__main__.TestTypeHints)\nRun documentation examples through mypy.\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"test/test_type_hints.py\", line 127, in test_doc_examples\n    self.fail(f\"mypy failed:\\n{stdout}\")\nAssertionError: mypy failed:\ntest/generated_type_hints_smoketest.py:851: error: Name 'tensor' is not defined  [name-defined]\ntest/generated_type_hints_smoketest.py:853: error: Name 'tensor' is not defined  [name-defined]\nFound 2 errors in 1 file (checked 1 source file)\n\n----------------------------------------------------------------------\nRan 1 test in 1.416s\n\nFAILED (failures=1)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56388\n\nTest Plan:\nBefore this PR, the first of the following two commands should fail (since that is essentially what is run in CI), but the second should fail:\n```\npython test/run_test.py -i test_type_hints\npython test/test_type_hints.py\n```\nAfter this PR, both commands should succeed.\n\nReviewed By: driazati\n\nDifferential Revision: D27860173\n\nPulled By: samestep\n\nfbshipit-source-id: efb82fffd7ccb04d0331824b40bdef7bbc319c98", "pr_number": "56388", "files_changed": ["test/test_type_hints.py", "torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: typing"]}, "fa7534788b": {"title": "Fix typo in gradcheck.py (#56368)", "body": "Summary:\nbetwen -> between\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56368\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27860450\n\nPulled By: albanD\n\nfbshipit-source-id: 86ef7b62e228c15319683a8d72b404b5f527666e", "pr_number": "56368", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "open source"]}, "7adc04d7b5": {"title": "Add more logging to debug test_reduce_sum_cuda_twice (#56406)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56406\n\nBeen hard to reproduce\nhttps://github.com/pytorch/pytorch/issues/50840, adding some debug log to get a\nbetter sense of the issue.\nghstack-source-id: 126874222\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27863328\n\nfbshipit-source-id: e6f125b77cfb636b90598eb54395609654f5e139", "pr_number": "56406", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "31677c5fcb": {"title": "[reland] .github: Add initial linux CI workflow (#56280)", "body": "Summary:\nThis reverts commit 6b5ed5ec454ecd8597ff0465305915dd1e09a805.\n\nThere'll also probably be fixes here, see diff from original PR: https://github.com/pytorch/pytorch/compare/f2abce0...ci-all/add-initial-linux-ci-gha\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56280\n\nReviewed By: walterddr\n\nDifferential Revision: D27826012\n\nPulled By: seemethere\n\nfbshipit-source-id: 71cad1d7f840ede5025b1bb4a33d628aa74686d1", "pr_number": "56280", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/scripts/install_nvidia_utils_linux.sh", ".github/scripts/report_git_status.sh", ".github/templates/linux_ci_workflow.yml.in", ".github/workflows/lint.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml", ".gitignore", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/common.sh", ".jenkins/pytorch/macos-test.sh", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test.sh", "Makefile"], "labels": ["Merged", "cla signed"]}, "5b4c3a9da1": {"title": "record Torch DP and DDP modules forward (#55578)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55578\n\nReviewed By: gdankel\n\nDifferential Revision: D27862392\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 18545d23e35a97c8f760707fecb696a24d47dc0a", "pr_number": "55578", "files_changed": ["torch/nn/parallel/data_parallel.py", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "module: ddp", "open source", "triaged"]}, "b1282bc109": {"title": "Use stack trace implementation in common/process on fbcode (#56400)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56400\n\nSee https://github.com/pytorch/pytorch/issues/56399\n\nI don't have time to fix this properly, so this is just to stem the\nbleeding.  Someone should go and figure out what it is that common/process\nis doing better.\nghstack-source-id: 126868405\n\nTest Plan:\nI manually patched this into D27765125 and triggered a\nexception and observed that everything symbolized good:\n\n```\n[9]   what():  new_refcount != 1INTERNAL ASSERT FAILED at \"caffe2/c10/util/intrusive_ptr.h\":234, please report a bug to PyTorch. intrusive_ptr: Cannot increase refcount after it reached zero.\nException raised from retain_ at caffe2/c10/util/intrusive_ptr.h:234 (most recent call first):\n# 0  c10::get_backtrace[abi:cxx11](unsigned long, unsigned long, bool)\n# 1  c10::(anonymous namespace)::GetFetchStackTrace[abi:cxx11]()::$_0::operator()[abi:cxx11]() const\n# 2  std::_Function_handler<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > (), c10::(anonymous namespace)::Ge\ntFetchStackTrace()::$_0>::_M_invoke(std::_Any_data const&)\n# 3  std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > ()>::operator()() const\n# 4  c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)\n# 5  c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocat\nor<char> > const&)\n# 6  c10::detail::torchInternalAssertFail(char const*, char const*, unsigned int, char const*, char const*)\n# 7  c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >::retain_()\n# 8  c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >::intrusive_ptr(c10::intrusiv\ne_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&)\n# 9  c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >& c10::intrusive_ptr<c10d::Pr\nocessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> >::operator=<c10d::ProcessGroup, c10::detail::intrusive_target\n_default_null_type<c10d::ProcessGroup> >(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessG\nroup> > const&) &\n```\n\nReviewed By: driazati\n\nDifferential Revision: D27861908\n\nfbshipit-source-id: 84c1dfb1ef28c460b020646f836c153562ad5c44", "pr_number": "56400", "files_changed": ["c10/util/Backtrace.cpp"], "labels": ["Merged", "cla signed"]}, "42f0fe1fe3": {"title": "fix misaligned access #56325 (#56403)", "body": "Summary:\nCC ngimel ptrblck\nref: https://github.com/pytorch/pytorch/issues/56325\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56403\n\nReviewed By: mruberry\n\nDifferential Revision: D27866625\n\nPulled By: ngimel\n\nfbshipit-source-id: 9dff0e9749f8de57fac6a653f685c14854611a02", "pr_number": "56403", "files_changed": ["aten/src/ATen/native/cuda/SoftMax.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source"]}, "07653b7fe0": {"title": "[SPMD] Remove ddp_gpu_size field from SyncBatchNorm (#55946)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55946\n\nAs `ddp_gpu_size` field of `SyncBatchNorm` will always be 1 for GPU modules, remove this field and the relevant code.\nghstack-source-id: 126883498\n\nTest Plan: waitforbuildbot\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27746021\n\nfbshipit-source-id: b4518c07e6f0c6943fbd7a7548500a7d4337126c", "pr_number": "55946", "files_changed": ["torch/nn/modules/batchnorm.py", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "513e9e0927": {"title": "Fix cxx11 abi (#55984)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55829\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55984\n\nReviewed By: agolynski\n\nDifferential Revision: D27809478\n\nPulled By: seemethere\n\nfbshipit-source-id: b00801e50c364b307009349594e396b934cc3a49", "pr_number": "55984", "files_changed": ["aten/src/ATen/native/cuda/Sort.cu"], "labels": ["Merged", "ci/binaries", "cla signed", "open source"]}, "7ae45403a1": {"title": "[static runtime] support aten::__getitem__ natively (#55310)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55310\n\nTest Plan:\nRun on the dper generated local/local_ro model\n```\n./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --scripted_model=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.local.local_ro.pt --pt_inputs=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.input_data.container.pt --iters=1000 --warmup_iters=10000 --num_threads=1 --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --pt_optimize_memory=1 --compare_results=0 --do_profile=0 --adsfinder_compatibility=1\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27569662\n\nfbshipit-source-id: df68c2fdd95e39a30aec35ddbaf1f5df0bc3a3da", "pr_number": "55310", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "c5c5230890": {"title": "Pytorch resolve bug around incorrect rdzv handler resolution (#56386)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56386\n\nThe diff resolves bug around incorrect handler resolution:\n_create_static_handler pointed towards etcd, and _create_etcd_handler pointed towards static.\n\nTest Plan:\nbuck test mode/dev-nosan //caffe2/test/distributed:test_launcher\n\nAdded test_launcher to the ci/cd tests\n\nReviewed By: cbalioglu\n\nDifferential Revision: D27858897\n\nfbshipit-source-id: 440155789958c091ce5755e7c9524e4bb704203a", "pr_number": "56386", "files_changed": ["test/distributed/bin/test_script.py", "test/distributed/test_launcher.py", "test/run_test.py", "torch/distributed/elastic/multiprocessing/api.py", "torch/distributed/elastic/multiprocessing/redirects.py", "torch/distributed/elastic/rendezvous/registry.py", "torch/distributed/elastic_launch.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "df8bb5a42b": {"title": "Add OpInfo for polygamma and remove torch_op_tests Infra (#51966)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\n* OpInfo entry for Polygamma\n* Removes infra of torch_op_tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51966\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27851858\n\nPulled By: mruberry\n\nfbshipit-source-id: 7f1d0273065e1df56a152f95a14513959af29a1b", "pr_number": "51966", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "test/test_fx.py", "test/test_unary_ufuncs.py", "tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "0ea4eb745b": {"title": "[opinfo] torch.lerp: move remaining cases from tensor_methods to opinfo (#55665)", "body": "Summary:\nFixes : https://github.com/pytorch/pytorch/issues/54304\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55665\n\nReviewed By: bdhirsh\n\nDifferential Revision: D27845528\n\nPulled By: mruberry\n\nfbshipit-source-id: 36bdf14c4923a83fb8e4f4d361467d9568784011", "pr_number": "55665", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "63dac82444": {"title": "Make grad mode error just a warning (#56401)", "body": "Summary:\nTemporary fix to give people extra time to finish the deprecation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56401\n\nReviewed By: xw285cornell, drdarshan\n\nDifferential Revision: D27862196\n\nPulled By: albanD\n\nfbshipit-source-id: ed460267f314a136941ba550b904dee0321eb0c6", "pr_number": "56401", "files_changed": ["test/cpp/api/grad_mode.cpp", "test/test_autograd.py", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "43c747859c": {"title": "Use c10 backtrace generation in caffe2 (#56198)", "body": "Summary:\nThis cuts out caffe2's old backtrace generation in favor of the one already in c10.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56198\n\nPulled By: driazati\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27868282\n\nfbshipit-source-id: aa9b9691271eaa3f95baab48773ffefebd924ae2", "pr_number": "56198", "files_changed": ["caffe2/utils/signal_handler.cc"], "labels": ["Merged", "cla signed"]}, "c61778355c": {"title": "Upgrade ShellCheck to v0.7.2 (#56445)", "body": "Summary:\n[First ShellCheck release in over a year!](https://github.com/koalaman/shellcheck/releases/tag/v0.7.2) I'm thankful for doing https://github.com/pytorch/pytorch/issues/55109 at the beginning of this month, because otherwise `master` would have just suddenly started failing a few hours ago.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56445\n\nTest Plan:\nCI. You can also run `shellcheck` locally; for instance, if you're on Mac and [installed it with Homebrew](https://github.com/koalaman/shellcheck/tree/v0.7.2#installing):\n```sh\nbrew upgrade shellcheck\nrm -r .extracted_scripts ; tools/extract_scripts.py --out=.extracted_scripts\ntools/run_shellcheck.sh .jenkins/pytorch .extracted_scripts\n```\n\nReviewed By: janeyx99\n\nDifferential Revision: D27874084\n\nPulled By: samestep\n\nfbshipit-source-id: 3bd871a368fe03aecd559e2f55bce36af49cfa27", "pr_number": "56445", "files_changed": [".github/workflows/auto_label.yml", ".github/workflows/lint.yml", ".jenkins/pytorch/macos-common.sh"], "labels": ["Merged", "cla signed"]}, "eacf6f1b51": {"title": "Updated the tech docs to be consistent with other two descriptions (#56338)", "body": "Summary:\nUpdated the Beta channel description to be consistent with other two channels (Stable, Prototype)\n\nThe screenshot attached is for reference before changes.\n\n![Screenshot 2021-04-18 12-36-55](https://user-images.githubusercontent.com/20245964/115137303-0c077380-a043-11eb-9532-c46486e8a75a.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56338\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27854350\n\nPulled By: bdhirsh\n\nfbshipit-source-id: a21208c11242e84de313d5b11269264756bf9029", "pr_number": "56338", "files_changed": ["docs/source/index.rst"], "labels": ["Merged", "cla signed", "open source"]}, "4e0760f41a": {"title": "Remove `is_variable` from tests (#56305)", "body": "Summary:\n`is_variable` spits out a deprecation warning during the build (if it's\nstill something that needs to be tested we can ignore deprecated\nwarnings for the whole test instead of this change).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56305\n\nPulled By: driazati\n\nReviewed By: ezyang\n\nDifferential Revision: D27834218\n\nfbshipit-source-id: c7bbea7e9d8099bac232a3a732a27e4cd7c7b950", "pr_number": "56305", "files_changed": ["test/cpp/api/tensor.cpp"], "labels": ["Merged", "cla signed"]}, "0e0a5471ef": {"title": "Remove an unused variable in SoftmaxWithLossOp (#56321)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56321\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27854332\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 1a9dcfdc63412069cee4444a595c3460815d3c6c", "pr_number": "56321", "files_changed": ["caffe2/operators/softmax_with_loss_op.cc"], "labels": ["Merged", "cla signed", "open source"]}, "b2dae294b6": {"title": "Fix distributed.test_jit_c10d flaky tests (#56410)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56410\n\nChanges:\n- Move create_tcp_store() helper function to common file\n- Update test_jit_c10d to retry TCP Store creation in case allocated port becomes used\n\nfixes https://github.com/pytorch/pytorch/issues/55053\n\nTest Plan: Imported from OSS\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27869560\n\nPulled By: H-Huang\n\nfbshipit-source-id: f4a6613049bb25e6f6f194214379a380968bb19c", "pr_number": "56410", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_jit_c10d.py", "torch/_C/_distributed_c10d.pyi", "torch/testing/_internal/common_distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "f74a346213": {"title": "Fix torch.hub.load(\"pytorch/vision\") fails to validate the master branch (#56138)", "body": "Summary:\nWe should iterate all pages of the branches API. Otherwise, even using \"pytorch/vision\" would fail to find master.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56138\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27872346\n\nPulled By: ailzhang\n\nfbshipit-source-id: 55881558f7980b1fb08b0d08ed6687a38df06edd", "pr_number": "56138", "files_changed": ["test/test_utils.py", "torch/hub.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "062e70590c": {"title": "Add OpInfo tests for torch.{dot, vdot, bmm, mv} (#56409)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56409\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27870769\n\nPulled By: anjali411\n\nfbshipit-source-id: a1a0e89856529a4739c7612c5b1e3c5ed2569126", "pr_number": "56409", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon", "module: testing"]}, "75651e3cc4": {"title": "Add remaining ToCs to ToC lint (#56487)", "body": "Summary:\nThe lint was originally added in https://github.com/pytorch/pytorch/issues/54974, but at the time I didn't realize that these other Markdown files also each have a table of contents:\n\n- `GLOSSARY.md`\n- `torch/csrc/jit/OVERVIEW.md`\n- `torch/csrc/jit/docs/serialization.md`\n- `torch/fx/OVERVIEW.md`\n\nThis PR adds those files to the lint, and also changes the rule from using a fixed list of filenames to a `git grep` command that finds all Markdown files containing this magic comment:\n\n```md\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56487\n\nTest Plan: The \"Lint / toc\" job in GitHub Actions.\n\nReviewed By: janeyx99\n\nDifferential Revision: D27884885\n\nPulled By: samestep\n\nfbshipit-source-id: 5462437502b17fba93abf5098e21754bf566a4fe", "pr_number": "56487", "files_changed": [".github/workflows/lint.yml", "GLOSSARY.md", "torch/csrc/jit/OVERVIEW.md", "torch/csrc/jit/docs/serialization.md", "torch/fx/OVERVIEW.md"], "labels": ["Merged", "cla signed"]}, "59b61f912a": {"title": "Switch assertWarnsOnceRegex logic to check any instead of all. (#56434)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56434\n\nIf we hit multiple TORCH_WARN from different sources when running the\nstatement, it makes more sense to me that we want to check the regex is\nmet in any one of the warning messages instead of all messages.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27871946\n\nPulled By: ailzhang\n\nfbshipit-source-id: 5940a8e43e4cc91aef213ef01e48d506fd9a1132", "pr_number": "56434", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "04de24d10a": {"title": "Separate profiling tests from p2p tests (#56412)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56412\n\nWe are investigating some flaky profiiling tests such as https://github.com/pytorch/pytorch/issues/56146. One issue is that the profiling tests are tightly coupled to these send/recv tests, hence if this test is disabled, we lose signal round send/recv collectives tests.\n\nTo mitigate this, separate the tests into ones that only test send/recv, and ones that test it with profiling. This way flakiness should not result in the send/recv only tests being disabled.\nghstack-source-id: 126920867\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D27864845\n\nfbshipit-source-id: 01f04a884482ec7741323218a7f8f4a8451eb4ae", "pr_number": "56412", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5017c5fcad": {"title": "[SPMD] Remove _specify_ddp_gpu_num method (#56425)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56425\n\nAs SPMD mode is gone, `_specify_ddp_gpu_num` becomes useless. It only checks if the module is a GPU module. This actually is already checked by the caller of this function (in fairscale and some other codebases).\n\nAdditionally also remove `enable_pytorch_sync_bn` wrapper that only calls this function and does nothing else.\nghstack-source-id: 126885376\n\nTest Plan: waitforbuildbot\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27866440\n\nfbshipit-source-id: d2fd5cf43eda25c0a2bd35f647848ec0dbd6ad0f", "pr_number": "56425", "files_changed": ["torch/nn/modules/batchnorm.py"], "labels": ["Merged", "cla signed"]}, "a8ea490f67": {"title": "Revert caffe2 print stack traces flag (#56496)", "body": "Summary:\nThis reverts the change in #56198 which broke some internal tests\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56496\n\nPulled By: driazati\n\nReviewed By: walterddr\n\nDifferential Revision: D27886611\n\nfbshipit-source-id: b04de01b3bcf886294ff7ae45776b5955ce19858", "pr_number": "56496", "files_changed": ["caffe2/utils/signal_handler.cc"], "labels": ["Merged", "cla signed"]}, "8868f9c8e3": {"title": "[TensorPipe] Use targetDevice in tensorpipe_agent. (#56346)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56346\n\nNow that TensorPipe's API has `targetDevice`, use that instead of\nmanually writing the CUDA device index in `metadata`.\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27703235\n\nfbshipit-source-id: c5b620e3b3ce619367412efdbe9fa3778f6b8869", "pr_number": "56346", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "ab20ba4427": {"title": "Fix issue with dispatch key: AutogradXPU (#56336)", "body": "Summary:\nAutomatically add dispatch key \"AutogradXPU\" with \"xpu\" tensor. And set \"fall through\" for AutogradXPU\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56336\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27872125\n\nPulled By: ailzhang\n\nfbshipit-source-id: c120c62becd577699f9aecb4c356c889bd37ad06", "pr_number": "56336", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp", "c10/core/Backend.h", "c10/core/DispatchKey.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "43eb21bff3": {"title": "[skip ci] Add simple local actions runner (#56439)", "body": "Summary:\nThis pulls out shell scripts from an action and runs them locally as a first pass at https://github.com/pytorch/pytorch/issues/55847. A helper script extracts specific steps in some order and runs them:\n\n```bash\n$ time -p make lint -j 5  # run lint with 5 CPUs\npython scripts/actions_local_runner.py \\\n        --file .github/workflows/lint.yml \\\n        --job 'flake8-py3' \\\n        --step 'Run flake8'\npython scripts/actions_local_runner.py \\\n        --file .github/workflows/lint.yml \\\n        --job 'mypy' \\\n        --step 'Run mypy'\npython scripts/actions_local_runner.py \\\n        --file .github/workflows/lint.yml \\\n        --job 'quick-checks' \\\n        --step 'Ensure no trailing spaces' \\\n        --step 'Ensure no tabs' \\\n        --step 'Ensure no non-breaking spaces' \\\n        --step 'Ensure canonical include' \\\n        --step 'Ensure no unqualified noqa' \\\n        --step 'Ensure no direct cub include' \\\n        --step 'Ensure correct trailing newlines'\npython scripts/actions_local_runner.py \\\n        --file .github/workflows/lint.yml \\\n        --job 'cmakelint' \\\n        --step 'Run cmakelint'\nquick-checks: Ensure no direct cub include\nquick-checks: Ensure canonical include\nquick-checks: Ensure no unqualified noqa\nquick-checks: Ensure no non-breaking spaces\nquick-checks: Ensure no tabs\nquick-checks: Ensure correct trailing newlines\ncmakelint: Run cmakelint\nquick-checks: Ensure no trailing spaces\nmypy: Run mypy\nSuccess: no issues found in 1316 source files\nSuccess: no issues found in 56 source files\nflake8-py3: Run flake8\n./test.py:1:1: F401 'torch' imported but unused\nreal 13.89\nuser 199.63\nsys 6.08\n```\n\nMypy/flake8 are by far the slowest, but that's mostly just because they're wasting a bunch of work linting the entire repo.\n\nIn followup, we could/should:\n* Improve ergonomics (i.e. no output unless there are errors)\n* Speed up lint by only linting files changes between origin and HEAD\n* Add clang-tidy\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56439\n\nReviewed By: samestep\n\nDifferential Revision: D27888027\n\nPulled By: driazati\n\nfbshipit-source-id: d6f2a59a45e9d725566688bdac8e909210175996", "pr_number": "56439", "files_changed": [".github/workflows/lint.yml", "CONTRIBUTING.md", "Makefile", "tools/actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "ea4af1511c": {"title": "[Pytorch] Better error message for bundling inputs a second time (#56086)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56086\n\nghstack-source-id: 126671245\n\nTest Plan: unittest\n\nReviewed By: dhruvbird\n\nDifferential Revision: D27778582\n\nfbshipit-source-id: 6b59aa7ddb25c1b3162bbffdf0dd212a96f22bd3", "pr_number": "56086", "files_changed": ["test/test_bundled_inputs.py", "torch/utils/bundled_inputs.py"], "labels": ["Merged", "cla signed"]}, "12b5e666b0": {"title": "add codegen subdirectories to mypy-strict.ini (#56523)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56523\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet, samestep\n\nDifferential Revision: D27890855\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 78cd725bcf534b8410bdfaf93d2eb681e8a56ff7", "pr_number": "56523", "files_changed": ["mypy-strict.ini"], "labels": ["Merged", "cla signed"]}, "c65284aa07": {"title": "Remove caption for Lang Reference (#56526)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56526\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar, gmagogsfm\n\nDifferential Revision: D27891208\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 50da4f08a01b5407c9a1ead535539a5a26aea0f7", "pr_number": "56526", "files_changed": ["docs/source/jit.rst"], "labels": ["Merged", "cla signed"]}, "7d4e9bdba1": {"title": "Add type hint for SequentialSampler (#56374)", "body": "Summary:\nAdd type hint for SequentialSampler\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56374\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27884528\n\nPulled By: ejguan\n\nfbshipit-source-id: 68eb900643098565743245c843e76e464f981458", "pr_number": "56374", "files_changed": ["torch/utils/data/dataloader.py", "torch/utils/data/sampler.py"], "labels": ["Merged", "cla signed", "module: dataloader", "open source", "triaged"]}, "1e03a2505f": {"title": "add channels last for MaxPool2d (#56361)", "body": "Summary:\nadd channels last support for MaxPool2d.\nthis one is a replacement of https://github.com/pytorch/pytorch/pull/48917\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56361\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27874142\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: bc9604def9c974d7b59621fc709a39948088b992", "pr_number": "56361", "files_changed": ["aten/src/ATen/cpu/vec256/vec256_base.h", "aten/src/ATen/cpu/vec256/vec256_double.h", "aten/src/ATen/cpu/vec256/vec256_float.h", "aten/src/ATen/cpu/vec256/vec256_float_neon.h", "aten/src/ATen/native/DilatedMaxPool2d.cpp", "aten/src/ATen/native/Pool.h", "aten/src/ATen/native/cpu/MaxPoolKernel.cpp", "aten/src/ATen/test/vec256_test_all_types.cpp", "test/test_nn.py", "tools/build_variables.bzl"], "labels": ["Merged", "ci/all", "cla signed", "open source"]}, "20e88401db": {"title": "Add monkey type config for JIT (#54513)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54513\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27881707\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: d318a5f3fc2deb7d9b2364962ec709c6bbb68b2c", "pr_number": "54513", "files_changed": ["torch/jit/_monkeytype_config.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "15734f5b6f": {"title": "Ignore warnings for record_function_ops (#56543)", "body": "Summary:\nThis hides the warnings from #35026 until we can fix them for real by migrating to custom classes\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56543\n\nPulled By: driazati\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27895085\n\nfbshipit-source-id: a325a5d8cefb20a5033c1a059e49c03c08514f18", "pr_number": "56543", "files_changed": ["caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "af7775ba26": {"title": "Types for caffe2/torch/testing/_internal/common_distributed.py (#55338)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55338\n\nTest Plan: Sandcastle\n\nReviewed By: pritamdamania87, ngimel\n\nDifferential Revision: D27575367\n\nfbshipit-source-id: ca8eb77967af71ce2734408b8e2e15bf64a5ab4a", "pr_number": "55338", "files_changed": ["torch/testing/_internal/common_distributed.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "3cc4dbb66d": {"title": "Expose nbins and ratio (#50398)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50398\n\nTest Plan: fbcode/caffe2/test/quantization/test_quantized_op.py\n\nDifferential Revision: D25873541\n\nfbshipit-source-id: 7c3cdbb38a1e943e7fa8943a4195dc65d9d95105", "pr_number": "50398", "files_changed": ["aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/library.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "1d8053655d": {"title": "Rename AutoNonVariableTypeMode to AutoDispatchBelowAutograd and add a warning. (#56422)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56422\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D27866608\n\nPulled By: ailzhang\n\nfbshipit-source-id: 507bbcaa4c25edf23e67162780efaa70f64ad14a", "pr_number": "56422", "files_changed": ["aten/src/ATen/core/LegacyTypeDispatch.h", "test/cpp/api/inference_mode.cpp"], "labels": ["Merged", "cla signed"]}, "3d904b56ec": {"title": "s/AutoNonVariableTypeMode/AutoDispatchBelowAutograd/ (#56423)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56423\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D27866606\n\nPulled By: ailzhang\n\nfbshipit-source-id: e3942356dc3133d1c5722de40ec0d45e6a60f2f1", "pr_number": "56423", "files_changed": ["aten/src/ATen/ScalarOps.cpp", "aten/src/ATen/TensorIndexing.cpp", "aten/src/ATen/TracerMode.h", "aten/src/ATen/core/boxing/impl/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_function_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_stackbased_test.cpp", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor_test.cpp", "aten/src/ATen/cpp_custom_type_hack.h", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/templates/Functions.cpp", "c10/core/impl/LocalDispatchKeySet.h", "caffe2/contrib/aten/gen_op.py", "test/custom_operator/test_custom_ops.cpp", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/variable_factories.h", "torch/csrc/api/include/torch/detail/TensorDataContainer.h", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/functions/comm.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/jit/codegen/cuda/executor.cpp", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "7fff71eb9a": {"title": "Fix warnings in tensor_flatten.cpp (#55956)", "body": "Summary:\nSwitch to use `TensorOptions` instead of deprecated `.type()` to fix compiler warnings as part of #55952\n](https://our.intern.facebook.com/intern/diff/27830504/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55956\n\nPulled By: driazati\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D27830504\n\nfbshipit-source-id: f705818ddb7d8b17c0f5383f22dc431203a194d9", "pr_number": "55956", "files_changed": ["torch/csrc/cuda/comm.cpp", "torch/csrc/utils/tensor_flatten.cpp", "torch/csrc/utils/tensor_flatten.h"], "labels": ["Merged", "cla signed"]}, "594c546b69": {"title": "[PyTorch Edge] Eliminate non-determinism when generating build YAML file (#56539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56539\n\nIt seems like a potential source of non-determinism when generating YAML files during the build stems from the fact that when we write out Python lists, they get written out in list order. This isn't a problem per-se, but if you look to see how these lists are generated, you'll see that they come from sets, which are inherently [not order preserving](https://stackoverflow.com/questions/1653970/does-python-have-an-ordered-set) in Python.\n\nI can't guarantee that this removes non-determinism, but it removes all non-determinism that I know of so far. The surface area of codegen isn't sprawling, and the YAML file is generated by converting the object `toDict()` and passing it into the YAML serializer, so this should cover it (I think). Dictionaries are serialized in key order by pyyaml, so that's not a problem.\n\nThis could be releated to the elevated Android build times being seen [here](https://fb.workplace.com/groups/pytorch.edge.users/permalink/841622146708080/).\nghstack-source-id: 126987721\n\nTest Plan: Build + Sandcastle.\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D27893058\n\nfbshipit-source-id: 6d7bcb09f34c05b71fbb4a0673bac1c4c33f23d7", "pr_number": "56539", "files_changed": ["tools/codegen/selective_build/selector.py"], "labels": ["Merged", "cla signed"]}, "a4626348bc": {"title": "fix unqualified noqa lint (#56548)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56548\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27898933\n\nPulled By: suo\n\nfbshipit-source-id: dc4dcd2ab8bb145e5a548566fc299fa6e7e1928e", "pr_number": "56548", "files_changed": ["torch/jit/_monkeytype_config.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a2422cc243": {"title": "Add stricter check for function schemas with varargs (#56509)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56509\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27889626\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 5ff81a313ff53a9519d7dc9f3d6f7234d58af8e2", "pr_number": "56509", "files_changed": ["test/test_function_schema.py", "torch/csrc/jit/frontend/function_schema_parser.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "33f206b865": {"title": "[StaticRuntime] Replace StorageImpl with TensorImpl in MemoryPlanner (#56447)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56447\n\nMemoryPlanner shouldn't manage StorageImpls; instead, it should manage the TensorImpls because the StorageImpl in Tensors can change.\n\nTest Plan: CI\n\nReviewed By: ajyu\n\nDifferential Revision: D27840361\n\nfbshipit-source-id: f22165d167c70165be2934c6717b5057a8bb4d29", "pr_number": "56447", "files_changed": ["benchmarks/static_runtime/deep_wide_pt.h", "caffe2/core/test_utils.cc", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "4575028f6c": {"title": "Update script API to take example inputs (#55376)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55376\n\nTest Plan: Imported from OSS\n\nReviewed By: driazati, gmagogsfm\n\nDifferential Revision: D27897350\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 4f63235b9eae898c8f4ccaec3fcf64b4b29c860e", "pr_number": "55376", "files_changed": ["torch/jit/__init__.py", "torch/jit/_script.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7929bc76a0": {"title": "[shape inference] Fix dim type for Cast", "body": "Summary: ATT\n\nTest Plan: unit test\n\nReviewed By: yinghai\n\nDifferential Revision: D27904584\n\nfbshipit-source-id: b62d2eb5da0be79091c82e6300dd0c075a0bf2fe", "pr_number": null, "files_changed": ["caffe2/opt/bound_shape_inferencer.cc"], "labels": []}, "d168eae114": {"title": "make torch.testing error messages more expressive (#55145)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55145\n\nRepeating the discussion from https://github.com/pytorch/pytorch/pull/54784#issuecomment-811792089\n\nThe error messages for mismatched values are directly adapted from the old `_compare_tensors_internal`:\n\nhttps://github.com/pytorch/pytorch/blob/50cb75edcef41b82fefe957f95b470dfffe53710/torch/testing/__init__.py#L104-L111\n\nA sample error message right now looks like this\n\n```\nWith rtol=1.3e-06 and atol=1e-05, found 1 different element(s) out of 12 (8.3%). The greatest difference of 4.0 (5.0 vs. 9.0) occurred at index (2, 3)\n```\n\nUsing the same data with `numpy.testing.assert_equal` gives the following output:\n\n```\nNot equal to tolerance rtol=1.3e-06, atol=1e-05\n\nMismatched elements: 1 / 12 (8.33%)\nMax absolute difference: 4.\nMax relative difference: 0.44444445\n x: array([[5., 5., 5., 5.],\n       [5., 5., 5., 5.],\n       [5., 5., 5., 5.]], dtype=float32)\n y: array([[5., 5., 5., 5.],\n       [5., 5., 5., 5.],\n       [5., 5., 5., 9.]], dtype=float32)\n```\n\nPros:\n\n- The info is presented in a list instead of a sentence. IMO this makes it more readable\n- The maximum relative difference is reported, which is beneficial in case a comparison fails due to the `rtol`\n\nCons:\n\n- The values of the inputs are reported (this can be disabled by passing `verbose=False`, but lets face it: most users will use the default setting). In case the inputs are large, the output gets truncated with `...`. Not only is it hard to visually find the mismatching values, they could also live within the truncated part, making the output completely useless.\n- Even when visually find the offending values it is hard to parse this back to the index in the inputs.\n\nThis implements a mix of both to get a short but expressive message:\n\n```\nTensors are not close according to rtol=1.3e-6 and atol=1e-05:\n\nMismatched elements: 1 / 12 (8.3%)\nMax. rel. diff.: 4.44e-1 at (2, 3)\nMax. abs. diff.: 4.0 at (2, 3)\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27877157\n\nPulled By: mruberry\n\nfbshipit-source-id: 6898a995f116f127e3ae8ed0bcb1ada63eadc45a", "pr_number": "55145", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "46a1ac40d9": {"title": "fix meta() calls for non-storage tensors (i.e. xla) (#56306)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56306\n\nIt turns out that TensorIteratorBase `meta()` calls don't work with XLA tensors, since the logic that builds up the `TensorIteratorBase` object also tries to grab/store the underlying tensors' data pointers. This doesn't work for XLA because they don't have storage.\n\nI think it's fine to just skip this bit of logic for tensors that don't have storage- since the data_ptr information isn't important to the meta call, and TensorIterator isn't actually used in the implementation for non-native kernels, i.e. XLA.\n\nTest Plan: Imported from OSS\n\nReviewed By: bhosmer\n\nDifferential Revision: D27883949\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 7db4358b94b23c504a383f9673dc509c4020a708", "pr_number": "56306", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed"]}, "75024e228c": {"title": "Add lint for unqualified `type: ignore` (#56290)", "body": "Summary:\nThe other half of https://github.com/pytorch/pytorch/issues/56272.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56290\n\nTest Plan:\nCI should pass on the tip of this PR, and we know that the lint works because the following CI runs (before this PR was finished) failed:\n\n- https://github.com/pytorch/pytorch/runs/2384511062\n- https://github.com/pytorch/pytorch/actions/runs/765036024\n\nReviewed By: seemethere\n\nDifferential Revision: D27867219\n\nPulled By: samestep\n\nfbshipit-source-id: e648f07b6822867e70833e23ddafe7fb7eaca235", "pr_number": "56290", "files_changed": [".github/workflows/lint.yml", "benchmarks/distributed/pipeline/pipe.py", "benchmarks/functional_autograd_benchmark/torchvision_models.py", "benchmarks/instruction_counts/core/expand.py", "test/distributed/elastic/rendezvous/etcd_rendezvous_backend_test.py", "test/distributed/optim/test_zero_redundancy_optimizer.py", "test/package/package_a/fake_interface.py", "test/package/test_analyze.py", "test/package/test_dependency_api.py", "test/package/test_glob_group.py", "test/package/test_importer.py", "test/package/test_mangling.py", "test/package/test_misc.py", "test/package/test_model.py", "test/package/test_package_fx.py", "test/package/test_package_script.py", "test/package/test_resources.py", "test/package/test_save_load.py", "test/test_bundled_inputs.py", "test/test_datapipe.py", "test/test_futures.py", "test/test_fx.py", "test/test_numpy_interop.py", "test/test_torch.py", "test/test_type_hints.py", "test/test_utils.py", "tools/autograd/gen_python_functions.py", "tools/autograd/load_derivatives.py", "tools/codegen/gen.py", "tools/nightly.py", "tools/pyi/gen_pyi.py", "torch/__init__.py", "torch/_jit_internal.py", "torch/_lobpcg.py", "torch/_vmap_internals.py", "torch/autograd/function.py", "torch/autograd/functional.py", "torch/autograd/gradcheck.py", "torch/autograd/profiler.py", "torch/autograd/variable.py", "torch/backends/cudnn/__init__.py", "torch/backends/cudnn/rnn.py", "torch/cuda/__init__.py", "torch/cuda/memory.py", "torch/distributed/benchmarks/benchmark_ddp_rpc.py", "torch/distributed/distributed_c10d.py", "torch/distributed/elastic/metrics/__init__.py", "torch/distributed/elastic/multiprocessing/api.py", "torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py", "torch/distributed/elastic/rendezvous/etcd_server.py", "torch/distributed/elastic/rendezvous/utils.py", "torch/distributed/launcher/api.py", "torch/distributed/optim/zero_redundancy_optimizer.py", "torch/distributed/pipeline/sync/batchnorm.py", "torch/distributed/pipeline/sync/checkpoint.py", "torch/distributed/pipeline/sync/copy.py", "torch/distributed/pipeline/sync/dependency.py", "torch/distributed/pipeline/sync/microbatch.py", "torch/distributed/pipeline/sync/pipe.py", "torch/distributed/pipeline/sync/skip/portal.py", "torch/distributed/pipeline/sync/skip/skippable.py", "torch/distributed/rpc/api.py", "torch/distributions/kl.py", "torch/fft/__init__.py", "torch/functional.py", "torch/futures/__init__.py", "torch/fx/__init__.py", "torch/fx/experimental/graph_manipulation.py", "torch/fx/experimental/optimization.py", "torch/fx/graph_module.py", "torch/fx/interpreter.py", "torch/fx/node.py", "torch/fx/operator_schemas.py", "torch/fx/passes/split_module.py", "torch/fx/proxy.py", "torch/fx/symbolic_trace.py", "torch/hub.py", "torch/jit/__init__.py", "torch/jit/_builtins.py", "torch/jit/_check.py", "torch/jit/_monkeytype_config.py", "torch/jit/_recursive.py", "torch/jit/_script.py", "torch/jit/_serialization.py", "torch/jit/_trace.py", "torch/jit/annotations.py", "torch/jit/mobile/__init__.py", "torch/jit/quantized.py", "torch/linalg/__init__.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/lazy.py", "torch/nn/modules/linear.py", "torch/nn/modules/module.py", "torch/nn/modules/rnn.py", "torch/nn/quantizable/modules/activation.py", "torch/nn/quantizable/modules/rnn.py", "torch/nn/quantized/dynamic/modules/rnn.py", "torch/nn/quantized/modules/conv.py", "torch/nn/quantized/modules/linear.py", "torch/nn/utils/parametrize.py", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset8.py", "torch/onnx/symbolic_opset9.py", "torch/onnx/utils.py", "torch/package/_package_pickler.py", "torch/package/_package_unpickler.py", "torch/package/importer.py", "torch/package/package_importer.py", "torch/profiler/__init__.py", "torch/quantization/_learnable_fake_quantize.py", "torch/quantization/_numeric_suite.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/fx/fuse.py", "torch/quantization/fx/graph_module.py", "torch/quantization/fx/quantization_patterns.py", "torch/quantization/fx/quantize.py", "torch/quantization/fx/utils.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/pattern_utils.py", "torch/quantization/ns/utils.py", "torch/quantization/ns/weight_utils.py", "torch/quantization/quantize_fx.py", "torch/special/__init__.py", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/common_methods_invocations.py", "torch/utils/_crash_handler.py", "torch/utils/benchmark/examples/spectral_ops_fuzz_test.py", "torch/utils/bundled_inputs.py", "torch/utils/cpp_extension.py", "torch/utils/data/_decorator.py", "torch/utils/data/_typing.py", "torch/utils/data/_utils/signal_handling.py", "torch/utils/data/_utils/worker.py", "torch/utils/data/dataloader.py", "torch/utils/data/datapipes/iter/callable.py", "torch/utils/data/datapipes/iter/combinatorics.py", "torch/utils/data/datapipes/iter/combining.py", "torch/utils/data/datapipes/iter/grouping.py", "torch/utils/data/datapipes/iter/readfilesfromtar.py", "torch/utils/data/datapipes/iter/readfilesfromzip.py", "torch/utils/data/datapipes/utils/decoder.py", "torch/utils/data/dataset.py", "torch/utils/data/distributed.py", "torch/utils/data/sampler.py", "torch/utils/show_pickle.py", "torch/utils/tensorboard/_pytorch_graph.py", "torch/utils/tensorboard/writer.py"], "labels": ["Merged", "cla signed"]}, "744360ce52": {"title": "Fix missing definitions in Vec256 for VSX (#56486)", "body": "Summary:\nShould fix https://github.com/pytorch/pytorch/issues/56474, although I have no Power PC system to test on.\n\nSleef has `copysign` support for vsx, according to https://sleef.org/ppc64.xhtml\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56486\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27890091\n\nPulled By: ezyang\n\nfbshipit-source-id: be0221f33a12f66f30d49a4cdea858ffcce1061f", "pr_number": "56486", "files_changed": ["aten/src/ATen/cpu/vec256/vsx/vec256_complex_double_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_complex_float_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_double_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_float_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_int16_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_int32_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_int64_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_qint32_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_qint8_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_quint8_vsx.h"], "labels": ["Merged", "cla signed", "open source"]}, "51d0212d0f": {"title": "generate xla codegen in-tree (#55050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55050\n\nnot ready for review yet\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D27708346\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 2289edd641f30277d7561cf2d48ec69c6a2137a9", "pr_number": "55050", "files_changed": ["aten/src/ATen/templates/aten_xla_type.h", "aten/src/ATen/templates/aten_xla_type_default.cpp", "aten/src/ATen/templates/aten_xla_type_default.h", "tools/codegen/api/cpp.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/types.py", "tools/codegen/context.py", "tools/codegen/dest/__init__.py", "tools/codegen/dest/gen_external_aten_fallbacks.py", "tools/codegen/dest/native_functions.py", "tools/codegen/gen.py", "tools/codegen/gen_backend_stubs.py", "tools/codegen/model.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "fe0e1c71a7": {"title": "Add type ignore lint to Makefile (#56587)", "body": "Summary:\nFollowup to https://github.com/pytorch/pytorch/issues/56290 which adds the new lint to the local runner from https://github.com/pytorch/pytorch/issues/56439.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56587\n\nTest Plan: Same as https://github.com/pytorch/pytorch/issues/56439.\n\nReviewed By: walterddr\n\nDifferential Revision: D27909889\n\nPulled By: samestep\n\nfbshipit-source-id: 8b67f3bc36c9b5567fe5a9e49904f2cf23a9f135", "pr_number": "56587", "files_changed": ["Makefile"], "labels": ["Merged", "cla signed"]}, "e51f73a03e": {"title": "Report test stats for macos_10_13 tests (#56429)", "body": "Summary:\nRun print_test_stats.py for macos_10_13 tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56429\n\nTest Plan: Make sure CI passes, specifically for macos_10_13\n\nReviewed By: samestep\n\nDifferential Revision: D27911557\n\nPulled By: janeyx99\n\nfbshipit-source-id: 178c0ff7786ab5c41dec9d8afa257eebda4f5a0f", "pr_number": "56429", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["Merged", "cla signed"]}, "a583b9cd86": {"title": "Fixing \"naive\" `forward` of `ModuleList` and `ModuleDict (#48785)", "body": "Summary:\n**Goal:** Making sure \"calling\"/\"forwarding\" a `ModuleList` or `ModuleDict` produce the intended `NotImpmentedError`.\n\n**Current behavior:**\nCurrently, when naively calling `forward`  user ends up with the confusing error message:\n```python\nTypeError: forward() takes 1 positional argument but 2 were given\n```\nInstead of the intended `NotImplementedError.`\nThis minor issue was brought up by vadimkantorov in issue https://github.com/pytorch/pytorch/issues/37718 [here][1], also by a confused stackoverflow user [here][2].\n\n**What this PR includes:**\nRemove `forward` altogether from `ModuleList` and `ModuleDict` to fall back on the `_forward_unimplemented` of `Module` that properly throws `NotImplementedError` regardless of input arguments.\n\nAppropriate test was added to `test_nn.py`\n\nFixes previous PR https://github.com/pytorch/pytorch/issues/48698 and PR https://github.com/pytorch/pytorch/issues/48783 (third time's a charm? I'm really sorry for the mess)\n\nTest added according to ngimel [request][3].\n\n[1]: https://github.com/pytorch/pytorch/issues/37718#issuecomment-736333345\n[2]: https://stackoverflow.com/q/65096679/1714410\n[3]: https://github.com/pytorch/pytorch/pull/48698#issuecomment-737398693\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48785\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D25359759\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 28f82386f2e9a2a9b0b0b81b16dba6b79398bd34", "pr_number": "48785", "files_changed": ["test/test_nn.py", "torch/nn/modules/container.py"], "labels": ["Merged", "cla signed", "open source"]}, "3e55fc91fd": {"title": "[pet] Remove additional @record in elastic_launch to fix file existing error", "body": "Summary:\nSince `launch_agent()` in api.py is already decorated with record, we can remove the usage in elastic_launch.\nIt also fix the bug for FileExistError on MAST\n\nWe run an experiment to count how many times record is invoked in D27901961 to ensure the assumption.\n\nTest Plan:\n```\nfbpkg build -E torchelastic_distributed_sum\n\nbuck run mode/dev-nosan //pytorch/elastic/torchelastic/tsm/fb/cli:tsm -- run_ddp --scheduler mast --fbpkg torchelastic_distributed_sum:fde7879   --nnodes 1 --nproc_per_node 1 --resource T1 --run_cfg hpcIdentity=oncall_dai_pet,hpcClusterUuid=MastNaoTestCluster main.par\n```\n\nhttps://www.internalfb.com/mast/job/tsm_wilsonhong-torchelastic_distributed_sum_a92f97e7\n\nReviewed By: borovsky-d\n\nDifferential Revision: D27902034\n\nfbshipit-source-id: e08b02d4b9c7a7c70fbb0dbcb24b95af55d2ea95", "pr_number": null, "files_changed": ["torch/distributed/elastic_launch.py"], "labels": []}, "02c9d2dc90": {"title": "Release GIL before destructing ProcessGroup classes (#56381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56381\n\nPart of fix for https://github.com/pytorch/pytorch/issues/56297\nghstack-source-id: 126943449\n\nTest Plan: sandcastle\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27855337\n\nfbshipit-source-id: 88bc9234685a6637318e35b25fa68ccbdc3cbc12", "pr_number": "56381", "files_changed": ["c10/util/intrusive_ptr.h", "torch/csrc/distributed/c10d/init.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e691f24079": {"title": "[sparsity] Moving only the C++ files from internal to OSS (#56553)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56553\n\nThis splits the previous diff into multiple parts. This introduces only the c++ files.\n\nThe unittests pass as part of the internal build. Will be put in the OSS in the later PRs\n\nTest Plan:\n`buck test mode/opt //caffe2/torch/fb/model_optimization:sparsity_test`\n\n```\nParsing buck files: finished in 2.0 sec\nCreating action graph: finished in 16.4 sec\nBuilding: finished in 55.0 sec (100%) 20264/20264 jobs, 16 updated\n  Total time: 01:13.6 min\nMore details at https://www.internalfb.com/intern/buck/build/c9c5e69e-ce00-4560-adce-58b68bc43e47\nTpx test run coordinator for Facebook. See https://fburl.com/tpx for details.\nRunning with tpx session id: 1e678a07-0689-45b4-96f3-54d0a3181996\nTrace available for this run at /tmp/tpx-20210415-161113.966600/trace.log\nStarted reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/3096224795029304\n    \u2713 ListingSuccess: caffe2/torch/fb/model_optimization:sparsity_test - main (4.186)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (1.752)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseKernels) (1.884)\n    \u2713 Pass: caffe2/torch/fb/model_optimization:sparsity_test - test_sparse_qlinear_serdes (caffe2.torch.fb.model_optimization.test.sparsity.quantized_test.TestQuantizedSparseLayers) (2.013)\nSummary\n  Pass: 3\n  ListingSuccess: 1\n```\n\nReviewed By: ailzhang\n\nDifferential Revision: D27833226\n\nfbshipit-source-id: a47707117de950a9794f79e50a544aa13542c1e1", "pr_number": "56553", "files_changed": ["aten/src/ATen/CMakeLists.txt", "aten/src/ATen/native/ao_sparse/README", "aten/src/ATen/native/ao_sparse/library.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/ao_sparse/quantized/cpu/packed_params.h", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_unpack.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qnnpack_utils.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "8a81c4dc27": {"title": "Update padding_idx docs for EmbeddingBag to better match Embedding's (#56065)", "body": "Summary:\nMatch updated `Embedding` docs from https://github.com/pytorch/pytorch/pull/54026 as closely as possible. Additionally, update the C++ side `Embedding` docs, since those were missed in the previous PR.\n\nThere are 6 (!) places for docs:\n1. Python module form in `sparse.py` - includes an additional line about newly constructed `Embedding`s / `EmbeddingBag`s\n2. Python `from_pretrained()` in `sparse.py` (refers back to module docs)\n3. Python functional form in `functional.py`\n4. C++ module options - includes an additional line about newly constructed `Embedding`s / `EmbeddingBag`s\n5. C++ `from_pretrained()` options\n6. C++ functional options\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56065\n\nReviewed By: malfet\n\nDifferential Revision: D27908383\n\nPulled By: jbschlosser\n\nfbshipit-source-id: c5891fed1c9d33b4b8cd63500a14c1a77d92cc78", "pr_number": "56065", "files_changed": ["torch/csrc/api/include/torch/nn/options/embedding.h", "torch/nn/functional.py", "torch/nn/modules/sparse.py"], "labels": ["Merged", "cla signed"]}, "7660cb880f": {"title": "Rename job to be py2-setup-validate-errormsg (#56593)", "body": "Summary:\nThis should clarify its purpose, which is:\n\n> to make sure that we give an appropriate error message when someone tries to use python2\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56593\n\nTest Plan: CI.\n\nReviewed By: gchanan\n\nDifferential Revision: D27913086\n\nPulled By: samestep\n\nfbshipit-source-id: e7555d5cab5696b19a17824383c92f25f91da2cf", "pr_number": "56593", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "4230040470": {"title": "torch: Fix flake8 errors from leftover import (#56614)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56614\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27917831\n\nPulled By: seemethere\n\nfbshipit-source-id: 90a3213080cc2c8da2bc63c8971e14f7823390a9", "pr_number": "56614", "files_changed": ["torch/distributed/elastic_launch.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8ee1347c3f": {"title": "Changes to support strides in addition to shape and dtype. (#56567)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56567\n\nThis adds stride information to the serialized JSON.\n\nThis also adds shape, dtype and stride to the graph that is printed out.\n\nTest Plan: Run unit tests.\n\nReviewed By: jfix71\n\nDifferential Revision: D27528988\n\nfbshipit-source-id: f0be92055ad7c8e525625bfd1332c2db11ba612d", "pr_number": "56567", "files_changed": ["torch/fx/experimental/graph_manipulation.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "11e26e7246": {"title": "[sparsity][refactor] Remove \"Sparsity\" from the function names (#56555)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56555\n\nRemove the \"sparse\" and \"sparsity\" from the function/variable names\n\nTest Plan: `buck test mode/opt //caffe2/torch/fb/model_optimization:sparsity_test`\n\nReviewed By: raghuramank100\n\nDifferential Revision: D27812205\n\nfbshipit-source-id: 1665253720467030b84b744f824fa7742a802542", "pr_number": "56555", "files_changed": ["aten/src/ATen/native/ao_sparse/library.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/ao_sparse/quantized/cpu/packed_params.h", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_unpack.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qnnpack_utils.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "5a09def9b0": {"title": "Support factory kwargs in torch.nn modules (#54508)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/pull/53144\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54508\n\nReviewed By: malfet\n\nDifferential Revision: D27909732\n\nPulled By: jbschlosser\n\nfbshipit-source-id: d8684b2403ab7eb336371d118799146a2520bd76", "pr_number": "54508", "files_changed": ["test/run_test.py", "test/test_module_init.py", "torch/nn/__init__.py", "torch/nn/modules/activation.py", "torch/nn/modules/adaptive.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/instancenorm.py", "torch/nn/modules/linear.py", "torch/nn/modules/normalization.py", "torch/nn/modules/rnn.py", "torch/nn/modules/sparse.py", "torch/nn/modules/transformer.py", "torch/nn/parameter.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/nn/quantizable/modules/activation.py", "torch/nn/quantizable/modules/rnn.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/activation.py", "torch/nn/quantized/modules/batchnorm.py", "torch/nn/quantized/modules/conv.py", "torch/nn/quantized/modules/normalization.py", "torch/quantization/observer.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "284e735b3f": {"title": "Set show_error_codes = True in mypy-strict.ini (#56616)", "body": "Summary:\nThis should make it easier to resolve issues surfaced by https://github.com/pytorch/pytorch/issues/56290. Also see https://github.com/pytorch/pytorch/pull/56559#discussion_r617828152 for context.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56616\n\nTest Plan:\nYou could add a type error in a strict-checked file like `tools/test_history.py`, and then run this command:\n```\n$ mypy --config=mypy-strict.ini tools/test_history.py\n```\n\nOutput before this PR:\n```\ntools/test_history.py:13:1: error: Function is missing a type annotation for one or more arguments\nFound 1 error in 1 file (checked 1 source file)\n```\n\nOutput after this PR:\n```\ntools/test_history.py:13:1: error: Function is missing a type annotation for one or more arguments  [no-untyped-def]\nFound 1 error in 1 file (checked 1 source file)\n```\n\nReviewed By: driazati\n\nDifferential Revision: D27918753\n\nPulled By: samestep\n\nfbshipit-source-id: 953926e019a7669da9004fd54498b414aec777a6", "pr_number": "56616", "files_changed": ["mypy-strict.ini"], "labels": ["Merged", "cla signed"]}, "5e4dfd0140": {"title": "Add quicklint make target (#56559)", "body": "Summary:\nThis queries the local git repo for changed files (any changed files, not just committed ones) and sends them to mypy/flake8 instead of the default (which is the whole repo, defined the .flake8 and mypy.ini files). This brings a good speedup (from 15 seconds with no cache to < 1 second from my local testing on this PR).\n\n```bash\nmake quicklint -j 6\n```\n\nIt should be noted that the results of this aren\u2019t exactly what\u2019s in the CI, since mypy and flake8 ignore the `include` and `exclude` parts of their config when an explicit list of files is passed in.\n](https://our.intern.facebook.com/intern/diff/27901577/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56559\n\nPulled By: driazati\n\nReviewed By: malfet\n\nDifferential Revision: D27901577\n\nfbshipit-source-id: 99f351cdfe5aba007948aea2b8a78f683c5d8583", "pr_number": "56559", "files_changed": [".github/workflows/lint.yml", "CONTRIBUTING.md", "Makefile", "mypy-strict.ini", "tools/actions_local_runner.py", "tools/test/test_actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "3e0c226eed": {"title": "Raise TypeErrors when IValue::getSubValues fails (#56510)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56510\n\nThe comment for `TORCH_INTERNAL_ASSERT` say to use it for \"enforcement of internal invariants in code\", meaning \"assuming no bugs in PyTorch, the conditions tested by this macro should always be true\". However this wasn't the case here, at least for the RPC code: CUDAFuture is calling the `getSubValues` method on a generic IValue of which it doesn't know (or care about) the type. It was thus sometimes triggering the internal assert when users provided non-inspectable types, which was producing an exception with a message containing \"please report a bug to PyTorch\", which was confusing to users.\n\nIt makes more sense to me to consider this a type error, which can thus be reported more clearly to the user (and, later on in this stack, to catch). Hence the difference introduced here is just the type and the message of the exception. I don't expect there to be any code depending on the old behavior (as it would mean depending on a violation of an internal invariant).\nghstack-source-id: 127035768\n\nTest Plan: Unit tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D27861066\n\nfbshipit-source-id: 6d41c922257cba5f37c7a4614d8e5ab5c7c87b92", "pr_number": "56510", "files_changed": ["aten/src/ATen/core/ivalue.cpp"], "labels": ["Merged", "cla signed"]}, "af23822112": {"title": "Gracefully handle failure of DataPtr extraction in CUDAFuture (#56511)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56511\n\nCUDAFuture needs to inspect the value in order to extract DataPtrs. Sometimes it's unable to do so. So far we've handled this by raising an error when `markCompleted` is called. In this PR I'm proposing a change, which makes `markCompleted` return successfully, but instead causes the Future to be set to an error if the DataPtr extraction fails.\n\nThe advantages I see are that user code calling `markCompleted` didn't expect it to throw, and thus wasn't catching and handle that error. Which in the best case could lead to a crash, and in the worst case could lead to the Future remaining incomplete, thus not unblocking any client waiting on it. With this change those clients would be woken up and would see the error.\nghstack-source-id: 127035772\n\nTest Plan: Unit tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D27861070\n\nfbshipit-source-id: 4bb6100a488ab35fbe3c2bc3ac6f98d166c60a0b", "pr_number": "56511", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/cuda/CUDAFuture.h"], "labels": ["Merged", "cla signed"]}, "5ddc2691d0": {"title": "Merge ivalue::Future's markCompleted and markCompletedWithDataPtrs (#56512)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56512\n\nI don't know if there was a reason to keep them separate, but since the former deferred to the latter, it seems to me that we can get the exact same behavior by merging them and making the `data_ptrs` argument optional (by giving it a default value).\nghstack-source-id: 127035767\n\nTest Plan: Unit tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D27861069\n\nfbshipit-source-id: 93a49d6959b65a8d4ab9b31accce90bf30cd441e", "pr_number": "56512", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "torch/csrc/distributed/rpc/python_functions.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7dec14a491": {"title": "Avoid defining RpcCUDAFuture subclass in TensorPipe agent (#56513)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56513\n\nThe RpcCUDAFuture class existed solely to support extracting DataPtrs from a Message class. However, this can be done more simply by using a vanilla CUDAFuture and just extracting those DataPtrs before marking it complete and passing them to markCompleted.\n\nThis allows to make the DataPtr extraction logic of CUDAFuture private again.\nghstack-source-id: 127035771\n\nTest Plan: Unit tests\n\nReviewed By: mrshenli\n\nDifferential Revision: D27861064\n\nfbshipit-source-id: b0b4df2cab7be6b4b16d5cfc888483c18fbce60e", "pr_number": "56513", "files_changed": ["aten/src/ATen/cuda/CUDAFuture.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "0911ee9108": {"title": "Split CUDAFuture into a .h and a .cpp file (#56514)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56514\n\nrohan-varma mentioned that having CUDAFuture entirely defined in a header meant having to rebuild a whole lot of things whenever it changed. In fact there's no reason not to use a .cpp file, so here I do so.\nghstack-source-id: 127035765\n\nTest Plan: Unit tests\n\nReviewed By: rohan-varma, mrshenli\n\nDifferential Revision: D27861071\n\nfbshipit-source-id: c209d54af9b52d3ad781db1b61f6fca02c637f32", "pr_number": "56514", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/CUDAFuture.cpp", "aten/src/ATen/cuda/CUDAFuture.h"], "labels": ["Merged", "cla signed"]}, "3ec6bf5d26": {"title": "Fix cuda launch error in reflection_pad2d (#56451)", "body": "Summary:\nFix https://github.com/pytorch/pytorch/issues/55222\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56451\n\nReviewed By: malfet\n\nDifferential Revision: D27912184\n\nPulled By: ngimel\n\nfbshipit-source-id: 3fc80273c30a68a247289d3fb698f99b92931731", "pr_number": "56451", "files_changed": ["aten/src/ATen/native/cuda/ReflectionPad.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source"]}, "27a0d6f1df": {"title": "AutoDispatchBelowAutograd takes no arguments. (#56424)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56424\n\nTest Plan: Imported from OSS\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D27866607\n\nPulled By: ailzhang\n\nfbshipit-source-id: b82cfb90af5bc7b4129266083fe31f8b335a5b41", "pr_number": "56424", "files_changed": ["aten/src/ATen/ScalarOps.cpp", "aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/boxing/impl/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_function_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_stackbased_test.cpp", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor_test.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "test/custom_operator/test_custom_ops.cpp", "tools/autograd/templates/variable_factories.h", "torch/csrc/api/include/torch/detail/TensorDataContainer.h", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/functions/comm.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "5e695b1271": {"title": "Use absolute path for local linter (#56633)", "body": "Summary:\nIn some cases the `__file__` here was relative, so in the linter script it ended up setting the repo root to `''`, which `asyncio` doesn't handle.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56633\n\nPulled By: driazati\n\nReviewed By: samestep\n\nDifferential Revision: D27922510\n\nfbshipit-source-id: 7e406fa374ec0e5c4917b7c11742b9457dd52668", "pr_number": "56633", "files_changed": ["tools/actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "1211bccc65": {"title": "[PyTorch] Fix const correctness for resize native functions (#55351)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55351\n\nWe incorrectly used `Tensor&` to mean \"the underlying\nTensorImpl cannot be changed\", as explained in\nhttps://github.com/zdevito/ATen/issues/27#issuecomment-330717839 .\nThis diff gets us on the path to fixing this problem: we have an\nincremental way to fix individual native functions so that we can\napply any handwritten fixes a few at a time. It gets the migration\nstarted with the `resize` family of native functions.\nghstack-source-id: 127092677\n\nTest Plan: fitsships\n\nReviewed By: ezyang\n\nDifferential Revision: D27583983\n\nfbshipit-source-id: 4eeeec85f5d268e9d0f1645eb9396914a9f9557f", "pr_number": "55351", "files_changed": ["aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/core/boxing/impl/boxing.h", "aten/src/ATen/native/README.md", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/Resize.h", "aten/src/ATen/native/ResizeCommon.h", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "tools/autograd/context.py", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_type.py", "tools/codegen/api/cpp.py", "tools/codegen/api/native.py", "tools/codegen/context.py", "tools/codegen/gen.py", "tools/codegen/local.py", "tools/codegen/model.py", "torch/csrc/autograd/TraceTypeManual.cpp", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "b79901f932": {"title": "[PyTorch] Remove non-const TensorIterator::tensor() method (#55420)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55420\n\nIt doesn't seem to be necessary, and it blocks using `c10::MaybeOwned` to support borrowing.\nghstack-source-id: 127092679\n\nTest Plan: fitsships\n\nReviewed By: ezyang\n\nDifferential Revision: D27607270\n\nfbshipit-source-id: a007e9896785c8708f8cc02035cc6f4607a0a31b", "pr_number": "55420", "files_changed": ["aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed"]}, "7e8f078a3d": {"title": "[PyTorch] Always update op.current_dtype in TensorIteratorBase::set_output (#55940)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55940\n\nSimpler way to keep current_dtype up to date than #55689.\nghstack-source-id: 127092676\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D27744064\n\nfbshipit-source-id: 23fccb8b0375f5b790439a9a1c9ac07d5fae391b", "pr_number": "55940", "files_changed": ["aten/src/ATen/TensorIterator.cpp"], "labels": ["Merged", "cla signed"]}, "01842d2bb0": {"title": "[PyTorch] Support borrowing in/out Tensors in TensorIterator (#55690)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55690\n\nJust change `OperandInfo::tensor` and\n`TensorIteratorConfig::tensors` to hold `c10::MaybeOwned<Tensor>`, and\ndeal with the consequent pointer syntax. Had to C10_ALWAYS_INLINE\nOperandInfo to preserve existing inlining behavior for whatever\ncompiler-idiosyncratic reason.\n\nThis is a separate diff from usage to enable measuring the cost of\nsupport, and because there is no reason not to send it separately.\n\nWe probably should not land this without a plan to migrate a lot of\nTensorIterator use cases to use either borrowing or structured kernels\n& borrowing.\nghstack-source-id: 127092681\n\nTest Plan:\nExisting CI for correctness.\n\nRan perf stat on existing add in-place C++ benchmark and compared to D27607270 (diff before last; previous diff is arguably part of supporting borrowing). This is a devbig with turbo off.\n\nBaseline:\n```\n Performance counter stats for '/tmp/cpp_benchmark.MaybeOwnedBaselineD27607270' (5 runs):\n\n          5,837.13 msec task-clock                #    1.000 CPUs utilized            ( +-  0.34% )\n               442      context-switches          #    0.076 K/sec                    ( +-  3.54% )\n                 5      cpu-migrations            #    0.001 K/sec                    ( +- 19.07% )\n            13,144      page-faults               #    0.002 M/sec                    ( +-  0.39% )\n    11,597,542,455      cycles                    #    1.987 GHz                      ( +-  0.32% )  (50.05%)\n    30,687,118,071      instructions              #    2.65  insn per cycle           ( +-  0.03% )  (50.08%)\n     6,247,677,215      branches                  # 1070.334 M/sec                    ( +-  0.04% )  (50.08%)\n         1,705,403      branch-misses             #    0.03% of all branches          ( +-  2.16% )  (50.05%)\n\n            # Table of individual measurements:\n            5.9025 (+0.0663) #\n            5.8276 (-0.0085) #\n            5.8151 (-0.0210) #\n            5.7842 (-0.0519) #\n            5.8511 (+0.0150) #\n\n            # Final result:\n            5.8361 +- 0.0198 seconds time elapsed  ( +-  0.34% )\n\n```\n\nAdd but don't use borrowing support:\n```\n Performance counter stats for '/tmp/cpp_benchmark.MeasureMaybeOwnedCost' (5 runs):\n\n          5,947.20 msec task-clock                #    0.999 CPUs utilized            ( +-  0.15% )\n               422      context-switches          #    0.071 K/sec                    ( +-  1.88% )\n                 3      cpu-migrations            #    0.001 K/sec                    ( +- 47.14% )\n            13,025      page-faults               #    0.002 M/sec                    ( +-  0.46% )\n    11,814,216,945      cycles                    #    1.987 GHz                      ( +-  0.12% )  (50.08%)\n    31,535,372,676      instructions              #    2.67  insn per cycle           ( +-  0.06% )  (50.09%)\n     6,482,809,438      branches                  # 1090.060 M/sec                    ( +-  0.04% )  (50.07%)\n         1,688,623      branch-misses             #    0.03% of all branches          ( +-  1.62% )  (50.07%)\n\n           # Table of individual measurements:\n           5.97105 (+0.01991) #\n           5.93649 (-0.01466) #\n           5.93568 (-0.01547) #\n           5.95940 (+0.00825) #\n           5.95310 (+0.00196) #\n\n           # Final result:\n           5.95114 +- 0.00679 seconds time elapsed  ( +-  0.11% )\n```\n\n1.87% cycles regression vs baseline\n2.76% instructions regression vs baseline\n\nReviewed By: ezyang\n\nDifferential Revision: D27607293\n\nfbshipit-source-id: 55b9873c15b0de689ae17f9c35eb4ba0d026cade", "pr_number": "55690", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed"]}, "6032ea0313": {"title": "[PyTorch] Migrate add operators to borrow in TensorIteratorBase (#55691)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55691\n\nAvoiding reference counting for these operations is\nroughly a 5% CPU time win vs not supporting borrowing at all.\nghstack-source-id: 127092680\n\nTest Plan:\nExisting CI for correctness.\n\nContinued perf stat experiment from previous diff. All results included below for reviewing convenience.\n\nBaseline:\n```\n Performance counter stats for '/tmp/cpp_benchmark.MaybeOwnedBaselineD27607270' (5 runs):\n\n          5,837.13 msec task-clock                #    1.000 CPUs utilized            ( +-  0.34% )\n               442      context-switches          #    0.076 K/sec                    ( +-  3.54% )\n                 5      cpu-migrations            #    0.001 K/sec                    ( +- 19.07% )\n            13,144      page-faults               #    0.002 M/sec                    ( +-  0.39% )\n    11,597,542,455      cycles                    #    1.987 GHz                      ( +-  0.32% )  (50.05%)\n    30,687,118,071      instructions              #    2.65  insn per cycle           ( +-  0.03% )  (50.08%)\n     6,247,677,215      branches                  # 1070.334 M/sec                    ( +-  0.04% )  (50.08%)\n         1,705,403      branch-misses             #    0.03% of all branches          ( +-  2.16% )  (50.05%)\n\n            # Table of individual measurements:\n            5.9025 (+0.0663) #\n            5.8276 (-0.0085) #\n            5.8151 (-0.0210) #\n            5.7842 (-0.0519) #\n            5.8511 (+0.0150) #\n\n            # Final result:\n            5.8361 +- 0.0198 seconds time elapsed  ( +-  0.34% )\n\n```\n\nAdd but don't use borrowing support:\n```\n Performance counter stats for '/tmp/cpp_benchmark.MeasureMaybeOwnedCost' (5 runs):\n\n          5,947.20 msec task-clock                #    0.999 CPUs utilized            ( +-  0.15% )\n               422      context-switches          #    0.071 K/sec                    ( +-  1.88% )\n                 3      cpu-migrations            #    0.001 K/sec                    ( +- 47.14% )\n            13,025      page-faults               #    0.002 M/sec                    ( +-  0.46% )\n    11,814,216,945      cycles                    #    1.987 GHz                      ( +-  0.12% )  (50.08%)\n    31,535,372,676      instructions              #    2.67  insn per cycle           ( +-  0.06% )  (50.09%)\n     6,482,809,438      branches                  # 1090.060 M/sec                    ( +-  0.04% )  (50.07%)\n         1,688,623      branch-misses             #    0.03% of all branches          ( +-  1.62% )  (50.07%)\n\n           # Table of individual measurements:\n           5.97105 (+0.01991) #\n           5.93649 (-0.01466) #\n           5.93568 (-0.01547) #\n           5.95940 (+0.00825) #\n           5.95310 (+0.00196) #\n\n           # Final result:\n           5.95114 +- 0.00679 seconds time elapsed  ( +-  0.11% )\n```\n\nNow, use the borrowing support (this diff):\n```\n Performance counter stats for '/tmp/cpp_benchmark.MakeAddBorrow' (5 runs):\n\n          5,528.58 msec task-clock                #    1.000 CPUs utilized            ( +-  0.33% )\n               451      context-switches          #    0.082 K/sec                    ( +-  4.29% )\n                 6      cpu-migrations            #    0.001 K/sec                    ( +- 34.65% )\n            13,155      page-faults               #    0.002 M/sec                    ( +-  0.32% )\n    10,985,806,260      cycles                    #    1.987 GHz                      ( +-  0.33% )  (50.09%)\n    30,657,224,792      instructions              #    2.79  insn per cycle           ( +-  0.02% )  (50.07%)\n     6,247,997,282      branches                  # 1130.127 M/sec                    ( +-  0.01% )  (50.04%)\n         1,732,507      branch-misses             #    0.03% of all branches          ( +-  1.04% )  (50.06%)\n\n            # Table of individual measurements:\n            5.5626 (+0.0356) #\n            5.4913 (-0.0357) #\n            5.5007 (-0.0263) #\n            5.5839 (+0.0569) #\n            5.4965 (-0.0305) #\n\n            # Final result:\n            5.5270 +- 0.0192 seconds time elapsed  ( +-  0.35% )\n\n```\n\n7.02% cycles improvement vs previous diff\n2.78% instructions improvement vs previous diff\n\n5.28% cycles improvement vs baseline\n0.1% instructions improvement vs baseline\n\nNote that instructions per cycle improved. This makes sense because we are avoiding memory accesses, and memory accesses manifest as instructions which take 3 (or many more in the case of a cache miss) cycles. This is also a great example of an effect that instruction counting is blind to.\n\nReviewed By: bhosmer\n\nDifferential Revision: D27607295\n\nfbshipit-source-id: 7a0205b4aba6b63febbb5966f0f5e2627815cbbe", "pr_number": "55691", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h", "aten/src/ATen/native/BinaryOps.cpp"], "labels": ["Merged", "cla signed"]}, "28f52649d8": {"title": "add dtype information for input (#55358)", "body": "Summary:\nadd dtype for all input besides input dimenstion.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55358\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27862346\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 656c5d6c9f23d723b27b44f0afc1a249ce1f3e44", "pr_number": "55358", "files_changed": ["torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d83ae5d1b7": {"title": "Add devices to TensorPipe options (#56405)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56405\n\nIf not provided, the `devices` field will be initialized to local\ndevices in local `device_maps` and corresponding devices in peers'\n`device_maps`. When processing CUDA RPC requests, the agent will\nuse a dedicated stream for each device in the devices list to 1)\naccept argument CUDA tensors 2) run user functions 3) send return\nvalue tensors.\n\ncloses #54017\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D27863133\n\nPulled By: mrshenli\n\nfbshipit-source-id: 5d078c3b6d1812f85d62b0eb0f89f2b6c82cb060", "pr_number": "56405", "files_changed": ["torch/_C/_distributed_rpc.pyi", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/options.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e0be76fb9b": {"title": "[static_runtime] fix num args for to_copy (#56441)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56441\n\nSince aten::to is overloaded, match schema to replace it with static_runtime::to_copy\n\nTest Plan:\n```\nMKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --c2_model=/data/users/ansha/tmp/adfinder/210494966_0.predictor.disagg.remote_request_only --c2_inputs=/data/users/ansha/tmp/adfinder/models/c2_remote_ro_input_data.pb --pred_net=/data/users/ansha/tmp/adfinder/models/c2_remote_ro_net2.pb --c2_sigrid_transforms_opt=1 --c2_apply_nomnigraph_passes=1 --c2_use_memonger=1 --scripted_model=/data/users/ansha/tmp/adfinder/models_dianshi/210494966_0.predictor.disagg.remote_request_only.pt --pt_inputs=/data/users/ansha/tmp/adfinder/models/remote_ro_wrapped_input_data.pt --pt_enable_static_runtime=1 --pt_cleanup_activations=1 --pt_enable_out_variant=1 --compare_results=1 --iters=1 --warmup_iters=1 --num_threads=1 --do_profile=1 --benchmark_c2_predictor=0 --do_benchmark=0\n```\n\n```\nTime per node type:\n       0.623426 ms.     55.337%. quantized::embedding_bag_4bit_rowwise_offsets (82 nodes)\n       0.331633 ms.    29.4367%. quantized::embedding_bag_byte_rowwise_offsets (71 nodes)\n       0.123163 ms.    10.9323%. aten::to (155 nodes)\n       0.038479 ms.     3.4155%. fb::lengths_to_offsets (155 nodes)\n       0.004169 ms.   0.370052%. aten::embedding_bag (2 nodes)\n       0.002549 ms.   0.226256%. static_runtime::to_copy (2 nodes)\n       0.002512 ms.   0.222972%. prim::TupleConstruct (1 nodes)\n       0.000667 ms.  0.0592048%. prim::dtype (2 nodes)\n         1.1266 ms. in Total\nStaticRuntime setup time: 0.009605 ms\nMemory allocation time: 0.001907 ms\nMemory deallocation time: 0.032401 ms\nOutputs deallocation time: 0.020876 ms\nTotal memory managed: 256 bytes\nTotal number of reused tensors: 159\n```\n\nI verified that all of the aten::to matches, for the local, local_ro, and remote_ro nets in opt and dev mode.\n\nOnly 2 of calls are replaced because the other 155 have either the input or the ouput of the op returned as an external output. This is a similar case for the other instances of aten::to in the local and local_ro nets.\n\nReviewed By: hlu1\n\nDifferential Revision: D27872350\n\nfbshipit-source-id: b72785ea2768be415faae2afcf9915aef07daec2", "pr_number": "56441", "files_changed": ["torch/csrc/jit/runtime/static/passes.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "73eaa0a5f5": {"title": "Fixing error in jit cuda on ROCm: non-constant-expression cannot be n\u2026 (#55243)", "body": "Summary:\nOn ROCm, the error when compiling was \"non-constant-expression cannot be narrowed from type 'int' to 'uint32_t'\"\nwhen compiling grid_reduction.cu.\n\nAdded typecast to fix issue.\n\nAlso, removed test skip with ROCm : re-enabling\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55243\n\nReviewed By: malfet\n\nDifferential Revision: D27917066\n\nPulled By: ngimel\n\nfbshipit-source-id: b0b7c5fc8ecd2624222b35fe060846f7d1670f07", "pr_number": "55243", "files_changed": ["test/test_jit_cuda_fuser.py", "torch/csrc/jit/codegen/cuda/runtime/grid_reduction.cu"], "labels": ["Merged", "cla signed", "module: rocm", "oncall: jit", "open source"]}, "5dcc7ac35c": {"title": "Add new scheduled job to circle-ci workflow (#55182)", "body": "Summary:\nUnder this setting the job should run 3 times a day.\n\nWhen the environment variable, `PYTORCH_TEST_WITH_SLOW_GRADCHECK` is set to `ON`, set the default value for `fast_mode` in gradchack wrapper as False. This would be overriden by whatever value the user explicitly passes in.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55182\n\nReviewed By: albanD\n\nDifferential Revision: D27919236\n\nPulled By: soulitzer\n\nfbshipit-source-id: 3a55ec6edcfc6e65fbc3a8a09c63aaea1bd1c5bf", "pr_number": "55182", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml", ".jenkins/pytorch/test.sh", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "6d7d36d255": {"title": "s/\u201cpad\u201d/\"pad\"/ in files introduced by #56065 (#56618)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56618\n\nReviewed By: albanD\n\nDifferential Revision: D27919343\n\nPulled By: malfet\n\nfbshipit-source-id: 2fac8ba5f399e050463141eba225da935c97a5ce", "pr_number": "56618", "files_changed": ["torch/csrc/api/include/torch/nn/options/embedding.h", "torch/nn/functional.py", "torch/nn/modules/sparse.py"], "labels": ["Merged", "cla signed"]}, "a970e525fd": {"title": "make ProcessGroup.Options.timeout argument private in python (#56531)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56531\n\nper discussions in\nhttps://github.com/pytorch/pytorch/pull/53663/files#r593409009, we need\nto make sure our API not confusing user by passing in both timeout in\nargument and timeout in processgroup.options. This PR tries to make the\n`ProcessGroup.Options.timeout` be a private field, and only be used in\nour test utils, for both `init_process_group` and `new_group`, we still\nallow user pass `timeout` as a separate argument. Since\n`ProcessGroupGloo.Options` only have a `timeout` config, both functions\nwill not allow passing in options for the GLOO backend.\n\nThis way we still preserve the only `timeout` API, and only allow user\nto use `ProcessGroupNCCL.Options` when needed.\n\ncc pritamdamania87 rohan-varma\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27893395\n\nPulled By: wanchaol\n\nfbshipit-source-id: cdd29c84648002226ef3d9f9f3ea67b795e64bc5", "pr_number": "56531", "files_changed": ["test/distributed/test_c10d.py", "test/distributed/test_c10d_spawn.py", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/distributed_c10d.py", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "43ad172c54": {"title": "make ProcessGroupDefaultTimeout the same as python (#56549)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56549\n\nThis make the `kProcessGroupDefaultTimeout` be the same as the python\nside, and python side directly use the pybind value instead\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27899190\n\nPulled By: wanchaol\n\nfbshipit-source-id: 388a7f42358b0abed75cf4934fb7b311fd33fee6", "pr_number": "56549", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/constants.py", "torch/lib/c10d/ProcessGroup.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "818ce1d0d2": {"title": "Add standardOps match more input type in ORT (#53813) (#56172)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56172\n\nEnable the standardOps include **Add\\Sub\\Mul\\Div\\Gemm\\Pow\\Mod**  with low precision input in ORT\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D27866136\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: f2cf5649fffefd68c0cc7b6dce94198751636727", "pr_number": "56172", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/passes/onnx/scalar_type_analysis.cpp", "torch/csrc/jit/passes/onnx/scalar_type_analysis.h", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/python/init.cpp", "torch/onnx/utils.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "0cc42809ce": {"title": "Enable skipped test for c10::complex on CUDA >= 11.2 (#50227)", "body": "Summary:\nThat test was skipped due to a compiler bug. That bug should be fixed in 11.2, so we should enable it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50227\n\nReviewed By: malfet\n\nDifferential Revision: D27909195\n\nPulled By: anjali411\n\nfbshipit-source-id: c802702079d0e521f53fc98cd0fc3ded0c12b455", "pr_number": "50227", "files_changed": ["c10/test/util/complex_test_common.h"], "labels": ["Merged", "cla signed", "open source"]}, "76fbd755c1": {"title": "Reland of \"D27708346: generate xla codegen in-tree\" (#56601)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56601\n\nUpdating it to ensure that RegistrationDeclarations.yaml is completely\nunchanged\n\nThis reverts commit 90e532f3ef17a9611e9e7a9f1f6189d4168bf084.\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27915305\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 491a025c44221690dad849f9a2166934130c0fec", "pr_number": "56601", "files_changed": ["aten/src/ATen/templates/aten_xla_type.h", "aten/src/ATen/templates/aten_xla_type_default.cpp", "aten/src/ATen/templates/aten_xla_type_default.h", "tools/codegen/api/cpp.py", "tools/codegen/api/dispatcher.py", "tools/codegen/api/types.py", "tools/codegen/context.py", "tools/codegen/dest/__init__.py", "tools/codegen/dest/gen_external_aten_fallbacks.py", "tools/codegen/dest/native_functions.py", "tools/codegen/gen.py", "tools/codegen/gen_backend_stubs.py", "tools/codegen/model.py"], "labels": ["Merged", "cla signed"]}, "df1dfd879e": {"title": "Fix errors when initializing Linear with 0 in_features (#56505)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/48152\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56505\n\nReviewed By: malfet\n\nDifferential Revision: D27919590\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 462ca280051f63c31ff588c38a9e436116c0f336", "pr_number": "56505", "files_changed": ["test/test_nn.py", "torch/nn/init.py", "torch/nn/modules/linear.py"], "labels": ["Merged", "cla signed", "module: nn", "open source", "triaged"]}, "a1299a2802": {"title": "Disable Windows GPU testing (#56655)", "body": "Summary:\nUntil https://github.com/pytorch/pytorch/issues/56654 is resolved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56655\n\nReviewed By: ilia-cher\n\nDifferential Revision: D27929002\n\nPulled By: malfet\n\nfbshipit-source-id: af741d67e4c938f632afad29e675533e1fcb445d", "pr_number": "56655", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": ["Merged", "cla signed"]}, "d24314bd2c": {"title": "Update Kineto submodule and use new metadata api (#56432)", "body": "Summary:\nUpdate Kineto submodule and use new metadata api\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56432\n\nTest Plan: CI\n\nReviewed By: chaekit\n\nDifferential Revision: D27871570\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 3556787f07a9c9e138666a62ee4cd23af6d7473b", "pr_number": "56432", "files_changed": ["third_party/kineto", "torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["Merged", "cla signed"]}, "5cc75e46fa": {"title": "Split test_c10d.py to test_c10d_common.py, test_c10d_gloo.py, test_c10d_nccl.py (#56598)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56598\n\nTest Plan: NA\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27913170\n\nfbshipit-source-id: 3439d18141131b02d55f2ca399a4c795cba2b04b", "pr_number": "56598", "files_changed": [".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/win-test-helpers/test_distributed.bat", "test/distributed/test_c10d.py", "test/distributed/test_c10d_common.py", "test/distributed/test_c10d_gloo.py", "test/distributed/test_c10d_nccl.py", "test/distributed/test_c10d_spawn.py", "test/run_test.py", "torch/distributed/CONTRIBUTING.md", "torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "426852b4f0": {"title": "Split test_c10d_spawn.py to test_c10d_spawn_gloo.py,test_c10d_spawn_nccl.py (#56599)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56599\n\nTest Plan: NA\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27913955\n\nfbshipit-source-id: 7206e589fb7d08c55d08a58a3d57dc3d210a795e", "pr_number": "56599", "files_changed": [".jenkins/pytorch/multigpu-test.sh", ".jenkins/pytorch/win-test-helpers/test_distributed.bat", "test/distributed/test_c10d_spawn_gloo.py", "test/distributed/test_c10d_spawn_nccl.py", "test/run_test.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "57cba8e601": {"title": "Use at::cpu in bench_approx (#56563)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56563\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D27902737\n\nPulled By: bertmaher\n\nfbshipit-source-id: 66962671afbb093d5ae0b9308a401536c06ce8f5", "pr_number": "56563", "files_changed": ["benchmarks/cpp/tensorexpr/bench_approx.cpp"], "labels": ["Merged", "cla signed"]}, "81b59211d4": {"title": "[static runtime] binding for aten::div_out (#56653)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56653\n\nTest Plan:\n```\n./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --scripted_model=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.local.local.pt --pt_inputs=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.input_data.container.pt --iters=500 --warmup_iters=500 --num_threads=1 --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --pt_optimize_memory=1 --compare_results=1 --do_profile=1 --adsfinder_compatibility=1\n```\n\n```\nTime per node type:\n        1.48563 ms.    35.9861%. fb::sigrid_transforms_torch_bind (1 nodes)\n        0.92385 ms.    22.3783%. aten::linear (6 nodes)\n       0.681066 ms.    16.4974%. aten::argmin (1 nodes)\n       0.239311 ms.    5.79679%. aten::matmul (1 nodes)\n       0.140157 ms.    3.39501%. fb::clip_ranges_gather_sigrid_hash_v3 (77 nodes)\n      0.0951568 ms.    2.30497%. fb::clip_ranges_gather (263 nodes)\n      0.0835801 ms.    2.02455%. aten::sub (1 nodes)\n       0.054081 ms.       1.31%. aten::repeat (1 nodes)\n      0.0424465 ms.    1.02818%. aten::norm (1 nodes)\n      0.0389049 ms.   0.942389%. fb::batch_box_cox (1 nodes)\n      0.0346992 ms.   0.840514%. aten::__getitem__ (506 nodes)\n      0.0341335 ms.    0.82681%. prim::TupleUnpack (254 nodes)\n      0.0306839 ms.   0.743252%. aten::sigmoid (2 nodes)\n      0.0280489 ms.   0.679426%. aten::mul (3 nodes)\n      0.0265321 ms.   0.642684%. fb::offsets_to_ranges (253 nodes)\n      0.0207622 ms.    0.50292%. aten::pow (1 nodes)\n      0.0202067 ms.   0.489465%. fb::simple_embedding_bag_sum (3 nodes)\n      0.0195497 ms.    0.47355%. fb::casted_batch_one_hot_lengths (1 nodes)\n      0.0184351 ms.   0.446551%. fb::concat_add_mul_replacenan_clip (1 nodes)\n       0.016382 ms.    0.39682%. aten::sum (3 nodes)\n      0.0158651 ms.   0.384299%. prim::TupleConstruct (1 nodes)\n      0.0150918 ms.   0.365567%. prim::DictConstruct (2 nodes)\n     0.00858005 ms.   0.207833%. aten::div (1 nodes)\n     0.00810684 ms.   0.196371%. fb::sigrid_hash_precompute (1 nodes)\n     0.00796325 ms.   0.192893%. static_runtime::to_copy (8 nodes)\n     0.00782038 ms.   0.189432%. prim::ListConstruct (4 nodes)\n      0.0057504 ms.   0.139291%. aten::contiguous (1 nodes)\n      0.0044688 ms.   0.108247%. aten::narrow (4 nodes)\n     0.00284054 ms.   0.068806%. aten::logit (1 nodes)\n     0.00265049 ms.  0.0642024%. aten::add (1 nodes)\n     0.00216242 ms.    0.05238%. aten::full (1 nodes)\n     0.00207732 ms.  0.0503187%. aten::relu (1 nodes)\n     0.00198412 ms.   0.048061%. fb::gather_ranges (4 nodes)\n     0.00176954 ms.  0.0428632%. aten::stack (1 nodes)\n     0.00175913 ms.  0.0426112%. static_runtime::reshape_copy (2 nodes)\n      0.0016996 ms.  0.0411692%. aten::clamp_min (1 nodes)\n     0.00128528 ms.  0.0311331%. aten::size (3 nodes)\n    0.000849156 ms.   0.020569%. aten::expand_as (1 nodes)\n    0.000757672 ms.   0.018353%. fb::clip_ranges (2 nodes)\n    0.000596224 ms.  0.0144423%. fb::lengths_to_offsets (3 nodes)\n    0.000442632 ms.  0.0107218%. static_runtime::flatten_copy (1 nodes)\n    0.000196158 ms. 0.00475151%. prim::device (1 nodes)\n        4.12833 ms. in Total\nStaticRuntime setup time: 0.000451 ms\nMemory allocation time: 0.0089336 ms\nMemory deallocation time: 0.0578358 ms\nOutputs deallocation time: 0.0431742 ms\nTotal memory managed: 947328 bytes\nTotal number of reused tensors: 31\nW0421 16:56:34.220682 1522800 PyTorchPredictorContainer.cpp:200] Failed to load metadata file\nW0421 16:56:34.220772 1522800 PyTorchPredictorContainer.cpp:457] Couldn't find model param config file xl_model_weights/model_param_config\nI0421 16:56:34.220791 1522800 PyTorchPredictorBenchLib.cpp:137] PyTorch predictor: number of prediction threads 1\nI0421 16:56:34.366667 1522800 PyTorchPredictorBenchLib.cpp:230] PyTorch run finished. Milliseconds per iter: 145.863. Iters per second: 6.85573\nI0421 16:56:34.514202 1522800 PtVsBlackBoxPredictorBenchLib.cpp:132] Finished comparing PT static runtime and jit interpreter results\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27927731\n\nfbshipit-source-id: 595883a31ba0cadf6449799d47bf2294a1d05b41", "pr_number": "56653", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "49df8993c4": {"title": "Port `scatter` and `scatter_add` to `OpInfo` (#56140)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54302\nTracking Issue https://github.com/pytorch/pytorch/issues/54261\n\n**Summary:**\n- Port `scatter` and `scatter_add` tests to `OpInfo`\n- `masked_scatter` was already ported to `OpInfo`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56140\n\nReviewed By: malfet\n\nDifferential Revision: D27918038\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 80b507fe8761cd15c967c85e0c289b568b877573", "pr_number": "56140", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "76214bb464": {"title": "Add OpInfo for torch.baddbmm (#56502)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56502\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27890939\n\nPulled By: anjali411\n\nfbshipit-source-id: 072647a05cf93aedb76df0367af71b534be77258", "pr_number": "56502", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "1f0223d6bb": {"title": "Fix bug in gaussian_nll_loss (#56469)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53964. cc albanD almson\n\n## Major changes:\n- Overhauled the actual loss calculation so that the shapes are now correct (in functional.py)\n- added the missing doc in nn.functional.rst\n\n## Minor changes (in functional.py):\n- I removed the previous check on whether input and target were the same shape. This is to allow for broadcasting, say when you have 10 predictions that all have the same target.\n- I added some comments to explain each shape check in detail. Let me know if these should be shortened/cut.\n\nScreenshots of updated docs attached.\nLet me know what you think, thanks!\n\n## Edit: Description of change of behaviour (affecting BC):\nThe backwards-compatibility is only affected for the `reduction='none'` mode. This was the source of the bug. For tensors with size (N, D), the old returned loss had size (N), as incorrect summation was happening. It will now have size (N, D) as expected.\n\n### Example\nDefine input tensors, all with size (2, 3).\n`input = torch.tensor([[0., 1., 3.], [2., 4., 0.]], requires_grad=True)`\n`target = torch.tensor([[1., 4., 2.], [-1., 2., 3.]])`\n`var = 2*torch.ones(size=(2, 3), requires_grad=True)`\n\nInitialise loss with reduction mode 'none'. We expect the returned loss to have the same size as the input tensors, (2, 3).\n`loss = torch.nn.GaussianNLLLoss(reduction='none')`\n\nOld behaviour:\n`print(loss(input, target, var)) `\n`# Gives tensor([3.7897, 6.5397], grad_fn=<MulBackward0>. This has size (2).`\n\nNew behaviour:\n`print(loss(input, target, var)) `\n`# Gives tensor([[0.5966, 2.5966, 0.5966], [2.5966, 1.3466, 2.5966]], grad_fn=<MulBackward0>)`\n`# This has the expected size, (2, 3).`\n\nTo recover the old behaviour, sum along all dimensions except for the 0th:\n`print(loss(input, target, var).sum(dim=1))`\n`# Gives tensor([3.7897, 6.5397], grad_fn=<SumBackward1>.`\n\n![doc1](https://user-images.githubusercontent.com/26558092/115391089-f7f47b00-a1d6-11eb-8726-e4da9057aee0.png)\n![doc2](https://user-images.githubusercontent.com/26558092/115391094-f925a800-a1d6-11eb-954b-afd187f42bc7.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56469\n\nReviewed By: jbschlosser, agolynski\n\nDifferential Revision: D27894170\n\nPulled By: albanD\n\nfbshipit-source-id: 197890189c97c22109491c47f469336b5b03a23f", "pr_number": "56469", "files_changed": ["docs/source/nn.functional.rst", "test/test_nn.py", "torch/nn/functional.py", "torch/nn/modules/loss.py", "torch/overrides.py"], "labels": ["Merged", "cla signed", "module: bc-breaking", "module: loss", "open source", "triaged"]}, "bdb421895a": {"title": "Remove some wildcards from mypy configs (#56645)", "body": "Summary:\nSee https://github.com/pytorch/pytorch/pull/56523#issuecomment-823562134 for context. Basically the idea is that people (including myself) keep assuming that the single-asterisk `*` wildcard means \"match in this directory and in its subdirectories\", which is _not_ true. Removing the wildcards thus reduces confusion.\n\nIdeally I would like to remove _all_ of these wildcards and then add a lint to disallow them in the future (and also greatly simplify the pattern-matching logic in `tools/mypy_wrapper.py`; see https://github.com/pytorch/pytorch/issues/55702 for context), but currently this one can't be removed:\n\n```\ntools/autograd/*.py,\n```\n\nThat is because there is a file called `tools/autograd/templates/annotated_fn_args.py` (added in https://github.com/pytorch/pytorch/issues/41575) which is not a valid Python file and thus cannot be checked by `mypy`. ezyang would it be possible to rename that file to use a suffix other than `.py`?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56645\n\nTest Plan:\n```\n$ mypy\nSuccess: no issues found in 1317 source files\n$ mypy --config=mypy-strict.ini\nSuccess: no issues found in 72 source files\n```\nThe numbers of source files should be the same before and after this PR.\n\nReviewed By: ezyang\n\nDifferential Revision: D27925207\n\nPulled By: samestep\n\nfbshipit-source-id: c17faf73665a75393d3109346a1138c2af023abb", "pr_number": "56645", "files_changed": ["mypy-strict.ini", "mypy.ini"], "labels": ["Merged", "cla signed"]}, "47d2edd597": {"title": "Fix quick-checks for operator-schemas (#56692)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56692\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27939830\n\nPulled By: malfet\n\nfbshipit-source-id: 67a054de5c58832fcd7d0df0dd37faf1ea1406fd", "pr_number": "56692", "files_changed": ["torch/fx/experimental/normalize.py", "torch/fx/node.py", "torch/fx/operator_schemas.py"], "labels": ["Merged", "cla signed", "fx"]}, "3355c30f91": {"title": "Always run all the grep-based quick-checks steps (#56700)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56700\n\nReviewed By: walterddr\n\nDifferential Revision: D27940638\n\nPulled By: samestep\n\nfbshipit-source-id: 54311ef45ec051ee29d934d501e83b3542bbb439", "pr_number": "56700", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "690c8b434f": {"title": "[static runtime] binding for aten::sub_out (#56656)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56656\n\nTest Plan:\n```\n./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --scripted_model=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.local.local.pt --pt_inputs=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.input_data.container.pt --iters=500 --warmup_iters=500 --num_threads=1 --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --pt_optimize_memory=1 --compare_results=1 --do_profile=1 --adsfinder_compatibility=1\n```\n```\nTime per node type:\n        1.85766 ms.    35.7817%. fb::sigrid_transforms_torch_bind (1 nodes)\n         1.1238 ms.    21.6464%. aten::linear (6 nodes)\n       0.858116 ms.    16.5288%. aten::argmin (1 nodes)\n       0.334183 ms.    6.43694%. aten::matmul (1 nodes)\n       0.173697 ms.     3.3457%. fb::clip_ranges_gather_sigrid_hash_v3 (77 nodes)\n       0.118827 ms.    2.28881%. fb::clip_ranges_gather (263 nodes)\n       0.101348 ms.    1.95215%. aten::sub (1 nodes)\n      0.0748209 ms.    1.44118%. aten::repeat (1 nodes)\n      0.0582576 ms.    1.12214%. aten::norm (1 nodes)\n      0.0474353 ms.   0.913686%. fb::batch_box_cox (1 nodes)\n      0.0457588 ms.   0.881393%. aten::__getitem__ (506 nodes)\n      0.0435175 ms.   0.838222%. prim::TupleUnpack (254 nodes)\n      0.0425416 ms.   0.819425%. aten::sigmoid (2 nodes)\n      0.0383822 ms.   0.739308%. fb::offsets_to_ranges (253 nodes)\n      0.0330187 ms.   0.635996%. aten::mul (3 nodes)\n       0.027534 ms.   0.530352%. fb::simple_embedding_bag_sum (3 nodes)\n      0.0274914 ms.   0.529532%. aten::pow (1 nodes)\n      0.0236733 ms.   0.455989%. fb::casted_batch_one_hot_lengths (1 nodes)\n       0.023348 ms.   0.449723%. fb::concat_add_mul_replacenan_clip (1 nodes)\n      0.0193511 ms.   0.372735%. aten::sum (3 nodes)\n      0.0188839 ms.   0.363737%. prim::DictConstruct (2 nodes)\n      0.0183191 ms.   0.352858%. prim::TupleConstruct (1 nodes)\n      0.0119029 ms.    0.22927%. aten::div (1 nodes)\n      0.0103263 ms.   0.198902%. static_runtime::to_copy (8 nodes)\n     0.00977658 ms.   0.188314%. prim::ListConstruct (4 nodes)\n     0.00924042 ms.   0.177986%. fb::sigrid_hash_precompute (1 nodes)\n     0.00692162 ms.   0.133322%. aten::contiguous (1 nodes)\n     0.00567485 ms.   0.109307%. aten::narrow (4 nodes)\n     0.00362285 ms.  0.0697823%. aten::logit (1 nodes)\n     0.00329995 ms.  0.0635627%. aten::add (1 nodes)\n     0.00285633 ms.  0.0550178%. aten::full (1 nodes)\n     0.00268469 ms.  0.0517118%. fb::gather_ranges (4 nodes)\n     0.00248577 ms.  0.0478803%. aten::stack (1 nodes)\n     0.00241782 ms.  0.0465715%. aten::relu (1 nodes)\n     0.00233674 ms.  0.0450096%. aten::clamp_min (1 nodes)\n     0.00222238 ms.  0.0428068%. static_runtime::reshape_copy (2 nodes)\n     0.00171177 ms.  0.0329716%. aten::size (3 nodes)\n     0.00120008 ms.  0.0231155%. aten::expand_as (1 nodes)\n     0.00112628 ms.  0.0216942%. fb::clip_ranges (2 nodes)\n     0.00103193 ms.  0.0198768%. fb::lengths_to_offsets (3 nodes)\n    0.000598624 ms.  0.0115305%. static_runtime::flatten_copy (1 nodes)\n    0.000236196 ms. 0.00454954%. prim::device (1 nodes)\n        5.19164 ms. in Total\nStaticRuntime setup time: 0.000868 ms\nMemory allocation time: 0.0109619 ms\nMemory deallocation time: 0.071791 ms\nOutputs deallocation time: 0.0560187 ms\nTotal memory managed: 1232320 bytes\nTotal number of reused tensors: 32\nW0421 17:40:52.053653 1746499 PyTorchPredictorContainer.cpp:200] Failed to load metadata file\nW0421 17:40:52.053757 1746499 PyTorchPredictorContainer.cpp:457] Couldn't find model param config file xl_model_weights/model_param_config\nI0421 17:40:52.053779 1746499 PyTorchPredictorBenchLib.cpp:137] PyTorch predictor: number of prediction threads 1\nI0421 17:40:52.185776 1746499 PyTorchPredictorBenchLib.cpp:230] PyTorch run finished. Milliseconds per iter: 131.985. Iters per second: 7.57661\nI0421 17:40:52.337853 1746499 PtVsBlackBoxPredictorBenchLib.cpp:132] Finished comparing PT static runtime and jit interpreter results\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27929253\n\nfbshipit-source-id: 5a7984ba3ce2d6d4bce0a0ab6c5e09e8c037b44e", "pr_number": "56656", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "9be2cabc45": {"title": "Pass contiguous weight to NNPACK convolution (#56569)", "body": "Summary:\nAdded TestNN.test_conv2d_discontiguous_weight to prevent further regressions\n\nFixes https://github.com/pytorch/pytorch/issues/55781\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56569\n\nReviewed By: ngimel\n\nDifferential Revision: D27926509\n\nPulled By: malfet\n\nfbshipit-source-id: fa5ce943c3e4db4aa4de1b1cba35bd399fb3c54d", "pr_number": "56569", "files_changed": ["aten/src/ATen/native/NNPACK.cpp", "test/test_nn.py"], "labels": ["Merged", "cla signed"]}, "78022aa62c": {"title": "Add more model symbolic tracing tests from torchvision (#55744)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55398\n\nGenerates tests that calls `symbolic_trace` on torchvision models and verifies the parity of outputs from eager model, `fx.GraphModule`, `jit.ScriptModule`.\n\nTest errors: GoogleNet and Inception models throw a type mismatch when scripting the traced `fx.GraphModule`.\n```\nReturn value was annotated as having type __torch__.torchvision.models.googlenet.GoogLeNetOutputs but is actually of type Tensor:\n    dropout = self.dropout(flatten);  flatten = None\n    fc = self.fc(dropout);  dropout = None\n    return fc\n    ~~~~~~~~~ <--- HERE\n```\n\nRelevant type-inconsistency https://github.com/pytorch/vision/blob/512ea299d4b2d2bbac3498a75a2d8c0190cfcb39/torchvision/models/googlenet.py#L200\n```\ntorch.jit.unused\n    def eager_outputs(self, x: Tensor, aux2: Tensor, aux1: Optional[Tensor]) -> GoogLeNetOutputs:\n        if self.training and self.aux_logits:\n            return _GoogLeNetOutputs(x, aux2, aux1)\n        else:\n            return x   # type: ignore[return-value]\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55744\n\nReviewed By: albanD\n\nDifferential Revision: D27920595\n\nPulled By: suraj813\n\nfbshipit-source-id: 01f6f2aef7badbde29b5162a7787b5af9398090d", "pr_number": "55744", "files_changed": ["test/test_fx.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "21fd5f4b79": {"title": "Document current deploy cpython build #56490 (#56600)", "body": "Summary:\nCall out the issues with cpython deps and suggest a workaround.\n\nFixes https://github.com/pytorch/pytorch/issues/56490\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56600\n\nReviewed By: albanD\n\nDifferential Revision: D27920647\n\nPulled By: wconstab\n\nfbshipit-source-id: 61a53a176eaf42a6166d649d3cb0fdfa2489e9d2", "pr_number": "56600", "files_changed": ["CMakeLists.txt", "torch/csrc/deploy/README.md"], "labels": ["Merged", "cla signed"]}, "bc3d892c20": {"title": "README: Minor improvements (#56193)", "body": "Summary:\n* Visual studio versions: clarify and shorten.\n* Remove obsolete note about a bug that has been fixed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56193\n\nReviewed By: albanD\n\nDifferential Revision: D27939766\n\nPulled By: ezyang\n\nfbshipit-source-id: e142ec04ba98d5468f28ddf2e8bba5d99d3cfc26", "pr_number": "56193", "files_changed": ["README.md"], "labels": ["Merged", "cla signed", "module: docs", "open source", "triaged"]}, "dfb65146e5": {"title": "Add RELEASE.md (#56520)", "body": "Summary:\nThe purpopse of this document is to outline our current release process\nso that users coming into the project have a better idea on how the\nactual release process works and how they can help contribute to it.\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56520\n\nReviewed By: janeyx99\n\nDifferential Revision: D27890571\n\nPulled By: seemethere\n\nfbshipit-source-id: 882a565ea8d9b9a46c9242be7cf79dede2bae63f", "pr_number": "56520", "files_changed": ["RELEASE.md"], "labels": ["Merged", "cla signed", "releng"]}, "036becf29c": {"title": "Disable TestComplexity.test_nn_module_test in fbcode (#56677)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56677\n\nThis has been failing with `RecursionError: maximum recursion depth\nexceeded while calling a Python object` in fbcode for a while now.  Obviously\nthis isn't a fix, but the test works in OSS, so...\nghstack-source-id: 127146338\n\nTest Plan:\n```\nbuck test mode/dev //caffe2/test:jit -- --exact 'caffe2/test:jit - test_nn_module_tests (jit.test_complexity.TestComplexity)' --run-disabled\n```\n\nReviewed By: Lilyjjo\n\nDifferential Revision: D27934963\n\nfbshipit-source-id: 21d9858dab9ca1ebb5b67f286e788662dd24a988", "pr_number": "56677", "files_changed": ["test/jit/test_complexity.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "187a524249": {"title": "Re-order tests based on changed files (#56666)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56666\n\nAddresses some of #56557 by checking for changed files when running tests. This will help deliver signal faster when a failing test is run. It should always be safe to at least try to re-order the tests, so there's no option to turn it off, and any error ends up bailing out of the sorting process. Time saved will change between tests, with more improvement for things that are further down the static list here:\n\nhttps://github.com/pytorch/pytorch/blob/1e9c7ad4cb1869ea3769e1c563c78bce95da5945/test/run_test.py#L32\n\nThe results vary from not much improvement ([before: 11m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307580/workflows/6ab3def6-8d63-4f41-9b8d-9c2c50f6266b/jobs/12712819/steps), [after: 10m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307578/workflows/157407b4-f850-431c-b641-d2ac97916a04/jobs/12712802/steps)) to a lot ([before: 75m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307580/workflows/6ab3def6-8d63-4f41-9b8d-9c2c50f6266b/jobs/12712884/steps), [after: 8m](https://app.circleci.com/pipelines/github/pytorch/pytorch/307578/workflows/157407b4-f850-431c-b641-d2ac97916a04/jobs/12712865/steps)), but overall there shouldn't be any regression in test timing. These results are also probably a little confounded since the test sharding will be different after re-ordering.\n\nAs a follow up we can use the target determination logic to figure out which tests to bring to front based on the actual code instead of just edits to test files\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D27934076\n\nPulled By: driazati\n\nfbshipit-source-id: 747d09ad732289d7693101803d46e9fa8e6d2f59", "pr_number": "56666", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed"]}, "0c544ebd24": {"title": "Revert to ANVTM in jni_lite due to Oculus failure.", "body": "Test Plan: FanW123 verified on her Oculus device\n\nReviewed By: FanW123\n\nDifferential Revision: D27943428\n\nfbshipit-source-id: ac1c1ca6b47937f8839ba23c9e3af0843ea086a3", "pr_number": null, "files_changed": ["android/pytorch_android/src/main/cpp/pytorch_jni_lite.cpp"], "labels": []}, "b85b89d246": {"title": "Re-enable test_device_maps_gpu (#56415)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56415\n\ncloses #53287\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D27865438\n\nPulled By: mrshenli\n\nfbshipit-source-id: 3f7fcba8b799966388cc98ffc349cb62f281c367", "pr_number": "56415", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8b3bf98cb8": {"title": "Tell codegen that SparseCsrCUDA is cuda (#56602)", "body": "Summary:\nFollow up to https://github.com/pytorch/pytorch/issues/50937, Fixes build failures in https://github.com/pytorch/pytorch/issues/56561\n\nCurrently SparseCsrCUDA is included in cpu build and also doesn't get code-generated device guards. This fixed both issues.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56602\n\nReviewed By: albanD\n\nDifferential Revision: D27921001\n\nPulled By: ezyang\n\nfbshipit-source-id: 2b3b0b66d0a7c5ef96e0817d8852d511dd954ae4", "pr_number": "56602", "files_changed": ["tools/codegen/model.py"], "labels": ["Merged", "cla signed", "open source"]}, "048087d942": {"title": "make beg_size output deterministic for EmbeddingBag (#56661)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56661\n\nUnder some conditions (requires_grad = false and mode=SUM) bag_size and max_indices will be created via at::empty and will not be modified, that is why corresponding outputs is not deterministic and causing tests to fail.\n\nTest Plan: buck test mode/opt //caffe2/benchmarks/static_runtime:static_runtime_cpptest -- --exact 'caffe2/benchmarks/static_runtime:static_runtime_cpptest - StaticRuntime.EmbeddingBag' --run-disabled\n\nReviewed By: hlu1\n\nDifferential Revision: D27931445\n\nfbshipit-source-id: fe9747094027e4e6f7c7b0771c1cd994f94fd554", "pr_number": "56661", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "2ee3f5f812": {"title": "Copy over test reports before running \"report results\" for linux test jobs (#56725)", "body": "Summary:\nThis way, if report results fail, the test reports are still saved as artifacts so we could use them to help us debug.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56725\n\nTest Plan: CI linux test to pass + see that the test reports are copied in the Run tests step\n\nReviewed By: samestep\n\nDifferential Revision: D27948434\n\nPulled By: janeyx99\n\nfbshipit-source-id: 597a2ba4fe1dca16c7b75a1399600b27f380f5cd", "pr_number": "56725", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed"]}, "679cc7eb13": {"title": "Re-enable fast winograd conv on IOS (#56021)", "body": "Summary:\nThis is the proper fix for https://github.com/pytorch/pytorch/issues/38186\nhusthyc tested locally that it indeed fixes the issue there.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56021\n\nReviewed By: ailzhang\n\nDifferential Revision: D27940362\n\nPulled By: albanD\n\nfbshipit-source-id: 020743315ce055633324ccd751c457e32ea3263d", "pr_number": "56021", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["Merged", "cla signed"]}, "2128a84a69": {"title": "Fix grad_fn bindings when saved variable freed (#56499)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54472\n\nAdds HANDLE_TH_ERRORS to python bindings for grad_fn attrs and updates tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56499\n\nReviewed By: albanD\n\nDifferential Revision: D27920742\n\nPulled By: soulitzer\n\nfbshipit-source-id: d4f7ac8c0aa2173d25517277c393f8c66de68951", "pr_number": "56499", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_autograd_functions.py", "tools/autograd/templates/python_functions.cpp", "torch/csrc/autograd/saved_variable.cpp"], "labels": ["Merged", "cla signed"]}, "5b01b3e8e8": {"title": "Introducing JitPlugin (#56708)", "body": "Summary:\nThis PR is step 1 to covering JIT'd methods and functions. Step 2 (using it in CI) is here: https://github.com/pytorch/pytorch/issues/56310.\n\n1. This PR introduces a package `coverage_plugins` that hosts JITPlugin.\n2. We also bring in a `.coveragerc` file that is used in CI to omit the files we don't want to report on (e.g., temporary directories or test or utils.)\n\n**Disclaimer: This PR does NOT use the plug-in. Nothing should change as a result.**\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56708\n\nTest Plan:\nCI. Coverage should not go down.\n\nIf you're interested in testing this plug-in locally, you should:\n`pip install -e tools/coverage_plugins_package` from the root directory.\nAdd the following lines to `.coveragerc` under `[run]`\n```\nplugins =\n    coverage_plugins.jit_plugin\n```\nAnd then try:\n`coverage run test/test_jit.py TestAsync.test_async_script_no_script_mod`\n\nYou should see `.coverage.jit` show up at the end. You can then run `coverage combine --append` and `coverage debug data` to see that some files in `torch/jit` are covered.\n\nReviewed By: samestep\n\nDifferential Revision: D27945570\n\nPulled By: janeyx99\n\nfbshipit-source-id: 78732940fcb498d5ec37d4075c4e7e08e96a8d55", "pr_number": "56708", "files_changed": [".coveragerc", ".gitignore", ".jenkins/pytorch/test.sh", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", ".jenkins/pytorch/win-test.sh", "tools/coverage_plugins_package/README.md", "tools/coverage_plugins_package/pyproject.toml", "tools/coverage_plugins_package/setup.py", "tools/coverage_plugins_package/src/coverage_plugins/__init__.py", "tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py"], "labels": ["Merged", "cla signed"]}, "31fe2bbb30": {"title": "Remove extraneous variables in windows report stats step (#56596)", "body": "Summary:\nTesting that the CIRCLE variables in the Windows test CI report stats step aren't needed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56596\n\nTest Plan: CI\n\nReviewed By: samestep\n\nDifferential Revision: D27948983\n\nPulled By: janeyx99\n\nfbshipit-source-id: 71f2ca08246eea7580e31fb632612b205fb995fc", "pr_number": "56596", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed"]}, "375687839e": {"title": "[sparsity] Moving the sparsity python files to OSS (#56617)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56617\n\nThis migrates the sparsity to the open source\n\nTest Plan: `buck test mode/opt //caffe2/test:ao`\n\nReviewed By: raghuramank100\n\nDifferential Revision: D27812207\n\nfbshipit-source-id: cc87d9d2b486269901a4ad9b483615741a1cd712", "pr_number": "56617", "files_changed": ["test/test_ao_sparse.py", "torch/ao/__init__.py", "torch/ao/nn/__init__.py", "torch/ao/nn/sparse/__init__.py", "torch/ao/nn/sparse/quantized/__init__.py", "torch/ao/nn/sparse/quantized/dynamic/__init__.py", "torch/ao/nn/sparse/quantized/dynamic/linear.py", "torch/ao/nn/sparse/quantized/linear.py", "torch/ao/nn/sparse/quantized/utils.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "3a44d269ac": {"title": "Add periodic_ prefix to all jobs run by cron (#56695)", "body": "Summary:\nTo make them more easily distinguishable in the HUD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56695\n\nReviewed By: walterddr, samestep\n\nDifferential Revision: D27939938\n\nPulled By: malfet\n\nfbshipit-source-id: e0abd1a6bc931a89f2aa5c6e2d8ebb471c461051", "pr_number": "56695", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "cla signed"]}, "3a4344a717": {"title": "Create helper function for RPC profiling in _invoke_rpc and remote (#56643)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56643\n\nRefactor enabling rpc profiling logic in `_invoke_rpc` and `remote()` into `_rpc_profiling()` helper function.\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27922286\n\nfbshipit-source-id: 27cfe662a401756f0ee8a3cd45978d933377f78f", "pr_number": "56643", "files_changed": ["torch/distributed/rpc/api.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "febff45900": {"title": "Support factory kwargs in torch.nn modules (#54508)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/pull/53144\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54508\n\nReviewed By: albanD\n\nDifferential Revision: D27939544\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 4bf517e5f74f093e27ca38a85e732da65e44d805", "pr_number": "54508", "files_changed": ["test/run_test.py", "test/test_module_init.py", "torch/nn/__init__.py", "torch/nn/modules/activation.py", "torch/nn/modules/adaptive.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/conv.py", "torch/nn/modules/instancenorm.py", "torch/nn/modules/linear.py", "torch/nn/modules/normalization.py", "torch/nn/modules/rnn.py", "torch/nn/modules/sparse.py", "torch/nn/modules/transformer.py", "torch/nn/parameter.py", "torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py", "torch/nn/quantizable/modules/activation.py", "torch/nn/quantizable/modules/rnn.py", "torch/nn/quantized/modules/__init__.py", "torch/nn/quantized/modules/activation.py", "torch/nn/quantized/modules/batchnorm.py", "torch/nn/quantized/modules/conv.py", "torch/nn/quantized/modules/normalization.py", "torch/quantization/observer.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "bac4cfd54d": {"title": "Fix mp serialization for integer nn.Parameter on CUDA (#56529)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/56342\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56529\n\nReviewed By: albanD\n\nDifferential Revision: D27896094\n\nPulled By: ngimel\n\nfbshipit-source-id: fe817781eb7139ea57c78acfd56e7c11b61eb4ed", "pr_number": "56529", "files_changed": ["test/test_multiprocessing.py", "torch/multiprocessing/reductions.py"], "labels": ["Merged", "cla signed", "open source"]}, "08ce2300bf": {"title": "torch: Add cpython as a dependency for torch_python_obj (#56740)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56740\n\nWas running into a race condition where the torch_python_obj was\nattempting to build before cpython was actually finished installing,\nthis should resolve that issue.\n\nOnly applicable on builds that use the `USE_DEPLOY=ON` option\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D27953782\n\nPulled By: seemethere\n\nfbshipit-source-id: 76dd7c4218870eac97fc4c14e20b46128d264b30", "pr_number": "56740", "files_changed": ["torch/csrc/deploy/interpreter/CMakeLists.txt"], "labels": ["Merged", "cla signed", "module: build", "module: deploy"]}, "5c752ead3e": {"title": "Print non-breaking space directly in lint.yml (#56726)", "body": "Summary:\nAfter some fun investigating, samestep found that `\\u1234` to produce a unicode character is only supported in bash > 4.2, but MacOS ship with bash/sh 3.2, so it was searching for the literal string `u1234`. This fixes the issue by printing out the char directly via its UTF-8 bytes and `printf`.\n](https://our.intern.facebook.com/intern/diff/27952866/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56726\n\nPulled By: driazati\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27952866\n\nfbshipit-source-id: 35871e959e250dfdbbdf8b121fc92212bc0614e8", "pr_number": "56726", "files_changed": [".github/workflows/lint.yml", "tools/actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "2ea3c24c06": {"title": "Disable flaky tests (#56279)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56279\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27916606\n\nPulled By: soulitzer\n\nfbshipit-source-id: 60c07024f6eb818f4aa6730a5f9ff90d7bc2b80f", "pr_number": "56279", "files_changed": ["test/test_linalg.py", "test/test_nn.py"], "labels": ["Merged", "cla signed"]}, "d01302431c": {"title": "Enable fast gradcheck for real inputs and outputs (#55237)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55237\n\nIn this PR, we reenable fast-gradcheck and resolve misc issues that arise:\nBefore landing this PR, land #55182 so that slow tests are still being run periodically.\n\nBolded indicates the issue is handled in this PR, otherwise it is handled in a previous PR.\n\n**Non-determinism issues**:\n- ops that do not have deterministic implementation (as documented https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms)\n  - test_pad_cuda (replication_pad2d) (test_nn)\n  - interpolate (test_nn)\n  - cummin, cummax (scatter_add_cuda_kernel) (test_ops)\n  - test_fn_gradgrad_prod_cpu_float64 (test_ops)\n\nRandomness:\n  - RRelu (new module tests) - we fix by using our own generator as to avoid messing with user RNG state (handled in #54480)\n\nNumerical precision issues:\n- jacobian mismatch: test_gelu (test_nn, float32, not able to replicate locally) - we fixed this by disabling for float32 (handled in previous  PR)\n- cholesky_solve (test_linalg): #56235 handled in previous PR\n- **cumprod** (test_ops) - #56275 disabled fast gradcheck\n\nNot yet replicated:\n - test_relaxed_one_hot_categorical_2d (test_distributions)\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27920906\n\nfbshipit-source-id: 894dd7bf20b74f1a91a5bc24fe56794b4ee24656", "pr_number": "55237", "files_changed": ["test/test_nn.py", "test/test_ops.py", "torch/autograd/gradcheck.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "2078836005": {"title": "Clean up raise exception logic (#55656)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55656\n\n### For release notes\nWhat:\n - All errors that are silenced by \"raise_exception=False\" are now GradcheckError (which inherits from RuntimeError).\n\nWhy:\n - Due to a refactor of gradcheck\n\nWorkaround:\n - If you catch for 'RuntimeError' with `except RuntimeError`, since GradcheckError inherits from RuntimeError, no changes are necessary. However if you explicitly check for the errors type via `type(error)`, you'll need to update your code to check for `GradcheckError` instead.\n\nFactors out all the logic handling involving `fail_test`, `raise_exception` into 1) a wrapper around gradcheck that uses try/except 2) gradcheck_helper that always raises exception.\nThis allows us to avoid having to write the `if not x: return False` logic that is scattered throughout gradcheck currently.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27920809\n\nPulled By: soulitzer\n\nfbshipit-source-id: 253aef6d9a3b147ee37a6e37a4ce06437981929a", "pr_number": "55656", "files_changed": ["test/test_autograd.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "f84a50109f": {"title": "Move windows testers to previous image (#56626)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56626\n\nReviewed By: seemethere\n\nDifferential Revision: D27920812\n\nPulled By: malfet\n\nfbshipit-source-id: faa739ca8500654df18cf963707b31c3345132cf", "pr_number": "56626", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/header-section.yml"], "labels": ["Merged", "cla signed"]}, "461e887d92": {"title": "CPU Convolution benchmark harness for some popular models (#56455)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56455\n\nCPU convolution performance is pretty important for inference, so\ntracking performance for CNNs often boils down to finding shapes that have\neither regressed or need optimization.  This diff adds a benchmark harness that\nlets you pretty easily add new sets of convolution parameters to benchmark.\n\nI've started with an exhaustive list of layers from MobileNetV3, ResNet-18 and\nResNet-50, which are fairly popular torchvision models.  More to come if these\nprove useful.\n\nI've also added four backend configurations:\n\n- native: uses at::conv2d, which applies its own backend selection heuristics\n- mkldnn_none: uses mkldnn but applies no prepacking; uses the NCHW default\n- mkldnn_weight: prepacks weights in an mkldnn-friendly format\n- mkldnn_input: also prepacks the inputs in NCHW16c\nghstack-source-id: 127027784\n\nTest Plan: Ran this on my Skylake Xeon\n\nReviewed By: ngimel\n\nDifferential Revision: D27876139\n\nfbshipit-source-id: 950e1dfa09a33cc3acc7efd579f56df8453af1f2", "pr_number": "56455", "files_changed": ["benchmarks/cpp/CMakeLists.txt", "benchmarks/cpp/convolution.cpp", "caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "1f04494c0e": {"title": "Consolidate nondeterministic error tests (#55631)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51498\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55631\n\nReviewed By: malfet\n\nDifferential Revision: D27909953\n\nPulled By: mruberry\n\nfbshipit-source-id: 9115b2433f9c276555be55bd51b270a7a2846829", "pr_number": "55631", "files_changed": ["aten/src/ATen/Context.h", "test/test_linalg.py", "test/test_nn.py", "test/test_reductions.py", "test/test_torch.py", "torch/__init__.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_nn.py"], "labels": ["Merged", "module: determinism", "open source"]}, "7c50852a60": {"title": "moved more lowerings over (#55372)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55372\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27884601\n\nPulled By: Chillee\n\nfbshipit-source-id: 91b00182abb5dcf60209425d2717fa0303cb4932", "pr_number": "55372", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a4e47ea152": {"title": "static runtime support for fb::equally_split (#56565)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56565\n\nfb::equally_split get fused with ListUnpack and all outputs from ListUnpack getting attached to fb::equally_split.\nSo fb::equal_split will have as many outputs as ListUnpack .\n\nTest Plan:\nbuck test caffe2/torch/fb/sparsenn:fb_operators_test\n\nbuck test caffe2/torch/fb/sparsenn:test -- test_equally_split_op\n\nReviewed By: hlu1\n\nDifferential Revision: D27902824\n\nfbshipit-source-id: 7855047c3bd46bbb74b7346ac384c70b6a3e1f46", "pr_number": "56565", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/passes.cpp", "torch/csrc/jit/runtime/static/passes.h"], "labels": ["Merged", "Reverted", "cla signed", "fb-exported", "oncall: jit"]}, "83c23703b7": {"title": "Some simple optimizations (#51831)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51831\n\nReviewed By: albanD\n\nDifferential Revision: D26379122\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: d3562232f8501f2ad0b291586bf7f828e9b47010", "pr_number": "51831", "files_changed": ["aten/src/ATen/core/qualified_name.h", "c10/util/Exception.cpp", "c10/util/Exception.h", "torch/csrc/jit/runtime/logging.cpp", "torch/csrc/jit/runtime/logging.h"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "e5fda07e80": {"title": "Fix: Compare input against beta * threshold in softplus backwards (#56484)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55587\n\nThe fix converts the binary `TensorIterator` used by softplus backwards to a ternary one, adding in the original input for comparison against `beta * threshold`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56484\n\nReviewed By: malfet\n\nDifferential Revision: D27908372\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 73323880a5672e0242879690514a17886cbc29cd", "pr_number": "56484", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed"]}, "02c3e6d98a": {"title": "addmm CPU inplace implementation shouldn't resize an input tensor (#56452)", "body": "Summary:\n`addmm` CPU inplace implementation shouldn't resize an input tensor.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56452\n\nReviewed By: malfet\n\nDifferential Revision: D27925216\n\nPulled By: ngimel\n\nfbshipit-source-id: 3a4cda62ea59774ddf89f2c0592e9faffa1afe43", "pr_number": "56452", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "f2fd91ccfd": {"title": "[PyTorch] Add & document borrow_from_optional_tensor (#56647)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56647\n\nThis should be more efficient than the old hacky wrapper for optional Tensor pattern. Despite appearances, the old pattern did a reference count bump for non-empty optionals. Following diff will contain an automated change to migrate callsites.\nghstack-source-id: 127112926\n\nTest Plan: Review, CI on following change\n\nReviewed By: bhosmer\n\nDifferential Revision: D27925838\n\nfbshipit-source-id: 2c6082c5930b1e71b853a75c52873088dbc48167", "pr_number": "56647", "files_changed": ["aten/src/ATen/core/op_registration/adaption.h", "aten/src/ATen/templates/TensorBody.h"], "labels": ["Merged", "cla signed"]}, "7b7a4750a9": {"title": "[PyTorch] Migrate hacky wrapper removal to borrow_from_optional_tensor (#56648)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56648\n\nGenerated with\n\n```\nfastmod -m \"^((?P<indent>\\s*)// See \\[Note: hacky wrapper removal for optional tensor\\])\n \\s*const Tensor& (?P<varname>[A-Za-z_]+) = c10::value_or_else\\((?P<optionalname>[A-Za-z_]+), \\[\\] \\{return Tensor\\(\\);\\}\\);\" \\\n '${1}\n ${indent}c10::MaybeOwned<Tensor> ${varname}_maybe_owned = c10::borrow_from_optional_tensor(${optionalname});\n ${indent}const Tensor& ${varname} = *${varname}_maybe_owned;'\n```\nghstack-source-id: 127112928\n\nTest Plan: CI\n\nReviewed By: wenleix\n\nDifferential Revision: D27925837\n\nfbshipit-source-id: 720a4f2e3b96e14c93466698c9c4a3b9c8446a69", "pr_number": "56648", "files_changed": ["aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/LegacyNNDefinitions.cpp", "aten/src/ATen/native/Linear.cpp", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/native/LossNLL.cpp", "aten/src/ATen/native/LossNLL2d.cpp", "aten/src/ATen/native/NNPACK.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/SummaryOps.cpp", "aten/src/ATen/native/cuda/DepthwiseConv3d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/Loss.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/layer_norm_kernel.cu", "aten/src/ATen/native/cudnn/BatchNorm.cpp", "aten/src/ATen/native/cudnn/ConvPlaceholders.cpp", "aten/src/ATen/native/cudnn/RNN.cpp", "aten/src/ATen/native/group_norm.cpp", "aten/src/ATen/native/layer_norm.cpp", "aten/src/ATen/native/miopen/BatchNorm_miopen.cpp", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/miopen/RNN_miopen.cpp", "aten/src/ATen/native/mkldnn/Conv.cpp", "aten/src/ATen/native/mkldnn/Linear.cpp", "aten/src/ATen/native/mkldnn/Normalization.cpp", "aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp"], "labels": ["Merged", "cla signed"]}, "d6a25a58f5": {"title": "add hardtanh(0,6) to the set of MKLDNN fusible ops for mobilenetv2 (#56203)", "body": "Summary:\nTODO: post the numbers for mobilenetv2\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56203\n\nReviewed By: malfet\n\nDifferential Revision: D27917557\n\nPulled By: Krovatkin\n\nfbshipit-source-id: acea0f933a7e8c7a036a494295f68222c46a36f7", "pr_number": "56203", "files_changed": ["aten/src/ATen/core/interned_strings.h", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_freezing.py", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "58d12eb75e": {"title": "Allow to specify a set of device for CUDAFuture (#56515)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56515\n\nIn https://github.com/pytorch/pytorch/pull/56405 we finally found a solution to support RPC remote user functions that created/used CUDA tensors on devices that were not used by their arguments, by defining a \"bounding set\" of devices when constructing the agent and allowing all functions to freely use any of those devices.\n\nWe had the same exact problem with the callbacks of CUDAFuture, and in this PR I'm adopting the same exact solution: I allow to specify a set of devices when constructing a CUDAFuture, and then every callback is allowed to use any of those devices. (These devices will also be propagated to child futures).\n\nI'm also making ProcessGroupNCCL pass these devices. I can't yet do it for TensorPipeAgent, until #56405 lands.\nghstack-source-id: 127261552\n\nTest Plan: Added a test for this later in the stack.\n\nReviewed By: mrshenli\n\nDifferential Revision: D27861067\n\nfbshipit-source-id: 8ab2c9d06a514c0407a7e96abc3704e8d5c5dc09", "pr_number": "56515", "files_changed": ["aten/src/ATen/cuda/CUDAFuture.cpp", "aten/src/ATen/cuda/CUDAFuture.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "15ca379bde": {"title": "Add CUDA support to a user-created torch.futures.Future (#56517)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56517\n\nCurrently a torch.futures.Future could wrap a CUDAFuture, but it could not create one from scratch. This prevented users from using CUDAFutures in some occasions, for example when using `rpc.functions.async_execution`, or in their own code. I don't see any reason for such a limitation, hence here I add support for this.\nghstack-source-id: 127261554\n\nTest Plan: Added a test later in the stack\n\nReviewed By: mrshenli\n\nDifferential Revision: D27887190\n\nfbshipit-source-id: ecbb39c1ad7cd189d478ded9c361448f05a270ad", "pr_number": "56517", "files_changed": ["torch/_C/__init__.pyi.in", "torch/csrc/jit/python/init.cpp", "torch/futures/__init__.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "369e8bc4bc": {"title": "Added support for uppercase letters in torch.einsum (#56475)", "body": "Summary:\nThis PR adds support for upper case letters in `torch.einsum` equation.\n\nAddresses PR https://github.com/pytorch/pytorch/pull/55013 here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56475\n\nReviewed By: ailzhang\n\nDifferential Revision: D27948362\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 51cf57b17c4c23d88fab5343f17ba3bfbe3607a5", "pr_number": "56475", "files_changed": ["aten/src/ATen/native/Linear.cpp", "test/test_linalg.py"], "labels": ["Merged", "cla signed"]}, "acca89e25f": {"title": "Add more RRef CUDA RPC tests (#56757)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56757\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D27959592\n\nPulled By: mrshenli\n\nfbshipit-source-id: b72c873bcaef4515b0fc8d48ae539477e1850a40", "pr_number": "56757", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "22b151a3ba": {"title": "Make sure full backward hook fire when no input requires grad (#56693)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/56380\n\nBC-breaking note:\nThis changes the behavior of full backward hooks as they will now fire properly even if no input to the Module require gradients.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56693\n\nReviewed By: ezyang\n\nDifferential Revision: D27947030\n\nPulled By: albanD\n\nfbshipit-source-id: e8353d769ba5a2c1b6bdf3b64e2d61308cf624a2", "pr_number": "56693", "files_changed": ["test/test_nn.py", "torch/utils/hooks.py"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "d1fe68e70b": {"title": "To add single and chained learning schedulers to docs (#56705)", "body": "Summary:\nIn the optimizer documentation, many of the learning rate schedulers [examples](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) are provided according to a generic template. In this PR we provide a precise simple use case example to show how to use learning rate schedulers. Moreover, in a followup example we show an example how to chain two schedulers next to each other.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56705\n\nReviewed By: ezyang\n\nDifferential Revision: D27966704\n\nPulled By: iramazanli\n\nfbshipit-source-id: f32b2d70d5cad7132335a9b13a2afa3ac3315a13", "pr_number": "56705", "files_changed": ["docs/source/optim.rst"], "labels": ["Merged", "cla signed"]}, "7e9f7fb980": {"title": "[Pytorch Edge] Prepack folding for functions besides forward (#56081)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56081\nghstack-source-id: 127205799\n\nTest Plan: unit test. Since I'm prepacking the weights of the same operators multiple times I wonder if its a just works thing?\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27777337\n\nfbshipit-source-id: 909d2a667d9eb51e205536b478a6668c33b3fb15", "pr_number": "56081", "files_changed": ["test/test_mobile_optimizer.py", "torch/csrc/jit/passes/prepack_folding.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7d2a9f2dc9": {"title": "Fix instance norm input size validation + test (#56659)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/45687\n\nFix changes the input size check for `InstanceNorm*d` to be more restrictive and correctly reject sizes with only a single spatial element, regardless of batch size, to avoid infinite variance.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56659\n\nReviewed By: pbelevich\n\nDifferential Revision: D27948060\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 21cfea391a609c0774568b89fd241efea72516bb", "pr_number": "56659", "files_changed": ["test/test_fx.py", "test/test_nn.py", "torch/nn/functional.py", "torch/testing/_internal/common_quantization.py"], "labels": ["Merged", "cla signed"]}, "798dd4665d": {"title": "Add a new API replace_input_with to node.py (#55887)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55887\n\nReviewed By: jfix71\n\nDifferential Revision: D27731389\n\nfbshipit-source-id: 754654e64c4f3a584dfea06322d833bc11bcc3cc", "pr_number": "55887", "files_changed": ["test/test_fx.py", "torch/fx/node.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "2e4c68a727": {"title": "[PyTorch][Edge] Add v4 and v5 models and remove unused model (#56751)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56751\n\n## Summary\n1. Add two models (v4 and v5) for testing runtime. (v5 will be introduced in https://github.com/pytorch/pytorch/pull/56002)\n2. Remove an unused model.\n\nSide note: these binaries are part of the test in https://github.com/pytorch/pytorch/pull/56002, and currently there is an ongoing issue to `ghexport` with binaries (post is https://fb.workplace.com/groups/533197713799375/permalink/1130109004108240/). `ghimport` can work with binary after checking temporary diff (D23336574).\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27958477\n\nPulled By: cccclai\n\nfbshipit-source-id: 2e6f985a988da55ad08fb9a5037434a2b6db0776", "pr_number": "56751", "files_changed": ["test/cpp/lite_interpreter_runtime/sequence.ptl"], "labels": ["Merged", "Reverted", "cla signed", "oncall: jit"]}, "6de1d9b2d0": {"title": "Fix bug in emitUse to drop all values that are marked as drop (#56652)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56652\n\nPrevious code doesn't drop prim::Constant values even when they are marked as drop.\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27927413\n\nfbshipit-source-id: 67cd52cf292e111be2830ccf93b0e7b089e49001", "pr_number": "56652", "files_changed": ["torch/csrc/jit/runtime/interpreter.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "0424f6af93": {"title": "Local lint fixes - missing steps, pin to bash (#56752)", "body": "Summary:\nFixes #56738\n\n* `setup_lint` now installs mypy / shellcheck\n* the shell used to execute commands is pinned to `bash` (on Ubuntu the default is `dash`, which was causing the false positives in #56738)\n* the emoji check marks don't always work, so use more basic ones instead\n* adds `Run autogen` step for mypy (for the `lint` step only since it's pretty slow)\n](https://our.intern.facebook.com/intern/diff/27972006/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56752\n\nPulled By: driazati\n\nReviewed By: samestep\n\nDifferential Revision: D27972006\n\nfbshipit-source-id: 624e6c1af2d4f7c8623f420516744922b6b829a5", "pr_number": "56752", "files_changed": [".github/workflows/lint.yml", "Makefile", "tools/actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "ed2104fe5c": {"title": "Fixing MAGMA with HIP issues (#56448)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55552\n\n* Root-caused issue to MAGMA kernels\n* Issue is fixed on master of MAGMA\nMAGMA issue: https://bitbucket.org/icl/magma/issues/43/zgetrf_batched-shfl-kernel-failure-seen-on\n* Changing PyTorch to use particular commit sha from master of MAGMA project\n* ~~Reactivating skipped ROCm tests~~ : We will reactivate tests in a different PR\n\nCorresponding PyTorch builder PR: https://github.com/pytorch/builder/pull/695\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56448\n\nReviewed By: seemethere\n\nDifferential Revision: D27974563\n\nPulled By: janeyx99\n\nfbshipit-source-id: 25e6f95a20a06d27a5199a623dd7c5db7ca8d6ea", "pr_number": "56448", "files_changed": [".circleci/docker/common/install_rocm.sh"], "labels": ["Merged", "cla signed", "open source"]}, "10fd7d8be6": {"title": "Add option to OpInfo to skip gradgrad check and empty cdist OpInfo (#56603)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56603\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27939204\n\nPulled By: albanD\n\nfbshipit-source-id: c7c80551ef3c34c822832891a99104440893ea4c", "pr_number": "56603", "files_changed": ["test/test_fx.py", "test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "3ddcc8d833": {"title": "Add more test cases for cdist OpInfo and TODOs (#56604)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56604\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27939203\n\nPulled By: albanD\n\nfbshipit-source-id: 197de148ba00d217eb0bfc5b5724d23cf6de0910", "pr_number": "56604", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "0d7e780eff": {"title": "Fix broadcasting of cdist backward (#56605)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56605\n\nFix https://github.com/pytorch/pytorch/issues/55370\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27939202\n\nPulled By: albanD\n\nfbshipit-source-id: a4ac50a7b504c24f47f5343414fb57523546a0c7", "pr_number": "56605", "files_changed": ["aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/cuda/DistanceKernel.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "e098515b89": {"title": "Fix cdist backward for empty inputs (#56606)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56606\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D27939201\n\nPulled By: albanD\n\nfbshipit-source-id: 7ac2b579577cc5b58e714935d791be26478eb83c", "pr_number": "56606", "files_changed": ["aten/src/ATen/native/Distance.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "c37095760d": {"title": "[torch distributed] Implementing all_gather_base (#56315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56315\n\nThis diff implements the all_gather_base in pytorch distributed.\n\nTest Plan: dist.all_gather_base(output, input)...\n\nReviewed By: agolynski, amylittleyang\n\nDifferential Revision: D27488999\n\nfbshipit-source-id: 937ec8bddf9527fa4d114f984d1d0f6a5b8c3936", "pr_number": "56315", "files_changed": ["test/cpp_extensions/cpp_c10d_extension.cpp", "test/cpp_extensions/cpp_c10d_extension.hpp", "test/distributed/test_c10d_nccl.py", "torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/distributed/__init__.py", "torch/distributed/distributed_c10d.py", "torch/lib/c10d/ProcessGroup.cpp", "torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupMPI.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/ProcessGroupRoundRobin.cpp", "torch/lib/c10d/ProcessGroupRoundRobin.hpp", "torch/lib/c10d/test/ProcessGroupNCCLTest.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "375ebd634a": {"title": "[PyTorch] Break up generated tag in source (#56503)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56503\n\nThe presence of `generated` causes Phabricator and hg to think the file is generated (e.g., hg won't prompt to resolve merge conflicts with an editor). Breaking up the tag is the traditional way to solve this.\nghstack-source-id: 126965382\n\nTest Plan: Review, builds\n\nReviewed By: ailzhang\n\nDifferential Revision: D27887691\n\nfbshipit-source-id: 394a38d50289d64f8801a13f9a28f6f0f37ca59d", "pr_number": "56503", "files_changed": ["tools/autograd/gen_variable_type.py"], "labels": ["Merged", "cla signed"]}, "be7a943bb8": {"title": "s/AutoDispatchBelowAutograd/AutoDispatchBelowInplaceOrView. (#56657)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56657\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27931526\n\nPulled By: ailzhang\n\nfbshipit-source-id: 3af718df3435e2b0b30bc62070dbdc5aeeecdfb4", "pr_number": "56657", "files_changed": ["aten/src/ATen/TensorIndexing.cpp", "aten/src/ATen/TracerMode.h", "aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/cpp_custom_type_hack.h", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/templates/Functions.cpp", "c10/core/DispatchKey.h", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/jit/codegen/cuda/executor.cpp", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "2041cd6707": {"title": "Enable forward/backward compatibility in TS mobile (#56079)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56079\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27828149\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 9291ddbf01853354fca0fa0a58b8115d5d2294da", "pr_number": "56079", "files_changed": ["test/cpp/jit/test_interpreter.cpp", "test/cpp/jit/test_utils.cpp", "test/cpp/jit/test_utils.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.h", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "5d940e2fbc": {"title": "[TSAN] Fix PythonEngine data-race-on-vptr. (#56808)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56808\n\nFor information about data-race-on-vptr in general, see https://www.internalfb.com/intern/wiki/TSAN/Common_Concurrency_Mistakes/Stopping_a_Thread_in_Destructor/\n\nEngine::~Engine() was previously tasked with stopping the threads. This causes a data race on the object's vptr when PythonEngine is being destructed. This fixes the data race by making ~PythonEngine trigger the thread stopping before going down to the base class's destructor.\n\nTest Plan:\nMany tests are affected, but here's one example:\n\nbuck test mode/dev-tsan -c fbcode.tsan_strict_mode=true //oculus/research/orcoptics/deep_learning/srg_nn/tests:test_grating_net -- 'test_train (oculus.research.orcoptics.deep_learning.srg_nn.tests.test_grating_net.TestGratingNet)' --run-disabled\n\nReviewed By: walterddr, albanD\n\nDifferential Revision: D27972384\n\nfbshipit-source-id: 8b70fec8d9326497c591a2777b355ea590a85082", "pr_number": "56808", "files_changed": ["torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_engine.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "bd3dda95fd": {"title": "Make old_gpu warning dynamic (#56621)", "body": "Summary:\nCompute minimum support CUDA architecture as oldest GPU arch_list\nsupported by current build\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56621\n\nReviewed By: soumith\n\nDifferential Revision: D27920141\n\nPulled By: malfet\n\nfbshipit-source-id: 71a42dd60c38a658ebad4544bcfb3d2d20e471b5", "pr_number": "56621", "files_changed": ["torch/cuda/__init__.py"], "labels": ["Merged", "cla signed"]}, "b2b9efb33a": {"title": ".github: Add initial Linux CI for CUDA (#56494)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56494\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D27953781\n\nPulled By: seemethere\n\nfbshipit-source-id: bce9298dc40d035bfbb5057e48b99d15c13733bc", "pr_number": "56494", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/scripts/install_nvidia_utils_linux.sh", ".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml", ".jenkins/pytorch/common_utils.sh", ".jenkins/pytorch/test.sh"], "labels": ["Merged", "cla signed", "module: ci"]}, "e4efc0c948": {"title": "[Static Runtime] Enable check_for_memory_leak in StaticRuntime::benchmark (#56839)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56839\n\nEnable check_for_memory_leak at the end of StaticRuntime::benchmark so this code is exercised more often.\n\nTest Plan: Checked with adindexer merge net model\n\nReviewed By: edvgha\n\nDifferential Revision: D27417911\n\nfbshipit-source-id: 5248942dc439fcc7301ffb0005da76374939fa96", "pr_number": "56839", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "a688b29750": {"title": "Support custom Python classes in CUDAFuture (#56516)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56516\n\nOne problem with CUDAFuture's extraction of DataPtrs from IValues is that it only supported Python objects that could be converted to \"regular\" IValues (e.g., lists/dicts/tuples of ints/strings/tensors/...). One notable exception are custom Python classes, which are in fact a very common data type transferred over RPC. The only solution we found for those is to use the Python pickler to extract the tensors contained in them.\n\nWe can't insert a Python dependency directly into CUDAFuture, so instead I'm proposing to use the same indirection technique used to support `getSubValues` on Python objects: define some methods on the abstract class `PyObjectHolder` (which can be used by CUDAFuture) but only implement them in the concrete subclass `ConcretePyObjectHolder` (which is only built when Python support is enabled).\n\nI am a bit worried about the performance toll of this (pickling isn't exactly known to be cheap) but I think we should start by providing a functionally complete API. We already have ideas on how to make this faster if needed, for example by having users provide a custom DataPtr extractor tailored to their class via a decorator. (Or just use TorchScript).\nghstack-source-id: 127295014\n\nTest Plan: Added a test later in the stack\n\nReviewed By: mrshenli\n\nDifferential Revision: D27887189\n\nfbshipit-source-id: 9d27e4e62390b836e5bb4f06f401cc002f0cf95b", "pr_number": "56516", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/cuda/CUDAFuture.cpp", "torch/_jit_internal.py", "torch/csrc/jit/python/python_ivalue.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c416167fb7": {"title": "Add tests for CUDAFuture (#56518)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56518\n\nI don't think we have any tests for CUDAFuture (I couldn't find any, and I didn't write any in the past). I think especially for the two latest features added by this stack we should have a test to ensure they properly work and to catch regressions. (These tests also add indirect coverage for the more \"basic\" features of CUDAFuture).\n\nI didn't know how/where to add tests for C++ ATen stuff, so instead I added these tests to the Python RPC suite, using the torch.futures.Future wrapper. (It made sense in my mind because RPC is the main user of CUDAFuture). I'll gladly accept pointers to better ways of doing this.\nghstack-source-id: 127295022\n\nTest Plan: The tests themselves.\n\nReviewed By: mrshenli\n\nDifferential Revision: D27887191\n\nfbshipit-source-id: 4ad6d81e676fe486aa8d329591ee1a3818fea059", "pr_number": "56518", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "45692fbef0": {"title": "[fx splitter][fx net_min] Move Splitter, Minimizer and necessary deps to OSS (#56201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56201\n\nRefactor Splitter and Minimizer to superclass `_SplitterBase` and `_MinimizerBase` and move them to OSS. This is needed to create an OSS example of GPU lowering with those tools.\n\nTest Plan: CI\n\nReviewed By: jackm321\n\nDifferential Revision: D27629598\n\nfbshipit-source-id: 0d4da02105ca509b31f1a6c4a39b1122c2bc7bf0", "pr_number": "56201", "files_changed": ["torch/fx/passes/graph_drawer.py", "torch/fx/passes/net_min_base.py", "torch/fx/passes/operator_support.py", "torch/fx/passes/split_utils.py", "torch/fx/passes/splitter_base.py", "torch/fx/passes/tools_common.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "6e5ce569bd": {"title": "DOC: add note for torch.clamp() special case min > max See #45664 (#56367)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/45664\n\nThis PR adds a note to the documentation for `torch.clamp()` to alert users to a special case: If `min` is greater than `max`, all values are set to the `max` value.\n\nAlso, an example was added after the first code example. And this one is referenced in the note.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56367\n\nReviewed By: ezyang\n\nDifferential Revision: D27960553\n\nPulled By: mruberry\n\nfbshipit-source-id: 9dc6016ccacebe87c809a0dd9f557b4aea0ae6f5", "pr_number": "56367", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: docs", "open source", "triaged"]}, "710288e413": {"title": "torch.fft: Document out argument (#56732)", "body": "Summary:\nAn oversight from https://github.com/pytorch/pytorch/issues/49335, the documentation was never updated to include `out` arguments.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56732\n\nReviewed By: ezyang\n\nDifferential Revision: D27960478\n\nPulled By: mruberry\n\nfbshipit-source-id: a342a4f590369d6d2e17bed014fa64e49ee72936", "pr_number": "56732", "files_changed": ["torch/fft/__init__.py"], "labels": ["Merged", "cla signed", "module: fft", "open source"]}, "dbf3451c6e": {"title": "Add support for checking tensor containers in `torch.testing` (#55385)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55385\n\nThis renames `assert_tensors_(equal|close)` to `_check_tensors_(equal|close)` and exposes two new functions: `assert_(equal|close)`. In addition to tensor pairs, the newly added functions also support the comparison of tensors in sequences or mappings. Otherwise their signature stays the same.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27903805\n\nPulled By: mruberry\n\nfbshipit-source-id: 719d19a1d26de8d14cb25846e3d22a6ac828c80a", "pr_number": "55385", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "27148db5df": {"title": "Add support for scalars and numpy in torch.testing (#55786)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55786\n\nAdd support to compare scalars as well as `np.ndarray`'s with torch.testing. We are reusing the mathcing functionality that is already in place for tensors, by casting the inputs. The approach can easily extended if we want to support other input types as long as they can be cast to a tensor.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27903814\n\nPulled By: mruberry\n\nfbshipit-source-id: fe3d063d0c9513cbd8b3408a2023e94c490c817e", "pr_number": "55786", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "edfbc989d1": {"title": "add support for equal_nan in torch.testing.assert_close (#55788)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55788\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27903821\n\nPulled By: mruberry\n\nfbshipit-source-id: c10254b2cdc7c1ae5a31b22913136013f0472b26", "pr_number": "55788", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "805129f957": {"title": "enable support for custom error messages in `torch.testing` (#55890)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55890\n\nProof-of-concept for https://github.com/pytorch/pytorch/pull/55145#issuecomment-817297273\n\nWith this the user is able to pass a custom error message to `assert_(equal|close)` which will be used in case the values mismatch. Optionally, a callable can be passed which will be called with mismatch diagnostics and should return an error message:\n\n```python\ndef make_msg(a, b, info):\n    return (\n        f\"Argh, we found {info.total_mismatches} mismatches! \"\n        f\"That is {info.mismatch_ratio:.1%}!\"\n    )\n\ntorch.testing.assert_equal(torch.tensor(1), torch.tensor(2), msg=make_msg)\n```\n\nIf you imagine `a` and `b` as the outputs of binary ufuncs, the error message could look like this:\n\n```python\ndef make_msg(input, torch_output, numpy_output, info):\n    return (\n        f\"For input {input} torch.binary_op() and np.binary_op() do not match: \"\n        f\"{torch_output} != {numpy_output}\"\n    )\n\ntorch.testing.assert_equal(\n    torch.binary_op(input),\n    numpy.binary_op(input),\n    msg=lambda a, b, info: make_msg(input, a, b, info),\n)\n```\n\nThis should make it much easier for developers to find out what is actually going wrong.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27903842\n\nPulled By: mruberry\n\nfbshipit-source-id: 4c82e3d969e9a621789018018bec6399724cf388", "pr_number": "55890", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "58fcf77712": {"title": "Port CPU torch.geqrf to ATen (#56249)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56249\n\nThis PR ports `torch.geqrf` from TH to ATen. CUDA path will be\nimplemented in a follow-up PR.\nWith ATen port support for complex and batched inputs is added.\nThere were no correctness tests, they are\nadded in this PR and I added OpInfo for this operation.\n\nWe can implement the QR decomposition as a composition of geqrf and\norgqr (torch.linalg.householder_product).\nAlso we can implement the least squares solver with geqrf + ormqr +\ntrtrs. So it's useful to have this function renewed at least for the\ninternal code.\n\nResolves https://github.com/pytorch/pytorch/issues/24705\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27907357\n\nPulled By: mruberry\n\nfbshipit-source-id: 94e1806078977417e7903db76eab9d578305f585", "pr_number": "56249", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THLapack.cpp", "aten/src/TH/generic/THLapack.h", "aten/src/TH/generic/THTensorLapack.cpp", "aten/src/TH/generic/THTensorLapack.h", "test/test_linalg.py", "torch/_torch_docs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "module: porting", "open source"]}, "e97c17afa0": {"title": "Update internal code for torch.geqrf (#56250)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56250\n\nMoved `apply_geqrf` to `BatchLinearAlgebraKernel.cpp`. Added\n`geqrf_stub` dispatch.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D27907362\n\nPulled By: mruberry\n\nfbshipit-source-id: 6719464aef29dcf3bbbde060edf79f1e32fc8ad6", "pr_number": "56250", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "d4707e260b": {"title": "Infer types (#56832)", "body": "Summary:\nAddresses:  Infer argument types for functions in JIT\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56832\n\nReviewed By: pbelevich\n\nDifferential Revision: D27979495\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 82156a516c7f96cdd3f7a067d41cb210a6d13a51", "pr_number": "56832", "files_changed": ["torch/jit/frontend.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "70d9be0f42": {"title": "Replace duplicative s with alpha (#56804)", "body": "Summary:\nIt is always easier to read a document when different objects / concepts denoted with different variables / representations.\nIn this PR we make sure the [complex autograd](https://pytorch.org/docs/master/notes/autograd.html#autograd-for-complex-numbers) documentation, the variable of output and step size diverge.\n\nFixes https://github.com/pytorch/pytorch/issues/53633\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56804\n\nReviewed By: anjali411\n\nDifferential Revision: D27989959\n\nPulled By: iramazanli\n\nfbshipit-source-id: c271590ee744c8aeeff62bfaa2295429765ef64e", "pr_number": "56804", "files_changed": ["docs/source/notes/autograd.rst"], "labels": ["Merged", "cla signed"]}, "2d2370bb61": {"title": "[Dist profiling] Fix ProcessGroupNCCL collective profiling (#55204)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55204\n\nImplements a fix discussed offline with pritamdamia87 to run end callbacks after `CUDAFuture`'s wrapCallback has ensured appropriate synchronization. Also enables the relevant distributed profiling tests that were previously disabled for ProcessGroupNCCL.\n\nNote that the profiling infrastructure has moved to primarily encourage the use of torch.profiler and CUPTI to trace CUDA kernels, support for distributed collectives for that will require further discussion with ilia-cher. However, this PR improves the usability of torch.autograd.profiler with respect to distributed collectives.\n\nghstack-source-id: 127357995\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D27491711\n\nfbshipit-source-id: cec7703a4c5d59b5023b0aa8fef4c2e3fb8d37d0", "pr_number": "55204", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7b74c3c70a": {"title": "Enable tests for dist profiling with torch.profiler (#56216)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56216\n\nVerifies that the newly added distributed profiling works as expected for torch.profiler.\n\nExample trace from `test_ddp_profiling`:\n\nNote that tests are disabled internally due to an unrelated hang issue but run in OSS.\nghstack-source-id: 127357993\n\nReviewed By: mrshenli\n\nDifferential Revision: D27645105\n\nfbshipit-source-id: 7ddba271acd8f7fbce1f9c5370830d5310314736", "pr_number": "56216", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "dde2bc4818": {"title": "Add OPENSSL_ROOT_DIR to cmake.py (#56846)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56846\n\nTest Plan: Imported from OSS\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27992923\n\nPulled By: pbelevich\n\nfbshipit-source-id: dc2d26d4bc9d17a5da441ae4db8241609ca97c6e", "pr_number": "56846", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["Merged", "cla signed"]}, "267b554b6f": {"title": "fx: Fix type_matches for Optional[List[int]] arguments (#56790)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56790\n\nIf the argument doesn't match `List[int]`, this code falls through to\n`issubclass(argument_type, List[int])` which is invalid and raises a\n`TypeError`. If this happens during the processing of a `Union` (e.g.\n`Optional`), the other union types aren't given the chance to match against the\nsignature.\n\nThis also stop normalize_function from indescriminately swallowing exceptions,\nwhich let this bug go unnoticed.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27987746\n\nPulled By: mruberry\n\nfbshipit-source-id: c5aa5f61a215f0f39925e7053f33bff4b5d5acc2", "pr_number": "56790", "files_changed": ["torch/fx/operator_schemas.py"], "labels": ["cla signed", "fx", "open source"]}, "298db67220": {"title": "[OpInfo] Add Function Variant and Opinfo for permute (#56125)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56125\n\nReviewed By: ezyang\n\nDifferential Revision: D27960312\n\nPulled By: mruberry\n\nfbshipit-source-id: b9dd89f7e69d7dff29f3b53828656c13df898fa5", "pr_number": "56125", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "test/test_torch.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "9e027d7ea3": {"title": "[OpInfo] Add opinfo for `transpose` and its aliases (#56122)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56122\n\nReviewed By: ezyang\n\nDifferential Revision: D27962878\n\nPulled By: mruberry\n\nfbshipit-source-id: cfd84bb0dcedeb98233a10e2c9754281f7cb76af", "pr_number": "56122", "files_changed": ["test/test_op_aliases.py", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "9eee14704a": {"title": "OpInfo: roll and rot90 (#56770)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56770\n\nReviewed By: ngimel\n\nDifferential Revision: D27987820\n\nPulled By: mruberry\n\nfbshipit-source-id: c6b86cdc1b89d91eeda2215020137582e7c20c65", "pr_number": "56770", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "7b31ba4708": {"title": "Fix cudnn ctc loss backward (#56639)", "body": "Summary:\nFix cudnn ctc loss backward\n\nFix https://github.com/pytorch/pytorch/issues/49046, which was working in pytorch 1.1\n\nOriginally modified in this PR in Oct 2019, https://github.com/pytorch/pytorch/pull/27039/files#diff-25ec2c1108ee03e2167622588ec31d167897ef1cccb12a4cfe77eb98777316daR2383-R2392\n\nAccording to the original code\n\nhttps://github.com/pytorch/pytorch/blob/90ffab6e3713a40f47f4c9d3d3b16d56e83f97c4/tools/autograd/derivatives.yaml#L1387-L1388\n\nand the code after PR\n\nhttps://github.com/pytorch/pytorch/blob/f461184505149560803855f3a40d9e0e54c64826/tools/autograd/templates/Functions.cpp#L2456-L2465\n\nThis `at::zeros({0}, raw_grad.options())` in line 2460 seems suspicious, and is causing `infer_size` runtime error\n\n```\nRuntimeError: The size of tensor a (0) must match the size of tensor b (177) at non-singleton dimension 2\nException raised from infer_size at ..\\aten\\src\\ATen\\ExpandUtils.cpp:24 (most recent call first):\n```\n\nI've modified that to `at::zeros_like(raw_grad)`, which looks more accurate.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56639\n\nReviewed By: mruberry\n\nDifferential Revision: D27987860\n\nPulled By: ngimel\n\nfbshipit-source-id: 5ad65e78d017c26894fb26318a5992b0878d04d5", "pr_number": "56639", "files_changed": ["test/test_nn.py", "torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "f27513e951": {"title": "Fix bug in torch.sparse.addmm on CUDA when beta != 0 or 1 (#56160)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55917, which caused `torch.sparse.addmm` to fail on CUDA whenever `beta` was different from 0 or 1\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56160\n\nReviewed By: ejguan\n\nDifferential Revision: D27825108\n\nPulled By: ngimel\n\nfbshipit-source-id: 2ade5ea38c5322768dc4dffb40c65fcbb17ec201", "pr_number": "56160", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "test/test_sparse.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ed9c7e187b": {"title": "Added OpInfo for addmm (#55920)", "body": "Summary:\nAdded an OpInfo for `addmm` & ported its `method_tests`\n\nSkipping `test_variant_consistency_eager` on CPU, as it's blocked by https://github.com/pytorch/pytorch/issues/56233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55920\n\nReviewed By: agolynski\n\nDifferential Revision: D27800325\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 311cd26c6b491b486f652cf64275c6901fea03c5", "pr_number": "55920", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "3e006fc57e": {"title": "Adding hsplit,vsplit and dsplit methods (#53536)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53536\n\nReviewed By: albanD\n\nDifferential Revision: D27938880\n\nPulled By: iramazanli\n\nfbshipit-source-id: f741119517783ec2bafa296622ee518b587dd127", "pr_number": "53536", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_fx_experimental.py", "test/test_tensor_creation_ops.py", "test/test_view_ops.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "5854e93bc9": {"title": "Fix derivative of sinc at x=0 (#56763)", "body": "Summary:\nAttempting to fix https://github.com/pytorch/pytorch/issues/56760\n\nThe derivative of `sinc(x)` at `x=0` should be special cased to 0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56763\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D27978135\n\nPulled By: albanD\n\nfbshipit-source-id: ede5e734613cf60e720f6bcc7387c3cd9c6ec233", "pr_number": "56763", "files_changed": ["test/test_autograd.py", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h"], "labels": ["Merged", "cla signed", "open source"]}, "f84f2063b4": {"title": "Port CUDA torch.geqrf to ATen (#56251)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56251\n\nThis PR ports `torch.geqrf` from TH to ATen for CUDA path.\n\nResolves https://github.com/pytorch/pytorch/issues/24569\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27960155\n\nPulled By: mruberry\n\nfbshipit-source-id: a8b010c41d703a5de4bf40b045c89e6b95b5a5ca", "pr_number": "56251", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/generic/THCTensorMathMagma.cu", "aten/src/THC/generic/THCTensorMathMagma.h", "test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "module: porting", "open source"]}, "27a8ece805": {"title": "Add cuSOLVER path for torch.geqrf (#56252)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56252\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27960152\n\nPulled By: mruberry\n\nfbshipit-source-id: 0510a302aab50623d7490efaba0133f740cd57c3", "pr_number": "56252", "files_changed": ["aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/CUDASolver.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "5b1f0ef622": {"title": "Add cuBLAS path for batched torch.geqrf (#56253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56253\n\n`geqrfBatched` from cuBLAS is used if\n```\n(input.size(-2) <= 256 && batchCount(input) >= std::max<int64_t>(2, input.size(-2) / 16))\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27960156\n\nPulled By: mruberry\n\nfbshipit-source-id: 3e438eff01cbf7c7e075fb7aef709b97698a4650", "pr_number": "56253", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "28a9483e36": {"title": "fix ddp logging test (#56640)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56640\n\nreset performance stats for current iteration, also fix ddp logging verifiction for sampled iterations.\nghstack-source-id: 127327708\n\nTest Plan: unit tests\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27923414\n\nfbshipit-source-id: aaa1b10f64a0c952ba345c789c864bcef5cf1ab0", "pr_number": "56640", "files_changed": ["torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "2f598b53dd": {"title": "catch xml parser error during report test result phase in CI (#56864)", "body": "Summary:\nFixes Report test result step fails the test suite entirely, such as:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/307916/workflows/4144870c-d1cf-4567-a6f8-93bb436471a4/jobs/12732796\nand\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/307388/workflows/a945940f-3325-43b3-bc14-c9f885b21f50/jobs/12705944\n\nand also not related but could be a problem that only partial test reports are uploaded:\nhttps://app.circleci.com/pipelines/github/pytorch/pytorch/308777/workflows/bdc37967-3863-448e-8264-311bf21ca381/jobs/12777741\n\nThis skips the parser error and move on to the next file.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56864\n\nTest Plan: CI\n\nReviewed By: janeyx99\n\nDifferential Revision: D28002166\n\nPulled By: walterddr\n\nfbshipit-source-id: 6fa48122ae9dd68e401daf3692821fb00082b3ae", "pr_number": "56864", "files_changed": ["tools/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "2639c4e6b3": {"title": "fix bug in rocm device type (#56646)", "body": "Summary:\nrelated to https://github.com/pytorch/pytorch/issues/56156.\n\nhttps://github.com/pytorch/pytorch/issues/55808 effectively turned dtypeIfROCM off but let some legacy issues unfixed. Given the fact that we still need to deal with discrepancy between the 2 platform. This PR turns dtypeIfROCM default pointing to dtypeIfCUDA and only override when user specifies.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56646\n\nReviewed By: mruberry\n\nDifferential Revision: D27968959\n\nPulled By: walterddr\n\nfbshipit-source-id: 6a11987b8ddf4417577b3d0d5054eaab169de42c", "pr_number": "56646", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: rocm"]}, "6155b0d9fa": {"title": "[reland] Trigger azure pipeline for multi gpu tests (#56128)", "body": "Summary:\nReland https://github.com/pytorch/pytorch/issues/52490 only for nightly builds\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56128\n\nReviewed By: anjali411\n\nDifferential Revision: D28002257\n\nPulled By: seemethere\n\nfbshipit-source-id: d32bf420fee13b809cee402362f98942234d380b", "pr_number": "56128", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".circleci/scripts/trigger_azure_pipeline.py", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml"], "labels": ["Merged", "cla signed", "module: ci", "module: windows", "open source", "triaged"]}, "7989f2ac87": {"title": "Clang format dist_utils.py and rpc/__init__.py (#56853)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56853\n\nghstack-source-id: 127412640\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27984669\n\nfbshipit-source-id: 8e89ba0c53107622b3ca29ea296226e260b251df", "pr_number": "56853", "files_changed": ["torch/distributed/rpc/__init__.py", "torch/testing/_internal/dist_utils.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "cb1e78038f": {"title": ".github: Add options to force unzip artifacts (#56929)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56929\n\nArtifacts were failing to unzip since they already existed in the\ncurrent tree so this just forces the zip to go through no matter what\n\nWas observing that test phases will fail if attempting to zip over an already existing directory, https://github.com/pytorch/pytorch/runs/2424525136?check_suite_focus=true\n\nIn the long run however it'd be good to have these binaries built out as part of the regular cmake process instead of being one off builds like they are now\n\n**NOTE**: This wouldn't be an issue if `--ephemeral` workers was a thing, see: https://github.com/actions/runner/pull/660\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D28004271\n\nPulled By: seemethere\n\nfbshipit-source-id: c138bc85caac5d411a0126d27cc42c60fe88de60", "pr_number": "56929", "files_changed": [".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "72c3ee073f": {"title": "add deterministic path for index_add_cuda (#56521)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56521\n\nindex_add_cuda is non-deterministic due to cuda atomicAdd. Here we add a deterministic code path with index_put(accumulate=True)\n\nTest Plan:\nbuck test mode/opt //caffe2/test:torch_cuda -- test_index_add_deterministic\n\n    \u2713 ListingSuccess: caffe2/test:torch_cuda - main (12.289)\n    \u2713 Pass: caffe2/test:torch_cuda - test_index_add_deterministic_cuda (test_torch.TestTorchDeviceTypeCUDA) (27.190)\n    \u2713 Pass: caffe2/test:torch_cuda - main (27.190)\nSummary\n  Pass: 2\n  ListingSuccess: 1\n\nbuck test mode/opt //caffe2/test:torch_cuda -- test_nondeterministic_alert\n\n    \u2713 ListingSuccess: caffe2/test:torch_cuda - main (16.088)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_ReflectionPad1d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_kthvalue_cuda_float64 (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_ReplicationPad1d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_bincount_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_index_put_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_EmbeddingBag_max_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_MaxPool3d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_AdaptiveAvgPool3d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_histc_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_interpolate_linear_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_AdaptiveMaxPool2d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_FractionalMaxPool3d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_AdaptiveAvgPool2d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_NLLLoss_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_put_accumulate_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_grid_sample_2d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_put_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_interpolate_trilinear_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_interpolate_bicubic_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_ReflectionPad2d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_scatter_add_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_AvgPool3d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_grid_sample_3d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_CTCLoss_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_FractionalMaxPool2d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_ReplicationPad3d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_index_copy_cuda_float64 (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_ReplicationPad2d_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_median_cuda_float64 (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_gather_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_interpolate_bilinear_cuda (test_torch.TestTorchDeviceTypeCUDA) (37.654)\n    \u2713 Pass: caffe2/test:torch_cuda - main (37.654)\nSummary\n  Pass: 32\n  ListingSuccess: 1\n\nReviewed By: ngimel\n\nDifferential Revision: D27861072\n\nfbshipit-source-id: c33731017b863751f3e3068a23135129c555b66f", "pr_number": "56521", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "d1088de522": {"title": "Let RRef getValue() synchronize CUDA streams (#56895)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56895\n\nPR #54932 fixes CUDA stream synchronization between RPC-created\nOwnerRRef and UserRRef when `to_here()` is invoked. However, there\nare two more gaps.\n\n1. RRef value can be accessed on the owner directly through\n    `local_value`, which bypasses the fix in #54932.\n2. When RRef is created directly through RRef ctor instead of RPC,\n    the OwnerRRef won't be able to correctly record CUDA events.\n\nThis PR fixes 1 by letting current streams wait for RRef recorded\nCUDA events before returning the value in `RRef::getValue()`.\n\nFor 2, more discussions is needed to decide whether we should add\na `devices` argument to RRef ctor, or should RRef ctor inspect the\ngiven values.\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D27992775\n\nPulled By: mrshenli\n\nfbshipit-source-id: ed0e5bfbf715460208c85e46dd3317deef17f8fe", "pr_number": "56895", "files_changed": ["torch/csrc/distributed/rpc/rref_impl.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "9b46b6b37a": {"title": "Added sm_75 to CUDA Arch List for Linux CI GPU builds (#56619)", "body": "Summary:\nThis PR adds `sm_75` CUDA architecture support for CircleCI GPU builds, so that generated artifacts from these builds can be installed and run on machines with CUDA capability `sm_75`.\n\nThis PR is currently to see how much longer the PR CI GPU builds will take with `TORCH_CUDA_ARCH_LIST=\"7.5\"` rather than `TORCH_CUDA_ARCH_LIST=\"5.2\"`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56619\n\nReviewed By: malfet\n\nDifferential Revision: D28012538\n\nPulled By: seemethere\n\nfbshipit-source-id: 3959736721eab7389984234d89eadcf04d163c37", "pr_number": "56619", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["Merged", "cla signed", "module: build", "module: ci", "open source"]}, "e810bed63f": {"title": "[Static Runtime] Clean up op implementations (#56841)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56841\n\n- Move arg checks to outside the lambda so we can perform these checks at Static Runtime initialization time\n- use `optional` where possible\n- support `to.other` overload, the 5-arg input load of `torch.to`.\n\nTest Plan:\n```\nbuck run //caffe2/benchmarks/static_runtime:static_runtime_cpptest\nbuck test mode/opt-clang //caffe2/caffe2/fb/predictor:ptvsc2_predictor_bench_test -- --run-disabled\n```\n\nReviewed By: edvgha\n\nDifferential Revision: D27933176\n\nfbshipit-source-id: 49d6249c8784c44146461e286e7a301596172d7c", "pr_number": "56841", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "0888b8726a": {"title": "[static runtime] binding for aten::clamp_min_out (#56635)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56635\n\nTest Plan:\n```\n./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --scripted_model=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.local.local.pt --pt_inputs=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.input_data.container.pt --iters=500 --warmup_iters=500 --num_threads=1 --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --pt_optimize_memory=1 --compare_results=1 --do_profile=0 --adsfinder_compatibility=1\n```\n\n```\nTime per node type:\n        1.50885 ms.    36.0064%. fb::sigrid_transforms_torch_bind (1 nodes)\n        0.92296 ms.    22.0251%. aten::linear (6 nodes)\n       0.695455 ms.     16.596%. aten::argmin (1 nodes)\n       0.237931 ms.    5.67787%. aten::matmul (1 nodes)\n       0.141634 ms.    3.37989%. fb::clip_ranges_gather_sigrid_hash_v3 (77 nodes)\n      0.0925469 ms.     2.2085%. fb::clip_ranges_gather (263 nodes)\n      0.0886556 ms.    2.11563%. aten::sub (1 nodes)\n      0.0549624 ms.     1.3116%. aten::repeat (1 nodes)\n       0.043996 ms.     1.0499%. aten::norm (1 nodes)\n      0.0403472 ms.   0.962826%. fb::batch_box_cox (1 nodes)\n      0.0371137 ms.   0.885664%. aten::sigmoid (2 nodes)\n       0.035054 ms.   0.836512%. aten::__getitem__ (506 nodes)\n      0.0338771 ms.   0.808427%. prim::TupleUnpack (254 nodes)\n      0.0288516 ms.   0.688502%. aten::mul (3 nodes)\n       0.026195 ms.   0.625106%. fb::offsets_to_ranges (253 nodes)\n      0.0243627 ms.   0.581381%. aten::pow (1 nodes)\n      0.0210347 ms.   0.501962%. fb::simple_embedding_bag_sum (3 nodes)\n      0.0195358 ms.   0.466192%. fb::casted_batch_one_hot_lengths (1 nodes)\n      0.0193484 ms.   0.461722%. fb::concat_add_mul_replacenan_clip (1 nodes)\n      0.0164265 ms.   0.391995%. aten::sum (3 nodes)\n      0.0157266 ms.   0.375291%. prim::TupleConstruct (1 nodes)\n      0.0156512 ms.   0.373493%. prim::DictConstruct (2 nodes)\n      0.0114427 ms.   0.273062%. aten::div (1 nodes)\n     0.00884876 ms.   0.211163%. static_runtime::to_copy (8 nodes)\n     0.00864496 ms.   0.206299%. prim::ListConstruct (4 nodes)\n     0.00803458 ms.   0.191734%. fb::sigrid_hash_precompute (1 nodes)\n     0.00619933 ms.   0.147938%. aten::contiguous (1 nodes)\n     0.00462827 ms.   0.110447%. aten::narrow (4 nodes)\n     0.00293105 ms.  0.0699452%. aten::logit (1 nodes)\n     0.00287083 ms.  0.0685082%. static_runtime::reshape_copy (2 nodes)\n     0.00250605 ms.  0.0598032%. aten::add (1 nodes)\n     0.00217015 ms.  0.0517875%. fb::gather_ranges (4 nodes)\n     0.00202655 ms.  0.0483607%. aten::full (1 nodes)\n     0.00200812 ms.  0.0479208%. aten::relu (1 nodes)\n     0.00175433 ms.  0.0418644%. aten::stack (1 nodes)\n     0.00174899 ms.   0.041737%. aten::clamp_min (1 nodes)\n     0.00134367 ms.  0.0320646%. aten::size (3 nodes)\n    0.000811416 ms.  0.0193633%. fb::clip_ranges (2 nodes)\n    0.000801096 ms.   0.019117%. aten::expand_as (1 nodes)\n    0.000541452 ms.   0.012921%. fb::lengths_to_offsets (3 nodes)\n    0.000477838 ms.  0.0114029%. static_runtime::flatten_copy (1 nodes)\n    0.000192906 ms. 0.00460342%. prim::device (1 nodes)\n        4.19049 ms. in Total\nStaticRuntime setup time: 0.000408 ms\nMemory allocation time: 0.00895982 ms\nMemory deallocation time: 0.0587527 ms\nOutputs deallocation time: 0.0430985 ms\nTotal memory managed: 947328 bytes\nTotal number of reused tensors: 28\nW0421 14:33:55.610956 836281 PyTorchPredictorContainer.cpp:200] Failed to load metadata file\nW0421 14:33:55.611043 836281 PyTorchPredictorContainer.cpp:457] Couldn't find model param config file xl_model_weights/model_param_config\nI0421 14:33:55.611063 836281 PyTorchPredictorBenchLib.cpp:137] PyTorch predictor: number of prediction threads 1\nI0421 14:33:55.736069 836281 PyTorchPredictorBenchLib.cpp:230] PyTorch run finished. Milliseconds per iter: 124.995. Iters per second: 8.0003\nI0421 14:33:55.874794 836281 PtVsBlackBoxPredictorBenchLib.cpp:132] Finished comparing PT static runtime and jit interpreter results\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27922570\n\nfbshipit-source-id: 095aa9bd0c425bc73eb48841653441d5c9e45744", "pr_number": "56635", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "f5c24cc891": {"title": "add deterministic path for index_copy_cpu (#56900)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56900\n\nuse serial copy with iter.serial_for_each in the deterministic mode\n\nTest Plan:\nbuck test mode/opt //caffe2/test:torch -- test_index_copy_deterministic\n\n    \u2713 Pass: caffe2/test:torch - test_index_copy_deterministic_cpu (test_torch.TestTorchDeviceTypeCPU) (5.581)\n\nbuck test mode/opt //caffe2/test:torch_cuda -- test_nondeterministic_alert_index_copy\n\n    \u2713 ListingSuccess: caffe2/test:torch_cuda - main (11.565)\n    \u2713 Pass: caffe2/test:torch_cuda - test_nondeterministic_alert_index_copy_cuda_float64 (test_torch.TestTorchDeviceTypeCUDA) (29.172)\n    \u2713 Pass: caffe2/test:torch_cuda - main (29.172)\n\nReviewed By: ngimel\n\nDifferential Revision: D27992992\n\nfbshipit-source-id: cebeefd8508553f9dbc4145819fe90dd625502f3", "pr_number": "56900", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "d405d41a7c": {"title": "ns for fx: enable user defined functions for graph matching (#56283)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56283\n\nExposes the `base_name_to_sets_of_related_ops` variable\nto the graph matching API, so that users can add relationships\nfor custom functions. This is needed to enable full support of\nexternal functions for custom backends.\n\nThe next PR will extend this to the NS APIs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher.test_user_defined_function\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27830410\n\nfbshipit-source-id: 8688cf697d388c52e3d18f108765edfca3c3d3aa", "pr_number": "56283", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py"], "labels": ["Merged", "cla signed"]}, "8dbf6ae8fa": {"title": "ns for fx: handling for user functions in weight and unshadowed act APIs (#56292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56292\n\nAdds hooks for specifying user defined functions to NS weight and\nunshadowed activation APIs.\n\nAdding it to shadowed activation APIs will be a bit more work, upcoming\nin a separate PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27830409\n\nfbshipit-source-id: 6bbddc3062c0b3e412a3147244795319c0785a92", "pr_number": "56292", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py"], "labels": ["Merged", "cla signed"]}, "93de80203d": {"title": "ns for fx: move node I/O dtype mapping to be local instead of global (#56296)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56296\n\nTo support shadows of custom functions, we need to allow user to\nspecify I/O type of the custom functions.\n\nThis PR is a cleanup in preparation for making the above happen.\nWe make the I/O dtype mappings be generated by a function instead\nof a global variable. In the next PR, we will add a hook so user\ncan modify these mappings.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27831996\n\nfbshipit-source-id: 782f5e77de0eef3899b9b7def0fdabd8dcafef12", "pr_number": "56296", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "1917350977": {"title": "ns for fx: allow user functions in shadowing (#56301)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56301\n\nAllows usage of user functions in NS shadow APIs. We expose the\ni/o mapping to the user APIs, and thread them throughout the code.\n\nNote: the format of the mapping is currently not the best.  Saving\nimproving that for a future PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27833189\n\nfbshipit-source-id: dac418e294d1c9b204efbf4071d5cc12a9e784c0", "pr_number": "56301", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "96a9eafcfb": {"title": "ns for fx: add fp16 function shadowing (#56311)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56311\n\nAdds functionality for shadowing user functions with fp16 I/O dtype.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27836064\n\nfbshipit-source-id: 37a434a04e2bd2593a892209bbae59f0f1f34319", "pr_number": "56311", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "f35540be38": {"title": "ns for fx: bug fix for shadowing fp16 emulation patterns (#56384)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56384\n\nEnables shadow copies of fp16 emulation patterns where weights\nare cast to fp16 before being passed to linear.  This previously\ndid not work because copying of `call_method` nodes was not implemented.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_linear_fp16_vs_linear_fp16_shadow_activations\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27857735\n\nfbshipit-source-id: 7c1a067f035acf7322175f8535876d0ead88a86a", "pr_number": "56384", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "c004346c88": {"title": "ns for fx: support binary ops when adding unshadowed loggers for inputs (#56408)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56408\n\nAdds the ability to log unshadowed inputs of binary ops such as `add`\nand `mul`, when indices 0, 1, or 0 and 1 are tensors.\n\nNote: making shadowing support this is saved for a future PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_add_mul_inputs_activations\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27864296\n\nfbshipit-source-id: 3cbeb728297aa192d1ea17e815299709fd9db056", "pr_number": "56408", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/ns_types.py", "torch/quantization/ns/utils.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "92c7aec5f5": {"title": "ns for fx: add option to skip matching classes and functions (#56493)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56493\n\nAdds a config option to skip matching classes by class type\nand functions by function type.\n\nThis is useful when users make custom modules which return\ntypes other than tensors. With the current implementation of\nLogger, these are not scriptable.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_module_scriptable\n```\n\nneeds more testing before land\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27886107\n\nfbshipit-source-id: ec92c4f7ab7141021bc022f07b3b558b42bbb986", "pr_number": "56493", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/mappings.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "502c58ad84": {"title": "ns for fx: allow comparing int8 to int8 for functionals (#56742)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56742\n\nFixes a bug to allow shadowing of linear and conv functionals.\nThe bug is to only detach tensors, not all objects.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_int8_shadows_int8_fun\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27960767\n\nfbshipit-source-id: abc911ca4b9edafd1effb9dada7731981538c2df", "pr_number": "56742", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "9bd14da6e4": {"title": "ns for fx: additional bugfix for user defined functions (#56762)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56762\n\nAdds a test case for wrapped sigmoid, and fixes the following issues\nto make it pass in NS:\n* allows comparing between x.sigmoid() and torch.sigmoid(x), if they are related\n* allows dtype cast from FP32_OR_INT8 to FP32, via dequantize (this will be improved later)\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D27960766\n\nfbshipit-source-id: 02935d2f400aa0b8f3d51bbf664a6c8ca89aa811", "pr_number": "56762", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "e909ad2dc4": {"title": "[static runtime] binding for aten::argmin_out (#56638)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56638\n\nTest Plan:\n```\n./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --scripted_model=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.local.local.pt --pt_inputs=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.input_data.container.pt --iters=500 --warmup_iters=500 --num_threads=1 --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --pt_optimize_memory=1 --compare_results=1 --do_profile=1 --adsfinder_compatibility=1\n```\n\n```\nTime per node type:\n        1.55901 ms.    35.3486%. fb::sigrid_transforms_torch_bind (1 nodes)\n       0.986321 ms.    22.3636%. aten::linear (6 nodes)\n       0.722277 ms.    16.3767%. aten::argmin (1 nodes)\n       0.256231 ms.    5.80971%. aten::matmul (1 nodes)\n       0.149653 ms.    3.39319%. fb::clip_ranges_gather_sigrid_hash_v3 (77 nodes)\n       0.105381 ms.    2.38938%. fb::clip_ranges_gather (263 nodes)\n      0.0911405 ms.    2.06649%. aten::sub (1 nodes)\n      0.0605429 ms.    1.37273%. aten::repeat (1 nodes)\n      0.0456569 ms.    1.03521%. aten::norm (1 nodes)\n      0.0421855 ms.   0.956501%. fb::batch_box_cox (1 nodes)\n      0.0370142 ms.   0.839249%. aten::__getitem__ (506 nodes)\n      0.0359091 ms.   0.814193%. prim::TupleUnpack (254 nodes)\n      0.0338332 ms.   0.767123%. aten::sigmoid (2 nodes)\n      0.0315159 ms.   0.714582%. aten::mul (3 nodes)\n      0.0297553 ms.   0.674662%. fb::offsets_to_ranges (253 nodes)\n      0.0279913 ms.   0.634666%. fb::simple_embedding_bag_sum (3 nodes)\n      0.0233521 ms.   0.529478%. aten::pow (1 nodes)\n       0.021296 ms.    0.48286%. fb::concat_add_mul_replacenan_clip (1 nodes)\n      0.0208991 ms.   0.473861%. fb::casted_batch_one_hot_lengths (1 nodes)\n      0.0183163 ms.   0.415298%. aten::sum (3 nodes)\n      0.0164318 ms.   0.372571%. prim::DictConstruct (2 nodes)\n      0.0160191 ms.   0.363211%. prim::TupleConstruct (1 nodes)\n      0.0126953 ms.   0.287849%. aten::div (1 nodes)\n      0.0106084 ms.   0.240532%. static_runtime::to_copy (8 nodes)\n      0.0092846 ms.   0.210516%. prim::ListConstruct (4 nodes)\n     0.00916175 ms.   0.207731%. fb::sigrid_hash_precompute (1 nodes)\n     0.00707015 ms.   0.160307%. aten::contiguous (1 nodes)\n     0.00621954 ms.    0.14102%. aten::narrow (4 nodes)\n     0.00302307 ms.  0.0685441%. aten::add (1 nodes)\n     0.00290759 ms.  0.0659259%. aten::full (1 nodes)\n     0.00283369 ms.  0.0642503%. aten::logit (1 nodes)\n     0.00239244 ms.  0.0542455%. fb::gather_ranges (4 nodes)\n     0.00220181 ms.  0.0499232%. aten::relu (1 nodes)\n     0.00211563 ms.  0.0479691%. static_runtime::reshape_copy (2 nodes)\n      0.0020059 ms.  0.0454812%. aten::stack (1 nodes)\n     0.00186682 ms.  0.0423276%. aten::clamp_min (1 nodes)\n     0.00172548 ms.   0.039123%. aten::size (3 nodes)\n      0.0011853 ms.  0.0268751%. aten::expand_as (1 nodes)\n    0.000881784 ms.  0.0199933%. fb::clip_ranges (2 nodes)\n    0.000835602 ms.  0.0189462%. fb::lengths_to_offsets (3 nodes)\n    0.000444376 ms.  0.0100757%. static_runtime::flatten_copy (1 nodes)\n    0.000197078 ms. 0.00446848%. prim::device (1 nodes)\n         4.4104 ms. in Total\nStaticRuntime setup time: 0.000702 ms\nMemory allocation time: 0.00943333 ms\nMemory deallocation time: 0.062704 ms\nOutputs deallocation time: 0.0477171 ms\nTotal memory managed: 831744 bytes\nTotal number of reused tensors: 31\nW0421 14:53:04.841202 929500 PyTorchPredictorContainer.cpp:200] Failed to load metadata file\nW0421 14:53:04.841315 929500 PyTorchPredictorContainer.cpp:457] Couldn't find model param config file xl_model_weights/model_param_config\nI0421 14:53:04.841341 929500 PyTorchPredictorBenchLib.cpp:137] PyTorch predictor: number of prediction threads 1\nI0421 14:53:04.971776 929500 PyTorchPredictorBenchLib.cpp:230] PyTorch run finished. Milliseconds per iter: 130.423. Iters per second: 7.66736\nI0421 14:53:05.122830 929500 PtVsBlackBoxPredictorBenchLib.cpp:132] Finished comparing PT static runtime and jit interpreter results\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27923172\n\nfbshipit-source-id: 05cf5497fb6ac39dd3ff24f583607a3dff8cae95", "pr_number": "56638", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "35f3feca28": {"title": "[RPC Framework] Supporting reading the input from the remote worker (#56943)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56943\n\nIf the module is placed on a CUDA device, then all the CPU tensors in `args` and `kwargs` will also be implicitly moved to the same CUDA device to run forward.\n\nCurrently still need to move the forward output from CUDA device back to CPU, until:\n1) Process group RPC backend is completely deprecated, and we always use TensorPipe RPC backend;\n2) A device map is explicitly provided to TensorPipe RPC backend.\n\nThese steps will be done in a separate PR.\n\n#Original PR issue: https://github.com/pytorch/pytorch/issues/51670\nghstack-source-id: 127457584\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_input_moved_to_cuda_device\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_input_moved_to_cuda_device_script\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- RemoteModule\n\nbuck test mode/dev-nosan //caffe2/torch/fb/training_toolkit/applications/sparse_nn/batch_distributed_inference/tests:batch_distributed_inference_test -- --exact 'caffe2/torch/fb/training_toolkit/applications/sparse_nn/batch_distributed_inference/tests:batch_distributed_inference_test - test_load_di_parts (caffe2.torch.fb.training_toolkit.applications.sparse_nn.batch_distributed_inference.tests.batch_distributed_inference_test.BatchDistributedInferenceTest)'\n\nReviewed By: wanchaol\n\nDifferential Revision: D27934791\n\nfbshipit-source-id: de27e27b905db83cc52800e63684fc6c942e9dc7", "pr_number": "56943", "files_changed": ["torch/distributed/nn/jit/templates/remote_module_template.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "a09bbe73fd": {"title": "static runtime support for fb::equally_split (#56812)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56812\n\nfb::equally_split get fused with ListUnpack and all outputs from ListUnpack getting attached to fb::equally_split.\nSo fb::equal_split will have as many outputs as ListUnpack .\n\nTest Plan:\nbuck test caffe2/benchmarks/static_runtime/fb:test_fb_operators\nbuck test caffe2/torch/fb/sparsenn:test -- test_equally_split_op\n\nReviewed By: hlu1\n\nDifferential Revision: D27974999\n\nfbshipit-source-id: b2ca19ff86aec76b977c1e3cfc56567adab66b35", "pr_number": "56812", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/passes.cpp", "torch/csrc/jit/runtime/static/passes.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "6ed5bbfb46": {"title": "[TensorPipe] Give higher priority to CPU-only channels. (#56908)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56908\n\nCUDA channels might implement CPU-to-CPU transfers, but will usually be\nless efficient for that purpose.\n\nTest Plan: CI\n\nReviewed By: lw\n\nDifferential Revision: D27994069\n\nfbshipit-source-id: fefa7f243eb43cf769864233df518f2a1819f949", "pr_number": "56908", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "dc8a8cea79": {"title": "Move caffe2 signal_handler to c10. (#56717)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56717\n\nThe signal_handler was under the caffe2 namespacee but was being used\nby PyTorch as well.\n\nI've fixed this my moving it to the c10 namespace where now both C2 and PyTorch\ncan use it.\n\nThe signal_handler interface in caffe2/utils/signal_handler.h is kept the same\nfor backward compatiblity for C2, but most of the commmon code is moved to c10.\nghstack-source-id: 127446929\n\nTest Plan: waitforbuildbot\n\nReviewed By: ezyang\n\nDifferential Revision: D27946738\n\nfbshipit-source-id: d6228d1a0108f4c807d405e7a0bb799c5375388f", "pr_number": "56717", "files_changed": ["c10/util/signal_handler.cpp", "c10/util/signal_handler.h", "caffe2/python/pybind_state.cc", "caffe2/utils/fatal_signal_asan_no_sig_test.cc", "caffe2/utils/signal_handler.cc", "caffe2/utils/signal_handler.h", "torch/_C/__init__.pyi.in", "torch/csrc/jit/python/init.cpp", "torch/testing/_internal/common_distributed.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c42dd8b257": {"title": "Revert \"Use at::cpu in bench_approx (#56563)\" (#56816)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56816\n\nThis doesn't actually work.  For some reason the linker can't find\nat::cpu::logit_out, and it's not worth digging into why not.\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27977406\n\nPulled By: bertmaher\n\nfbshipit-source-id: d0235a393f25243e2c8a011e9baf267daf483ae4", "pr_number": "56816", "files_changed": ["benchmarks/cpp/tensorexpr/bench_approx.cpp"], "labels": ["Merged", "cla signed"]}, "780f454297": {"title": "Add some functions for manipulating mkldnn tensors to TORCH_API (#56954)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56954\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D28010327\n\nPulled By: bertmaher\n\nfbshipit-source-id: 59872a40c7bc06187f0d87046446dd39193a1d71", "pr_number": "56954", "files_changed": ["aten/src/ATen/native/mkldnn/MKLDNNCommon.h"], "labels": ["Merged", "cla signed"]}, "a0483cd06b": {"title": "Back out \"fx: Fix type_matches for Optional[List[int]] arguments\" (#56991)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56991\n\nOriginal commit changeset: c5aa5f61a215\n\nDiff: D27987746 (https://github.com/pytorch/pytorch/commit/267b554b6fd7d8dfdfb9885c663fbe48fb26b2f8)\n\nTest Plan: `buck test` under the glow-buck target is the target that this reversion is intended to fix\n\nReviewed By: jfix71\n\nDifferential Revision: D28019659\n\nfbshipit-source-id: 37584ff404fc9195b309a5a6afdb4edbc2b4f088", "pr_number": "56991", "files_changed": ["torch/fx/operator_schemas.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "f7fba854bf": {"title": "Implement module.to_empty() (#56610)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54600\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56610\n\nReviewed By: malfet\n\nDifferential Revision: D27921653\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 10734b3eaa5b84bb4ba6eeba1043cfc8bb570a17", "pr_number": "56610", "files_changed": ["test/test_nn.py", "torch/nn/modules/module.py"], "labels": ["Merged", "cla signed"]}, "0d777a808c": {"title": "Make test_randperm work with meta device (#56976)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56976\n\nBand-aid fix for #54282\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D28020401\n\nPulled By: ezyang\n\nfbshipit-source-id: 50546d5275eade408d65e9c883999fb3b65ff55a", "pr_number": "56976", "files_changed": ["test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed"]}, "ab1457ad14": {"title": "Remove C++17 only optional include (#56782)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56782\n\nFixes #56749\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D28000019\n\nPulled By: ezyang\n\nfbshipit-source-id: 87f86a402dac87e6c101aef8c78a928ce7d21340", "pr_number": "56782", "files_changed": ["torch/csrc/cuda/comm.cpp"], "labels": ["Merged", "cla signed"]}, "57e37080cd": {"title": "Added OpInfo for torch.einsum (#56276)", "body": "Summary:\nAdds OpInfo testing for torch.einsum.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56276\n\nReviewed By: mruberry\n\nDifferential Revision: D27967095\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 60524273d2ca885e7eeb932db3e7fd697ae5ca8e", "pr_number": "56276", "files_changed": ["aten/src/ATen/native/Linear.cpp", "test/test_fx.py", "test/test_fx_experimental.py", "test/test_jit.py", "test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "268cc117a8": {"title": "Add OpInfos for torch.{complex, view_as_real, view_as_complex} (#56524)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56524\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27909165\n\nPulled By: anjali411\n\nfbshipit-source-id: 38592cdb357386549c8309792ef7c3218665d286", "pr_number": "56524", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: testing"]}, "7fe6e8e5a2": {"title": "Refactor C->C to C->R twice (#55692)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55692\n\n### Release notes\nget_numerical_jacobian and get_analytical_jacobian only support `grad_out=1` and `fn` no longer accepts functions that return complex output\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D28004614\n\nPulled By: soulitzer\n\nfbshipit-source-id: 9592c9c69584b4035b39be62252f138dce39d3b5", "pr_number": "55692", "files_changed": ["test/test_autograd.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "201ad938b2": {"title": "Enable fixed fast_mode for complex (#55699)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55699\n\nTodo:\n- error message should be updated to say whether the failure is for fn's real or imaginary component\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D28007887\n\nPulled By: soulitzer\n\nfbshipit-source-id: 1819201f59c8586a1d9631db05983969438bde66", "pr_number": "55699", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "759cfb7495": {"title": "add missing comma to `run_test.py` (#57010)", "body": "Summary:\nFactored out from https://github.com/pytorch/pytorch/pull/57008#discussion_r621137121:\n\n> Without this comma, the strings are concatenated to `test_binary_ufuncstest_numpy_interop`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57010\n\nReviewed By: malfet\n\nDifferential Revision: D28028061\n\nPulled By: walterddr\n\nfbshipit-source-id: 97c64b79a6aaaf0242def03c8808c1a032537258", "pr_number": "57010", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "e138987818": {"title": ".github: Build test binaries in build/ directory (#56941)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56941\n\nSets the custom test binaries we build in .jenkins/pytorch/build.sh to\nbe built in the `build` directory instead of the directory above the\nworkspace.\n\nThis should alleviate any weirdness we were seeing before with test\nbinaries having to be overwritten\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D28018453\n\nPulled By: seemethere\n\nfbshipit-source-id: 74add11037a622e011d00fb6292bfe20e1d55d9e", "pr_number": "56941", "files_changed": [".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml", ".jenkins/pytorch/build.sh", ".jenkins/pytorch/test.sh"], "labels": ["Merged", "cla signed", "module: ci"]}, "8d29ac2033": {"title": ".github: Bump linux.2xlarge runners to 500 (#56945)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56945\n\nIn preparation to turn these on for CI\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D28018454\n\nPulled By: seemethere\n\nfbshipit-source-id: fa94d666499877f2cdd7b8fd3fc8b2d8127f61e8", "pr_number": "56945", "files_changed": [".github/scale-config.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "cfbd06d7a1": {"title": "add all pools, Batchnorm and Tanh (i.e. all ideeped MKLDNN ops) to MKLDNNFuser (#56541)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56541\n\nReviewed By: pbelevich\n\nDifferential Revision: D27930353\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 4d5b932bad4154e8bdd6e35498354e13b39c87a1", "pr_number": "56541", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ed617a61ce": {"title": "Adjust computeLRWorkDim() to work with Accelerate.framework (#56847)", "body": "Summary:\nAccording to `vecLib.framework/Headers/clapack.h` Accelerate.framework's LAPACK implementation is based on 3.2.1, and so LRWORK should be computed using following formula (from\n```\n*>          If JOBZ = 'N', LRWORK >= 7*min(M,N).\n*>          Otherwise,\n*>          LRWORK >= min(M,N)*max(5*min(M,N)+7,2*max(M,N)+2*min(M,N)+1)\n```\n\nFound while looking at test_linalg.py crashes on M1, but would have happen on x86 as well, if Pytorch+Accelerate framework are to be tested on x86_64\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56847\n\nReviewed By: albanD\n\nDifferential Revision: D27983352\n\nPulled By: malfet\n\nfbshipit-source-id: f757c515c85b32c1e09d00a91bc20fe4b390a75a", "pr_number": "56847", "files_changed": ["aten/src/ATen/native/LinearAlgebraUtils.h"], "labels": ["Merged", "cla signed"]}, "11d455fa8b": {"title": ".github: Enable Linux CPU GHA on PRs (#56942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56942\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D28018455\n\nPulled By: seemethere\n\nfbshipit-source-id: 2b4ba3d616c217b4e960871f1428dda03f2ad92a", "pr_number": "56942", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/workflows/cancel_redundant_workflows.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "a93ceb333d": {"title": "Workaround intermittent gcc-7.5 ICE in cpp tests (#57016)", "body": "Summary:\ngcc-7.5 optimizer can hit internal compiler error if both `-fopenmp` and\n`-faligned-new` are passed:\n```\n/var/lib/jenkins/workspace/test/cpp/api/transformer.cpp: In function 'void transformer_decoder_test_helper(bool)':\n/var/lib/jenkins/workspace/test/cpp/api/transformer.cpp:609:6: internal compiler error: in equal_mem_array_ref_p, at tree-ssa-scopedtables.c:429\n void transformer_decoder_test_helper(bool is_cuda) {\n      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n```\n\nFixes https://github.com/pytorch/pytorch/issues/40941\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57016\n\nReviewed By: walterddr\n\nDifferential Revision: D28027670\n\nPulled By: malfet\n\nfbshipit-source-id: 834e34b95e09bcae39ada25e02749f479a7e9013", "pr_number": "57016", "files_changed": ["test/cpp/api/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "9d54475032": {"title": "Hide module paths leaking in the documentation. (#54585)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54354\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54585\n\nReviewed By: H-Huang\n\nDifferential Revision: D28027037\n\nPulled By: mruberry\n\nfbshipit-source-id: 219874e143221f5e8349d007f88464e0be1a6243", "pr_number": "54585", "files_changed": ["docs/source/conf.py", "torch/fx/symbolic_trace.py", "torch/serialization.py"], "labels": ["Merged", "cla signed", "fx", "module: doc infra", "open source", "triaged"]}, "6c37788cb1": {"title": "[torch] Add cuda support for segment reduction 'max' (#56704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56704\n\nThis is re submit of PR: https://github.com/pytorch/pytorch/pull/54175\n\nMain changes compared to original PR:\n- Switch to importing \"<ATen/cuda/cub.cuh>\"\n- Use CUB_WRAPPER to reduce boiler plate code.\n\nTest Plan:\nWill check CI status to make sure a\n\nAdded unit test\n\nReviewed By: ngimel\n\nDifferential Revision: D27941257\n\nfbshipit-source-id: 24a0e0c7f6c46126d2606fe42ed03dca15684415", "pr_number": "56704", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/cub.cuh", "aten/src/ATen/native/SegmentReduce.cpp", "aten/src/ATen/native/SegmentReduce.h", "aten/src/ATen/native/cuda/SegmentReduce.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_segment_reductions.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "6e91e90b4d": {"title": "Use OpInfo for unsqueeze test (#56924)", "body": "Summary:\nThis PR is ready for https://github.com/pytorch/pytorch/issues/56774.\n\n(cc: mruberry, emcastillo, kmaehashi)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56924\n\nReviewed By: H-Huang\n\nDifferential Revision: D28026529\n\nPulled By: mruberry\n\nfbshipit-source-id: 3afb33bb2999110c565728cd761d3e7d9d3fc82b", "pr_number": "56924", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "cf17fd6dd5": {"title": "Fix multinomial CUDA misalignment and non-deterministic behavior (#55364)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/46702\n\n- fails on probability distribution with odd items\n  - trying to access an `acc_type` (`float`) in a `scalar_t` (`float16`) aligned memory\n- produce unrepeatable result for large input tensor\n  - parallel cumsum not monotonic at some positions\n\n### Fixes\n- computing cumsum on `acc_type` (`float`) instead of using `scalar_t` (`float16`) fixed both issues\n- the non-monotonic behavior may happen even using `float`, though\n  - in these cases, deterministic behavior may be achieved by eliminating the race condition when writing the result, using the atomic function `atomicMax`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55364\n\nReviewed By: mruberry\n\nDifferential Revision: D28031666\n\nPulled By: ngimel\n\nfbshipit-source-id: 0fc6289e0b9ea2d31ef3771e7ca370de8f5c02de", "pr_number": "55364", "files_changed": ["aten/src/ATen/native/cuda/MultinomialKernel.cu", "aten/src/ATen/native/cuda/block_reduce.cuh", "test/test_torch.py"], "labels": ["Merged", "cla signed", "open source"]}, "a18f3aacee": {"title": "Vectorize floating point floor_divide (#55380)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55380\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27993499\n\nPulled By: mruberry\n\nfbshipit-source-id: 45ea9c3295e4d85316bae9487db20914e0cbe3ed", "pr_number": "55380", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "6c602eb099": {"title": "Don't hold ThreadPool lock when destructing task (#56817)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56817\n\nFix https://github.com/pytorch/pytorch/issues/56701 and https://github.com/pytorch/pytorch/issues/56786\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D27975642\n\nPulled By: ezyang\n\nfbshipit-source-id: b7f4a6c18a4fa65c38bacc7c46246f0865c95f86", "pr_number": "56817", "files_changed": ["c10/core/thread_pool.cpp"], "labels": ["Merged", "cla signed"]}, "3de86b951d": {"title": "Migrate thrust->cub for index put (#55693)", "body": "Summary:\n64bit indexing is not supported, because if `num_indices = 2^31`, then 4 long tensors of `num_indices` elements will take 64GB RAM. I don't think anybody will be interested in running `index_put` with 64GB GPU RAM.\n\nBenchmark on CUDA 11.3 RTX3090:\n```python\nimport torch\nimport itertools\n\ndef run50_sync(f):\n    for _ in range(50):\n        f()\n    torch.cuda.synchronize()\n\nrun50_sync(lambda: torch.randperm(1000000, device='cuda'))\n\ndef benchmark(M, L):\n    a = torch.randn(M, device='cuda')\n    i1 = torch.randint(M, (L,), dtype=torch.long, device='cuda')\n    v = torch.randn(L, device='cuda')\n\n    torch.cuda.synchronize()\n\n    %timeit run50_sync(lambda:a.index_put_((i1,), v, True))\n\nfor M, L in itertools.product((100, 100000, 10000000), repeat=2):\n    print(M, L)\n    benchmark(M, L)\n```\n\nBefore\n```\n100 100\n5.13 ms \u00b1 91 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n100 100000\n30.2 ms \u00b1 471 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n100 10000000\n3.17 s \u00b1 14.5 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n100000 100\n5.19 ms \u00b1 61.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n100000 100000\n11.9 ms \u00b1 200 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n100000 10000000\n712 ms \u00b1 3.49 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n10000000 100\n5.07 ms \u00b1 66.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10000000 100000\n12.1 ms \u00b1 76.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10000000 10000000\n627 ms \u00b1 7.65 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\nAfter\n```\n100 100\n3.75 ms \u00b1 49.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n100 100000\n26.2 ms \u00b1 154 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n100 10000000\n2.81 s \u00b1 23.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n100000 100\n3.85 ms \u00b1 16.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n100000 100000\n9.74 ms \u00b1 40.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n100000 10000000\n444 ms \u00b1 1.86 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n10000000 100\n3.85 ms \u00b1 14.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10000000 100000\n10.7 ms \u00b1 116 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n10000000 10000000\n396 ms \u00b1 2.63 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55693\n\nReviewed By: albanD\n\nDifferential Revision: D27895967\n\nPulled By: ngimel\n\nfbshipit-source-id: 0616ce33395ce46f1a4161dfd38940b8e54fedc2", "pr_number": "55693", "files_changed": ["aten/src/ATen/cuda/cub.cuh", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LegacyThrustHelpers.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cuda_cub_test.cu", "aten/tools/run_tests.sh", "test/test_indexing.py"], "labels": ["Merged", "cla signed", "open source"]}, "cea265b8d8": {"title": "Support layer_norm for static runtime (#56444)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56444\n\nAdded out version for layer_norm\n\nTest Plan:\nbuck test caffe2/aten:math_kernel_test -- NativeLayerNorm\n\nbuck test caffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nReviewed By: hlu1\n\nDifferential Revision: D27873846\n\nfbshipit-source-id: 53ee9fec4ff9a4e78198b031e86b5afd013626dd", "pr_number": "56444", "files_changed": ["aten/src/ATen/native/layer_norm.cpp", "aten/src/ATen/native/layer_norm.h", "benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "f9e7e2e20e": {"title": "Remove unnecessary noCuda arg from AtomicJitFuture (#56973)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56973\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D28020918\n\nPulled By: mrshenli\n\nfbshipit-source-id: 99d0e4306f7650be97f73af00d89bdbb762595bc", "pr_number": "56973", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5b3c0ae563": {"title": "Use a FutureFactoryRegistry to allow libtorch_cpu files to create CUDAFuture (#56984)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56984\n\nThis is a preparation PR before we can create CUDAFuture in rref_impl.cpp.\n\nThe solution is adding a `FutureFactoryRegistry` in `rpc/utils.*`. The\nTensorPipe RPC agent is responsible for registering `CUDAFuture` factory\nand `ivalue::Future` factory. The reason that we need this change instead\nof directly using `USE_CUDA` macro in RRef files is as follows. There are\nthree build targets: `torch_cpu`, `torch_cuda`, and `torch_python`.\n`torch_python` is built on top of the other two. `torch_cpu` is CPU-only,\nwhich contains no CUDA-related code, and hence no `USE_CUDA` macro.\n`tensorpipe_*` files are in `torch_python` which does have access to CUDA.\nHowever RRef source files are in `torch_cpu`, which cannot contain CUDA\ncode. The recommended solution is to allow dynamic dispatching. Therefore,\nwe had this PR.\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D28020917\n\nPulled By: mrshenli\n\nfbshipit-source-id: e67c76a273074aebb61877185cc5e6bc0a1a5448", "pr_number": "56984", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/utils.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "9da0f2e95e": {"title": "Support `__pos__` and `positive` (#55891)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55604.\n\nThis PR implements `torch.Tensor.__pos__` and `torch.positive` for the compatibility with NumPy\u2019s interface. (cc: mruberry, rgommers, emcastillo and kmaehashi)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55891\n\nReviewed By: H-Huang\n\nDifferential Revision: D28025928\n\nPulled By: mruberry\n\nfbshipit-source-id: e43e329a802f31bf8805f6efab5c2c7ef34c88b9", "pr_number": "55891", "files_changed": ["aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: numpy", "oncall: jit", "open source"]}, "092eeedcb7": {"title": "[profier] Fix double printing of FLOPs (#56974)", "body": "Summary:\nCall table() shouldn't modify the events\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56974\n\nTest Plan:\n```\nimport torch\nfrom torch import nn\nfrom torch.profiler import profile, record_function\n\nmodel = nn.Conv2d(8, 64, 3, padding=1)\ninput = torch.randn(1, 8, 272, 272)\n\nwith profile(record_shapes=True, with_flops=True) as prof:\n    with record_function(\"model_inference\"):\n        model(input)\n\nevents = prof.key_averages(group_by_input_shape=True)\nprint(events.table())\nprint(events.table())\n```\n\n```\n----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------  ------------\n                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                   Input Shapes      GFLOPS/s\n----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------  ------------\n                 aten::zeros         0.78%      68.000us         1.16%     101.000us     101.000us             1                           [[], [], [], [], []]            --\n                 aten::empty         0.49%      43.000us         0.49%      43.000us      14.333us             3                       [[], [], [], [], [], []]            --\n                 aten::zero_         0.23%      20.000us         0.23%      20.000us      20.000us             1                                          [[1]]            --\n             model_inference        13.67%       1.195ms        98.84%       8.639ms       8.639ms             1                                             []            --\n                aten::conv2d         0.42%      37.000us        85.13%       7.440ms       7.440ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [        91.645\n           aten::convolution         0.15%      13.000us        84.70%       7.403ms       7.403ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [            --\n          aten::_convolution         0.48%      42.000us        84.55%       7.390ms       7.390ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [            --\n    aten::mkldnn_convolution        83.47%       7.295ms        84.07%       7.348ms       7.348ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [            --\n           aten::as_strided_         0.31%      27.000us         0.31%      27.000us      27.000us             1                [[1, 64, 272, 272], [], [], []]            --\n----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------  ------------\nSelf CPU time total: 8.740ms\n\n----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------  ------------\n                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                   Input Shapes      GFLOPS/s\n----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------  ------------\n                 aten::zeros         0.78%      68.000us         1.16%     101.000us     101.000us             1                           [[], [], [], [], []]            --\n                 aten::empty         0.49%      43.000us         0.49%      43.000us      14.333us             3                       [[], [], [], [], [], []]            --\n                 aten::zero_         0.23%      20.000us         0.23%      20.000us      20.000us             1                                          [[1]]            --\n             model_inference        13.67%       1.195ms        98.84%       8.639ms       8.639ms             1                                             []            --\n                aten::conv2d         0.42%      37.000us        85.13%       7.440ms       7.440ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [        91.645\n           aten::convolution         0.15%      13.000us        84.70%       7.403ms       7.403ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [            --\n          aten::_convolution         0.48%      42.000us        84.55%       7.390ms       7.390ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [            --\n    aten::mkldnn_convolution        83.47%       7.295ms        84.07%       7.348ms       7.348ms             1  [[1, 8, 272, 272], [64, 8, 3, 3], [64], [], [            --\n           aten::as_strided_         0.31%      27.000us         0.31%      27.000us      27.000us             1                [[1, 64, 272, 272], [], [], []]            --\n----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------  ------------\nSelf CPU time total: 8.740ms\n```\n\nFixes https://github.com/pytorch/pytorch/issues/55606\n\nReviewed By: xuzhao9\n\nDifferential Revision: D28019925\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 7e7d7ed496059caf917a3dd8dea2daaceb5db920", "pr_number": "56974", "files_changed": ["torch/autograd/profiler.py"], "labels": ["Merged", "cla signed"]}, "26ed4b4756": {"title": "OpInfo : index_fill (port remaining method_tests) (#57009)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/53237\n\nBefore PR (around 90s) (most time consuming tests in details)\n\n<details>\n\n```\npytest test/test_ops.py -k _index_fill --durations=20\n========================================================================= test session starts ==========================================================================\nplatform linux -- Python 3.8.6, pytest-6.1.2, py-1.9.0, pluggy-0.13.1\nplugins: hypothesis-5.38.1\ncollected 19327 items / 19225 deselected / 102 selected\n\ntest/test_ops.py s..................ssssssssssssssssssss..................ss....ssssssssssssssss....sssss....ssssss....                                          [100%]\n\n=========================================================================== warnings summary ===========================================================================\n========================================================================= slowest 20 durations =========================================================================\n44.14s call     test/test_ops.py::TestGradientsCUDA::test_fn_gradgrad_index_fill_cuda_complex128\n13.08s call     test/test_ops.py::TestGradientsCPU::test_fn_gradgrad_index_fill_cpu_complex128\n7.36s call     test/test_ops.py::TestGradientsCUDA::test_fn_grad_index_fill_cuda_complex128\n4.20s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_index_fill_cuda_float32\n3.42s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_index_fill_cpu_float32\n2.93s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_index_fill_cuda_complex64\n2.32s call     test/test_ops.py::TestGradientsCPU::test_fn_grad_index_fill_cpu_complex128\n2.18s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_index_fill_cpu_complex64\n1.03s call     test/test_ops.py::TestOpInfoCUDA::test_duplicate_method_tests_index_fill_cuda_float32\n0.84s call     test/test_ops.py::TestGradientsCUDA::test_fn_grad_index_fill_cuda_float64\n0.64s call     test/test_ops.py::TestGradientsCUDA::test_fn_gradgrad_index_fill_cuda_float64\n0.41s call     test/test_ops.py::TestOpInfoCUDA::test_supported_backward_index_fill_cuda_complex128\n0.41s call     test/test_ops.py::TestOpInfoCUDA::test_supported_backward_index_fill_cuda_bfloat16\n0.39s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_eager_index_fill_cuda_complex64\n0.38s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_eager_index_fill_cuda_float32\n0.36s call     test/test_ops.py::TestOpInfoCUDA::test_supported_backward_index_fill_cuda_complex64\n0.36s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_float16\n0.35s call     test/test_ops.py::TestOpInfoCUDA::test_supported_backward_index_fill_cuda_float16\n0.35s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_int16\n0.35s call     test/test_ops.py::TestOpInfoCUDA::test_supported_backward_index_fill_cuda_float32\n======================================================================= short test summary info ========================================================================\n=============================================== 52 passed, 50 skipped, 19225 deselected, 8 warnings in 97.31s (0:01:37) ================================================\n```\n</details>\n\nAfter PR (around 90s) (most time consuming tests in details)\n\n<details>\n\n```\npytest test/test_ops.py -k _index_fill --durations=20\n========================================================================= test session starts ==========================================================================\nplatform linux -- Python 3.8.6, pytest-6.1.2, py-1.9.0, pluggy-0.13.1\nplugins: hypothesis-5.38.1\ncollected 19327 items / 19225 deselected / 102 selected\n\ntest/test_ops.py s..................ssssssssssssssssssss..................ss....ssssssssssssssss....sssss....ssssss....                                          [100%]\n\n=========================================================================== warnings summary ===========================================================================\n========================================================================= slowest 20 durations =========================================================================\n40.88s call     test/test_ops.py::TestGradientsCUDA::test_fn_gradgrad_index_fill_cuda_complex128\n13.12s call     test/test_ops.py::TestGradientsCPU::test_fn_gradgrad_index_fill_cpu_complex128\n7.03s call     test/test_ops.py::TestGradientsCUDA::test_fn_grad_index_fill_cuda_complex128\n3.48s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_index_fill_cuda_complex64\n3.01s call     test/test_ops.py::TestCommonCUDA::test_variant_consistency_jit_index_fill_cuda_float32\n2.55s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_index_fill_cpu_complex64\n2.43s call     test/test_ops.py::TestGradientsCPU::test_fn_grad_index_fill_cpu_complex128\n2.38s call     test/test_ops.py::TestCommonCPU::test_variant_consistency_jit_index_fill_cpu_float32\n1.10s call     test/test_ops.py::TestOpInfoCUDA::test_duplicate_method_tests_index_fill_cuda_float32\n0.76s call     test/test_ops.py::TestGradientsCUDA::test_fn_grad_index_fill_cuda_float64\n0.67s call     test/test_ops.py::TestGradientsCUDA::test_fn_gradgrad_index_fill_cuda_float64\n0.50s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_bfloat16\n0.50s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_uint8\n0.49s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_float64\n0.49s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_float16\n0.49s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_complex128\n0.49s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_bool\n0.49s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_float32\n0.49s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_int32\n0.48s call     test/test_ops.py::TestOpInfoCUDA::test_supported_dtypes_index_fill_cuda_complex64\n======================================================================= short test summary info ========================================================================\n\n=============================================== 52 passed, 50 skipped, 19225 deselected, 8 warnings in 93.31s (0:01:33) ================================================\n```\n\n</details>\n\nTODO:\n* [x] Add test timings (Before and After)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57009\n\nReviewed By: H-Huang\n\nDifferential Revision: D28027095\n\nPulled By: mruberry\n\nfbshipit-source-id: 6509ff726c8d954171cc0921b803ba261091a0e9", "pr_number": "57009", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "dd84224edc": {"title": ".github: Switch alpine to ECR image instead (#57060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57060\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D28040144\n\nPulled By: seemethere\n\nfbshipit-source-id: f7590256c9f067add5d5e7b61a2c44beb2482d71", "pr_number": "57060", "files_changed": [".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "d16ed1ee8a": {"title": "Add first draft of gradcheck note (#55966)", "body": "Summary:\nYou can find the latest rendered version in the `python_doc_build` CI job below, in the artifact tab of that build on circle CI\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55966\n\nReviewed By: H-Huang\n\nDifferential Revision: D28032446\n\nPulled By: albanD\n\nfbshipit-source-id: 227ad37b03d39894d736c19cae3195b4d56fc62f", "pr_number": "55966", "files_changed": ["docs/source/notes/gradcheck.rst"], "labels": ["Merged", "cla signed"]}, "daef60c3b7": {"title": "Initial support for sparse complex tensors constructors for CPU/CUDA (#54153)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54153\n\nCurrently, sparse tensors only support real floating point tensors. Complex support is added in this PR for CPU/CUDA.\n\n- [x] add complex support (torch.cfloat and torch.cdouble) to torch.sparse_coo_tensor constructors\n- [x] add complex support to coalesce function\n- [x] add complex support to to_dense function\n- [x] add complex support to to_sparse function\n- [x] add complex support to sparse_add function\n- [x] add unit tests\n\nNote: This PR contains only complex support for torch.sparse_coo_tensor fordward function and the related ops used with this function (coalesce, to_dense, to_sparse, and sparse_add). The following PRs in ghstack should cover other sparse operations to have a more complex sparse support, specifically related with the use of specific APIs for accelerated linear algebra.\n\nNote: Before using ghstack the original PR  was  #50984\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D27765618\n\nPulled By: ezyang\n\nfbshipit-source-id: a9cdd31d5c7a7dafd790f6cc148f3df26e884c89", "pr_number": "54153", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/native/cuda/Nonzero.cu", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/TH/THTensor.h", "aten/src/TH/THTensorEvenMoreMath.cpp", "aten/src/TH/THVector.h", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "test/test_autograd.py", "test/test_sparse.py", "test/test_type_promotion.py", "tools/autograd/gen_variable_type.py"], "labels": ["Merged", "Reverted", "cla signed", "open source"]}, "4a899bb3c4": {"title": "Fix: Incorrect example output in sparse_csr_tensor doc-string (#56722)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56722\n\nFix: Incorrect example output in sparse_csr_tensor doc-string\ncloses gh-56685\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D27999920\n\nPulled By: ezyang\n\nfbshipit-source-id: 0b344f7ddab4be8aadde540ce010b75df4433f4b", "pr_number": "56722", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed", "open source"]}, "ecaa208fd6": {"title": "Fix: sparse_csr_tensor segfaults when crow_indices or col_indices are non-tensors (#56723)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56723\n\nWIP gh-56687\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D27999919\n\nPulled By: ezyang\n\nfbshipit-source-id: 7eb23c8f45f3c459efe65793caecaa6b67a187c9", "pr_number": "56723", "files_changed": ["torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "module: sparse", "open source"]}, "522dca4ab0": {"title": "Port `topk` from THC to ATen, migrate most of sort as well (#55392)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/24648\n\nThe large tensor codepath is ported, but there is a legacy codepath that depends on an inplace sort in THC that is not callable from `at::`. At first glance, THC `topk` seems to be the only function that uses this `sortKeyValueInplace`.\nIs the correct change to wrap `sortKeyValueInplace` in legacy functions for visibility in the `at::` namespace?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55392\n\nReviewed By: ezyang\n\nDifferential Revision: D28014257\n\nPulled By: ngimel\n\nfbshipit-source-id: e297423c763f0691151cb62a4f5eff4cb31fb2b3", "pr_number": "55392", "files_changed": ["BUILD.bazel", "aten/src/ATen/LegacyTHFunctionsCUDA.h", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "aten/src/ATen/native/cuda/Sort.cu", "aten/src/ATen/native/cuda/SortUtils.cuh", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCSortUtils.cuh", "aten/src/THC/THCTensorMath.h", "aten/src/THC/THCTensorSort.cu", "aten/src/THC/THCTensorSort.cuh", "aten/src/THC/THCTensorTopK.cuh", "aten/src/THC/generated/THCTensorSortBFloat16.cu", "aten/src/THC/generated/THCTensorSortByte.cu", "aten/src/THC/generated/THCTensorSortChar.cu", "aten/src/THC/generated/THCTensorSortDouble.cu", "aten/src/THC/generated/THCTensorSortFloat.cu", "aten/src/THC/generated/THCTensorSortHalf.cu", "aten/src/THC/generated/THCTensorSortInt.cu", "aten/src/THC/generated/THCTensorSortLong.cu", "aten/src/THC/generated/THCTensorSortShort.cu", "aten/src/THC/generated/THCTensorTopKBFloat16.cu", "aten/src/THC/generated/THCTensorTopKByte.cu", "aten/src/THC/generated/THCTensorTopKChar.cu", "aten/src/THC/generated/THCTensorTopKDouble.cu", "aten/src/THC/generated/THCTensorTopKFloat.cu", "aten/src/THC/generated/THCTensorTopKHalf.cu", "aten/src/THC/generated/THCTensorTopKInt.cu", "aten/src/THC/generated/THCTensorTopKLong.cu", "aten/src/THC/generated/THCTensorTopKShort.cu", "aten/src/THC/generic/THCTensorSort.cu", "aten/src/THC/generic/THCTensorSort.h", "aten/src/THC/generic/THCTensorTopK.cu", "aten/src/THC/generic/THCTensorTopK.h", "test/test_sort_and_select.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "60a5ebfac2": {"title": "[Pytorch Edge] Remove methods_to_optimize arg (#57045)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57045\n\nWent back and adjusted the previous optimizations to just be applied to every function.\nCleaned up api to match.\n\nghstack-source-id: 127214412\nghstack-source-id: 127536155\n\nTest Plan: unit test\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27950859\n\nfbshipit-source-id: 214e83d5a19b452747fe223615815c10fa4aee58", "pr_number": "57045", "files_changed": ["test/test_mobile_optimizer.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/passes/xnnpack_rewrite.cpp", "torch/csrc/jit/passes/xnnpack_rewrite.h", "torch/csrc/jit/python/init.cpp", "torch/utils/mobile_optimizer.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "09feb5f579": {"title": "Delete grandfathered Caffe2 dispatch keys. (#56939)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56939\n\nThese never have kernels registered to them and are effectively useless.\nWhat I am not so sure if we allocate tensors to them or not; if we do\nI cannot use asserts and I need to ensure we just return undefined\nor something equivalent.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: ailzhang\n\nDifferential Revision: D28006160\n\nPulled By: ezyang\n\nfbshipit-source-id: f8e2b61b8bd928fb2c0ac0b534bd4af076423f71", "pr_number": "56939", "files_changed": ["c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.h", "c10/core/TensorOptions.h"], "labels": ["Merged", "cla signed"]}, "18c89a904b": {"title": "Modernize test-suite in sparse tensor CSR (#56392)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56392\n\nFixes for gh-56371 and gh-56369\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D27913212\n\nPulled By: mruberry\n\nfbshipit-source-id: 2c78fe9fa4b6c6b566d9eb01f71e6016d672a545", "pr_number": "56392", "files_changed": ["aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "test/expect/TestSparseCSRCPU.test_sparse_csr_print_cpu.expect", "test/test_sparse_csr.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "module: sparse", "open source"]}, "0d41122e61": {"title": "Eliminate global usage of torch.set_default_dtype in sparse test (#56393)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56393\n\nFixes for  gh-56369\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27913266\n\nPulled By: mruberry\n\nfbshipit-source-id: 2c590d3a2188aae251184f08c1a6a2c4c570d150", "pr_number": "56393", "files_changed": ["test/expect/TestCudaSparse.test_print.expect", "test/expect/TestCudaUncoalescedSparse.test_print.expect", "test/expect/TestSparse.test_print.expect", "test/expect/TestSparseCPU.test_print_coalesced_cpu_float64.expect", "test/expect/TestSparseCPU.test_print_uncoalesced_cpu_float64.expect", "test/expect/TestSparseCUDA.test_print_coalesced_cuda_float64.expect", "test/expect/TestSparseCUDA.test_print_uncoalesced_cuda_float64.expect", "test/expect/TestUncoalescedSparse.test_print.expect", "test/test_sparse.py"], "labels": ["Merged", "cla signed", "module: sparse", "open source"]}, "fa57191b16": {"title": "fix #56822 (#56967)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/56822\n\nThere was an off by one in CPU randperm when checking the limits of the requested range. Also shows up in the \"CUDA\" version as it will fallback to CPU for small input sizes.\n\nCC zasdfgbnm\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56967\n\nReviewed By: mruberry\n\nDifferential Revision: D28031819\n\nPulled By: ngimel\n\nfbshipit-source-id: 4d25995628997f164aafe94e7eae6c54f018e4e5", "pr_number": "56967", "files_changed": ["aten/src/ATen/native/TensorFactories.h", "test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c307379170": {"title": "Output tensor specified via out= must be on the same device as inputs for dot & vdot (#56334)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55561.\n\n1. Added checks to ensure that Output tensor specified via out= must be on the same device as inputs for `dot` & `vdot`.\n2. Unskipped `test_out` for `dot` & `vdot`.\n3. Also changed the `tensordot` implementation to check if both input tensors are on the same device as the output tensor.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56334\n\nReviewed By: H-Huang\n\nDifferential Revision: D27993778\n\nPulled By: mruberry\n\nfbshipit-source-id: 36dee41ceef123c29d0cc52d6b09c3c440e8e60e", "pr_number": "56334", "files_changed": ["aten/src/ATen/native/Linear.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c4bec76bec": {"title": "ns for fx: move node I/O dtype mapping to be local instead of global (#57021)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57021\n\nTo support shadows of custom functions, we need to allow user to\nspecify I/O type of the custom functions.\n\nThis PR is a cleanup in preparation for making the above happen.\nWe make the I/O dtype mappings be generated by a function instead\nof a global variable. In the next PR, we will add a hook so user\ncan modify these mappings.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030094\n\nPulled By: vkuzo\n\nfbshipit-source-id: 3cbb617f034ef385c2875c4ec7fed13ca30bfc57", "pr_number": "57021", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "782a0a1469": {"title": "ns for fx: allow user functions in shadowing (#57022)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57022\n\nAllows usage of user functions in NS shadow APIs. We expose the\ni/o mapping to the user APIs, and thread them throughout the code.\n\nNote: the format of the mapping is currently not the best.  Saving\nimproving that for a future PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030095\n\nPulled By: vkuzo\n\nfbshipit-source-id: 2863312362223ad276437e2aeeec4a3f71b691c7", "pr_number": "57022", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "2acc19eca1": {"title": "ns for fx: add fp16 function shadowing (#57023)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57023\n\nAdds functionality for shadowing user functions with fp16 I/O dtype.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030092\n\nPulled By: vkuzo\n\nfbshipit-source-id: 642792398a76bd62593fa439ab14901e9dbdf4f8", "pr_number": "57023", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/utils.py"], "labels": ["Merged", "cla signed"]}, "ddedeab66d": {"title": "ns for fx: bug fix for shadowing fp16 emulation patterns (#57024)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57024\n\nEnables shadow copies of fp16 emulation patterns where weights\nare cast to fp16 before being passed to linear.  This previously\ndid not work because copying of `call_method` nodes was not implemented.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_linear_fp16_vs_linear_fp16_shadow_activations\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030096\n\nPulled By: vkuzo\n\nfbshipit-source-id: 13a39ea6c106180df6d750246672286b58b4d04c", "pr_number": "57024", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py"], "labels": ["Merged", "cla signed"]}, "e8a5490c0a": {"title": "ns for fx: support binary ops when adding unshadowed loggers for inputs (#57025)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57025\n\nAdds the ability to log unshadowed inputs of binary ops such as `add`\nand `mul`, when indices 0, 1, or 0 and 1 are tensors.\n\nNote: making shadowing support this is saved for a future PR.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_add_mul_inputs_activations\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030098\n\nPulled By: vkuzo\n\nfbshipit-source-id: fd46760faac153975cd7688e70c44991ec1d5dff", "pr_number": "57025", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/ns_types.py", "torch/quantization/ns/utils.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "a359cfac22": {"title": "ns for fx: add option to skip matching classes and functions (#57026)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57026\n\nAdds a config option to skip matching classes by class type\nand functions by function type.\n\nThis is useful when users make custom modules which return\ntypes other than tensors. With the current implementation of\nLogger, these are not scriptable.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_module_scriptable\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030093\n\nPulled By: vkuzo\n\nfbshipit-source-id: 71dc54dd935d2071c4b017260ea2a1e5c2298bfe", "pr_number": "57026", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/mappings.py"], "labels": ["Merged", "cla signed"]}, "da2cef6a40": {"title": "ns for fx: allow comparing int8 to int8 for functionals (#57027)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57027\n\nFixes a bug to allow shadowing of linear and conv functionals.\nThe bug is to only detach tensors, not all objects.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_int8_shadows_int8_fun\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030090\n\nPulled By: vkuzo\n\nfbshipit-source-id: 0a38c4b232e007d7822eee818b0af99d98335d22", "pr_number": "57027", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_passes.py"], "labels": ["Merged", "cla signed"]}, "9fe2673d1c": {"title": "ns for fx: additional bugfix for user defined functions (#57028)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57028\n\nAdds a test case for wrapped sigmoid, and fixes the following issues\nto make it pass in NS:\n* allows comparing between x.sigmoid() and torch.sigmoid(x), if they are related\n* allows dtype cast from FP32_OR_INT8 to FP32, via dequantize (this will be improved later)\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs.test_user_defined_function\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28030089\n\nPulled By: vkuzo\n\nfbshipit-source-id: b237353e2d564a4476f409df461746a259015a4b", "pr_number": "57028", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/graph_passes.py", "torch/quantization/ns/weight_utils.py"], "labels": ["Merged", "cla signed"]}, "e8c268746b": {"title": "Remove sync for randperm on small tensors. (#54113)", "body": "Summary:\nFor small tensors, it is known that GPU operates slower than CPU. However, offloading to CPU causes host <--> device sync. As a result, although offloading to CPU has better microbenchmarks, it often hurts instead of benefits the end-to-end performance, and it could be a blocker for CUDA graphs. After discussion with mcarilli and ptrblck, we think it might be good to just remove this piece of code and let it be slow.\n\nMicrobenchmarks:\n\n```python\ndef run50_sync(f):\n    for _ in range(50):\n        f()\n    torch.cuda.synchronize()\n\ntorch.cuda.synchronize()\n%timeit run50_sync(lambda: torch.randperm(3, device='cuda'))\n%timeit run50_sync(lambda: torch.randperm(30, device='cuda'))\n%timeit run50_sync(lambda: torch.randperm(300, device='cuda'))\n%timeit run50_sync(lambda: torch.randperm(3000, device='cuda'))\n%timeit run50_sync(lambda: torch.randperm(30000, device='cuda'))\n%timeit run50_sync(lambda: torch.randperm(300000, device='cuda'))\n%timeit run50_sync(lambda: torch.randperm(3000000, device='cuda'))\n%timeit run50_sync(lambda: torch.randperm(30000000, device='cuda'))\n```\n\nBefore this PR:\n```\n5.79 ms \u00b1 51.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5.78 ms \u00b1 92.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n6.17 ms \u00b1 87.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.65 ms \u00b1 69.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n17.6 ms \u00b1 133 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n21 ms \u00b1 120 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n104 ms \u00b1 880 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n944 ms \u00b1 3.49 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\nAfter this PR:\n```\n7.22 ms \u00b1 11.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.28 ms \u00b1 9.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n7.25 ms \u00b1 10.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.19 ms \u00b1 5.83 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n9.76 ms \u00b1 162 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n12.3 ms \u00b1 11.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n69.3 ms \u00b1 42.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n716 ms \u00b1 1.01 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54113\n\nReviewed By: ezyang\n\nDifferential Revision: D28017958\n\nPulled By: ngimel\n\nfbshipit-source-id: 660992d43ca449e61ce0cb0aa1dae554c9560a4e", "pr_number": "54113", "files_changed": ["aten/src/ATen/native/cuda/Randperm.cu", "aten/src/ATen/native/cuda/Randperm.cuh", "aten/src/ATen/test/cuda_distributions_test.cu", "test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "0319b64ea0": {"title": "[aten][simple] Optimize atrepeat (#56994)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56994\n\n- Use `DimVector` in place of `std::vector<int64_t>` to remove heap allocations for tensors with ndim <= 5\n- Use `sizes()[i]` in place of `size(i)` where we know i is positive\n\nTest Plan: CI\n\nReviewed By: edvgha, swolchok\n\nDifferential Revision: D28022355\n\nfbshipit-source-id: ef20ac73c0a330192ebc41ab9c92374ed8e2484a", "pr_number": "56994", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "6e826cac67": {"title": "To fix inconsistency of digamma with SciPy (#56689)", "body": "Summary:\nFixes {https://github.com/pytorch/pytorch/issues/49015}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56689\n\nReviewed By: mruberry\n\nDifferential Revision: D28014563\n\nPulled By: iramazanli\n\nfbshipit-source-id: 4d311e6a32737e44ebfabfc1a4b9414b0de7b46e", "pr_number": "56689", "files_changed": ["aten/src/ATen/native/Math.h", "aten/src/ATen/native/cuda/Math.cuh", "test/test_unary_ufuncs.py"], "labels": ["Merged", "cla signed"]}, "808850b6de": {"title": "[ARM] Do not use depthwise3x3 conv in grad mode (#56889)", "body": "Summary:\ncpu_depthwise3x3_winograd is not grad aware and therefore should not be used if grad is expected on the input\n\nFixes https://github.com/pytorch/pytorch/issues/56145\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56889\n\nReviewed By: ngimel\n\nDifferential Revision: D27990448\n\nPulled By: malfet\n\nfbshipit-source-id: 9c649f14b8f514eb1dfb7f0eb8e3357c09ddb299", "pr_number": "56889", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["Merged", "cla signed"]}, "1c0617bb54": {"title": "Fix clang-tidy for native CPU ops (#57037)", "body": "Summary:\nAttempts to call clang-tidy on any source file in\n`aten/src/ATen/cpu/native` would fail with series of\n```\n/Users/nshulga/git/pytorch-worktree/aten/src/ATen/native/cpu/Activation.cpp:637:1: warning: variable 'REGISTER_DISPATCH' is non-const and globally accessible, consider making it const [cppcoreguidelines-avoid-non-const-global-variables]\n/Users/nshulga/git/pytorch-worktree/aten/src/ATen/native/cpu/Activation.cpp:638:1: error: C++ requires a type specifier for all declarations [clang-diagnostic-error]\nREGISTER_DISPATCH(log_sigmoid_backward_cpu_stub, &log_sigmoid_backward_cpu_kernel);\n```\nbecause those macros are only defined for cpu-arch specific compilation of above mentioned files.\nFix this by introducing `map_filename` function that will map source\nfile to its copy in `build` folder, run clang-tidy over the copy and\nthan map it back\n\nFind it while working on https://github.com/pytorch/pytorch/pull/56892\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57037\n\nReviewed By: walterddr\n\nDifferential Revision: D28033760\n\nPulled By: malfet\n\nfbshipit-source-id: b67cd007000574ecc165ab4b285c0c102cbceadd", "pr_number": "57037", "files_changed": ["tools/clang_tidy.py"], "labels": ["Merged", "cla signed"]}, "c91ea7d488": {"title": "[PyTorch][Edge] Add binarires for unittests (#57039)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57039\n\n## Summary\nAdd two models (v4 and v5) for testing runtime. (v5 will be introduced in https://github.com/pytorch/pytorch/pull/56002)\n\n## Test plan\nCI\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D28047615\n\nPulled By: cccclai\n\nfbshipit-source-id: 47f7df3094dadb7e013ed57bc713cc8b3d1c8ce0", "pr_number": "57039", "files_changed": ["test/cpp/jit/script_module_v4.ptl", "test/cpp/jit/script_module_v5.ptl"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "51e6ebb5b7": {"title": "Add missing vec256<>::isnan() for VSX float and double vectors (#56658)", "body": "Summary:\nObviously, have no way of testing it\nFixes https://github.com/pytorch/pytorch/issues/56650\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56658\n\nReviewed By: walterddr\n\nDifferential Revision: D27929750\n\nPulled By: malfet\n\nfbshipit-source-id: a4e3fe75cfeeb35f47590c940ef17b2ba4172cd5", "pr_number": "56658", "files_changed": ["aten/src/ATen/cpu/vec256/vsx/vec256_double_vsx.h", "aten/src/ATen/cpu/vec256/vsx/vec256_float_vsx.h"], "labels": ["Merged", "cla signed"]}, "5536cda19a": {"title": "Update floor_divide behavior in line with NumPy 1.20 (#56893)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56893\n\nFixes gh-56814\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D28025814\n\nPulled By: mruberry\n\nfbshipit-source-id: 8654978ea1d5aa7c12bcf5a8c939966287a2d34e", "pr_number": "56893", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "test/test_binary_ufuncs.py"], "labels": ["Merged", "cla signed", "module: bc-breaking", "open source"]}, "1e77ba36db": {"title": "change ddpLoggingData struct to map or dict (#56641)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56641\n\ncurrently ddpLoggingData is flat struct, which requires internal DDP developers and external users to know about the struct field names. This is not flexible to delete or add new fields in the future. also it is hard to access ddpLoggingData.\n\nWith maps/dict, developers and users can easily access the fields without knowing the field names, also easier to add/remove a new/old field.\n\nSince C++ does not support map values to be different types, right now ddpLoggingData containes two types of maps.\nghstack-source-id: 127482694\n\nTest Plan: unit tests\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27923723\n\nfbshipit-source-id: c90199c14925fc50ef219000e2f809dc7601cce1", "pr_number": "56641", "files_changed": ["c10/util/Logging.cpp", "c10/util/Logging.h", "test/distributed/test_c10d_common.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "89377e3e45": {"title": "model_dump tool for model inspection (#56868)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56868\n\nSee __init__.py for a summary of the tool.\nThe following sections are present in this initial version\n- Model Size.  Show the total model size, as well as a breakdown by\n  stored files, compressed files, and zip overhead.  (I expect this\n  breakdown to be a bit more useful once data.pkl is compressed.)\n- Model Structure.  This is basically the output of\n  `show_pickle(data.pkl)`, but as a hierarchical structure.\n  Some structures cause this view to crash right now, but it can be\n  improved incrementally.\n- Zip Contents.  This is basically the output of `zipinfo -l`.\n- Code.  This is the TorchScript code.  It's integrated with a blame\n  window at the bottom, so you can click \"Blame Code\", then click a bit\n  of code to see where it came from (based on the debug_pkl).  This\n  currently doesn't render properly if debug_pkl is missing or\n  incomplete.\n- Extra files (JSON).  JSON dumps of each json file under /extra/, up to\n  a size limit.\n- Extra Pickles.  For each .pkl file in the model, we safely unpickle it\n  with `show_pickle`, then render it with `pprint` and include it here\n  if the size is not too large.  We aren't able to install the pprint\n  hack that thw show_pickle CLI uses, so we get one-line rendering for\n  custom objects, which is not very useful.  Built-in types look fine,\n  though.  In particular, bytecode.pkl seems to look fine (and we\n  hard-code that file to ignore the size limit).\n\nI'm checking in the JS dependencies to avoid a network dependency at\nruntime.  They were retrieved from the following URLS, then passed\nthrough a JS minifier:\n  https://unpkg.com/htm@3.0.4/dist/htm.module.js?module\n  https://unpkg.com/preact@10.5.13/dist/preact.module.js?module\n\nTest Plan:\nManually ran on a few models I had lying around.\nMostly tested in Chrome, but I also poked around in Firefox.\n\nReviewed By: dhruvbird\n\nDifferential Revision: D28020849\n\nPulled By: dreiss\n\nfbshipit-source-id: 421c30ed7ca55244e9fda1a03b8aab830466536d", "pr_number": "56868", "files_changed": ["setup.py", "test/run_test.py", "test/test_model_dump.py", "torch/utils/model_dump/__init__.py", "torch/utils/model_dump/__main__.py", "torch/utils/model_dump/code.js", "torch/utils/model_dump/htm.mjs", "torch/utils/model_dump/preact.mjs", "torch/utils/model_dump/skeleton.html"], "labels": ["Merged", "cla signed"]}, "4638bd0f0f": {"title": "Fix ProcessGroupMPITest.cpp Gather, Scatter and SendRecv. Enable ProcessGroupMPITest (#56709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56709\n\nRight now, ProcessGroupMPITest testGather() fails with\n\n ```\nwhat():  Gather: number of output tensors should be 0 for non-root\n[devgpu025:429730] *** Process received signal ***\n\n```\n\nthere is a similar issue with testScatter() where number of input/output tensors on source/destination respectively should be 0.\n\nIn addition testSendRecv(true); fails with\n\n```\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  src rank is wrong for recvAnysource\n\n```\n\nsince we never populate `srcRanks`\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D28001963\n\nPulled By: agolynski\n\nfbshipit-source-id: c381dfc6f417ee78fbbaf884e567b0485076dfc8", "pr_number": "56709", "files_changed": [".jenkins/pytorch/test.sh", "torch/lib/c10d/test/ProcessGroupMPITest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "46321cb937": {"title": "[static runtime] binding for aten::norm_out (#56636)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56636\n\nTest Plan:\nTest it runs on the aug_1x model, which has aten::norm, and verify jit/sr results\n```\n./buck-out/opt/gen/caffe2/caffe2/fb/predictor/ptvsc2_predictor_bench --scripted_model=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.local.local.pt --pt_inputs=/data/users/ansha/tmp/adfinder/aug_1x/210616848_0.predictor.disagg.input_data.container.pt --iters=500 --warmup_iters=500 --num_threads=1 --pt_enable_static_runtime=1 --pt_cleanup_activations=true --pt_enable_out_variant=1 --pt_optimize_memory=1 --compare_results=1 --do_profile=1 --adsfinder_compatibility=1\n```\n\n```\nTime per node type:\n        1.53159 ms.    35.8619%. fb::sigrid_transforms_torch_bind (1 nodes)\n         0.9481 ms.    22.1996%. aten::linear (6 nodes)\n       0.704806 ms.    16.5029%. aten::argmin (1 nodes)\n       0.252252 ms.    5.90643%. aten::matmul (1 nodes)\n       0.140869 ms.    3.29842%. fb::clip_ranges_gather_sigrid_hash_v3 (77 nodes)\n       0.100014 ms.    2.34181%. fb::clip_ranges_gather (263 nodes)\n      0.0880838 ms.    2.06247%. aten::sub (1 nodes)\n      0.0553556 ms.    1.29614%. aten::repeat (1 nodes)\n      0.0438464 ms.    1.02665%. aten::norm (1 nodes)\n      0.0395956 ms.   0.927124%. fb::batch_box_cox (1 nodes)\n       0.035834 ms.   0.839045%. aten::__getitem__ (506 nodes)\n      0.0345233 ms.   0.808357%. prim::TupleUnpack (254 nodes)\n      0.0316876 ms.   0.741959%. aten::sigmoid (2 nodes)\n      0.0293246 ms.   0.686629%. aten::mul (3 nodes)\n      0.0287696 ms.   0.673635%. fb::offsets_to_ranges (253 nodes)\n      0.0242373 ms.   0.567511%. aten::pow (1 nodes)\n      0.0224204 ms.    0.52497%. fb::simple_embedding_bag_sum (3 nodes)\n      0.0200074 ms.   0.468469%. fb::casted_batch_one_hot_lengths (1 nodes)\n      0.0190264 ms.   0.445499%. fb::concat_add_mul_replacenan_clip (1 nodes)\n      0.0167253 ms.    0.39162%. prim::TupleConstruct (1 nodes)\n      0.0164962 ms.   0.386255%. aten::sum (3 nodes)\n      0.0158986 ms.   0.372262%. prim::DictConstruct (2 nodes)\n      0.0109372 ms.   0.256093%. aten::div (1 nodes)\n     0.00910563 ms.   0.213207%. prim::ListConstruct (4 nodes)\n     0.00876917 ms.   0.205328%. static_runtime::to_copy (8 nodes)\n     0.00822567 ms.   0.192603%. fb::sigrid_hash_precompute (1 nodes)\n     0.00622559 ms.   0.145771%. aten::contiguous (1 nodes)\n     0.00460064 ms.   0.107723%. aten::narrow (4 nodes)\n     0.00297164 ms.  0.0695804%. static_runtime::reshape_copy (2 nodes)\n     0.00287099 ms.  0.0672237%. aten::logit (1 nodes)\n     0.00277557 ms.  0.0649894%. aten::add (1 nodes)\n     0.00264978 ms.  0.0620441%. aten::clamp_min (1 nodes)\n     0.00215832 ms.  0.0505366%. aten::relu (1 nodes)\n     0.00213779 ms.   0.050056%. fb::gather_ranges (4 nodes)\n     0.00195846 ms.  0.0458571%. aten::full (1 nodes)\n     0.00177333 ms.  0.0415222%. aten::stack (1 nodes)\n     0.00147449 ms.   0.034525%. aten::size (3 nodes)\n    0.000762524 ms.  0.0178544%. aten::expand_as (1 nodes)\n    0.000757406 ms.  0.0177345%. fb::clip_ranges (2 nodes)\n    0.000614798 ms.  0.0143954%. fb::lengths_to_offsets (3 nodes)\n    0.000407952 ms. 0.00955212%. static_runtime::flatten_copy (1 nodes)\n    0.000159918 ms. 0.00374445%. prim::device (1 nodes)\n         4.2708 ms. in Total\nStaticRuntime setup time: 0.000407 ms\nMemory allocation time: 0.0089714 ms\nMemory deallocation time: 0.0592135 ms\nOutputs deallocation time: 0.0458097 ms\nTotal memory managed: 947328 bytes\nTotal number of reused tensors: 28\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D27922070\n\nfbshipit-source-id: 538b39b7fff0638fc994b7983bf32d9e9f15d016", "pr_number": "56636", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "ef2bb784da": {"title": "Replace raw cudaMalloc calls with CUDACachingAllocator (#57083)", "body": "Summary:\nReplace raw cudaMalloc calls with CUDACachingAllocator\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57083\n\nReviewed By: zou3519\n\nDifferential Revision: D28058989\n\nPulled By: ezyang\n\nfbshipit-source-id: 84e2d0937e3ad5e3db9ae5a5e584d8c90954e213", "pr_number": "57083", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "5d7e48c9fc": {"title": "Disable one test in rocm (#56951)", "body": "Summary:\nThe test seems to be failing in ROCM 4.1 on CI node.  Disabling the same for now. The test will be    re-enabled for ROCM when CI transitions to 4.2.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56951\n\nReviewed By: zou3519\n\nDifferential Revision: D28059808\n\nPulled By: ezyang\n\nfbshipit-source-id: a9b064b7525ae6dce89c51fe29ff07f37b7ac796", "pr_number": "56951", "files_changed": ["test/test_torch.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "4d72538f80": {"title": "Give Tensor a trivial (for now) metaclass _TensorMeta (#56147)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56147\n\nThis is support of #55686, you can see the broader context of the metaclass in\na more complete PR #56017.  The short story is that in the future I want to\ngive Tensor a non-trivial metaclass, so to derisk the change first I give it a\ntrivial metaclass to shake out any bugs that might be caused by it.  The\nmetaclass shouldn't have any performance impact on Tensor as it only gets\ninvoked upon subclass creation.\n\nBy the way, it was totally not documented how to create metaclasses in the Python\nC API, and it took a good bit of trial error to figure it out (and the answer is\nnow immortalized in https://stackoverflow.com/q/67077317/23845 -- the things\nthat I got wrong in earlier versions of the PR included setting tp_basicsize\nincorrectly, incorrectly setting Py_TPFLAGS_HAVE_GC on the metaclass--you want\nto leave it unset so that it inherits, and determining that tp_init is what\nactually gets called when you construct a class, not tp_call as another\nnot-to-be-named StackOverflow question suggests).\n\nAside: Ordinarily, adding a metaclass to a class is a user visible change, as\nit means that it is no longer valid to mixin another class with a different\nmetaclass. However, because _C._TensorBase is a C extension object, it will\ntypically conflict with most other metaclasses, so this is not BC breaking.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D28028747\n\nPulled By: ezyang\n\nfbshipit-source-id: c1e35a986aeb3db540c73d188f53dce951eeed33", "pr_number": "56147", "files_changed": ["test/test_jit.py", "torch/_C/__init__.pyi.in", "torch/csrc/autograd/python_variable.cpp"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "e362ee6f8a": {"title": "Make it illegal to directly construct _TensorBase (#56150)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56150\n\nSee #56017 for full context; the short story is that by making\nit illegal to directly construct _TensorBase, we need only\nwrite a *single* tp_dealloc function which will work universally\nfor all _TensorBase subclasses, rather than having to write two\nversions, one for _TensorBase itself, and others for Python subclasses\nof _TensorBase.  This means simpler code.\n\nThe subtlety here is that we only install our custom `tp_new` for direct subclasses of TensorBase.  This is important, because overriding the `tp_new` also overrides any user defined constructor.  Fortunately class Tensor(_TensorBase) has no nontrivial constructors and doesn't mind, but other subclasses like Parameter definitely mind!\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D28028746\n\nPulled By: ezyang\n\nfbshipit-source-id: 3c03a14666ad1ded1145fe676afb0a7623cdb9bb", "pr_number": "56150", "files_changed": ["test/test_torch.py", "torch/csrc/autograd/python_variable.cpp"], "labels": ["Merged", "cla signed"]}, "6ee5e490d4": {"title": "[BE][SyncBN] Avoid sync stats in eval mode (#56982)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56982\n\nSyncBatchNorm should behave as a regular BN layer in eval model, this\nchange ensures that this is the case.\n\nIn particular, the bug was when `track_running_stats=False`, `bn_training` would be set to True in eval mode, but this would trigger a collective sync in syncBN.\n\nHowever, in eval mode syncBN should behave like a regular BN layer and not do this sync.\n\nCloses https://github.com/pytorch/pytorch/issues/48988\n\nEnsured with unittest that when used for inference on a single rank, stats sync is not triggered.\nghstack-source-id: 127544421\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27579297\n\nfbshipit-source-id: 26406e2793f0be14f2daa46ae66f97a8494182ed", "pr_number": "56982", "files_changed": ["torch/nn/modules/batchnorm.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ec0fa40f0f": {"title": "Release GIL before destructing RPCAgent subclasses. (#57029)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57029\n\nPartially addresses https://github.com/pytorch/pytorch/issues/56297\n\nThis fixes deadlocks when the threads the RPCAgent are blocking\non try to take the GIL.  This also adds a general utility for\nmaking shared_ptr run destructors without GIL.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28030294\n\nPulled By: ezyang\n\nfbshipit-source-id: 628c066eebbb70bda5b914645a109dce35d73c8d", "pr_number": "57029", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/testing/init.cpp", "torch/csrc/utils/pybind.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "4b8ccc6a0f": {"title": ".circleci: Add /opt/openssl to CI images (#57071)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57071\n\nAdds /opt/openssl v1.1.1 to cpu CI images to enable testing for Gloo\nTCP_TLS\n\nSimilar to https://github.com/pytorch/builder/pull/712\n\nEnables https://github.com/pytorch/pytorch/pull/56442\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D28061203\n\nPulled By: seemethere\n\nfbshipit-source-id: 222a824b30de96c1064da11ce8ce4dc6c851111e", "pr_number": "57071", "files_changed": [".circleci/docker/common/install_openssl.sh", ".circleci/docker/ubuntu-cuda/Dockerfile", ".circleci/docker/ubuntu/Dockerfile"], "labels": ["Merged", "cla signed", "module: docker"]}, "18337fec7e": {"title": "Remove glaringlee from C++ frontend codeowners (#57130)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57130\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D28059800\n\nPulled By: ezyang\n\nfbshipit-source-id: dc8a28761acaf19bc5620912c016c67bdd3a4e5b", "pr_number": "57130", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed"]}, "911852ffe2": {"title": ".github: Only add @generated on generated workflows (#57063)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57063\n\nRemoves the generated tag from the original template so the diff shows\nup correctly on internal Phab\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D28040694\n\nPulled By: seemethere\n\nfbshipit-source-id: c6ec0520fbc4ea169abefc7df2ff925ecc0474cc", "pr_number": "57063", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "ce79bd255d": {"title": "Fix doc issues (#57153)", "body": "Summary:\nFixes inconsistencies in the TorchScript Language reference.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57153\n\nReviewed By: zou3519, gmagogsfm\n\nDifferential Revision: D28061449\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: a055c7b1417391afe00ec0b35e1042acb049feed", "pr_number": "57153", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "efd451385c": {"title": "Add gzip format support for chrome tracing (#56554)", "body": "Summary:\nadd gzip format support when exporting chrome tracing\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56554\n\nReviewed By: xuzhao9\n\nDifferential Revision: D28019111\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 7d522481912bc9e93b4b31b17f01b1b069c7d2b6", "pr_number": "56554", "files_changed": ["test/test_profiler.py", "torch/profiler/profiler.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "610c984d2e": {"title": "[CUDA graphs] Avoid sync errors when graph capturing cudnn rnn calls that use cudnn dropout (#56433)", "body": "Summary:\nCudnn rnn calls that use use cudnn dropout maintain a \"state\" buffer across calls. [DropoutState](https://github.com/pytorch/pytorch/blob/fe3f6f2da2cb2ddde1a277cd5e99f898933a3c5d/aten/src/ATen/native/cudnn/RNN.cpp#L1388-L1402)'s lock() and unlock() ensure the current call's use of the state buffer syncs with the end of the previous call's use of the state buffer (in case the previous call was on a different stream).\n\nTelling a capturing stream to wait on an event recorded in a non-capturing stream is an error (1). Telling a non-capturing stream to wait on an event recorded during capture is also an error (2). So DropoutState's flow can error in either of two simple use cases:\n```python\nrnn = nn.LSTM(512, 512, 2, dropout=0.5).cuda()\n\nout1 = rnn(in1)\n\n# calling cudnn rnn with dropout in capture after calling it uncaptured triggers 1\ncapture_stream.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(capture_stream):\n    graph.capture_begin()\n    out2 = rnn(in2)\n    graph.capture_end()\ntorch.cuda.current_stream().wait_stream(capture_stream)\n\n# calling cudnn rnn with dropout uncaptured after calling it in capture triggers 2\nout3 = rnn(in3)\n```\n\nThis PR fixes both cases by telling `DropoutState::lock()`: \"if the most recent end-of-usage event was in a different capture state (ie, we crossed a capturing<->noncapturing border) or in a different capture, don't sync on it.\" While considering the fix I had two assumptions in mind:\n- only one capture using the RNN can be underway at a time in this process\n- no noncapturing ops in this process are issuing RNN calls while the capture using the RNN is underway.\n\nThat second assumption seems brittle if, for example, someone wants to capture an internal region of the forward method of a model wrapped with DataParallel: multiple threads could be issuing RNN calls with some currently capturing and some not. We should talk about whether that use case seems realistic.\n\n(Bigger-picture thoughts: I don't know if forcing calls to serialize on using the shared state buffer is the best design. And if we want to do it that way, we might as well run all cudnn rnns with dropout on a dedicated side stream synced with the surrounding stream (capturing or not), in which case I don't think this PR's event-handling diffs would be needed.)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56433\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D27966444\n\nPulled By: ezyang\n\nfbshipit-source-id: fe0df843c521e0d48d7f2c81a17aff84c5497e20", "pr_number": "56433", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp", "test/test_cuda.py"], "labels": ["Merged", "Reverted", "cla signed", "module: cuda graphs", "module: cudnn", "module: rnn", "open source", "triaged"]}, "5a10ee71d6": {"title": "[Reland] TCPStore add watchKey method and new listener thread (#56217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56217\n\nReland of https://github.com/pytorch/pytorch/pull/54264\n\nChanges:\n- Update socket send() to use flag MSG_NOSIGNAL to prevent SIGPIPE because error in return is already capturad\n- Update watchKey to block until callback has been registered on master.\n- Fix race condition in testWatchKeyCallback which caused flaky test failures.\n\nTest:\nRan TCPStoreTest 100 times locally with no errors, running [ci-all tests](https://github.com/pytorch/pytorch/pull/56219)\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D27824802\n\nPulled By: H-Huang\n\nfbshipit-source-id: c32230ce726d7d848b9896a63aa52b8eb04a0a2d", "pr_number": "56217", "files_changed": ["torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/Utils.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "4cb534f92e": {"title": "Make PyTorch code-base clang-tidy compliant (#56892)", "body": "Summary:\nThis is an automatic change generated by the following script:\n```\n#!/usr/bin/env python3\nfrom subprocess import check_output, check_call\nimport os\n\ndef get_compiled_files_list():\n    import json\n    with open(\"build/compile_commands.json\") as f:\n        data = json.load(f)\n    files = [os.path.relpath(node['file']) for node in data]\n    for idx, fname in enumerate(files):\n        if fname.startswith('build/') and fname.endswith('.DEFAULT.cpp'):\n            files[idx] = fname[len('build/'):-len('.DEFAULT.cpp')]\n    return files\n\ndef run_clang_tidy(fname):\n    check_call([\"python3\", \"tools/clang_tidy.py\", \"-c\", \"build\", \"-x\", fname,\"-s\"])\n    changes = check_output([\"git\", \"ls-files\", \"-m\"])\n    if len(changes) == 0:\n        return\n    check_call([\"git\", \"commit\",\"--all\", \"-m\", f\"NOLINT stubs for {fname}\"])\n\ndef main():\n    git_files = check_output([\"git\", \"ls-files\"]).decode(\"ascii\").split(\"\\n\")\n    compiled_files = get_compiled_files_list()\n    for idx, fname in enumerate(git_files):\n        if fname not in compiled_files:\n            continue\n        if fname.startswith(\"caffe2/contrib/aten/\"):\n            continue\n        print(f\"[{idx}/{len(git_files)}] Processing {fname}\")\n        run_clang_tidy(fname)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56892\n\nReviewed By: H-Huang\n\nDifferential Revision: D27991944\n\nPulled By: malfet\n\nfbshipit-source-id: 5415e1eb2c1b34319a4f03024bfaa087007d7179", "pr_number": "56892", "files_changed": ["aten/src/ATen/BatchedFallback.cpp", "aten/src/ATen/BatchedTensorImpl.cpp", "aten/src/ATen/BatchingRegistrations.cpp", "aten/src/ATen/CPUGeneratorImpl.cpp", "aten/src/ATen/Context.cpp", "aten/src/ATen/DLConvertor.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/NamedTensorUtils.cpp", "aten/src/ATen/ParallelNative.cpp", "aten/src/ATen/ParallelThreadPoolNative.cpp", "aten/src/ATen/SequenceNumber.cpp", "aten/src/ATen/SparseTensorUtils.cpp", "aten/src/ATen/TensorIndexing.h", "aten/src/ATen/TensorNames.cpp", "aten/src/ATen/Utils.cpp", "aten/src/ATen/VmapMode.cpp", "aten/src/ATen/VmapTransforms.cpp", "aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/benchmarks/quantize_per_channel.cpp", "aten/src/ATen/benchmarks/stateful_conv1d.cpp", "aten/src/ATen/benchmarks/tensor_add.cpp", "aten/src/ATen/core/Dimname.cpp", "aten/src/ATen/core/Formatting.cpp", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/core/NamedTensor.cpp", "aten/src/ATen/core/TensorAccessor.h", "aten/src/ATen/core/TensorImpl_test.cpp", "aten/src/ATen/core/VariableHooksInterface.cpp", "aten/src/ATen/core/Vitals.cpp", "aten/src/ATen/core/blob.h", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_function_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_stackbased_test.cpp", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor_test.cpp", "aten/src/ATen/core/dispatch/CppSignature_test.cpp", "aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/backend_fallback_test.cpp", "aten/src/ATen/core/interned_strings.cpp", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/library.cpp", "aten/src/ATen/core/op_registration/infer_schema.cpp", "aten/src/ATen/core/op_registration/op_registration.cpp", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/core/register_symbols.cpp", "aten/src/ATen/core/type.cpp", "aten/src/ATen/cpu/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec256/vec256_qint.h", "aten/src/ATen/detail/CPUGuardImpl.cpp", "aten/src/ATen/detail/CUDAHooksInterface.cpp", "aten/src/ATen/detail/HIPHooksInterface.cpp", "aten/src/ATen/detail/MetaGuardImpl.cpp", "aten/src/ATen/metal/Context.cpp", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/AdaptiveAveragePooling.cpp", "aten/src/ATen/native/AdaptiveAveragePooling3d.cpp", "aten/src/ATen/native/AdaptiveMaxPooling2d.cpp", "aten/src/ATen/native/AveragePool2d.cpp", "aten/src/ATen/native/AveragePool3d.cpp", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/Batching.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/BlasKernel.cpp", "aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/ConvolutionTBC.cpp", "aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/Cross.cpp", "aten/src/ATen/native/DilatedMaxPool2d.cpp", "aten/src/ATen/native/DilatedMaxPool3d.cpp", "aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Embedding.cpp", "aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/Fill.cpp", "aten/src/ATen/native/FractionalMaxPool2d.cpp", "aten/src/ATen/native/FractionalMaxPool3d.cpp", "aten/src/ATen/native/FunctionOfAMatrixUtils.cpp", "aten/src/ATen/native/GatedLinearUnit.cpp", "aten/src/ATen/native/GridSampler.cpp", "aten/src/ATen/native/IndexingUtils.cpp", "aten/src/ATen/native/Integration.cpp", "aten/src/ATen/native/Lerp.cpp", "aten/src/ATen/native/Linear.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/LossCTC.cpp", "aten/src/ATen/native/LossMultiLabelMargin.cpp", "aten/src/ATen/native/LossMultiMargin.cpp", "aten/src/ATen/native/LossNLL.cpp", "aten/src/ATen/native/LossNLL2d.cpp", "aten/src/ATen/native/MaxPooling.cpp", "aten/src/ATen/native/MaxUnpooling.cpp", "aten/src/ATen/native/MetaTensor.cpp", "aten/src/ATen/native/NNPACK.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/PixelShuffle.cpp", "aten/src/ATen/native/PointwiseOps.cpp", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/QuantizedLinear.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/ReduceAllOps.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReflectionPad.cpp", "aten/src/ATen/native/ReplicationPadding.cpp", "aten/src/ATen/native/Resize.cpp", "aten/src/ATen/native/SegmentReduce.cpp", "aten/src/ATen/native/SobolEngineOps.cpp", "aten/src/ATen/native/SobolEngineOpsUtils.cpp", "aten/src/ATen/native/SoftMax.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorConversions.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorIteratorReduce.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/TensorTransformations.cpp", "aten/src/ATen/native/TriangularOps.cpp", "aten/src/ATen/native/TypeProperties.cpp", "aten/src/ATen/native/Unfold2d.cpp", "aten/src/ATen/native/UnfoldBackward.cpp", "aten/src/ATen/native/UpSample.cpp", "aten/src/ATen/native/UpSampleBicubic2d.cpp", "aten/src/ATen/native/UpSampleBilinear2d.cpp", "aten/src/ATen/native/UpSampleLinear1d.cpp", "aten/src/ATen/native/UpSampleNearest1d.cpp", "aten/src/ATen/native/UpSampleNearest2d.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cpu/BlasKernel.cpp", "aten/src/ATen/native/cpu/CatKernel.cpp", "aten/src/ATen/native/cpu/ComplexKernel.cpp", "aten/src/ATen/native/cpu/CopyKernel.cpp", "aten/src/ATen/native/cpu/CrossKernel.cpp", "aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp", "aten/src/ATen/native/cpu/DistanceOpsKernel.cpp", "aten/src/ATen/native/cpu/FillKernel.cpp", "aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cpu/LerpKernel.cpp", "aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp", "aten/src/ATen/native/cpu/MaxPoolKernel.cpp", "aten/src/ATen/native/cpu/MaxPooling.cpp", "aten/src/ATen/native/cpu/MultinomialKernel.cpp", "aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp", "aten/src/ATen/native/cpu/PowKernel.cpp", "aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp", "aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cpu/ScatterGatherKernel.cpp", "aten/src/ATen/native/cpu/SoftMaxKernel.cpp", "aten/src/ATen/native/cpu/SortingKernel.cpp", "aten/src/ATen/native/cpu/StackKernel.cpp", "aten/src/ATen/native/cpu/SumKernel.cpp", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cpu/Unfold2d.cpp", "aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp", "aten/src/ATen/native/cpu/UpSampleKernel.cpp", "aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp", "aten/src/ATen/native/cpu/batch_norm_kernel.cpp", "aten/src/ATen/native/cpu/group_norm_kernel.cpp", "aten/src/ATen/native/cpu/layer_norm_kernel.cpp", "aten/src/ATen/native/group_norm.cpp", "aten/src/ATen/native/layer_norm.cpp", "aten/src/ATen/native/metal/MetalGuardImpl.cpp", "aten/src/ATen/native/metal/MetalPrepackOpRegister.cpp", "aten/src/ATen/native/mkl/SpectralOps.cpp", "aten/src/ATen/native/mkldnn/IDeepRegistration.cpp", "aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp", "aten/src/ATen/native/mkldnn/MkldnnTensorMath.cpp", "aten/src/ATen/native/mkldnn/Normalization.cpp", "aten/src/ATen/native/mkldnn/Pooling.cpp", "aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/native/quantized/affine_quantizer.cpp", "aten/src/ATen/native/quantized/affine_quantizer_base.cpp", "aten/src/ATen/native/quantized/cpu/conv_serialization.h", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.h", "aten/src/ATen/native/quantized/cpu/int_repr_quant.cpp", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/q_adaavgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool.cpp", "aten/src/ATen/native/quantized/cpu/q_avgpool3d.cpp", "aten/src/ATen/native/quantized/cpu/qadd.cpp", "aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp", "aten/src/ATen/native/quantized/cpu/qclamp.cpp", "aten/src/ATen/native/quantized/cpu/qconcat.cpp", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qconv_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qelu.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp", "aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/qhardswish.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qmul.cpp", "aten/src/ATen/native/quantized/cpu/qnnpack/include/conv_utils.h", "aten/src/ATen/native/quantized/cpu/qnnpack_utils.h", "aten/src/ATen/native/quantized/cpu/qnormalization.cpp", "aten/src/ATen/native/quantized/cpu/qpool.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/qsigmoid.cpp", "aten/src/ATen/native/quantized/cpu/qsort.cpp", "aten/src/ATen/native/quantized/cpu/qtanh.cpp", "aten/src/ATen/native/quantized/cpu/qthreshold.cpp", "aten/src/ATen/native/quantized/cpu/qupsample_bilinear2d.cpp", "aten/src/ATen/native/quantized/cpu/tensor_operators.cpp", "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "aten/src/ATen/native/sparse/SoftMax.cpp", "aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp", "aten/src/ATen/native/sparse/SparseMatMul.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/xnnpack/Convolution.cpp", "aten/src/ATen/native/xnnpack/Init.cpp", "aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp", "aten/src/ATen/nnapi/nnapi_bind.cpp", "aten/src/ATen/nnapi/nnapi_model_loader.cpp", "aten/src/ATen/nnapi/nnapi_wrapper.cpp", "aten/src/ATen/quantized/QTensorImpl.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/record_function.cpp", "aten/src/ATen/test/Dict_test.cpp", "aten/src/ATen/test/Dimname_test.cpp", "aten/src/ATen/test/MaybeOwned_test.cpp", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/ATen/test/apply_utils_test.cpp", "aten/src/ATen/test/atest.cpp", "aten/src/ATen/test/basic.cpp", "aten/src/ATen/test/broadcast_test.cpp", "aten/src/ATen/test/cpu_caching_allocator_test.cpp", "aten/src/ATen/test/cpu_generator_test.cpp", "aten/src/ATen/test/cpu_profiling_allocator_test.cpp", "aten/src/ATen/test/cpu_rng_test.cpp", "aten/src/ATen/test/dlconvertor_test.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "aten/src/ATen/test/half_test.cpp", "aten/src/ATen/test/ivalue_test.cpp", "aten/src/ATen/test/math_kernel_test.cpp", "aten/src/ATen/test/memory_format_test.cpp", "aten/src/ATen/test/memory_overlapping_test.cpp", "aten/src/ATen/test/mobile_memory_cleanup.cpp", "aten/src/ATen/test/native_test.cpp", "aten/src/ATen/test/pow_test.cpp", "aten/src/ATen/test/quantized_test.cpp", "aten/src/ATen/test/reduce_ops_test.cpp", "aten/src/ATen/test/scalar_tensor_test.cpp", "aten/src/ATen/test/scalar_test.cpp", "aten/src/ATen/test/tensor_interop_test.cpp", "aten/src/ATen/test/tensor_iterator_test.cpp", "aten/src/ATen/test/test_parallel.cpp", "aten/src/ATen/test/test_thread_pool_guard.cpp", "aten/src/ATen/test/thread_init_test.cpp", "aten/src/ATen/test/type_test.cpp", "aten/src/ATen/test/undefined_tensor_test.cpp", "aten/src/ATen/test/variant_test.cpp", "aten/src/ATen/test/vec256_test_all_types.cpp", "aten/src/ATen/test/vitals.cpp", "aten/src/ATen/test/vmap_test.cpp", "aten/src/ATen/test/vulkan_test.cpp", "aten/src/ATen/test/weakref_test.cpp", "aten/src/ATen/test/wrapdim_test.cpp", "aten/src/ATen/test/xla_tensor_test.cpp", "aten/src/ATen/vulkan/Context.cpp", "aten/src/TH/THAllocator.cpp", "aten/src/TH/THBlas.cpp", "aten/src/TH/THGeneral.cpp", "aten/src/TH/THLapack.cpp", "aten/src/TH/THStorageFunctions.cpp", "aten/src/TH/THTensor.cpp", "aten/src/TH/THTensorEvenMoreMath.cpp", "aten/src/TH/THTensorLapack.cpp", "aten/src/TH/THTensorMath.cpp", "aten/src/TH/THTensorMoreMath.cpp", "benchmarks/cpp/convolution.cpp", "c10/core/GradMode.cpp", "c10/core/TensorImpl.cpp", "c10/test/util/C++17_test.cpp", "c10/util/intrusive_ptr.h", "caffe2/core/blob_serialization.cc", "caffe2/core/blob_test.cc", "caffe2/core/common.cc", "caffe2/core/common_test.cc", "caffe2/core/context.cc", "caffe2/core/context_base.cc", "caffe2/core/context_test.cc", "caffe2/core/db.cc", "caffe2/core/event.cc", "caffe2/core/event_test.cc", "caffe2/core/export_c10_op_to_caffe2.cc", "caffe2/core/export_c10_op_to_caffe2.h", "caffe2/core/graph.cc", "caffe2/core/graph_test.cc", "caffe2/core/init.cc", "caffe2/core/init_denormals.cc", "caffe2/core/init_intrinsics_check.cc", "caffe2/core/init_omp.cc", "caffe2/core/init_test.cc", "caffe2/core/int8_serialization.cc", "caffe2/core/memonger.cc", "caffe2/core/module_test.cc", "caffe2/core/net.cc", "caffe2/core/net_async_base.cc", "caffe2/core/net_async_scheduling.cc", "caffe2/core/net_async_task.cc", "caffe2/core/net_async_task_future.cc", "caffe2/core/net_async_tracing.cc", "caffe2/core/net_async_tracing.h", "caffe2/core/net_async_tracing_test.cc", "caffe2/core/net_dag_utils_test.cc", "caffe2/core/net_parallel.cc", "caffe2/core/net_simple.cc", "caffe2/core/net_simple_refcount.cc", "caffe2/core/net_simple_refcount_test.cc", "caffe2/core/net_test.cc", "caffe2/core/nomnigraph/Representations/NeuralNet.cc", "caffe2/core/nomnigraph/tests/AlgorithmsTest.cc", "caffe2/core/nomnigraph/tests/BinaryMatchImplTest.cc", "caffe2/core/nomnigraph/tests/GraphTest.cc", "caffe2/core/nomnigraph/tests/MatchTest.cc", "caffe2/core/nomnigraph/tests/NeuralNetTest.cc", "caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc", "caffe2/core/nomnigraph/tests/TarjansImplTest.cc", "caffe2/core/nomnigraph/tests/TopoSortTest.cc", "caffe2/core/observer_test.cc", "caffe2/core/operator.cc", "caffe2/core/operator_schema.cc", "caffe2/core/operator_schema_test.cc", "caffe2/core/operator_test.cc", "caffe2/core/parallel_net_test.cc", "caffe2/core/plan_executor.cc", "caffe2/core/plan_executor_test.cc", "caffe2/core/prof_dag_counters.cc", "caffe2/core/qtensor.h", "caffe2/core/qtensor_serialization.cc", "caffe2/core/serialization_test.cc", "caffe2/core/stats.cc", "caffe2/core/stats_test.cc", "caffe2/core/tensor.cc", "caffe2/core/test_utils.h", "caffe2/core/timer_test.cc", "caffe2/core/transform.cc", "caffe2/core/transform_test.cc", "caffe2/core/workspace.cc", "caffe2/core/workspace_test.cc", "caffe2/db/create_db_op.cc", "caffe2/db/protodb.cc", "caffe2/distributed/file_store_handler.cc", "caffe2/distributed/file_store_handler_op.cc", "caffe2/distributed/store_handler.cc", "caffe2/distributed/store_ops.cc", "caffe2/ideep/operators/adam_op.cc", "caffe2/ideep/operators/channel_shuffle_op.cc", "caffe2/ideep/operators/concat_split_op.cc", "caffe2/ideep/operators/conv_op.cc", "caffe2/ideep/operators/conv_transpose_op.cc", "caffe2/ideep/operators/dropout_op.cc", "caffe2/ideep/operators/elementwise_sum_op.cc", "caffe2/ideep/operators/expand_squeeze_dims_op.cc", "caffe2/ideep/operators/fully_connected_op.cc", "caffe2/ideep/operators/local_response_normalization_op.cc", "caffe2/ideep/operators/momentum_sgd_op.cc", "caffe2/ideep/operators/operator_fallback_ideep.cc", "caffe2/ideep/operators/order_switch_ops.cc", "caffe2/ideep/operators/pool_op.cc", "caffe2/ideep/operators/quantization/int8_add_op.cc", "caffe2/ideep/operators/quantization/int8_conv_op.cc", "caffe2/ideep/operators/quantization/int8_dequantize_op.cc", "caffe2/ideep/operators/quantization/int8_fully_connected_op.cc", "caffe2/ideep/operators/quantization/int8_given_tensor_fill_op.cc", "caffe2/ideep/operators/quantization/int8_pool_op.cc", "caffe2/ideep/operators/quantization/int8_quantize_op.cc", "caffe2/ideep/operators/quantization/int8_relu_op.cc", "caffe2/ideep/operators/queue_ops.cc", "caffe2/ideep/operators/relu_op.cc", "caffe2/ideep/operators/reshape_op.cc", "caffe2/ideep/operators/shape_op.cc", "caffe2/ideep/operators/sigmoid_op.cc", "caffe2/ideep/operators/spatial_batch_norm_op.cc", "caffe2/ideep/operators/transpose_op.cc", "caffe2/ideep/operators/utility_ops.cc", "caffe2/ideep/utils/ideep_register.cc", "caffe2/observers/time_observer.cc", "caffe2/observers/time_observer_test.cc", "caffe2/onnx/backend.cc", "caffe2/onnx/helper.cc", "caffe2/onnx/offline_tensor.cc", "caffe2/onnx/onnx_exporter.cc", "caffe2/onnx/ssa_test.cc", "caffe2/onnx/torch_ops/defs.cc", "caffe2/onnx/torch_ops/schema.cc", "caffe2/operators/abs_op.cc", "caffe2/operators/accumulate_op.cc", "caffe2/operators/accuracy_op.cc", "caffe2/operators/acos_op.cc", "caffe2/operators/affine_channel_op.cc", "caffe2/operators/alias_with_name.cc", "caffe2/operators/apmeter_op.cc", "caffe2/operators/arg_ops.cc", "caffe2/operators/asin_op.cc", "caffe2/operators/assert_op.cc", "caffe2/operators/async_net_barrier_op.cc", "caffe2/operators/atan_op.cc", "caffe2/operators/atomic_ops.cc", "caffe2/operators/batch_box_cox_op.cc", "caffe2/operators/batch_bucketize_op.cc", "caffe2/operators/batch_gather_ops.cc", "caffe2/operators/batch_matmul_op.cc", "caffe2/operators/batch_matmul_op_test.cc", "caffe2/operators/batch_moments_op.cc", "caffe2/operators/batch_permutation_op.cc", "caffe2/operators/batch_sparse_to_dense_op.cc", "caffe2/operators/bbox_transform_op.cc", "caffe2/operators/bisect_percentile_op.cc", "caffe2/operators/boolean_mask_ops.cc", "caffe2/operators/boolean_unmask_ops.cc", "caffe2/operators/boolean_unmask_ops_test.cc", "caffe2/operators/box_with_nms_limit_op.cc", "caffe2/operators/bucketize_op.cc", "caffe2/operators/byte_weight_dequant_op.cc", "caffe2/operators/cast_op.cc", "caffe2/operators/cbrt_op.cc", "caffe2/operators/cc_bmm_bg_op.cc", "caffe2/operators/ceil_op.cc", "caffe2/operators/channel_backprop_stats_op.cc", "caffe2/operators/channel_shuffle_op.cc", "caffe2/operators/channel_stats_op.cc", "caffe2/operators/clip_op.cc", "caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc", "caffe2/operators/communicator_op.cc", "caffe2/operators/concat_split_op.cc", "caffe2/operators/conditional_op.cc", "caffe2/operators/conv_gradient_op.cc", "caffe2/operators/conv_op.cc", "caffe2/operators/conv_op_eigen.cc", "caffe2/operators/conv_op_impl.h", "caffe2/operators/conv_op_shared.cc", "caffe2/operators/conv_pool_op_base.h", "caffe2/operators/conv_transpose_gradient_op.cc", "caffe2/operators/conv_transpose_op.cc", "caffe2/operators/conv_transpose_op_mobile_test.cc", "caffe2/operators/conv_transpose_unpool_op_base.h", "caffe2/operators/copy_op.cc", "caffe2/operators/copy_rows_to_tensor_op.cc", "caffe2/operators/copy_rows_to_tensor_op.h", "caffe2/operators/cos_op.cc", "caffe2/operators/cosh_op.cc", "caffe2/operators/cosine_embedding_criterion_op.cc", "caffe2/operators/counter_ops.cc", "caffe2/operators/create_scope_op.cc", "caffe2/operators/crf_viterbi_op.cc", "caffe2/operators/cross_entropy_op.cc", "caffe2/operators/ctc_beam_search_decoder_op.cc", "caffe2/operators/ctc_greedy_decoder_op.cc", "caffe2/operators/cube_op.cc", "caffe2/operators/data_couple.cc", "caffe2/operators/dataset_ops.cc", "caffe2/operators/deform_conv_gradient_op.cc", "caffe2/operators/deform_conv_op.cc", "caffe2/operators/dense_vector_to_id_list_op.cc", "caffe2/operators/distance_op.cc", "caffe2/operators/do_op.cc", "caffe2/operators/do_op.h", "caffe2/operators/dropout_op.cc", "caffe2/operators/elementwise_add_gradient_op.cc", "caffe2/operators/elementwise_add_op.cc", "caffe2/operators/elementwise_div_gradient_op.cc", "caffe2/operators/elementwise_div_op.cc", "caffe2/operators/elementwise_linear_op.cc", "caffe2/operators/elementwise_logical_ops.cc", "caffe2/operators/elementwise_mul_gradient_op.cc", "caffe2/operators/elementwise_mul_op.cc", "caffe2/operators/elementwise_op_test.cc", "caffe2/operators/elementwise_ops.cc", "caffe2/operators/elementwise_ops_schema.cc", "caffe2/operators/elementwise_ops_utils.cc", "caffe2/operators/elementwise_sub_gradient_op.cc", "caffe2/operators/elementwise_sub_op.cc", "caffe2/operators/elementwise_sum_op.cc", "caffe2/operators/elu_op.cc", "caffe2/operators/enforce_finite_op.cc", "caffe2/operators/ensure_clipped_op.cc", "caffe2/operators/ensure_cpu_output_op.cc", "caffe2/operators/erf_op.cc", "caffe2/operators/exp_op.cc", "caffe2/operators/expand_op.cc", "caffe2/operators/expand_squeeze_dims_op.cc", "caffe2/operators/expand_squeeze_dims_op.h", "caffe2/operators/feature_maps_ops.cc", "caffe2/operators/feed_blob_op.cc", "caffe2/operators/filler_op.cc", "caffe2/operators/find_duplicate_elements_op.cc", "caffe2/operators/find_op.cc", "caffe2/operators/flatten_op.cc", "caffe2/operators/flexible_top_k.cc", "caffe2/operators/floor_op.cc", "caffe2/operators/free_op.cc", "caffe2/operators/fully_connected_op.cc", "caffe2/operators/fused_rowwise_8bit_conversion_ops.cc", "caffe2/operators/fused_rowwise_8bit_conversion_ops.h", "caffe2/operators/fused_rowwise_nbit_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbit_conversion_ops.h", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.h", "caffe2/operators/fused_rowwise_random_quantization_ops.cc", "caffe2/operators/gather_fused_8bit_rowwise_op.cc", "caffe2/operators/gather_op.cc", "caffe2/operators/gather_ranges_to_dense_op.cc", "caffe2/operators/gelu_op.cc", "caffe2/operators/generate_proposals_op.cc", "caffe2/operators/generate_proposals_op_test.cc", "caffe2/operators/generate_proposals_op_util_boxes_test.cc", "caffe2/operators/generate_proposals_op_util_nms.h", "caffe2/operators/generate_proposals_op_util_nms_test.cc", "caffe2/operators/given_tensor_byte_string_to_uint8_fill_op.cc", "caffe2/operators/given_tensor_byte_string_to_uint8_fill_op.h", "caffe2/operators/given_tensor_fill_op.cc", "caffe2/operators/given_tensor_fill_op.h", "caffe2/operators/glu_op.cc", "caffe2/operators/group_norm_op.cc", "caffe2/operators/gru_unit_op.cc", "caffe2/operators/h_softmax_op.cc", "caffe2/operators/half_float_ops.cc", "caffe2/operators/half_float_ops_test.cc", "caffe2/operators/hard_sigmoid_op.cc", "caffe2/operators/heatmap_max_keypoint_op.cc", "caffe2/operators/histogram_op.cc", "caffe2/operators/histogram_op.h", "caffe2/operators/if_op.cc", "caffe2/operators/im2col_op.cc", "caffe2/operators/index_hash_ops.cc", "caffe2/operators/index_hash_ops.h", "caffe2/operators/index_ops.cc", "caffe2/operators/index_ops.h", "caffe2/operators/inference_lstm_op.cc", "caffe2/operators/instance_norm_gradient_op.cc", "caffe2/operators/instance_norm_op.cc", "caffe2/operators/integral_image_op.cc", "caffe2/operators/is_empty_op.cc", "caffe2/operators/jsd_op.cc", "caffe2/operators/key_split_ops.cc", "caffe2/operators/last_n_window_collector.cc", "caffe2/operators/layer_norm_op.cc", "caffe2/operators/layer_norm_op.h", "caffe2/operators/leaky_relu_op.cc", "caffe2/operators/length_split_op.cc", "caffe2/operators/lengths_pad_op.cc", "caffe2/operators/lengths_reducer_fused_8bit_rowwise_ops.cc", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.cc", "caffe2/operators/lengths_reducer_ops.cc", "caffe2/operators/lengths_reducer_rowwise_8bit_ops.cc", "caffe2/operators/lengths_tile_op.cc", "caffe2/operators/lengths_top_k_op.cc", "caffe2/operators/listwise_l2r_op.cc", "caffe2/operators/load_save_op.cc", "caffe2/operators/load_save_op.h", "caffe2/operators/local_response_normalization_op.cc", "caffe2/operators/locally_connected_op.cc", "caffe2/operators/log1p_op.cc", "caffe2/operators/log_op.cc", "caffe2/operators/logit_op.cc", "caffe2/operators/loss_op.cc", "caffe2/operators/lp_pool_op.cc", "caffe2/operators/lpnorm_op.cc", "caffe2/operators/lstm_unit_op.cc", "caffe2/operators/map_ops.cc", "caffe2/operators/margin_ranking_criterion_op.cc", "caffe2/operators/matmul_op.cc", "caffe2/operators/mean_op.cc", "caffe2/operators/merge_id_lists_op.cc", "caffe2/operators/merge_id_lists_op.h", "caffe2/operators/minmax_gradient_ops.cc", "caffe2/operators/minmax_ops.cc", "caffe2/operators/mish_op.cc", "caffe2/operators/mod_op.cc", "caffe2/operators/moments_op.cc", "caffe2/operators/multi_class_accuracy_op.cc", "caffe2/operators/negate_gradient_op.cc", "caffe2/operators/negative_op.cc", "caffe2/operators/ngram_ops.cc", "caffe2/operators/norm_planar_yuv_op.cc", "caffe2/operators/normalize_l1_op.cc", "caffe2/operators/normalize_op.cc", "caffe2/operators/numpy_tile_op.cc", "caffe2/operators/numpy_tile_op.h", "caffe2/operators/one_hot_ops.cc", "caffe2/operators/onnx_while_op.cc", "caffe2/operators/onnx_while_op.h", "caffe2/operators/order_switch_ops.cc", "caffe2/operators/pack_rnn_sequence_op.cc", "caffe2/operators/pack_segments.cc", "caffe2/operators/pad_op.cc", "caffe2/operators/partition_ops.cc", "caffe2/operators/percentile_op.cc", "caffe2/operators/perplexity_op.cc", "caffe2/operators/piecewise_linear_transform_op.cc", "caffe2/operators/pool_gradient_op.cc", "caffe2/operators/pool_op.cc", "caffe2/operators/pow_op.cc", "caffe2/operators/prelu_op.cc", "caffe2/operators/prepend_dim_op.cc", "caffe2/operators/prepend_dim_op.h", "caffe2/operators/quant_decode_op.cc", "caffe2/operators/quantile_op.cc", "caffe2/operators/quantized/int8_add_op.cc", "caffe2/operators/quantized/int8_average_pool_op.cc", "caffe2/operators/quantized/int8_channel_shuffle_op.cc", "caffe2/operators/quantized/int8_concat_op.cc", "caffe2/operators/quantized/int8_conv_op.cc", "caffe2/operators/quantized/int8_conv_op_relu.cc", "caffe2/operators/quantized/int8_conv_transpose_op.cc", "caffe2/operators/quantized/int8_dequantize_op.cc", "caffe2/operators/quantized/int8_fc_op.cc", "caffe2/operators/quantized/int8_flatten_op.cc", "caffe2/operators/quantized/int8_given_tensor_fill_op.cc", "caffe2/operators/quantized/int8_leaky_relu_op.cc", "caffe2/operators/quantized/int8_max_pool_op.cc", "caffe2/operators/quantized/int8_quantize_op.cc", "caffe2/operators/quantized/int8_relu_op.cc", "caffe2/operators/quantized/int8_reshape_op.cc", "caffe2/operators/quantized/int8_resize_nearest_op.cc", "caffe2/operators/quantized/int8_roi_align_op.cc", "caffe2/operators/quantized/int8_roi_align_op_test.cc", "caffe2/operators/quantized/int8_sigmoid_op.cc", "caffe2/operators/quantized/int8_slice_op.cc", "caffe2/operators/quantized/int8_softmax_op.cc", "caffe2/operators/quantized/int8_test.cc", "caffe2/operators/quantized/int8_transpose_op.cc", "caffe2/operators/rank_loss_op.cc", "caffe2/operators/reciprocal_gradient_op.cc", "caffe2/operators/reciprocal_op.cc", "caffe2/operators/reduce_front_back_max_ops.cc", "caffe2/operators/reduce_front_back_mean_ops.cc", "caffe2/operators/reduce_front_back_sum_ops.cc", "caffe2/operators/reduce_ops.cc", "caffe2/operators/reduction_ops.cc", "caffe2/operators/relu_n_op.cc", "caffe2/operators/relu_op.cc", "caffe2/operators/remove_data_blocks_op.cc", "caffe2/operators/replace_nan_op.cc", "caffe2/operators/reservoir_sampling.cc", "caffe2/operators/reshape_op.cc", "caffe2/operators/reshape_op.h", "caffe2/operators/resize_3d_op.cc", "caffe2/operators/resize_op.cc", "caffe2/operators/reverse_packed_segs_op.cc", "caffe2/operators/rmac_regions_op.cc", "caffe2/operators/rms_norm_op.cc", "caffe2/operators/rnn/recurrent_network_blob_fetcher_op.cc", "caffe2/operators/rnn/recurrent_network_blob_fetcher_op.h", "caffe2/operators/rnn/recurrent_network_executor.cc", "caffe2/operators/rnn/recurrent_network_executor_incl.h", "caffe2/operators/rnn/recurrent_network_op.cc", "caffe2/operators/rnn/recurrent_network_op.h", "caffe2/operators/roi_align_gradient_op.cc", "caffe2/operators/roi_align_op.cc", "caffe2/operators/roi_align_rotated_gradient_op.cc", "caffe2/operators/roi_align_rotated_op.cc", "caffe2/operators/roi_pool_op.cc", "caffe2/operators/rowmul_op.cc", "caffe2/operators/rsqrt_op.cc", "caffe2/operators/scale_blobs_op.cc", "caffe2/operators/scale_op.cc", "caffe2/operators/segment_reduction_op.cc", "caffe2/operators/segment_reduction_op.h", "caffe2/operators/self_binning_histogram_op.cc", "caffe2/operators/selu_op.cc", "caffe2/operators/sequence_ops.cc", "caffe2/operators/shape_op.cc", "caffe2/operators/sigmoid_gradient_op.cc", "caffe2/operators/sigmoid_op.cc", "caffe2/operators/sin_op.cc", "caffe2/operators/sinh_op.cc", "caffe2/operators/sinusoid_position_encoding_op.cc", "caffe2/operators/slice_op.cc", "caffe2/operators/softmax_op.cc", "caffe2/operators/softmax_with_loss_op.cc", "caffe2/operators/softplus_op.cc", "caffe2/operators/softsign_op.cc", "caffe2/operators/space_batch_op.cc", "caffe2/operators/sparse_dropout_with_replacement_op.cc", "caffe2/operators/sparse_lp_regularizer_op.cc", "caffe2/operators/sparse_normalize_op.cc", "caffe2/operators/sparse_to_dense_mask_op.cc", "caffe2/operators/sparse_to_dense_mask_op.h", "caffe2/operators/sparse_to_dense_op.cc", "caffe2/operators/spatial_batch_norm_gradient_op.cc", "caffe2/operators/spatial_batch_norm_op.cc", "caffe2/operators/spatial_softmax_with_loss_op.cc", "caffe2/operators/sqr_op.cc", "caffe2/operators/sqrt_op.cc", "caffe2/operators/square_root_divide_op.cc", "caffe2/operators/square_root_divide_op.h", "caffe2/operators/stats_ops.cc", "caffe2/operators/stats_put_ops.cc", "caffe2/operators/stats_put_ops.h", "caffe2/operators/stop_gradient.cc", "caffe2/operators/string_ops.cc", "caffe2/operators/string_ops_test.cc", "caffe2/operators/stump_func_op.cc", "caffe2/operators/stylizer_ops.cc", "caffe2/operators/summarize_op.cc", "caffe2/operators/swish_op.cc", "caffe2/operators/tan_op.cc", "caffe2/operators/tanh_gradient_op.cc", "caffe2/operators/tanh_op.cc", "caffe2/operators/tensor_protos_db_input.cc", "caffe2/operators/text_file_reader.cc", "caffe2/operators/text_file_reader_utils.cc", "caffe2/operators/text_file_reader_utils_test.cc", "caffe2/operators/thresholded_relu_op.cc", "caffe2/operators/tile_op.cc", "caffe2/operators/top_k.cc", "caffe2/operators/transpose_op.cc", "caffe2/operators/transpose_op.h", "caffe2/operators/tt_linear_op.cc", "caffe2/operators/tt_linear_op.h", "caffe2/operators/unique_ops.cc", "caffe2/operators/unsafe_coalesce.cc", "caffe2/operators/upsample_op.cc", "caffe2/operators/utility_ops.cc", "caffe2/operators/utility_ops.h", "caffe2/operators/utility_ops_test.cc", "caffe2/operators/variable_length_sequence_padding.cc", "caffe2/operators/weighted_multi_sampling_op.cc", "caffe2/operators/weighted_sample_op.cc", "caffe2/operators/while_op.cc", "caffe2/operators/workspace_ops.cc", "caffe2/operators/zero_gradient_op.cc", "caffe2/opt/backend_cutting.cc", "caffe2/opt/backend_cutting_test.cc", "caffe2/opt/backend_transformer_base.cc", "caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/converter.cc", "caffe2/opt/converter_nomigraph_test.cc", "caffe2/opt/dead_code_elim.cc", "caffe2/opt/dead_code_elim_test.cc", "caffe2/opt/device.cc", "caffe2/opt/device_test.cc", "caffe2/opt/distributed_converter.cc", "caffe2/opt/distributed_test.cc", "caffe2/opt/fakefp16_transform.cc", "caffe2/opt/fusion.cc", "caffe2/opt/glow_net_transform.cc", "caffe2/opt/mobile.cc", "caffe2/opt/mobile_test.cc", "caffe2/opt/onnxifi_op.cc", "caffe2/opt/onnxifi_op.h", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/optimize_ideep.cc", "caffe2/opt/passes.cc", "caffe2/opt/shape_info.cc", "caffe2/opt/split_slss_test.cc", "caffe2/opt/tvm_transformer.cc", "caffe2/perfkernels/embedding_lookup.cc", "caffe2/perfkernels/embedding_lookup_avx2.cc", "caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc", "caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc", "caffe2/perfkernels/embedding_lookup_idx_avx2.cc", "caffe2/perfkernels/fused_8bit_rowwise_embedding_lookup.cc", "caffe2/perfkernels/fused_nbit_rowwise_conversion.cc", "caffe2/perfkernels/math_cpu_avx2.cc", "caffe2/perfkernels/math_cpu_base.cc", "caffe2/perfkernels/typed_axpy.cc", "caffe2/perfkernels/typed_axpy_avx.cc", "caffe2/perfkernels/typed_axpy_avx2.cc", "caffe2/predictor/emulator/data_filler.cc", "caffe2/predictor/emulator/data_filler_test.cc", "caffe2/predictor/predictor.cc", "caffe2/predictor/predictor_test.cc", "caffe2/predictor/predictor_utils.cc", "caffe2/python/pybind_state.cc", "caffe2/python/pybind_state_dlpack.cc", "caffe2/python/pybind_state_ideep.cc", "caffe2/python/pybind_state_int8.cc", "caffe2/python/pybind_state_nomni.cc", "caffe2/python/pybind_state_registry.cc", "caffe2/quantization/server/activation_distribution_observer.cc", "caffe2/quantization/server/batch_matmul_dnnlowp_op.cc", "caffe2/quantization/server/caffe2_dnnlowp_utils.cc", "caffe2/quantization/server/channel_shuffle_dnnlowp_op.cc", "caffe2/quantization/server/concat_dnnlowp_op.cc", "caffe2/quantization/server/conv_dnnlowp_acc16_op.cc", "caffe2/quantization/server/conv_dnnlowp_op.cc", "caffe2/quantization/server/conv_relu_op.cc", "caffe2/quantization/server/dequantize_dnnlowp_op.cc", "caffe2/quantization/server/dnnlowp.cc", "caffe2/quantization/server/dynamic_histogram.cc", "caffe2/quantization/server/elementwise_add_dnnlowp_op.cc", "caffe2/quantization/server/elementwise_linear_dnnlowp_op.cc", "caffe2/quantization/server/elementwise_mul_dnnlowp_op.cc", "caffe2/quantization/server/elementwise_sum_dnnlowp_op.cc", "caffe2/quantization/server/elementwise_sum_dnnlowp_op_avx2.cc", "caffe2/quantization/server/elementwise_sum_relu_op.cc", "caffe2/quantization/server/fb_fc_packed_op.cc", "caffe2/quantization/server/fbgemm_fp16_pack_op.cc", "caffe2/quantization/server/fbgemm_pack_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_acc16_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_op.cc", "caffe2/quantization/server/fully_connected_fake_lowp_op.cc", "caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc", "caffe2/quantization/server/group_norm_dnnlowp_op.cc", "caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc", "caffe2/quantization/server/int8_gen_quant_params.cc", "caffe2/quantization/server/kl_minimization.cc", "caffe2/quantization/server/lstm_unit_dnnlowp_op.cc", "caffe2/quantization/server/mmio.h", "caffe2/quantization/server/norm_minimization.cc", "caffe2/quantization/server/norm_minimization_avx2.cc", "caffe2/quantization/server/p99.cc", "caffe2/quantization/server/pool_dnnlowp_op.cc", "caffe2/quantization/server/pool_dnnlowp_op_avx2.cc", "caffe2/quantization/server/quantize_dnnlowp_op.cc", "caffe2/quantization/server/relu_dnnlowp_op.cc", "caffe2/quantization/server/sigmoid.cc", "caffe2/quantization/server/sigmoid_dnnlowp_op.cc", "caffe2/quantization/server/spatial_batch_norm_dnnlowp_op.cc", "caffe2/quantization/server/spatial_batch_norm_dnnlowp_op_avx2.cc", "caffe2/quantization/server/tanh.cc", "caffe2/quantization/server/tanh_dnnlowp_op.cc", "caffe2/quantization/server/transpose.cc", "caffe2/quantization/server/utility_dnnlowp_ops.cc", "caffe2/queue/blobs_queue.cc", "caffe2/queue/blobs_queue_db.cc", "caffe2/queue/queue_ops.cc", "caffe2/queue/queue_ops.h", "caffe2/queue/rebatching_queue.cc", "caffe2/queue/rebatching_queue_ops.cc", "caffe2/serialize/file_adapter.cc", "caffe2/serialize/inline_container.cc", "caffe2/serialize/inline_container_test.cc", "caffe2/serialize/istream_adapter.cc", "caffe2/serialize/read_adapter_interface.cc", "caffe2/sgd/adadelta_op.cc", "caffe2/sgd/adagrad_fused.cc", "caffe2/sgd/adagrad_op.cc", "caffe2/sgd/adam_op.cc", "caffe2/sgd/clip_tensor_op.cc", "caffe2/sgd/decay_adagrad_op.cc", "caffe2/sgd/ftrl_op.cc", "caffe2/sgd/gftrl_op.cc", "caffe2/sgd/iter_op.cc", "caffe2/sgd/iter_op.h", "caffe2/sgd/lars_op.cc", "caffe2/sgd/learning_rate_adaption_op.cc", "caffe2/sgd/learning_rate_op.cc", "caffe2/sgd/math_lp.cc", "caffe2/sgd/momentum_sgd_op.cc", "caffe2/sgd/rmsprop_op.cc", "caffe2/sgd/rowwise_adagrad_fused.cc", "caffe2/sgd/rowwise_counter.cc", "caffe2/sgd/storm_op.cc", "caffe2/sgd/weight_scale_op.cc", "caffe2/sgd/wngrad_op.cc", "caffe2/sgd/yellowfin_op.cc", "caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc", "caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc", "caffe2/share/contrib/nnpack/conv_op.cc", "caffe2/share/contrib/nnpack/nnpack_test.cc", "caffe2/transforms/common_subexpression_elimination.cc", "caffe2/transforms/common_subexpression_elimination_test.cc", "caffe2/transforms/conv_to_nnpack_transform.cc", "caffe2/transforms/conv_to_nnpack_transform_test.cc", "caffe2/transforms/pattern_net_transform.cc", "caffe2/transforms/pattern_net_transform_test.cc", "caffe2/utils/bench_utils.cc", "caffe2/utils/cast_test.cc", "caffe2/utils/cpuid.cc", "caffe2/utils/cpuid_test.cc", "caffe2/utils/filler.h", "caffe2/utils/fixed_divisor_test.cc", "caffe2/utils/math/elementwise.cc", "caffe2/utils/math/reduce.cc", "caffe2/utils/math/utils.cc", "caffe2/utils/math_cpu.cc", "caffe2/utils/math_test.cc", "caffe2/utils/murmur_hash3.cc", "caffe2/utils/proto_utils.cc", "caffe2/utils/proto_utils_test.cc", "caffe2/utils/signal_handler.cc", "caffe2/utils/simple_queue_test.cc", "caffe2/utils/smart_tensor_printer.cc", "caffe2/utils/string_utils.cc", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/pthreadpool-cpp.cc", "caffe2/utils/threadpool/thread_pool_guard.cpp", "modules/detectron/group_spatial_softmax_op.cc", "modules/detectron/ps_roi_pool_op.cc", "modules/detectron/roi_pool_f_op.cc", "modules/detectron/sample_as_op.cc", "modules/detectron/select_smooth_l1_loss_op.cc", "modules/detectron/sigmoid_cross_entropy_loss_op.cc", "modules/detectron/sigmoid_focal_loss_op.cc", "modules/detectron/smooth_l1_loss_op.cc", "modules/detectron/softmax_focal_loss_op.cc", "modules/detectron/spatial_narrow_as_op.cc", "modules/detectron/upsample_nearest_op.cc", "modules/module_test/module_test_dynamic.cc", "modules/observers/net_observer_reporter_print.cc", "modules/observers/observer_config.cc", "modules/observers/perf_observer.cc", "test/cpp/api/any.cpp", "test/cpp/api/autograd.cpp", "test/cpp/api/dataloader.cpp", "test/cpp/api/enum.cpp", "test/cpp/api/expanding-array.cpp", "test/cpp/api/fft.cpp", "test/cpp/api/functional.cpp", "test/cpp/api/grad_mode.cpp", "test/cpp/api/inference_mode.cpp", "test/cpp/api/init.cpp", "test/cpp/api/integration.cpp", "test/cpp/api/jit.cpp", "test/cpp/api/memory.cpp", "test/cpp/api/misc.cpp", "test/cpp/api/module.cpp", "test/cpp/api/moduledict.cpp", "test/cpp/api/modulelist.cpp", "test/cpp/api/modules.cpp", "test/cpp/api/namespace.cpp", "test/cpp/api/nn_utils.cpp", "test/cpp/api/optim.cpp", "test/cpp/api/ordered_dict.cpp", "test/cpp/api/parallel_benchmark.cpp", "test/cpp/api/parameterdict.cpp", "test/cpp/api/parameterlist.cpp", "test/cpp/api/rnn.cpp", "test/cpp/api/sequential.cpp", "test/cpp/api/serialize.cpp", "test/cpp/api/special.cpp", "test/cpp/api/static.cpp", "test/cpp/api/support.cpp", "test/cpp/api/tensor.cpp", "test/cpp/api/tensor_cuda.cpp", "test/cpp/api/tensor_indexing.cpp", "test/cpp/api/tensor_options.cpp", "test/cpp/api/tensor_options_cuda.cpp", "test/cpp/api/torch_include.cpp", "test/cpp/api/transformer.cpp", "test/cpp/jit/test_alias_analysis.cpp", "test/cpp/jit/test_argument_spec.cpp", "test/cpp/jit/test_autodiff.cpp", "test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_compiler_lib.cpp", "test/cpp/jit/test_backend_compiler_preprocess.cpp", "test/cpp/jit/test_backend_lib.cpp", "test/cpp/jit/test_class_import.cpp", "test/cpp/jit/test_class_parser.cpp", "test/cpp/jit/test_class_type.cpp", "test/cpp/jit/test_cleanup_passes.cpp", "test/cpp/jit/test_code_template.cpp", "test/cpp/jit/test_constant_pooling.cpp", "test/cpp/jit/test_create_autodiff_subgraphs.cpp", "test/cpp/jit/test_custom_class.cpp", "test/cpp/jit/test_custom_class_registrations.cpp", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_dce.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_graph_executor.cpp", "test/cpp/jit/test_inliner.cpp", "test/cpp/jit/test_interface.cpp", "test/cpp/jit/test_interpreter.cpp", "test/cpp/jit/test_ir.cpp", "test/cpp/jit/test_irparser.cpp", "test/cpp/jit/test_jit_type.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/test_memory_dag.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_mobile_type_parser.cpp", "test/cpp/jit/test_module_api.cpp", "test/cpp/jit/test_peephole_optimize.cpp", "test/cpp/jit/test_qualified_name.cpp", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/test_schema_matching.cpp", "test/cpp/jit/test_subgraph_matcher.cpp", "test/cpp/jit/test_subgraph_rewriter.cpp", "test/cpp/jit/test_subgraph_utils.cpp", "test/cpp/jit/test_utils.cpp", "test/cpp/jit/torch_python_test.cpp", "test/cpp/tensorexpr/padded_buffer.cpp", "test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_conv.cpp", "test/cpp/tensorexpr/test_cpp_codegen.cpp", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_external_calls.cpp", "test/cpp/tensorexpr/test_ir_printer.cpp", "test/cpp/tensorexpr/test_ir_verifier.cpp", "test/cpp/tensorexpr/test_kernel.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_memdependency.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/test_te_fuser_pass.cpp", "test/cpp/tensorexpr/test_train.cpp", "test/cpp/tensorexpr/test_train_impl.cpp", "test/cpp/tensorexpr/test_type.cpp", "test/cpp/tensorexpr/tutorial.cpp", "third_party/miniz-2.0.8/miniz.c", "torch/csrc/DataLoader.cpp", "torch/csrc/DataLoader.h", "torch/csrc/Device.cpp", "torch/csrc/Dtype.cpp", "torch/csrc/DynamicTypes.cpp", "torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h", "torch/csrc/Generator.cpp", "torch/csrc/Layout.cpp", "torch/csrc/MemoryFormat.cpp", "torch/csrc/Module.cpp", "torch/csrc/QScheme.cpp", "torch/csrc/Size.cpp", "torch/csrc/Size.h", "torch/csrc/Storage.cpp", "torch/csrc/Stream.cpp", "torch/csrc/TypeInfo.cpp", "torch/csrc/TypeInfo.h", "torch/csrc/api/include/torch/data/dataloader.h", "torch/csrc/api/include/torch/data/dataloader/base.h", "torch/csrc/api/include/torch/data/dataloader/stateful.h", "torch/csrc/api/include/torch/data/dataloader/stateless.h", "torch/csrc/api/include/torch/data/datasets/chunk.h", "torch/csrc/api/include/torch/data/datasets/map.h", "torch/csrc/api/include/torch/data/datasets/mnist.h", "torch/csrc/api/include/torch/data/samplers/distributed.h", "torch/csrc/api/include/torch/data/transforms/tensor.h", "torch/csrc/api/include/torch/data/worker_exception.h", "torch/csrc/api/include/torch/expanding_array.h", "torch/csrc/api/include/torch/linalg.h", "torch/csrc/api/include/torch/nn/cloneable.h", "torch/csrc/api/include/torch/nn/functional/activation.h", "torch/csrc/api/include/torch/nn/functional/distance.h", "torch/csrc/api/include/torch/nn/functional/embedding.h", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/csrc/api/include/torch/nn/functional/padding.h", "torch/csrc/api/include/torch/nn/functional/upsampling.h", "torch/csrc/api/include/torch/nn/functional/vision.h", "torch/csrc/api/include/torch/nn/init.h", "torch/csrc/api/include/torch/nn/module.h", "torch/csrc/api/include/torch/nn/modules/activation.h", "torch/csrc/api/include/torch/nn/modules/adaptive.h", "torch/csrc/api/include/torch/nn/modules/batchnorm.h", "torch/csrc/api/include/torch/nn/modules/container/any_value.h", "torch/csrc/api/include/torch/nn/modules/container/functional.h", "torch/csrc/api/include/torch/nn/modules/container/moduledict.h", "torch/csrc/api/include/torch/nn/modules/container/modulelist.h", "torch/csrc/api/include/torch/nn/modules/container/parameterdict.h", "torch/csrc/api/include/torch/nn/modules/container/parameterlist.h", "torch/csrc/api/include/torch/nn/modules/container/sequential.h", "torch/csrc/api/include/torch/nn/modules/conv.h", "torch/csrc/api/include/torch/nn/modules/distance.h", "torch/csrc/api/include/torch/nn/modules/dropout.h", "torch/csrc/api/include/torch/nn/modules/embedding.h", "torch/csrc/api/include/torch/nn/modules/fold.h", "torch/csrc/api/include/torch/nn/modules/instancenorm.h", "torch/csrc/api/include/torch/nn/modules/linear.h", "torch/csrc/api/include/torch/nn/modules/loss.h", "torch/csrc/api/include/torch/nn/modules/normalization.h", "torch/csrc/api/include/torch/nn/modules/padding.h", "torch/csrc/api/include/torch/nn/modules/pixelshuffle.h", "torch/csrc/api/include/torch/nn/modules/pooling.h", "torch/csrc/api/include/torch/nn/modules/rnn.h", "torch/csrc/api/include/torch/nn/modules/transformer.h", "torch/csrc/api/include/torch/nn/modules/transformercoder.h", "torch/csrc/api/include/torch/nn/modules/transformerlayer.h", "torch/csrc/api/include/torch/nn/modules/upsampling.h", "torch/csrc/api/include/torch/nn/options/activation.h", "torch/csrc/api/include/torch/nn/options/adaptive.h", "torch/csrc/api/include/torch/nn/options/batchnorm.h", "torch/csrc/api/include/torch/nn/options/distance.h", "torch/csrc/api/include/torch/nn/options/dropout.h", "torch/csrc/api/include/torch/nn/options/embedding.h", "torch/csrc/api/include/torch/nn/options/fold.h", "torch/csrc/api/include/torch/nn/options/instancenorm.h", "torch/csrc/api/include/torch/nn/options/loss.h", "torch/csrc/api/include/torch/nn/options/normalization.h", "torch/csrc/api/include/torch/nn/options/transformer.h", "torch/csrc/api/include/torch/nn/options/transformerlayer.h", "torch/csrc/api/include/torch/nn/pimpl.h", "torch/csrc/api/include/torch/nn/utils/clip_grad.h", "torch/csrc/api/include/torch/nn/utils/convert_parameters.h", "torch/csrc/api/include/torch/nn/utils/rnn.h", "torch/csrc/api/include/torch/optim/adagrad.h", "torch/csrc/api/include/torch/optim/adam.h", "torch/csrc/api/include/torch/optim/adamw.h", "torch/csrc/api/include/torch/optim/lbfgs.h", "torch/csrc/api/include/torch/optim/optimizer.h", "torch/csrc/api/include/torch/optim/rmsprop.h", "torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h", "torch/csrc/api/include/torch/optim/sgd.h", "torch/csrc/api/src/data/datasets/mnist.cpp", "torch/csrc/api/src/data/samplers/distributed.cpp", "torch/csrc/api/src/nn/init.cpp", "torch/csrc/api/src/nn/modules/_functions.cpp", "torch/csrc/api/src/nn/modules/activation.cpp", "torch/csrc/api/src/nn/modules/adaptive.cpp", "torch/csrc/api/src/nn/modules/conv.cpp", "torch/csrc/api/src/nn/modules/embedding.cpp", "torch/csrc/api/src/nn/modules/linear.cpp", "torch/csrc/api/src/nn/modules/loss.cpp", "torch/csrc/api/src/nn/modules/normalization.cpp", "torch/csrc/api/src/nn/modules/pooling.cpp", "torch/csrc/api/src/nn/modules/rnn.cpp", "torch/csrc/api/src/nn/modules/transformer.cpp", "torch/csrc/api/src/optim/adamw.cpp", "torch/csrc/api/src/optim/lbfgs.cpp", "torch/csrc/api/src/python/init.cpp", "torch/csrc/api/src/serialize/input-archive.cpp", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/autograd/TraceTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/anomaly_mode.cpp", "torch/csrc/autograd/anomaly_mode.h", "torch/csrc/autograd/cpp_hook.cpp", "torch/csrc/autograd/custom_function.h", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/function.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/functions/basic_ops.cpp", "torch/csrc/autograd/functions/basic_ops.h", "torch/csrc/autograd/functions/init.cpp", "torch/csrc/autograd/profiler_legacy.h", "torch/csrc/autograd/python_anomaly_mode.h", "torch/csrc/autograd/python_cpp_function.cpp", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_function.h", "torch/csrc/autograd/python_hook.cpp", "torch/csrc/autograd/python_legacy_variable.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/autograd/saved_variable.cpp", "torch/csrc/autograd/saved_variable.h", "torch/csrc/autograd/utils/wrap_outputs.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h", "torch/csrc/copy_utils.h", "torch/csrc/generic/Storage.cpp", "torch/csrc/generic/Storage.h", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp", "torch/csrc/jit/api/module.cpp", "torch/csrc/jit/api/module.h", "torch/csrc/jit/backends/backend.h", "torch/csrc/jit/backends/backend_interface.h", "torch/csrc/jit/codegen/fuser/codegen.cpp", "torch/csrc/jit/codegen/fuser/compiler.cpp", "torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp", "torch/csrc/jit/codegen/fuser/cpu/resource_strings.h", "torch/csrc/jit/codegen/fuser/cuda/resource_strings.h", "torch/csrc/jit/codegen/fuser/executor.cpp", "torch/csrc/jit/codegen/fuser/fallback.cpp", "torch/csrc/jit/codegen/fuser/fused_kernel.h", "torch/csrc/jit/codegen/fuser/interface.cpp", "torch/csrc/jit/codegen/fuser/partition_desc.h", "torch/csrc/jit/codegen/fuser/tensor_desc.h", "torch/csrc/jit/codegen/fuser/tensor_info.h", "torch/csrc/jit/frontend/builtin_functions.cpp", "torch/csrc/jit/frontend/code_template.h", "torch/csrc/jit/frontend/convert_to_ssa.cpp", "torch/csrc/jit/frontend/edit_distance.cpp", "torch/csrc/jit/frontend/error_report.cpp", "torch/csrc/jit/frontend/exit_transforms.cpp", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/frontend/lexer.cpp", "torch/csrc/jit/frontend/lexer.h", "torch/csrc/jit/frontend/mini_environment.h", "torch/csrc/jit/frontend/parse_string_literal.h", "torch/csrc/jit/frontend/parser.cpp", "torch/csrc/jit/frontend/parser_constants.h", "torch/csrc/jit/frontend/schema_type_parser.cpp", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/frontend/source_range.cpp", "torch/csrc/jit/frontend/strtod.cpp", "torch/csrc/jit/frontend/sugared_value.cpp", "torch/csrc/jit/frontend/sugared_value.h", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/jit/frontend/tree.h", "torch/csrc/jit/frontend/tree_views.h", "torch/csrc/jit/frontend/versioned_symbols.cpp", "torch/csrc/jit/ir/alias_analysis.cpp", "torch/csrc/jit/ir/ir.cpp", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/interpreter.h", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/mobile/observer.h", "torch/csrc/jit/mobile/optim/sgd.h", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/constant_propagation.cpp", "torch/csrc/jit/passes/create_autodiff_subgraphs.cpp", "torch/csrc/jit/passes/create_functional_graphs.cpp", "torch/csrc/jit/passes/dead_code_elimination.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/freeze_module.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/passes/inline_autodiff_subgraphs.h", "torch/csrc/jit/passes/liveness.h", "torch/csrc/jit/passes/lower_tuples.cpp", "torch/csrc/jit/passes/onnx/cast_all_constant_to_floating.cpp", "torch/csrc/jit/passes/onnx/constant_fold.cpp", "torch/csrc/jit/passes/onnx/constant_map.h", "torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp", "torch/csrc/jit/passes/onnx/helper.cpp", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/csrc/jit/passes/pass_manager.h", "torch/csrc/jit/passes/peephole.cpp", "torch/csrc/jit/passes/peephole_alias_sensitive.cpp", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/insert_observers.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/csrc/jit/passes/quantization/quantization_patterns.h", "torch/csrc/jit/passes/remove_mutation.cpp", "torch/csrc/jit/passes/requires_grad_analysis.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/passes/utils/check_alias_annotation.cpp", "torch/csrc/jit/python/pybind.h", "torch/csrc/jit/python/python_custom_class.cpp", "torch/csrc/jit/python/python_interpreter.cpp", "torch/csrc/jit/python/python_ir.cpp", "torch/csrc/jit/python/python_ir.h", "torch/csrc/jit/python/python_ivalue.h", "torch/csrc/jit/python/python_sugared_value.h", "torch/csrc/jit/python/python_tree_views.cpp", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/python/update_graph_executor_opt.cpp", "torch/csrc/jit/runtime/argument_spec.cpp", "torch/csrc/jit/runtime/autodiff.cpp", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/graph_executor_impl.h", "torch/csrc/jit/runtime/instruction.cpp", "torch/csrc/jit/runtime/instruction.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/logging.cpp", "torch/csrc/jit/runtime/logging.h", "torch/csrc/jit/runtime/print_handler.cpp", "torch/csrc/jit/runtime/register_c10_ops.cpp", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/jit/runtime/register_special_ops.cpp", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/impl.h", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/passes.cpp", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/csrc/jit/runtime/vararg_functions.cpp", "torch/csrc/jit/serialization/export.cpp", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/import_export_helpers.cpp", "torch/csrc/jit/serialization/import_legacy.cpp", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/pickler.h", "torch/csrc/jit/serialization/python_print.cpp", "torch/csrc/jit/serialization/unpickler.cpp", "torch/csrc/jit/serialization/unpickler.h", "torch/csrc/jit/tensorexpr/hash_provider.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/operators/conv2d.cpp", "torch/csrc/jit/tensorexpr/tensorexpr_init.cpp", "torch/csrc/jit/testing/file_check.cpp", "torch/csrc/jit/testing/hooks_for_testing.cpp", "torch/csrc/multiprocessing/init.cpp", "torch/csrc/onnx/onnx.h", "torch/csrc/python_dimname.cpp", "torch/csrc/serialization.cpp", "torch/csrc/tensor/python_tensor.cpp", "torch/csrc/utils.cpp", "torch/csrc/utils/byte_order.cpp", "torch/csrc/utils/cuda_lazy_init.cpp", "torch/csrc/utils/disable_torch_function.cpp", "torch/csrc/utils/invalid_arguments.cpp", "torch/csrc/utils/pycfunction_helpers.h", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/tensor_dtypes.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_qschemes.cpp", "torch/csrc/utils/throughput_benchmark-inl.h", "torch/csrc/utils/throughput_benchmark.cpp", "torch/csrc/utils/throughput_benchmark.h", "torch/lib/libshm/core.cpp", "torch/lib/libshm/manager.cpp"], "labels": ["Merged", "cla signed"]}, "28fc59d13d": {"title": "Add xnnpack hardswish op (#56714)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56714\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55800\n\nFor mobile use xnnpack implementation of hardswish\n\nTest Plan: buck test //xplat/caffe2:pt_xnnpack_test\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27712306\n\nfbshipit-source-id: c7f0b70482aeef2aaa1966e2c669f79ecd29caa7", "pr_number": "56714", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/xnnpack/Activation.cpp", "aten/src/ATen/native/xnnpack/Engine.h", "aten/src/ATen/test/xnnpack_test.cpp", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "fb-exported"]}, "aac2e68515": {"title": "Add inplace hardswish xnnpack op (#56715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56715\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55801\n\nRefactor to add inplace version of xnnpack hardswish op\n\nTest Plan: buck test //xplat/caffe2:pt_xnnpack_test\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27712305\n\nfbshipit-source-id: ed1dba22b026251f891fe7b88fbaa9a42985ef2c", "pr_number": "56715", "files_changed": ["aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/xnnpack/Activation.cpp", "aten/src/ATen/native/xnnpack/Engine.h", "aten/src/ATen/test/xnnpack_test.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "3483049d58": {"title": "Add xnnpack global average pool op (#55791)", "body": "Summary:\nAdaptive average pool with output size (1, 1) is a global average pool\nFor mobile use xnnpack to speed up that path\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55791\n\nTest Plan:\nbuck test //xplat/caffe2:pt_xnnpack_test\n\npytest test/test_xnnpack_integration.py::TestXNNPACKOps\nFixes #{issue number}\n\nReviewed By: kimishpatel\n\nDifferential Revision: D27711082\n\nPulled By: axitkhurana\n\nfbshipit-source-id: 8757042c4a31a60451d8ba5fb6bf8cfbaf0a8d10", "pr_number": "55791", "files_changed": ["aten/src/ATen/native/AdaptiveAveragePooling.cpp", "aten/src/ATen/native/xnnpack/AveragePooling.cpp", "aten/src/ATen/native/xnnpack/Engine.h", "aten/src/ATen/test/xnnpack_test.cpp", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed"]}, "78736a72a5": {"title": "Fix default dtype for randperm, triu/tril_indices inside TorchScript (#57105)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/56676\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57105\n\nReviewed By: ezyang\n\nDifferential Revision: D28060969\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 6b074418306377f5f906aafd121b614964972fc3", "pr_number": "57105", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/jit/test_tensor_creation_ops.py", "test/test_jit.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "73453f1de1": {"title": "Swap CUDA-10.2 and CUDA-11.1 master-only status (#57207)", "body": "Summary:\nCUDA-11.1 build and tests will now run on PR and master, but 10.2 will\nbe master only\n\nAlso, delete remaining CUDA-10.1 build\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57207\n\nReviewed By: ngimel\n\nDifferential Revision: D28077271\n\nPulled By: malfet\n\nfbshipit-source-id: 633945bf85091575efa34280e04a6b9d68a53138", "pr_number": "57207", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/config.yml"], "labels": ["Merged", "cla signed"]}, "4049732811": {"title": "Enable clang-tidy on master (#57213)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57213\n\nReviewed By: seemethere\n\nDifferential Revision: D28078846\n\nPulled By: malfet\n\nfbshipit-source-id: adffa292c9f5d75b5f4840f9129d0184763d96a6", "pr_number": "57213", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "Reverted", "cla signed"]}, "2dc3dc2324": {"title": "Enhance error message for Future.setErrorIfNeeded. (#56631)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56631\n\n`setErrorIfNeeded` did not mention whether the future was already\ncompleted or there was some other exception. This particular change ensures\nthat we also print out the original exception as part of the error message.\n\nThis would help in debugging issues where this codepath is triggered.\nghstack-source-id: 127248844\n\nTest Plan: waitforbuildbot\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27919974\n\nfbshipit-source-id: 2273a93f3475929b14f721c976f194f33a5aa746", "pr_number": "56631", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "1ee54cc7b4": {"title": "Add devices argument to RRef constructor (#57085)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57085\n\nPR #54932 fixed the CUDA RPC for RRef when RRef is created through\nRPC. But besides that use case, RRef can also be created locally\nby directly passing in a value, which would bypass the CUDA stream\nsynchronization in #54932.\n\nThis commit covers the above gap by adding a `devices` argument\nto RRef constructor. The RRef will then use this argument to\nchoose between `CUDAFutre` and `ivalue::Future` to hold the value.\nWhen `devices` is specified and non-empty, `CUDAFuture` will be\nused, and the `devices` will be passed to that `CUDAFuture`.\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D28050001\n\nPulled By: mrshenli\n\nfbshipit-source-id: 2316b419fa69aa4dcd444050f0b74e61c3d0af1e", "pr_number": "57085", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/testing/_internal/distributed/rpc/jit/rpc_test.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "d0ea3183c1": {"title": "Remove debugging print in randperm (#57218)", "body": "Summary:\nSorry that I forget to delete this. Thank xwang233 for finding this.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57218\n\nReviewed By: mruberry\n\nDifferential Revision: D28081292\n\nPulled By: ngimel\n\nfbshipit-source-id: a75867aa82d8644ef3a863d94f225c37babfe249", "pr_number": "57218", "files_changed": ["aten/src/ATen/native/cuda/Randperm.cuh"], "labels": ["Merged", "cla signed", "open source"]}, "54eee04226": {"title": "support discontiguous tensors only for contiguous output format (#57177)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57122\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57177\n\nReviewed By: zou3519\n\nDifferential Revision: D28072674\n\nPulled By: ngimel\n\nfbshipit-source-id: 1f0b1d6916eb9739c35a5ac5aba33e70c1c43a34", "pr_number": "57177", "files_changed": ["aten/src/ATen/native/cuda/Shape.cu", "test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed"]}, "565b034237": {"title": "changed parametric type error in normalize to a warning (#57183)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57183\n\nPreviously, if it was unable to support matching against a type, it would throw an error.\n\nHowever, this exposes the user to arbitrary Torchscript schemas, which may or may not be problematic. Although we may support these in the future, for now we just return False (which will simply eliminate that schema from the candidates).\n\nTest Plan: T89661626 and T89664016\n\nReviewed By: spaugh, khabinov\n\nDifferential Revision: D28072018\n\nfbshipit-source-id: 83017d1e96d19912163edc74a5e43b2816783218", "pr_number": "57183", "files_changed": ["torch/fx/operator_schemas.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "eac02f85cf": {"title": "Fix more clang-tidy errors (#57235)", "body": "Summary:\nIn my last PR I've missed CUDA and distributed folders, fixing this now\nThis change is autogenerated by `python tool/clang_tidy.py -s`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57235\n\nReviewed By: janeyx99\n\nDifferential Revision: D28084444\n\nPulled By: malfet\n\nfbshipit-source-id: bf222f69ee90c7872c3cb0931e8cdb84f0cb3cda", "pr_number": "57235", "files_changed": [".github/workflows/lint.yml", "c10/util/StringUtil.h", "torch/csrc/CudaIPCTypes.cpp", "torch/csrc/CudaIPCTypes.h", "torch/csrc/Module.cpp", "torch/csrc/TypeInfo.cpp", "torch/csrc/api/include/torch/data/datasets/chunk.h", "torch/csrc/api/include/torch/nn/modules/container/any.h", "torch/csrc/autograd/functions/comm.cpp", "torch/csrc/autograd/profiler_cuda.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/csrc/autograd/profiler_legacy.h", "torch/csrc/autograd/python_function.cpp", "torch/csrc/cuda/Event.cpp", "torch/csrc/cuda/Module.cpp", "torch/csrc/cuda/Stream.cpp", "torch/csrc/cuda/Stream.h", "torch/csrc/cuda/comm.cpp", "torch/csrc/cuda/nccl.h", "torch/csrc/cuda/utils.cpp", "torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/example/benchmark.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/test_deploy.cpp", "torch/csrc/distributed/autograd/context/container.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp", "torch/csrc/distributed/autograd/rpc_messages/propagate_gradients_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_req.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_profiling_resp.cpp", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/c10d/init.cpp", "torch/csrc/distributed/rpc/metrics/RpcMetricsHandler.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/profiler/remote_profiler_manager.h", "torch/csrc/distributed/rpc/profiler/server_process_global_profiler.h", "torch/csrc/distributed/rpc/python_rpc_handler.h", "torch/csrc/distributed/rpc/request_callback.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rpc_command_base.h", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/rref_proto.h", "torch/csrc/distributed/rpc/script_call.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/csrc/jit/codegen/cuda/arith.cpp", "torch/csrc/jit/codegen/cuda/codegen.cpp", "torch/csrc/jit/codegen/cuda/executor.h", "torch/csrc/jit/codegen/cuda/executor_kernel_arg.h", "torch/csrc/jit/codegen/cuda/executor_utils.cpp", "torch/csrc/jit/codegen/cuda/graph_fuser.cpp", "torch/csrc/jit/codegen/cuda/index_compute.cpp", "torch/csrc/jit/codegen/cuda/instrumentation.h", "torch/csrc/jit/codegen/cuda/ir_base_nodes.h", "torch/csrc/jit/codegen/cuda/ir_cloner.h", "torch/csrc/jit/codegen/cuda/ir_interface_nodes.h", "torch/csrc/jit/codegen/cuda/ir_internal_nodes.h", "torch/csrc/jit/codegen/cuda/ir_iostream.h", "torch/csrc/jit/codegen/cuda/ir_nodes.cpp", "torch/csrc/jit/codegen/cuda/iter_visitor.h", "torch/csrc/jit/codegen/cuda/kernel_cache.cpp", "torch/csrc/jit/codegen/cuda/kernel_cache.h", "torch/csrc/jit/codegen/cuda/kernel_ir.cpp", "torch/csrc/jit/codegen/cuda/kernel_ir.h", "torch/csrc/jit/codegen/cuda/kernel_ir_builder.cpp", "torch/csrc/jit/codegen/cuda/lower2device.cpp", "torch/csrc/jit/codegen/cuda/lower_loops.cpp", "torch/csrc/jit/codegen/cuda/parser.cpp", "torch/csrc/jit/codegen/cuda/predicate_compute.cpp", "torch/csrc/jit/codegen/cuda/scheduler.cpp", "torch/csrc/jit/codegen/cuda/transform_iter.h", "torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp", "torch/csrc/jit/passes/onnx/constant_fold.cpp", "torch/csrc/jit/passes/onnx/eval_peephole.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp", "torch/csrc/jit/runtime/register_cuda_ops.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/cuda_codegen.h", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/ir_simplifier.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_strings.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e903e16d40": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D28088724\n\nfbshipit-source-id: 3a350580427b92719a3c300bec310aea78375996", "pr_number": null, "files_changed": ["test/cpp/jit/test_utils.cpp", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/testing/init.cpp"], "labels": []}, "2aadeac0ff": {"title": "Remove duplicate entry for filter in language ref v2 (#57154)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57154\n\nReviewed By: zou3519\n\nDifferential Revision: D28061690\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: b895238c0425cc6b60f5e19c67fc5bc6e0115d7f", "pr_number": "57154", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "ca814904b4": {"title": "Handle error reporting when reply file already exists (#57217)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57217\n\nIn torch multiprocessing error handler, we try to remove the file if it already exists. Before removing, we try to log the contents of the file. Here the assumption is that the contents would be valid json.\nHowever, in some cases, it isn't and then we end up not clearing the file.\nLet's handle this error and make sure that the file is cleaned irrespective of the contents of the file.\n\nReviewed By: devashisht\n\nDifferential Revision: D28041470\n\nfbshipit-source-id: da96d11b8f7091715cf0152cccd3ecc08b688eae", "pr_number": "57217", "files_changed": ["torch/distributed/elastic/multiprocessing/errors/error_handler.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "5c8ceefe46": {"title": "Pytorch add agent api tests (#56985)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56985\n\nPytorch add agent api tests\n\nTest Plan: ci/cd\n\nReviewed By: cbalioglu\n\nDifferential Revision: D28020485\n\nfbshipit-source-id: e6acf095f26ce4b99cddfbf7641fb4fa885b0c86", "pr_number": "56985", "files_changed": ["test/distributed/elastic/agent/server/test/api_test.py", "test/run_test.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "17b961b8bc": {"title": "[PyTorch][Edge] Fix mypy error (#56999)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56999\n\n## Summary\nCurrently\n\n## Test\n![image](https://user-images.githubusercontent.com/16430979/116294682-19acaf80-a74d-11eb-9596-3a1d697ae835.png)\nNote: there are still some other mypy failure for other functions in other repo\n\nDifferential Revision: D28023671\n\nTest Plan:\nSee the test image above\nAlso CI\n\nReviewed By: dhruvbird\n\nPulled By: cccclai\n\nfbshipit-source-id: d59da32b8b5a12c3f13bc5f4e02794db01132be3", "pr_number": "56999", "files_changed": ["torch/_C/__init__.pyi.in", "torch/jit/mobile/__init__.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "82e50f4757": {"title": "Update test_overrides for gradcheck (#57155)", "body": "Summary:\nRun both fast and slow mode for test overrides and fix failure in slow_mode\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57155\n\nReviewed By: albanD\n\nDifferential Revision: D28076483\n\nPulled By: soulitzer\n\nfbshipit-source-id: ef942d787d986ba881329e9515e5de6194f3782b", "pr_number": "57155", "files_changed": ["test/test_overrides.py"], "labels": ["Merged", "cla signed"]}, "fda8561944": {"title": "Adding vector_norm to the C++ API (#57055)", "body": "Summary:\n## BC Breaking Note\nThis PR removes the redundant linalg_ prefix from torch::linalg::linalg_det and torch::linalg::linalg_norm C++ API.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57055\n\nReviewed By: H-Huang\n\nDifferential Revision: D28041140\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 65ab32efbcf92010439881bd8a292cdb5b39c579", "pr_number": "57055", "files_changed": ["torch/csrc/api/include/torch/linalg.h"], "labels": ["Merged", "Reverted", "cla signed", "module: bc-breaking"]}, "36ebd0f65d": {"title": "Improve LeftRight documentation (#57164)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57164\n\nGive some more indications about its performance characteristics\nand when it is appropriate to use.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D28064685\n\nPulled By: ezyang\n\nfbshipit-source-id: dbf5e041088d7921db2111d287feb9079466f1b5", "pr_number": "57164", "files_changed": ["c10/util/LeftRight.h"], "labels": ["Merged", "cla signed"]}, "b232659765": {"title": "Replaced _lstsq_helper with internal dispatch (#54724)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54724\n\nRemoved at::_lstsq_helper; it is replaced with DEFINE/DECLARE_DISPATCH.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D27993747\n\nPulled By: mruberry\n\nfbshipit-source-id: dc8b884fd33b3dd18d9a8e4c582b869ac5391de5", "pr_number": "54724", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed", "open source"]}, "63533478bd": {"title": "Fix misleading messages in test_jit_c10d (#57256)", "body": "Summary:\nTCPStore is now available on Windows.\n\nBefore: `TCPStore not available on Windows`\nAfter:  `c10d was not compiled with the NCCL backend`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57256\n\nReviewed By: gchanan\n\nDifferential Revision: D28092539\n\nPulled By: H-Huang\n\nfbshipit-source-id: 1e48cfe29b33b102bc97f51268ac1bbda596397d", "pr_number": "57256", "files_changed": ["test/distributed/test_jit_c10d.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "6fdf092cad": {"title": "Add getStreamFromPool to DeviceGuardImplInterface (#57046)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57046\n\nWe intend to merge CUDAFuture into ivalue::Future by using DeviceGuardImplInterface to avoid explicitly referring to CUDA. For that we need to add two methods to DeviceGuardImplInterface. In this PR, we add a method to get a stream from the global ATen pool.\nghstack-source-id: 127713137\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: Used later in this stack\n\nReviewed By: ezyang\n\nDifferential Revision: D28029159\n\nfbshipit-source-id: 5055d84c1f3c2a4d86442f3149455c5ebd976dea", "pr_number": "57046", "files_changed": ["aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "c10/core/impl/DeviceGuardImplInterface.h", "c10/core/impl/VirtualGuardImpl.h", "c10/cuda/impl/CUDAGuardImpl.h"], "labels": ["Merged", "cla signed"]}, "ea64c90ecc": {"title": "Add recordDataPtrOnStream to DeviceGuardImplInterface (#57047)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57047\n\nWe intend to merge CUDAFuture into ivalue::Future by using DeviceGuardImplInterface to avoid explicitly referring to CUDA. For that we need to add two methods to DeviceGuardImplInterface. In this PR, we add a method to record a DataPtr onto a stream with the caching allocator.\nghstack-source-id: 127713135\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: Used later in this stack\n\nReviewed By: ezyang\n\nDifferential Revision: D28029161\n\nfbshipit-source-id: ff337ab8ccc98437b5594b2f263476baa1ae93e7", "pr_number": "57047", "files_changed": ["aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "c10/core/impl/DeviceGuardImplInterface.h", "c10/core/impl/VirtualGuardImpl.h", "c10/cuda/impl/CUDAGuardImpl.h"], "labels": ["Merged", "cla signed"]}, "381698f900": {"title": "Simplify CUDAMultiStreamGuard (#57048)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57048\n\nCUDAMultiStreamGuard had a default constructor and a `original_devices()` method which were only used in a test. I'm removing them here to simplify the API and make it easier to manipulate this class later. One extra benefit is that this class used to get and store the current stream of _all_ devices, whereas now it only does so for the relevant devices.\nghstack-source-id: 127713136\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28029160\n\nfbshipit-source-id: 185ef9a7ac909cd0ae6507dad9826fe978e67308", "pr_number": "57048", "files_changed": ["aten/src/ATen/cuda/CUDAMultiStreamGuard.h", "aten/src/ATen/test/cuda_stream_test.cpp"], "labels": ["Merged", "cla signed"]}, "682476022f": {"title": "Introduce generic MultiStreamGuard (#57049)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57049\n\nThere was a comment above CUDAMultiStreamGuard which said \"TODO: Implement this generically in c10\". This is what I'm doing here.\n\nThe new generic MultiStreamGuard class is able to take a vector of device-agnostic c10::Streams and is able to support any device type (CUDA, but also ROCm and others) by using a VirtualGuardImpl. A class called CUDAMultiStreamGuard is still kept around, for convenience, and slightly for performance as it avoids a vtable lookup.\nghstack-source-id: 127713139\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28029158\n\nfbshipit-source-id: 2f3181371f8cb0d77a3b2e6aa510f1dd74e8f69b", "pr_number": "57049", "files_changed": ["aten/src/ATen/cuda/CUDAFuture.cpp", "aten/src/ATen/cuda/CUDAMultiStreamGuard.h", "aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "c10/core/StreamGuard.h", "c10/core/impl/InlineStreamGuard.h", "c10/cuda/CUDAGuard.h", "c10/test/core/impl/InlineStreamGuard_test.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp", "torch/lib/c10d/test/ProcessGroupGlooAsyncTest.cpp", "torch/lib/c10d/test/ProcessGroupNCCLTest.cpp", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "cf1595c48b": {"title": "Use only generic helpers in CUDAFuture (#57050)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57050\n\nAvoid (nearly*) any explicit mention of CUDA in CUDAFuture, and instead use \"generic\" classes like c10::Event, c10::Stream and most notably c10::impl::DeviceGuardImplInterface which allow us to indirectly manipulate CUDA entities. This is a preparation step to make CUDAFuture device-agnostic and thus become able to merge it into ivalue::Future.\n\n* The one exception is when we construct the c10::impl::DeviceGuardImplInterface, where for now we still hardcode CUDA. This will be fixed in the very next PR\nghstack-source-id: 127713133\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28032710\n\nfbshipit-source-id: a240ecc32bda481e8ecf85dab94933e24f832bb0", "pr_number": "57050", "files_changed": ["aten/src/ATen/cuda/CUDAFuture.cpp", "aten/src/ATen/cuda/CUDAFuture.h"], "labels": ["Merged", "cla signed"]}, "71c2f88b90": {"title": "Make CUDAFuture handle any kind of device type (#57051)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57051\n\nMake CUDAFuture autodetect the devicetype from its arguments (which thus change from DeviceIndices to full Devices). This in fact transforms CUDAFuture into a AnythingFuture, since it's not tied to CUDA in any way anymore. Having made it fully device-agnostic, we'll merge it into ivalue::Future in the next PR.\nghstack-source-id: 127713134\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28032711\n\nfbshipit-source-id: 8ba23b1b0d97f61db8693cd5f3c7bae7989a9bcd", "pr_number": "57051", "files_changed": ["aten/src/ATen/cuda/CUDAFuture.cpp", "aten/src/ATen/cuda/CUDAFuture.h", "torch/_C/__init__.pyi.in", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/jit/python/init.cpp", "torch/futures/__init__.py", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "311ad5e3af": {"title": "Merge CUDAFuture into ivalue::Future (#57052)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57052\n\nThis PR caps a stack whose goal was to merge CUDAFuture into ivalue::Future. CUDAFuture used to be a subclass of ivalue::Future, which was already pretty good, but it meant that in several places we needed `#ifdef`s or registries in order to create the right type of class, which was annoying. We've made CUDAFuture device-agnostic, by using generic helpers, so that it doesn't depend on CUDA. Now all its code can be inserted into ivalue::Future.\n\nThis PR does this very naively, by copy-pasting CUDAFuture's code into the (previously empty) virtual methods of ivalue::Future. This helps ensure the correctness of this PR, as it's straightforward to see it behaves exactly like before. However we probably want to polish it a bit later to iron out so wrinkles.\nghstack-source-id: 127713138\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28036829\n\nfbshipit-source-id: 3e5b16402f5dc245c1fcb9d7bf06db64dcb0d2a3", "pr_number": "57052", "files_changed": ["BUILD.bazel", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/cuda/CUDAFuture.cpp", "aten/src/ATen/cuda/CUDAFuture.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/csrc/jit/python/init.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/ProcessGroupNCCL.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "e96667175e": {"title": ".circleci: Switch libtorch builds to use smaller image (#56937)", "body": "Summary:\nThese weren't using the smaller images so we should probably let them\nuse the smaller images\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56937\n\nReviewed By: walterddr\n\nDifferential Revision: D28077747\n\nPulled By: seemethere\n\nfbshipit-source-id: da0245bc3b4f564fcd392630542777b2b668b98f", "pr_number": "56937", "files_changed": [".circleci/cimodel/data/binary_build_definitions.py", ".circleci/config.yml"], "labels": ["Merged", "ci/binaries", "cla signed", "module: ci"]}, "2c8ea63cbb": {"title": "add a test for grad view with torch amp (#56730)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56730\n\nadd a test to verify DDP with torch map will result in the same results when using grad_as_bucket_view=true and false.\n\ntorch.amp scale factor does not have dependencies on old gradients, thus it is not affected by grad_as_bucket_view=true or false, see\nhow torch.amp is implemeted here https://github.com/pytorch/pytorch/pull/33366/files.\n\nThis diff verified ddp can work as expected with amp.GradScaler and amp.autocast when when using grad_as_bucket_view=true and false.\nghstack-source-id: 127526358\n\nTest Plan: unit tests\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27950132\n\nfbshipit-source-id: 8ed26935fdcb4514fccf01bb510e31bf6aedac69", "pr_number": "56730", "files_changed": ["torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "9486fc3229": {"title": "[PyTorch][Edge] share readArchiveAndTensors between mobile and jit (#57098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57098\n\n1. Separate `readArchiveAndTensors()` from `jit/import.cpp` to a new file `jit/import_read.cpp`.\n2. Use `readArchiveAndTensors()` in `mobile/import.cpp`\nghstack-source-id: 127703081\n3. Add a util function in cpp that could read .pkl files directly instead of loading the entire module\n\nTest Plan: CI\n\nReviewed By: raziel, iseeyuan\n\nDifferential Revision: D28052193\n\nfbshipit-source-id: c8d57f3270bdcf2e52a32f7c111899bd5da7cac2", "pr_number": "57098", "files_changed": ["tools/build_variables.bzl", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/import.h", "torch/csrc/jit/serialization/import_read.cpp", "torch/csrc/jit/serialization/import_read.h", "torch/csrc/jit/serialization/pickle.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ee71584236": {"title": "Update compare_set implementation for FileStore and HashStore (#57175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57175\n\nUpdate other Store implementations to add the value when current value is empty to match the amendment made to TCPStore (#55636). Added test to cover this case.\n\nTest:\n`pytest -vs test/distributed/test_c10d_common.py -k compare_set`\n\nTest Plan: Imported from OSS\n\nReviewed By: cbalioglu\n\nDifferential Revision: D28069380\n\nPulled By: H-Huang\n\nfbshipit-source-id: eac703edb41faee32a4e7cda61107e2a0e726326", "pr_number": "57175", "files_changed": ["test/distributed/test_c10d_common.py", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/HashStore.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c72f01ab6b": {"title": "Add CI workflow and script to test torchbench. (#56957)", "body": "Summary:\nThis PR adds TorchBench (pytorch/benchmark) CI workflow to pytorch. It tests PRs whose body contains a line staring with \"RUN_TORCHBENCH: \" followed by a list of torchbench model names. For example, this PR will create a Torchbench job of running pytorch_mobildnet_v3 and yolov3 model.\n\nFor security reasons, only the branch on pytorch/pytorch will run. It will not work on forked repositories.\n\nThe model names have to match the exact names in pytorch/benchmark/torchbenchmark/models, separated by comma symbol. Only the first line starting with \"RUN_TORCHBENCH: \" is respected. If nothing is specified after the magic word, no test will run.\n\nKnown issues:\n1. Build PyTorch from scratch and do not reuse build artifacts from other workflows. This is because GHA migration is still in progress.\n2. Currently there is only one worker, so jobs are serialized. We will review the capacity issue after this is deployed.\n3. If the user would like to rerun the test, she has to push to the PR. Simply updating the PR body won't work.\n4. Only supports environment CUDA 10.2 + python 3.7\n\nRUN_TORCHBENCH: yolov3, pytorch_mobilenet_v3\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56957\n\nReviewed By: janeyx99\n\nDifferential Revision: D28079077\n\nPulled By: xuzhao9\n\nfbshipit-source-id: e9ea73bdd9f35e650b653009060d477b22174bba", "pr_number": "56957", "files_changed": [".github/scripts/run_torchbench.py", ".github/workflows/cancel_redundant_workflows.yml", ".github/workflows/run_torchbench.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "2e2c0099eb": {"title": "Support type inference of nn.Module methods using PDT (#57165)", "body": "Summary:\nAdds support for type inference of nn.Module methods using monkeytype in JIT\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57165\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D28064983\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 303eaf8d7a27e74be09874f70f519b4c1081645b", "pr_number": "57165", "files_changed": ["test/jit/test_pdt.py", "test/test_jit.py", "torch/jit/_script.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "fd67088a57": {"title": "[Distributed test]Enable ddp_control_flow tests for ROCm (#57159)", "body": "Summary:\nSigned-off-by: Jagadish Krishnamoorthy <jagdish.krishna@gmail.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57159\n\nReviewed By: zou3519\n\nDifferential Revision: D28074244\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 03e66cf5f546987b3d6d1b9c5feafcdf8292573e", "pr_number": "57159", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "module: rocm", "oncall: distributed", "open source"]}, "ec86f96e91": {"title": "Fix for derivative of sinc(x) when x is positive but very very small (#56986)", "body": "Summary:\nProblem arises for sinc'(x) where x != 0, but x ** 2 == 0, which happens for some very small floats.\n\nI realized that my solution from https://github.com/pytorch/pytorch/issues/56763 was incomplete when I did a quick implementation using `torch.autograd.Function` and still got a `NaN` from my derivative.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56986\n\nReviewed By: gchanan\n\nDifferential Revision: D28093507\n\nPulled By: albanD\n\nfbshipit-source-id: 2a30e1065b08c5c60de843a0778dedeb0fb295f4", "pr_number": "56986", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ac86e0a0e5": {"title": "fix: index_fill_ formula to support duplicate indices (#57101)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57006\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57101\n\nReviewed By: gchanan\n\nDifferential Revision: D28076988\n\nPulled By: albanD\n\nfbshipit-source-id: 1c1bd396282ca030b2445e4f3e1912f3c5a42b6c", "pr_number": "57101", "files_changed": ["tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "a1d2bd56a0": {"title": "[PyTorch] Make as_strided_ use_const_ref_for_mutable_tensors (#55875)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55875\n\nOne less const-incorrect function.\nghstack-source-id: 127409720\n\nTest Plan: fitsships\n\nReviewed By: ezyang\n\nDifferential Revision: D27686995\n\nfbshipit-source-id: 6ba3fe86be9957770920177649f586da8134a09a", "pr_number": "55875", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/mkl/SpectralOps.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed"]}, "fb2f3cd172": {"title": "[PyTorch] Migrate copy_ to borrow input/output (#56031)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56031\n\nCopy kernels just immediately do the copy; borrowing should\nbe fine.\nghstack-source-id: 127409719\n\nTest Plan: CI, review\n\nReviewed By: ezyang, walterddr\n\nDifferential Revision: D27768310\n\nfbshipit-source-id: 7651731fd3dea14adbdb3fef95a6d67c02175508", "pr_number": "56031", "files_changed": ["aten/src/ATen/native/Copy.cpp"], "labels": ["Merged", "cla signed"]}, "dd9f4c8cc9": {"title": "[PyTorch] Reduce move overhead in inferExpandGeometry (#56032)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56032\n\nProfiling & assembly inspection showed that we weren't\ngetting NRVO with `inferExpandGeometry_dimvector` returning\n`std::tuple`. I added a custom type with constructors so that, as the\ncomment says, we could be sure to get NRVO.\nghstack-source-id: 127409717\n\nTest Plan:\nInspected new assembly, no more move construction (which is\na copy for on-stack DimVectors!) upon returning\n\nReviewed By: ezyang\n\nDifferential Revision: D27768312\n\nfbshipit-source-id: d1d53a36508be92585802e1467d8a42d1ae05d80", "pr_number": "56032", "files_changed": ["aten/src/ATen/ExpandUtils.cpp", "aten/src/ATen/ExpandUtils.h", "aten/src/ATen/native/TensorShape.cpp", "torch/csrc/jit/passes/shape_analysis.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "d3ffe9ab6b": {"title": "[PyTorch] Allocate correctly-sized output tensor in addmm_cuda (#56033)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56033\n\nThere doesn't seem to be any reason not to size the output\ncorrectly, and it avoids a round of dispatch for resize.\nghstack-source-id: 127409715\n\nTest Plan:\nInspected GPU trace for simple nn.Linear in a loop. No more\nresize operator invocation.\n\nExisting CI should let us know if this is incorrect\n\nReviewed By: ngimel\n\nDifferential Revision: D27768311\n\nfbshipit-source-id: fb48ec50f3cffc1015ef03d528e9007274b4dd3a", "pr_number": "56033", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["Merged", "cla signed"]}, "21be40b390": {"title": "Add torch_cpu specific flag for debug info (#57190)", "body": "Summary:\nRight now we are using `REL_WITH_DEB_INFO=1` on Linux CI binary builds. This is causing intermittent failures on CUDA builds since the debug information increases the load on the linker. This adds a workaround by a flag to enable debug info only for the target we actually want it for (`libtorch_cpu.so`, all the other binaries are stripped over their debug info after building).\n\nExample failures (from [the hud](https://ezyang.github.io/pytorch-ci-hud/build2/pytorch-nightly?mode=nightly)):\n* https://app.circleci.com/pipelines/github/pytorch/pytorch/311785/workflows/df640957-54b0-4592-aeef-6d5baee503ae/jobs/12932229\n* https://app.circleci.com/pipelines/github/pytorch/pytorch/311784/workflows/e3b487d6-fb46-4a5d-a2d5-22eec328b678/jobs/12932228\n* https://app.circleci.com/pipelines/github/pytorch/pytorch/311784/workflows/e3b487d6-fb46-4a5d-a2d5-22eec328b678/jobs/12932227\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57190\n\nPulled By: driazati\n\nReviewed By: janeyx99\n\nDifferential Revision: D28085550\n\nfbshipit-source-id: 0fc5b3e769b10c0dd3811717f968d0c933667361", "pr_number": "57190", "files_changed": [".circleci/scripts/binary_checkout.sh", "CMakeLists.txt", "caffe2/CMakeLists.txt"], "labels": ["Merged", "ci/binaries", "cla signed"]}, "4b96fc060b": {"title": "Remove distutils (#57040)", "body": "Summary:\n[distutils](https://docs.python.org/3/library/distutils.html) is on its way out and will be deprecated-on-import for Python 3.10+ and removed in Python 3.12 (see [PEP 632](https://www.python.org/dev/peps/pep-0632/)). There's no reason for us to keep it around since all the functionality we want from it can be found in `setuptools` / `sysconfig`. `setuptools` includes a copy of most of `distutils` (which is fine to use according to the PEP), that it uses under the hood, so this PR also uses that in some places.\n\nFixes #56527\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57040\n\nPulled By: driazati\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D28051356\n\nfbshipit-source-id: 1ca312219032540e755593e50da0c9e23c62d720", "pr_number": "57040", "files_changed": ["caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "scripts/build_android.sh", "scripts/build_ios.sh", "scripts/build_mobile.sh", "scripts/get_python_cmake_flags.py", "setup.py", "test/test_spectral_ops.py", "tools/build_pytorch_libs.py", "tools/generate_torch_version.py", "tools/setup_helpers/cmake.py", "torch/testing/_internal/common_cuda.py", "torch/testing/_internal/common_methods_invocations.py", "torch/utils/cpp_extension.py", "torch/utils/tensorboard/__init__.py"], "labels": ["Merged", "cla signed"]}, "95f393f212": {"title": "Add compare_set to trampoline class, add typing and formatting (#57191)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57191\n\nChanged Store::compareSet() to a pure virtual function and added compareSet definition to PythonStore. Rest of changes are from clang-format.\n\nTest Plan: Imported from OSS\n\nReviewed By: cbalioglu\n\nDifferential Revision: D28076557\n\nPulled By: H-Huang\n\nfbshipit-source-id: 379636cf8b031088341a032250ba410d84ccf692", "pr_number": "57191", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/Store.hpp", "torch/lib/c10d/test/FileStoreTest.cpp", "torch/lib/c10d/test/HashStoreTest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a6fa6a6cda": {"title": "[fx minimizer] Add an option to minimizer to allow return all intermediate results (#57279)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57279\n\nAdded an option \"return_intermediate\". If true, when building the submodule we want to run , we will replace the output with all the nodes, so that intermediate results of all the nodes will be returned as output.\n\nThis is recommended to use with `run_node()` function.\n\nTest Plan: `buck test glow/fb/nnpi/lowering:net_min_tests`\n\nReviewed By: khabinov\n\nDifferential Revision: D27913887\n\nfbshipit-source-id: 5a3eab02da05214fb9adeb25656c267b58075b1d", "pr_number": "57279", "files_changed": ["torch/fx/passes/net_min_base.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "e31265dfb3": {"title": "Fix path handling on Win32 in rendezvous.py (#57000)", "body": "Summary:\nFixes test failure after https://github.com/pytorch/pytorch/issues/56598\n\nIntroduced by https://github.com/pytorch/pytorch/issues/45335.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57000\n\nReviewed By: zou3519\n\nDifferential Revision: D28030360\n\nPulled By: seemethere\n\nfbshipit-source-id: 4871d51e6b80dceef8bf95c6c658441287575f63", "pr_number": "57000", "files_changed": ["torch/distributed/rendezvous.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "149000c3f0": {"title": "Update compare_set docs (#57203)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57203\n\nUpdate documentation to remove warning. Refactored arguments from `old_value` -> `expected_value` and `new_value` -> `desired_value`\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan, cbalioglu\n\nDifferential Revision: D28076556\n\nPulled By: H-Huang\n\nfbshipit-source-id: 5fcc5bcfff89cad51d8dc0b74a234964f1af20ed", "pr_number": "57203", "files_changed": ["torch/_C/_distributed_c10d.pyi", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/FileStore.cpp", "torch/lib/c10d/FileStore.hpp", "torch/lib/c10d/HashStore.cpp", "torch/lib/c10d/HashStore.hpp", "torch/lib/c10d/PrefixStore.cpp", "torch/lib/c10d/PrefixStore.hpp", "torch/lib/c10d/TCPStore.cpp", "torch/lib/c10d/TCPStore.hpp", "torch/lib/c10d/test/FileStoreTest.cpp", "torch/lib/c10d/test/HashStoreTest.cpp", "torch/lib/c10d/test/StoreTestCommon.hpp", "torch/lib/c10d/test/TCPStoreTest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "6ed90ed1ac": {"title": "Added OpInfos for sub & mul (#56227)", "body": "Summary:\n`OpInfo`s for `sub` & `mul` operators. Both of them will reuse the sample inputs function added for `add` via another PR.\n\nA https://github.com/pytorch/pytorch/issues/54261 task.\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56227\n\nReviewed By: H-Huang\n\nDifferential Revision: D27993889\n\nPulled By: mruberry\n\nfbshipit-source-id: 7b2da02b0edba3cc37b5b1b88ca32f7dd369ca60", "pr_number": "56227", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: numpy", "module: testing", "open source", "triaged"]}, "c2fbd96735": {"title": "[RPC Framework] Expose a Python API for device map getter (#57179)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57179\n\nExpose a Python API to get the device map and unblock RemoteModule work.\n\nSee: https://github.com/pytorch/pytorch/pull/56854#issuecomment-827762398\n\nAdditionally, add a const decorator for the C++ getter.\n\n#Original PR issue: https://github.com/pytorch/pytorch/issues/51670\nghstack-source-id: 127684266\n\nTest Plan: waitforbuildbot\n\nReviewed By: mrshenli\n\nDifferential Revision: D28070160\n\nfbshipit-source-id: 624d14552d82b99487f72e16428fa75c7a47f61f", "pr_number": "57179", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d1def93166": {"title": "[torch/debuggability] use log.info() in addition to print() in timeoutguard (#57296)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57296\n\nSeems many trainers disable print(), so we cannot see the thread dumps with CompleteInTimeOrDie(). So log.info() also.\n\nTest Plan: sandcastle\n\nReviewed By: aalmah\n\nDifferential Revision: D28098738\n\nfbshipit-source-id: dfdca8801bacf5c7bccecc2387cb7ef41dadfa46", "pr_number": "57296", "files_changed": ["caffe2/python/timeout_guard.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "e27740b38e": {"title": "[torch] Add backward support for segment reduce (CPU only)", "body": "Summary:\nThis is to setup boiler plate code for backward and CPU implementation.\n\nNext Steps in order:\n- Add backward support for CUDA\n- Add support for more aggregation types\n- Benchmarking (for cuda mainly)/more testing/documentation\n- Support for multi dimension\n\nTest Plan:\nUpdated unit test to also check correctness of backward.\n\nWait for CI signal\n\nReviewed By: ngimel\n\nDifferential Revision: D27970340\n\nfbshipit-source-id: 3e608c7fe3628b0a761dd8affc6aad8f65a6ef7f", "pr_number": null, "files_changed": ["aten/src/ATen/native/SegmentReduce.cpp", "aten/src/ATen/native/SegmentReduce.h", "aten/src/ATen/native/native_functions.yaml", "test/test_segment_reductions.py", "tools/autograd/derivatives.yaml"], "labels": []}, "995161203b": {"title": "Fix sort for slow gradcheck (#57192)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57166\n\nGenerate new inputs until we get one where we know that x + eps won't change its sorted order.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57192\n\nReviewed By: albanD\n\nDifferential Revision: D28102361\n\nPulled By: soulitzer\n\nfbshipit-source-id: a12377cc135b0bd92adf0914a100969317b97e8c", "pr_number": "57192", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "16fc18bf82": {"title": "port neg to structure kernel (#57212)", "body": "Summary:\n`negative` alias is not ported.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57212\n\nReviewed By: driazati\n\nDifferential Revision: D28095043\n\nPulled By: walterddr\n\nfbshipit-source-id: 6c7bcd727800bb1db7add43a152de7b58f4ccf43", "pr_number": "57212", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp"], "labels": ["Merged", "cla signed"]}, "49dbe1798f": {"title": "[kineto] Deprecate ClientTraceActivity and merge it with GenericTraceActivity (#56743)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56743\n\nPull Request resolved: https://github.com/pytorch/kineto/pull/184\n\nas part of the migration to ClientTraceActivity -> GenericTraceActivity, now that all CTA mirrors GTA's data structure, we can safely swap out the symbol name.\n\nTest Plan:\n- `buck build kineto`\n- sandcastle to catch any other breakage in depdendees\n\nTook before and after of `fastrnns` bench\n`buck run mode/opt //caffe2/benchmarks/fastrnns:bench -- --cnns resnet50 --group cnns --nloops 1000`\n\nBefore\nhttps://fburl.com/perfdoctor/9n0izgji\n\n{F611729029}\n\nAfter\nhttps://fburl.com/perfdoctor/h9d9tlmp\n{F611725475}\n\nSample ParamComms traces\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1619503816%2F127.0.0.1%2Flibkineto_activities_4003656.json.gz&bucket=gpu_traces\n\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1619503816%2F127.0.0.1%2Flibkineto_activities_4003657.json.gz&bucket=gpu_traces\n\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1619503816%2F127.0.0.1%2Flibkineto_activities_4003658.json.gz&bucket=gpu_traces\n\nReviewed By: gdankel\n\nDifferential Revision: D27353973\n\nfbshipit-source-id: 7012c6524c3c75079029ac290c1dd722ac187ec5", "pr_number": "56743", "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "65968ab817": {"title": "Revert \"Remove sync for randperm on small tensors. (#54113)\" (#57299)", "body": "Summary:\nThis reverts commit e8c268746b297efa988e03abc61ff22203bf3980.\nIt occasionally produces wrong results.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57299\n\nReviewed By: wat3rBro\n\nDifferential Revision: D28102706\n\nPulled By: ngimel\n\nfbshipit-source-id: d7618e104d854c3b96aa502fb4e30041b9aab5df", "pr_number": "57299", "files_changed": ["aten/src/ATen/native/cuda/Randperm.cu", "aten/src/ATen/native/cuda/Randperm.cuh", "aten/src/ATen/test/cuda_distributions_test.cu", "test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed"]}, "c44cbc63cc": {"title": "Ignore more compiler warnings, unify WERROR options (#56630)", "body": "Summary:\nThis adds some more compiler warnings ignores for everything that happens on a standard CPU build (CUDA builds still have a bunch of warnings so we can't turn on `-Werror` everywhere yet).\n](https://our.intern.facebook.com/intern/diff/28005063/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56630\n\nPulled By: driazati\n\nReviewed By: malfet\n\nDifferential Revision: D28005063\n\nfbshipit-source-id: 541ed415eb0470ddf7e08c22c5eb6da9db26e9a0", "pr_number": "56630", "files_changed": ["CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "setup.py", "torch/CMakeLists.txt", "torch/lib/c10d/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "ac72881f3f": {"title": "Fix a numerical issue of CUDA channels-last SyncBatchNorm (#57077)", "body": "Summary:\nFix a numerical issue of CUDA channels-last SyncBatchNorm\n\nThe added test is a repro for the numerical issue. Thanks for the help from jjsjann123 who identified the root cause. Since pytorch SBN channels-last code was migrated from [nvidia/apex](https://github.com/nvidia/apex), apex SBN channels-last also has this issue. We will submit a fix there soon.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57077\n\nReviewed By: mruberry\n\nDifferential Revision: D28107672\n\nPulled By: ngimel\n\nfbshipit-source-id: 0c80e79ddb48891058414ad8a9bedd80f0f7f8df", "pr_number": "57077", "files_changed": ["aten/src/ATen/native/cuda/Normalization.cuh", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source"]}, "b3e1802439": {"title": "Static runtime support for fb::expand_dims (#57282)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57282\n\nAdded support for fb::expand_dims for SR.\n\nTest Plan:\nbuck test caffe2/torch/fb/sparsenn:gpu_test -- test_expand_dims\n\nbuck test caffe2/benchmarks/static_runtime/fb:test_fb_operators\n\nReviewed By: hlu1\n\nDifferential Revision: D28043049\n\nfbshipit-source-id: 01f59db7b507f027b220f044d6ff23602adbdb06", "pr_number": "57282", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "df69b0d060": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D28115855\n\nfbshipit-source-id: 20434a96dae636db53fae089042342000fc103c7", "pr_number": null, "files_changed": ["torch/csrc/distributed/rpc/process_group_agent.cpp"], "labels": []}, "db32b69591": {"title": "quote str kwarg values in `test_ops.py::TestCommon::test_jit_alias_remapping` (#57120)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57119.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57120\n\nReviewed By: gchanan\n\nDifferential Revision: D28086601\n\nPulled By: mruberry\n\nfbshipit-source-id: 566a53c2365f2d128da49ac58463e37b36455831", "pr_number": "57120", "files_changed": ["test/test_ops.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "5b3e7638ca": {"title": "Expand Kineto profiler support (part 1) (#57333)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57333\n\nPull Request resolved: https://github.com/pytorch/kineto/pull/193\n\nExpanding Kineto support to more platforms\n\nTest Plan:\nCI and OSS CI:\nhttps://github.com/pytorch/pytorch/pull/56323\n\nReviewed By: gdankel\n\nDifferential Revision: D27873669\n\nfbshipit-source-id: 4a72a589f958440cbfff247751b7f4e1910a10c7", "pr_number": "57333", "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "d4ddb47719": {"title": "[special] Add `xlog1py` (#55138)", "body": "Summary:\nReference : https://github.com/pytorch/pytorch/issues/50345\n\n* [x] Check Rendered Document (https://12494173-65600975-gh.circle-artifacts.com/0/docs/special.html#torch.special.xlog1py)\n* [x] Tests in Binary Ufunc\n* [x] OpInfo\n* [x] Structured Kernel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55138\n\nReviewed By: ngimel\n\nDifferential Revision: D27961461\n\nPulled By: mruberry\n\nfbshipit-source-id: 30a8f41970a829bf50254aadf5615e8ce4148c7e", "pr_number": "55138", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml", "docs/source/special.rst", "test/test_binary_ufuncs.py", "tools/autograd/derivatives.yaml", "torch/_torch_docs.py", "torch/csrc/api/include/torch/special.h", "torch/overrides.py", "torch/special/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: numpy", "module: special", "open source", "triaged"]}, "6fa1d880b6": {"title": "make external codegen aware of autogen'd composite kernels (#56960)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56960\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D28012667\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 56da050c5d46b8952ddecfa83ebd5fe8454acffe", "pr_number": "56960", "files_changed": ["tools/codegen/dest/__init__.py", "tools/codegen/dest/gen_external_aten_fallbacks.py", "tools/codegen/gen.py"], "labels": ["Merged", "cla signed"]}, "c91bd25e90": {"title": "Fix use of allow_tensor_metadata in view variable creation (#57069)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57069\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D28071507\n\nPulled By: albanD\n\nfbshipit-source-id: 44f0e09846fdc569cf1a62a6f80ca88911e7e45c", "pr_number": "57069", "files_changed": ["torch/csrc/autograd/variable.h"], "labels": ["Merged", "cla signed"]}, "b016bc1c91": {"title": "fix InplaceOrView implementation for manual functions (#57152)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57152\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D28071506\n\nPulled By: albanD\n\nfbshipit-source-id: ef015593dd81be11bc08714d07e0ac4f26e188ec", "pr_number": "57152", "files_changed": ["torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["Merged", "cla signed"]}, "83f186717b": {"title": "Improve perf for forward AD view handling (#57057)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57057\n\nThis PR performs optimization on the ViewInfo handling to remove the need for the \"no forward AD mode\".\n- When the forward and backward ViewInfo are the same, create and store only one of them\n\nCode for timing:\n```python\ntimer = Timer(\n    stmt='a.view(-1)',\n    setup='''\\\nimport torch\na = torch.rand(4)''')\n\nres = timer.collect_callgrind(repeats=2, number=10)[1]\n```\n\nDifference between master and this PR:\n```\n# Benchmark at master\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fe33be83690>\na.view(-1)\nsetup:\n  import torch\n  a = torch.rand(4)\n\n                           All          Noisy symbols removed\n    Instructions:        69286                      68442\n    Baseline:             1332                       1188\n10 runs per measurement, 1 thread\n\n# Benchmark at this branch\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fe33bd7ec30>\na.view(-1)\nsetup:\n  import torch\n  a = torch.rand(4)\n\n                           All          Noisy symbols removed\n    Instructions:        69437                      68562\n    Baseline:             1363                       1188\n10 runs per measurement, 1 thread\n\n# Difference between the two\n<torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fe1216e9a00>\n    160  ???:0x000000000a11c8d0\n     60  torch::autograd::DifferentiableViewMeta::DifferentiableViewMeta\n     60  ???:torch::autograd::as_view(at::Tensor const&, at::Tensor const&, bool, bool, std::function<at::Tensor (at::Tensor const&)>, torch::autograd::CreationMeta, bool)\n     40  ???:0x0000000008e14f50\n     40  ???:0x0000000008e05bd0\n     40  ???:0x0000000008e05480\n     40  ???:0x0000000008e036d0\n     40  ???:0x0000000008e02720\n     30  make_variable_differentiable_view\n    ...\n    -20  ???:0x0000000008e02060\n    -20  ???:0x0000000008e01fd0\n    -30  ???:torch::autograd::isForwardADEnabled()\n    -40  ???:0x0000000008e14f90\n    -40  ???:0x0000000008e05c00\n    -40  ???:0x0000000008e054a0\n    -40  ???:0x0000000008e036f0\n    -40  ???:0x0000000008e02740\n   -160  ???:0x000000000a11d8d0\n\nTotal: 120\n\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D28071505\n\nPulled By: albanD\n\nfbshipit-source-id: 672b1bdf87d516b6de4f2e36656819cfd6f4c9b9", "pr_number": "57057", "files_changed": ["torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["Merged", "cla signed"]}, "95dc2b6e9b": {"title": "Remove unused forward AD flag (#57058)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57058\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D28071504\n\nPulled By: albanD\n\nfbshipit-source-id: df694ac6b9fbb4aed269d61cd9522f8602fdae0c", "pr_number": "57058", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/forward_grad.cpp", "torch/csrc/autograd/forward_grad.h", "torch/csrc/autograd/init.cpp"], "labels": ["Merged", "cla signed"]}, "095c328d9f": {"title": "Add supported backward_dtype to OpInfo (#56156)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/55601.\n\n- [x] removed complex autograd checker in `test_supported_backward`\n- [x] created `backward_dtype[If<Device>]` that inherits from normal `dtype[If<Device>]` by default\n- [x] removed all skip for backward test, instead added backward dtype\n- [x] change complex autograd to a function call: `support_complex_autograd(device_type)` that depends on `backward_dtype*` since they essentially mean the same thing for complex types\n\nTODO for next PR\n- add `test_unsupported_backward` to verify they are actually unsupported.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56156\n\nReviewed By: mruberry\n\nDifferential Revision: D27926717\n\nPulled By: walterddr\n\nfbshipit-source-id: 9a4af8612278ca44a97b6f1510b6b175852c893b", "pr_number": "56156", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "731cc472c5": {"title": "refactor autocast to be extensible for devices (#57104)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57104\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D28094173\n\nPulled By: ezyang\n\nfbshipit-source-id: a5fb62b9a4e58f30d2756bba4331d5fc88136b89", "pr_number": "57104", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/TensorImpl.cpp", "c10/core/impl/LocalDispatchKeySet.h"], "labels": ["Merged", "cla signed", "open source"]}, "52805a0f4f": {"title": "[PyTorch] Include hip_runtime.h in macros.h (#57070)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57070\n\nSee code comment.\nghstack-source-id: 127564865\n\nTest Plan: CI, should unbreak build of following formatting diff\n\nReviewed By: ngimel\n\nDifferential Revision: D28044331\n\nfbshipit-source-id: f571e60b2534313fb9e7dd13dd98d2441b9ce8b8", "pr_number": "57070", "files_changed": ["c10/macros/Macros.h"], "labels": ["Merged", "cla signed"]}, "b49e079a2a": {"title": "Fix string_view::equals_ compilation by CUDA-11.3 (#57322)", "body": "Summary:\n__builtin_memcmp is not a constexpr for character arrays for NVCC-11.3 compiler.\nAttempts to compile this code results in the following error:\n```\n/opt/conda/lib/python3.6/site-packages/torch/include/c10/util/string_view.h(585): note: constexpr memory comparison is only supported for top-level integer or array-of-integer objects\n/opt/conda/lib/python3.6/site-packages/torch/include/c10/util/string_view.h(340): note: called from:\n/opt/conda/lib/python3.6/site-packages/torch/include/c10/util/string_view.h(369): note: called from:\n\n```\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57322\n\nReviewed By: janeyx99\n\nDifferential Revision: D28119125\n\nPulled By: malfet\n\nfbshipit-source-id: e5ff6ac7bb42022e86c9974919e055cf82c2ea83", "pr_number": "57322", "files_changed": ["c10/util/string_view.h"], "labels": ["Merged", "cla signed"]}, "d50a969f2a": {"title": "reduce inline autodiff threshold so we can caputre smaller fusions (#57062)", "body": "Summary:\nThis should let us fuse simpler expressions like\n\n```cpp\n              torch.jit.script\n                def foo(x):\n                    return torch.sigmoid(torch.sigmoid(x))\n```\n\nRUN_TORCHBENCH: alexnet attention_is_all_you_need_pytorch Background_Matting BERT_pytorch demucs densenet121 dlrm fastNLP gen_torchvision_benchmarks.py LearningToPaint maml mnasnet1_0 mobilenet_v2 mobilenet_v2_quantized_qat moco pyhpc_equation_of_state pyhpc_isoneutral_mixing pytorch_CycleGAN_and_pix2pix pytorch_mobilenet_v3 pytorch_stargan pytorch_struct resnet18 resnet50 resnext50_32x4d shufflenet_v2_x1_0 squeezenet1_1 Super_SloMo tacotron2 vgg16 yolov3\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57062\n\nReviewed By: zou3519\n\nDifferential Revision: D28053608\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 6871c3d2a81dd326a481e7ecfaf2ffefffce4a89", "pr_number": "57062", "files_changed": ["test/jit/test_autodiff_subgraph_slicing.py", "torch/csrc/jit/runtime/profiling_graph_executor_impl.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "d896d1f4ce": {"title": "[fx splitter] Fix fusion group utility (#57280)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57280\n\nWe've found an issue that fusion group would results in circular dependency. For example\n```\na -> b -> c -> d\n|              ^\n+ -------------+\n\nOnly a has non tensor output and currently we would create a fusion group (a, b, d). This results in circular dependency because now the fusion group depends on c while c depends on the fusion group as well.\n```\n\nThis diff implement the solution discussed before. When we add a node to fusion group, we add all the nodes that are in the middle of the fusion group and this newly added node.\n\nUse the same logic in minimizer to build fusion group.\n\nTest Plan: split_tests and net_min_tests\n\nReviewed By: khabinov\n\nDifferential Revision: D27917432\n\nfbshipit-source-id: a3d99fe5929dbc9f8eb0f45bccd83fd7b173795a", "pr_number": "57280", "files_changed": ["torch/fx/passes/net_min_base.py", "torch/fx/passes/operator_support.py", "torch/fx/passes/split_utils.py", "torch/fx/passes/splitter_base.py", "torch/fx/passes/tools_common.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "b87d3fa432": {"title": "[PyTorch][jit] Don't allow create() on singleton types (#56807)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56807\n\nIf I understand correctly, there's no reason to create your own instance of these global singleton types.\nghstack-source-id: 127312270\n\nTest Plan: CI\n\nReviewed By: SplitInfinity\n\nDifferential Revision: D27973447\n\nfbshipit-source-id: f12df69d185f1baaa45f2ac6eac70570a7a65912", "pr_number": "56807", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "aten/src/ATen/test/type_test.cpp", "test/cpp/rpc/e2e_test_base.h", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/passes/onnx/fixup_onnx_controlflow.cpp", "torch/csrc/jit/python/pybind_utils.h"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "0a9c9cc674": {"title": "Update DLPack to 0.4 (#55365)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55090\n\nI included the header directly, but I am not sure if we should add this as a git submodule, what do you guys think?\nAlso regarding the implementation, in ATen lanes seems not to be supported, but from CuPy complex types are exported with 2 lanes, I am not sure wether this is correct or not. However, in PyTorch this seems to be working properly, so I forgive 2 lanes for complex datatypes.\n\nTODO: add tests for complex and bfloat\n\nEasy test script against cupy\n\n```python\nimport cupy\nimport torch\n\nfrom torch.utils.dlpack import to_dlpack\nfrom torch.utils.dlpack import from_dlpack\n\n# Create a PyTorch tensor.\ntx1 = torch.tensor(\n    [2 + 1j, 3 + 2j, 4 + 3j, 5 + 4j], dtype=torch.complex128\n).cuda()\n\n# Convert it into a DLPack tensor.\ndx = to_dlpack(tx1)\n\n# Convert it into a CuPy array.\ncx = cupy.fromDlpack(dx)\n\n# Convert it back to a PyTorch tensor.\ntx2 = from_dlpack(cx.toDlpack())\ntorch.testing.assert_allclose(tx1, tx2)\n```\n\nThanks to leofang who updated CuPy's dlpack version and his PR served me as the guide for this one.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55365\n\nReviewed By: ngimel\n\nDifferential Revision: D27724923\n\nPulled By: mruberry\n\nfbshipit-source-id: 481eadb882ff3dd31e7664e08e8908c60a960f66", "pr_number": "55365", "files_changed": ["aten/src/ATen/DLConvertor.cpp", "aten/src/ATen/dlpack.h", "aten/src/ATen/test/cuda_dlconvertor_test.cpp", "test/test_torch.py"], "labels": ["Merged", "cla signed", "module: numpy", "open source", "triaged"]}, "e62cdae469": {"title": "Static Runtime support for aten::matmul (#57291)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57291\n\naten::matmul support for static runtime\n\nTest Plan: buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest -- IndividualOps_Binary_MatMul\n\nReviewed By: hlu1\n\nDifferential Revision: D28099671\n\nfbshipit-source-id: 784035060c8c24953df47ca4227d2bca5094da22", "pr_number": "57291", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "5f2b9b1df9": {"title": "refactor autograd_hook (#54981)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54981\n\nput part of codes in autograd_hook into functions, so that they can be used in the static graph training later on.\nghstack-source-id: 127755405\n\nTest Plan: unit tests\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27439508\n\nfbshipit-source-id: a02a4b029841f5e7f11cfc5496bb7972ef53d878", "pr_number": "54981", "files_changed": ["torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3f81912885": {"title": "static graph api skeleton (#54995)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54995\n\nprovide an DDP private API to explicitly set the training is static, also set this flag in logger\nghstack-source-id: 127755713\n\nTest Plan: unit tests\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27444965\n\nfbshipit-source-id: 06ef1c372296815944b2adb33fbdf4e1217c1359", "pr_number": "54995", "files_changed": ["torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/logger.cpp", "torch/lib/c10d/logger.hpp", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "788aefd7cc": {"title": "Propagate information on torch_shm_manager failures to parent process (#57307)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57307\n\nExtend the `\"ERROR\"` message that `torch_shm_manager` writes to the pipe when it encounters a fatal error with some extra context (specifically, the `what()` on a caught `std::exception`), allowing the parent process to gain some insight into the cause of the failure.\n\nAlso, simply return from `main()` with an error exit code when a fatal exception is caught rather than re-throwing, because re-throwing leads to premature process termination that may prevent standard output from being flushed (and therefore the parent process from being able to read the error context from the pipe).\n\nReviewed By: ejguan\n\nDifferential Revision: D28047916\n\nfbshipit-source-id: d423ee8ed1b2bf7831db877e8f8515ec6d6aa169", "pr_number": "57307", "files_changed": ["torch/lib/libshm/core.cpp", "torch/lib/libshm/manager.cpp"], "labels": ["Merged", "cla signed"]}, "7eed5410cd": {"title": "Make c10::TempFile non-copyable but movable (#57308)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57308\n\nThis diff makes `c10::TempFile` non-copyable but movable. `torch_shm_manager` was previously dependent upon some hidden behavior that was a result of copying `TempFile`s, which is also being made more explicit now that they can be moved but not copied.\n\nContext:\n\n`c10::TempFile` is currently copyable, which leads to surprising behavior. A seemingly valid `TempFile` may in fact be invalid if the original it was copied from has already been destroyed, resulting in the file descriptor to be closed and the filename being unlinked without the user knowing about it.\n\n**In fact, both `c10::try_make_tempfile` and `c10::make_tempfile` cause copies of `TempFile` to be made**, which can easily be verified by explicitly deleting the copy constructor of `TempFile` and attempting to compile. This means that in practice, users of these functions are getting temporary files that have already been closed and unlinked.\n\nThis copying of `TempFile` is particularly interesting in the case of `torch_shm_manager`, which uses `try_make_tempfile` to generate the name of a Unix domain socket to communicate with clients. In order for `bind()` on the socket name to be successful, a file with that same name must not be linked in the filesystem, or `EADDRINUSE` will result. Happily, beacuse `try_make_tempfile` previously created a copy of the `TempFile` while destroying the original, `torch_shm_manager` did not encounter this. With this change, howevrer, `torch_shm_manager` must now explicitly destroy the `TempFile` before attempting to `bind()`. Unfortunately, this exposes a race condition--**other code can re-generate the same-named temporary file after the one created by `torch_shm_manager` is explicitly unlinked but before `torch_shm_manager` binds it to the server socket.** To be clear: this race condition already existed before this diff, but this makes things more explicit. The real fix will be in a follow-up change.\n\nReviewed By: ejguan\n\nDifferential Revision: D28047915\n\nfbshipit-source-id: e8a1b6bb50419fe65620cfecdb67c566a4cf9056", "pr_number": "57308", "files_changed": ["c10/util/tempfile.h", "torch/lib/libshm/manager.cpp"], "labels": ["Merged", "cla signed"]}, "2c2aa9e030": {"title": "Address temp file/bind race condition in torch_shm_manager (#57309)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57309\n\nAddressing a race condition that can occur in `torch_shm_manager` between the time its temporary file is unlinked and when it `bind()`s the manager server socket to that same name. In that time window, other threads/processes can re-create another temporary file with the same name, causing `bind()` to fail with `EADDRINUSE`.\n\nThis diff introduces `c10::TempDir` and associated helper functions that mirror those of `c10::TempFile` and generates the manager socket name using a combination of a temporary directory, which will be valid for the lifetime of `torch_shm_manager`, and a well-known file name within that directory that will never be used outside of `bind()`.\n\nReviewed By: ejguan\n\nDifferential Revision: D28047914\n\nfbshipit-source-id: 148d54818add44159881d3afc2ffb31bd73bcabf", "pr_number": "57309", "files_changed": ["c10/test/util/tempfile_test.cpp", "c10/util/tempfile.h", "torch/lib/libshm/manager.cpp"], "labels": ["Merged", "cla signed"]}, "e68c46bb3a": {"title": "Propagate information on torch_shm_manager execl failure to parent process (#57310)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57310\n\nIf we fail to exec `torch_shm_manager`, write an appropriate error message to stdout so that the parent process can have some context on the failure.\n\nReviewed By: ejguan\n\nDifferential Revision: D28047917\n\nfbshipit-source-id: 68bf357df7a6b318c036f4f62cbb428a62cb139e", "pr_number": "57310", "files_changed": ["torch/lib/libshm/core.cpp"], "labels": ["Merged", "cla signed"]}, "f54aa85a6c": {"title": "Fix MAGMA qr for empty batched inputs (#56257)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56257\n\nCPU and cuSOLVER path were fixed with refactoring of\n`_linalg_qr_helper_default`.\n\nResolves https://github.com/pytorch/pytorch/issues/50576\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27960157\n\nPulled By: mruberry\n\nfbshipit-source-id: f923f3067a35e65218889e64c6a886364c3d1759", "pr_number": "56257", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "3a777b6792": {"title": "[PyTorch] Optimize intrusive_ptr(TTarget*) ctor (pybind) (#57053)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57053\n\nThis ctor is intended for pybind use. It increments weakcount when creating a strong reference, which is only correct if you know that the value was previously zero. So, I consolidated make() with this ctor.\nghstack-source-id: 127537070\n\nTest Plan: existing CI\n\nReviewed By: ezyang\n\nDifferential Revision: D28037206\n\nfbshipit-source-id: eec57a99e3e032830f156c1e6258760f6465137b", "pr_number": "57053", "files_changed": ["c10/util/intrusive_ptr.h"], "labels": ["Merged", "cla signed"]}, "bbc3cc6718": {"title": "[CUDA graphs] [BC-breaking] Makes torch.cuda.amp.GradScaler scale updates in-place for better composability with graph capture (#55562)", "body": "Summary:\nI'd like the following pattern (a natural composition of Amp with full fwd+bwd capture) to work:\n```python\n# Create \"static_input\" with dummy data, run warmup iterations,\n# call optimizer.zero_grad(set_to_none=True), then\ng = torch.cuda._Graph()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    optimizer.zero_grad(set_to_none=True)\n    g.capture_begin()\n    with autocast():\n        out = model(static_input)\n        loss = loss_fn(out)\n    scaler.scale(loss).backward()\n    g.capture_end()\ntorch.cuda.current_stream().wait_stream(s)\n\n# Training loop:\nfor b in data:\n    # optimizer.zero_grad() deliberately omitted, replay()'s baked-in backward will refill statically held .grads\n    static_input.copy_(b)\n    g.replay()\n    scaler.step(optimizer)\n    scaler.update()\n```\n\nRight now `GradScaler` can't work with this pattern because `update()` creates the scale tensor for the next iteration out of place. This PR changes `update()` to act in place on a long-lived scale tensor that stays static across iterations.\n\nI'm not sure how this change affects XLA (see https://github.com/pytorch/pytorch/pull/48570), so we shouldn't merge without approval from ailzhang yaochengji.\n\nTagged bc-breaking because it's a change to the amp update utility function in native_functions.yaml. The function was never meant to be user-facing though.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55562\n\nReviewed By: zou3519\n\nDifferential Revision: D28046159\n\nPulled By: ngimel\n\nfbshipit-source-id: 02018c221609974546c562f691e20ab6ac611910", "pr_number": "55562", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/cuda/AmpKernels.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_cuda.py", "tools/code_analyzer/default_op_deps.yaml", "torch/cuda/amp/grad_scaler.py"], "labels": ["Merged", "cla signed", "module: amp (automated mixed precision)", "module: bc-breaking", "module: cuda graphs", "open source", "triaged"]}, "42b3fc29f4": {"title": "Fix NVRTC versioning for CUDA 11.X (X>=3), CUDA 12 and later (#57204)", "body": "Summary:\nNVRTC versioning has changed starting 11.3, and will change again for CUDA 12.X. See comment in code for detail. As a result, jit on CUDA 11.3 is broken.\n\nAlso, the error message is misleading: When both `libname` and `alt_libname` are non-empty, the error message is only reporting `alt_libname`, it should report both.\n\nTo reproduce the error, you can use:\n\n```python\nimport torch\n\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_override_can_fuse_on_cpu(True)\ntorch._C._jit_override_can_fuse_on_gpu(True)\n\ntorch.jit.script\ndef jit_relu_dropout(x, prob) :\n    # type: (Tensor, float) -> Tensor\n    x = torch.nn.functional.relu(x)\n    x = torch.nn.functional.dropout(x, p=prob, training=True)\n    return x\n\nx = torch.randn((64, 40, 12, 1024), device=\"cuda:0\", dtype=torch.float16, requires_grad=True)\ny = jit_relu_dropout(x, 0.5)\n```\nwith CUDA 11.3, and you will see\n```\nTraceback (most recent call last):\n  File \"/home/gaoxiang/misc/nvrtc-failure.py\", line 16, in <module>\n    y = jit_relu_dropout(x, 0.5)\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: Error in dlopen or dlsym: libnvrtc-8aa72235.so.11.3: cannot open shared object file: No such file or directory\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57204\n\nReviewed By: ngimel\n\nDifferential Revision: D28122083\n\nPulled By: malfet\n\nfbshipit-source-id: fd387cf79f33a6d5a5b93d54c9f21e9c23731045", "pr_number": "57204", "files_changed": ["aten/src/ATen/DynamicLibrary.cpp", "aten/src/ATen/cuda/detail/LazyNVRTC.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "4c3283da0d": {"title": "Fix binary_checkout to use master (#57389)", "body": "Summary:\nThese lines never should have been committed, remove themh\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57389\n\nPulled By: driazati\n\nReviewed By: seemethere, samestep\n\nDifferential Revision: D28129673\n\nfbshipit-source-id: 2de4b4d94c569177fec0c9eac8b7e9a8e59b550b", "pr_number": "57389", "files_changed": [".circleci/scripts/binary_checkout.sh"], "labels": ["Merged", "cla signed"]}, "400ca7677c": {"title": "[StaticRuntime] Use NNC's call_raw API to reduce call overheads. (#57329)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57329\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D28110358\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 94b87130a1ffdb4acf171ddcea3895e8a75c34ac", "pr_number": "57329", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "Reverted", "cla signed", "oncall: jit"]}, "293830bc19": {"title": "Fix min() and max() for empty tensors (#52565)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/34907\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52565\n\nReviewed By: anjali411\n\nDifferential Revision: D27999955\n\nPulled By: ezyang\n\nfbshipit-source-id: 30e88cc8d84806198500e3001ecf58fa764536dd", "pr_number": "52565", "files_changed": ["aten/src/ATen/native/ReduceAllOps.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/cuda/Sorting.cu", "test/test_linalg.py", "test/test_reductions.py"], "labels": ["Merged", "cla signed", "module: TensorIterator", "module: xla", "open source", "triaged"]}, "208f81b787": {"title": "[PyTorch] ifdef out ATen tests that fail with static dispatch (#57379)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57379\n\nReviewed By: cspanda\n\nDifferential Revision: D27576223\n\nfbshipit-source-id: 6f77f1ac8b92f955d654231527eee2a8b7a1ff3d", "pr_number": "57379", "files_changed": ["aten/src/ATen/core/dispatch/backend_fallback_test.cpp", "aten/src/ATen/test/basic.cpp", "aten/src/ATen/test/cpu_rng_test.cpp"], "labels": ["Merged", "cla signed"]}, "3a5f85465b": {"title": "[pytorch] fewer cuda sync in unique by using cub instead of thrust (#57323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57323\n\nUse cub library instead of thrust to reduce # of cuda stream synchronize.\n\nReviewed By: ngimel\n\nDifferential Revision: D28088029\n\nfbshipit-source-id: b616294cd776aa5643c153e172258a0153a42b6a", "pr_number": "57323", "files_changed": ["aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cuda/UniqueCub.cu", "aten/src/ATen/native/cuda/UniqueCub.cuh", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "233f2cd29f": {"title": "Maintain submodule references during subgraph rewriting (#55463)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55463\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D27621650\n\nPulled By: ansley\n\nfbshipit-source-id: e3558c64cdc2c1d846355fa58307a18c0714874b", "pr_number": "55463", "files_changed": ["test/fx/test_subgraph_rewriter.py", "torch/fx/graph.py", "torch/fx/subgraph_rewriter.py"], "labels": ["Merged", "cla signed", "fx"]}, "bbdadab306": {"title": "Refactor fast gradcheck (#55871)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55871\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28096549\n\nPulled By: soulitzer\n\nfbshipit-source-id: ee8b71fbd03ee581e71cdfcfd5e2258adefe15a6", "pr_number": "55871", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "2b54cec7e8": {"title": "Clean up naming and comments (#56964)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56964\n\nThis PR does many things but does not update any logic:\n - Prefixes all function names that are not `gradcheck`, `gradgradcheck`, `get_numerical_jacobian`, and `get_analytical_jacobian` with underscore to indicate that they aren't part of the public API (https://github.com/pytorch/pytorch/issues/55714).\n - Improve naming to avoid referencing Jacobian rows or Jacobian cols when we really mean vjp and jvp as suggested by zou3519\n - Try to reduce comment line length so they are more consistent and easier to read\n - Other misc improvements to documentaiton\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28096571\n\nPulled By: soulitzer\n\nfbshipit-source-id: d372b5f8ee080669e525a987402ded72810baa0c", "pr_number": "56964", "files_changed": ["torch/autograd/gradcheck.py", "torch/testing/_internal/common_nn.py"], "labels": ["Merged", "cla signed"]}, "bd347012ec": {"title": "Added sm_75 support for CI Xenial CUDA 11.1 cuDNN 8 builds (#57320)", "body": "Summary:\nThis PR adds `sm_75` CUDA architecture support for the PR CI build Xenial CUDA 11.1 cuDNN 8, with build name:`pytorch_linux_xenial_cuda11_1_cudnn8_py3_gcc7_build`, so that generated artifacts from these builds can be installed and run on machines with CUDA capability sm_75.\n\nIn PR https://github.com/pytorch/pytorch/issues/57207, the Xenial CUDA 10.2 cuDNN 7 build `pytorch_linux_xenial_cuda10_2_cudnn7_py3_gcc7_build` was taken off the list of builds done for PRs to `master`. PR https://github.com/pytorch/pytorch/issues/56619 has added `sm_75` support for this build. This PR removes this support for the Xenial CUDA 10.2 cuDNN7 builds, and adds it for the current PR CI build Xenial CUDA 11.1 cuDNN 8 `pytorch_linux_xenial_cuda11_1_cudnn8_py3_gcc7_build`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57320\n\nReviewed By: astaff\n\nDifferential Revision: D28125542\n\nPulled By: malfet\n\nfbshipit-source-id: f220b8f3279054c98cab9eef1e0d7e37161a946f", "pr_number": "57320", "files_changed": [".jenkins/pytorch/build.sh"], "labels": ["Merged", "cla signed", "module: build", "module: ci", "open source"]}, "20eac093a7": {"title": "[torch][segment_reduce] Add support for initial value (#56923)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56923\n\nNext Steps in order:\n- Add backward support for CUDA\n- Add support for more aggregation types\n- Benchmarking (for cuda mainly)/more testing/documentation\n- Support for multi dimension\n\nTest Plan: Updated unit test to include 0 length segment as well.\n\nReviewed By: ngimel\n\nDifferential Revision: D27992228\n\nfbshipit-source-id: 28851811f8a784a63162721c511d69e617a93727", "pr_number": "56923", "files_changed": ["aten/src/ATen/native/SegmentReduce.cpp", "aten/src/ATen/native/SegmentReduce.h", "aten/src/ATen/native/cuda/SegmentReduce.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_segment_reductions.py", "tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed", "fb-exported"]}, "13dbb77b7a": {"title": "[RPC Framework] Enable RemoteModule to directly send GPU tensors over the wire on TensorPipe RPC backend if a device map is provided (#57288)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57288\n\nIf the device map provided by RemoteModue is not empty, then TensorPipe RPC backend can support directly sending GPU tensors over the wire.\n\nAlso add pybind of `_get_device_map`.\n\nThe changes in unit test setup is separated out as a follow-up PR, as currently it breaks some tests in `distributed/rpc/test_faulty_agent.py`.\n\nStill need to fix test_load_di_parts in `torch/fb/training_toolkit/applications/sparse_nn/batch_distributed_inference/tests:batch_distributed_inference_test`. Currently an early return is used to bypass this test failure.\n\n#Original PR issue: https://github.com/pytorch/pytorch/issues/51670\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_input_moved_to_cuda_device\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_input_moved_to_cuda_device_script\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- RemoteModule -j 1\n\nCAUTION: This one actually fails and now it is bypassed. See FIXME in `_remote_forward`.\nbuck test mode/dev-nosan caffe2/torch/fb/training_toolkit/applications/sparse_nn/batch_distributed_inference/tests:batch_distributed_inference_test -- test_load_di_parts\n\nReviewed By: wanchaol\n\nDifferential Revision: D28021672\n\nfbshipit-source-id: a89245dc35e1d9479811ec6f98d9f34116837d79", "pr_number": "57288", "files_changed": ["torch/_C/_distributed_rpc.pyi", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/distributed/nn/api/remote_module.py", "torch/distributed/nn/jit/templates/remote_module_template.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "b11a24209f": {"title": "[PyTorch] Take advantage of string literals in TORCH_WARN (#54032)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54032\n\nAdd a `const char*` override to c10::Warning::warn so that we can avoid wrapping plain C string literals in std::string.\nghstack-source-id: 125544720\n\nTest Plan: Buildsizebot some iOS apps?\n\nReviewed By: ezyang\n\nDifferential Revision: D27061983\n\nfbshipit-source-id: dc11150c911a4317a8edac75e50c5ba43511ff24", "pr_number": "54032", "files_changed": ["c10/util/Exception.cpp", "c10/util/Exception.h"], "labels": ["Merged", "cla signed"]}, "44cc873fba": {"title": "[PyTorch] Autoformat c10 (#56830)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56830\n\nOpt into formatting on GitHub and format everything. This is a trial run before turning on formatting for more and eventually all of the codebase.\n\nTest Plan: CI\n\nReviewed By: zertosh\n\nDifferential Revision: D27979080\n\nfbshipit-source-id: a80f0c48691c08ae8ca0af06377b87e6a2351151", "pr_number": "56830", "files_changed": ["c10/benchmark/intrusive_ptr_benchmark.cpp", "c10/core/Allocator.cpp", "c10/core/Allocator.h", "c10/core/Backend.h", "c10/core/CompileTimeFunctionPointer.h", "c10/core/CopyBytes.cpp", "c10/core/DefaultDtype.cpp", "c10/core/DefaultDtype.h", "c10/core/DefaultTensorOptions.h", "c10/core/Device.cpp", "c10/core/Device.h", "c10/core/DeviceGuard.h", "c10/core/DeviceType.cpp", "c10/core/DeviceType.h", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/Event.h", "c10/core/GeneratorImpl.cpp", "c10/core/GeneratorImpl.h", "c10/core/GradMode.h", "c10/core/InferenceMode.cpp", "c10/core/InferenceMode.h", "c10/core/MemoryFormat.h", "c10/core/QEngine.h", "c10/core/QScheme.h", "c10/core/Scalar.cpp", "c10/core/Scalar.h", "c10/core/ScalarType.h", "c10/core/ScalarTypeToTypeMeta.h", "c10/core/Storage.cpp", "c10/core/Storage.h", "c10/core/StorageImpl.h", "c10/core/Stream.h", "c10/core/StreamGuard.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/TensorOptions.cpp", "c10/core/TensorOptions.h", "c10/core/UndefinedTensorImpl.cpp", "c10/core/UndefinedTensorImpl.h", "c10/core/WrapDimMinimal.h", "c10/core/impl/DeviceGuardImplInterface.cpp", "c10/core/impl/DeviceGuardImplInterface.h", "c10/core/impl/FakeGuardImpl.h", "c10/core/impl/InlineDeviceGuard.h", "c10/core/impl/InlineEvent.h", "c10/core/impl/InlineStreamGuard.h", "c10/core/impl/LocalDispatchKeySet.cpp", "c10/core/impl/LocalDispatchKeySet.h", "c10/core/impl/SizesAndStrides.cpp", "c10/core/impl/SizesAndStrides.h", "c10/core/impl/VirtualGuardImpl.h", "c10/core/thread_pool.cpp", "c10/core/thread_pool.h", "c10/cuda/CUDACachingAllocator.cpp", "c10/cuda/CUDACachingAllocator.h", "c10/cuda/CUDAException.h", "c10/cuda/CUDAFunctions.cpp", "c10/cuda/CUDAGraphsC10Utils.h", "c10/cuda/CUDAGuard.h", "c10/cuda/CUDAStream.cpp", "c10/cuda/CUDAStream.h", "c10/cuda/impl/CUDAGuardImpl.cpp", "c10/cuda/impl/CUDAGuardImpl.h", "c10/cuda/impl/CUDATest.cpp", "c10/cuda/impl/CUDATest.h", "c10/macros/Export.h", "c10/macros/Macros.h", "c10/mobile/CPUCachingAllocator.h", "c10/mobile/CPUProfilingAllocator.cpp", "c10/mobile/CPUProfilingAllocator.h", "c10/test/core/CompileTimeFunctionPointer_test.cpp", "c10/test/core/DispatchKeySet_test.cpp", "c10/test/core/impl/InlineDeviceGuard_test.cpp", "c10/test/core/impl/InlineStreamGuard_test.cpp", "c10/test/core/impl/SizesAndStrides_test.cpp", "c10/test/util/Array_test.cpp", "c10/test/util/C++17_test.cpp", "c10/test/util/LeftRight_test.cpp", "c10/test/util/Metaprogramming_test.cpp", "c10/test/util/TypeIndex_test.cpp", "c10/test/util/TypeList_test.cpp", "c10/test/util/TypeTraits_test.cpp", "c10/test/util/accumulate_test.cpp", "c10/test/util/bfloat16_test.cpp", "c10/test/util/complex_math_test_common.h", "c10/test/util/complex_test_common.h", "c10/test/util/either_test.cpp", "c10/test/util/exception_test.cpp", "c10/test/util/intrusive_ptr_test.cpp", "c10/test/util/irange_test.cpp", "c10/test/util/logging_test.cpp", "c10/test/util/optional_test.cpp", "c10/test/util/ordered_preserving_dict_test.cpp", "c10/test/util/string_view_test.cpp", "c10/test/util/tempfile_test.cpp", "c10/test/util/typeid_test.cpp", "c10/util/Array.h", "c10/util/ArrayRef.h", "c10/util/BFloat16-inl.h", "c10/util/BFloat16-math.h", "c10/util/BFloat16.h", "c10/util/Backtrace.cpp", "c10/util/Bitset.h", "c10/util/C++17.h", "c10/util/Deprecated.h", "c10/util/Flags.h", "c10/util/FunctionRef.h", "c10/util/Half-inl.h", "c10/util/Half.h", "c10/util/IdWrapper.h", "c10/util/LeftRight.h", "c10/util/MathConstants.h", "c10/util/MaybeOwned.h", "c10/util/Metaprogramming.h", "c10/util/Optional.cpp", "c10/util/Optional.h", "c10/util/Registry.h", "c10/util/SmallVector.h", "c10/util/StringUtil.cpp", "c10/util/StringUtil.h", "c10/util/ThreadLocalDebugInfo.cpp", "c10/util/ThreadLocalDebugInfo.h", "c10/util/TypeCast.h", "c10/util/TypeIndex.h", "c10/util/TypeList.h", "c10/util/TypeTraits.h", "c10/util/Unicode.cpp", "c10/util/Unicode.h", "c10/util/UniqueVoidPtr.h", "c10/util/accumulate.h", "c10/util/complex.h", "c10/util/complex_math.cpp", "c10/util/complex_math.h", "c10/util/complex_utils.h", "c10/util/copysign.h", "c10/util/either.h", "c10/util/flags_use_no_gflags.cpp", "c10/util/flat_hash_map.h", "c10/util/hash.h", "c10/util/in_place.h", "c10/util/intrusive_ptr.h", "c10/util/irange.h", "c10/util/llvmMathExtras.h", "c10/util/logging_is_not_google_glog.h", "c10/util/math_compat.h", "c10/util/numa.cpp", "c10/util/order_preserving_flat_hash_map.h", "c10/util/overloaded.h", "c10/util/quint4x2.h", "c10/util/reverse_iterator.h", "c10/util/sparse_bitset.h", "c10/util/string_utils.h", "c10/util/string_view.h", "c10/util/tempfile.h", "c10/util/typeid.cpp", "c10/util/typeid.h", "c10/util/variant.h", "c10/util/win32-headers.h", "tools/clang_format_all.py", "tools/clang_format_ci.sh"], "labels": ["Merged", "cla signed"]}, "f7f8540794": {"title": "Fix tensor device in test_kthvalue_overlap (#56869)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56869\n\nghstack-source-id: 127914015\n\nTest Plan: auto test\n\nReviewed By: ezyang\n\nDifferential Revision: D27986559\n\nfbshipit-source-id: f4a638d737b06dd5f384b54e20490d76543d4e78", "pr_number": "56869", "files_changed": ["test/test_shape_ops.py", "test/test_sort_and_select.py"], "labels": ["Merged", "cla signed"]}, "183320df96": {"title": "Add device_check place holder for functions (#56870)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56870\n\nAutomatic generation of device check code will be supported in\nfollowing PRs.\n\nChanged are generaetd via:\n\n1. Codemod\n```\nfastmod '  device_guard: False' '  device_check: NoCheck\n  device_guard: False' aten/src/ATen/native/native_functions.yaml\n```\n\n2. Python script: https://gist.github.com/wenleix/be20c34bbbfcee0b289cdea2cf15b16c\nghstack-source-id: 127914016\n\nTest Plan: auto test\n\nReviewed By: ezyang\n\nDifferential Revision: D27986427\n\nfbshipit-source-id: 4e598a30306b80b5ade27af70d3e58770e401fc2", "pr_number": "56870", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "tools/codegen/model.py"], "labels": ["Merged", "cla signed"]}, "20085f6d23": {"title": "Support auto generation of device check (#56872)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56872\n\nghstack-source-id: 127914018\n\nTest Plan: auto test\n\nReviewed By: ezyang\n\nDifferential Revision: D27986429\n\nfbshipit-source-id: 0da8413b0b8e6810fcea27ed1de499f11f68bd1f", "pr_number": "56872", "files_changed": ["aten/src/ATen/core/adaption.cpp", "aten/src/ATen/core/op_registration/adaption.h", "aten/src/ATen/native/README.md", "aten/src/ATen/native/native_functions.yaml", "test/test_binary_ufuncs.py", "test/test_cuda.py", "test/test_linalg.py", "test/test_tensor_creation_ops.py", "test/test_torch.py", "tools/codegen/dest/register_dispatch_key.py", "tools/codegen/model.py"], "labels": ["Merged", "cla signed"]}, "d536e6c684": {"title": "Fix variable names in torch.fft examples (#57290)", "body": "Summary:\nApparently normal reST doctests aren't run in CI, because of this line in the `conf.py`:\nhttps://github.com/pytorch/pytorch/blob/ac86e0a0e5ee805592a7804eee05ed8d5be85e5a/docs/source/conf.py#L366\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57290\n\nReviewed By: astaff\n\nDifferential Revision: D28118198\n\nPulled By: mruberry\n\nfbshipit-source-id: 7af621c4fef4e5d37e0fc62b9fd4382cc1698d89", "pr_number": "57290", "files_changed": ["torch/fft/__init__.py"], "labels": ["Merged", "cla signed", "open source"]}, "2dffa8cdf8": {"title": "Fix CUDA Stream synchronization when arguments contains RRefs (#57394)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57394\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D28131325\n\nPulled By: mrshenli\n\nfbshipit-source-id: 7174942d4c8dabe13f8eb1ba7fea599922a022c0", "pr_number": "57394", "files_changed": ["torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "58bc003487": {"title": "Add pybind type caster for c10::Device (#57292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57292\n\nIn Future (and soon in other places too) we need to receive a list of devices from Python-land. We don't want to just take their indices because we need full devices in order to infer the type from them. torch.device is not defined through pybind, it's defined through a plain `PyModule_AddObject` call with CPython, thus pybind isn't naturally able to understand and convert it. However we can provide a custom type caster which fixes that. We have this already for at::Tensor, at::Generator, ...\nghstack-source-id: 127916268\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28092732\n\nfbshipit-source-id: 1c31d0b85a4d5c9e7bde8161efbb7574d505157c", "pr_number": "57292", "files_changed": ["torch/csrc/jit/python/init.cpp", "torch/csrc/utils/pybind.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6697ef51b2": {"title": "Add device() method to c10::Event (#57293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57293\n\nIt's just a nice convenience.\nghstack-source-id: 127916265\n\nTest Plan: It builds\n\nReviewed By: mrshenli\n\nDifferential Revision: D28092731\n\nfbshipit-source-id: 99c8c33fd6e245915f2ed0c0482de132d7c75bf5", "pr_number": "57293", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "c10/core/Event.h"], "labels": ["Merged", "cla signed"]}, "0c3e79b5b9": {"title": "Rename DeviceGuardImplInteface's getStreamFromPool method (#57345)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57345\n\nAlready back in https://github.com/pytorch/pytorch/pull/57046 we realized that calling this method `getStreamFromPool` could cause issues because that name gets HIPified and thus in some callsites we'd end up calling a method that doesn't exist. In the end we got away with it because the places where we were calling that method weren't HIPified. However in the next PR we'll use this method inside RPC, and that will start causing problems, hence here I rename it to something that should not cause conflicts. This is a private API (since it's inside `impl`) thus there's no backwards compatibility concerns.\nghstack-source-id: 127916484\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28114923\n\nfbshipit-source-id: e027ad08a8e02090c08c6407c2db5a7fde104812", "pr_number": "57345", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h", "c10/core/impl/DeviceGuardImplInterface.h", "c10/core/impl/VirtualGuardImpl.h", "c10/cuda/impl/CUDAGuardImpl.h"], "labels": ["Merged", "cla signed"]}, "0422e67336": {"title": "Use Devices instead of DeviceIndexes in TensorPipe agent (#57294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57294\n\nWith the advent of CPUs in the device maps, and to be more generic (e.g., to support AMD GPUs), and to avoid conversions when passing to Future and RRef and such, it's easier to use Devices instead of DeviceIndices. This started by just migrating the TensorPipe agent but the RPC layer is quite intertwined so I had to migrate a lot of stuff.\nghstack-source-id: 127916562\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28092733\n\nfbshipit-source-id: 024dcb3648c5898ab13e770413c43958f04f1a8a", "pr_number": "57294", "files_changed": ["test/cpp/rpc/e2e_test_base.h", "test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/_C/_distributed_rpc.pyi", "torch/csrc/distributed/autograd/functions/recvrpc_backward.cpp", "torch/csrc/distributed/autograd/functions/recvrpc_backward.h", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.cpp", "torch/csrc/distributed/autograd/rpc_messages/rpc_with_autograd.h", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/autograd/utils.h", "torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.h", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.cpp", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.h", "torch/csrc/distributed/rpc/utils.cpp", "torch/csrc/distributed/rpc/utils.h", "torch/distributed/rpc/backend_registry.py", "torch/distributed/rpc/options.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "fb7469fb7f": {"title": "Use Devices instead of DeviceIndexes in Future (#57353)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57353\n\nEven though we merged CUDAFuture into ivalue::Future, the resulting methods still had basically two distinct codepaths (i.e., an \"early exit\" if `impl_ == nullptr` for CPU, and then some code for CUDA). This works but it risks creating divergence and inconsistencies when the same class is used in those two modes. Ideally we should have the same codepath, and have the stream operations be no-ops for CPU. Luckily, this is exactly what happens when using a CPU DeviceGuardImplInterface!\n\nHence here I do that, and for convenience I also use c10::Devices instead of c10::DeviceIndexes (like we did in https://github.com/pytorch/pytorch/pull/57294 for RPC).\nghstack-source-id: 127920097\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28100525\n\nfbshipit-source-id: cfac73894220ef5fa8a0389b5533c5d69ba1cf04", "pr_number": "57353", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "82d245faef": {"title": "Inline hooks in ivalue::Future (#57354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57354\n\nThe ivalue::Future class used to have some hooks, defined as separate protected virtual methods, so that they could be overridden by the CUDAFuture subclass. Now that CUDAFuture has been merged into ivalue::Future those hooks can be \"inlined\" to where they're used, hopefully making the code more readable as it puts related things closer together.\nghstack-source-id: 127920096\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28117199\n\nfbshipit-source-id: f749cd842c3bdc44a08f0a33bef972dfbf08afdd", "pr_number": "57354", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "4350d4af77": {"title": "Immediately mark DLPack capsule as used after stealing the ownership (#56789)", "body": "Summary:\nAfter stealing the ownership of the tensor passed via DLPack capsule, PyTorch should immediately mark it as used (by changing its name to `used_dltensor`). This fix is needed because the following line may raise an exception:\n\n```cpp\npy::module::import(\"torch.cuda\").attr(\"init\")();\n```\n\nWhen an exception is raised, Tensor created by `at::fromDLPack` calls the `deleter`. However as the causple is not consumed, the producer (a library that created the capsule) also calls the `deleter`, causing a double free.\n\nReprodcuer (I'm running this code on A100 GPU + PyTorch wheel which does not include `sm_80` support; in this configuration `torch.cuda.init` will raise a warning):\n```py\n$ python -Werror\n>>> import torch.utils.dlpack\n>>> import cupy\n>>> tensor = torch.utils.dlpack.from_dlpack(cupy.arange(10).toDlpack())\nfree(): double free detected in tcache 2\nzsh: abort (core dumped)  python -Werror\n```\n\nOnce this fix is merged users can now see the exception correctly:\n\n```\nA100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\nIf you want to use the A100-PCIE-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56789\n\nReviewed By: astaff\n\nDifferential Revision: D28118512\n\nPulled By: mruberry\n\nfbshipit-source-id: 56992f7a3fc78d94c69513e864a473ae9587a9c8", "pr_number": "56789", "files_changed": ["torch/csrc/Module.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "6d681d064f": {"title": "ROCM: Re-enable test_norm_fro_2_equivalence_old (#57170)", "body": "Summary:\nThis test was disabled for ROCM 3.9. With latest updates, the test is passing in ROCM 4.1. Hence enabling this test in test/test_linalg.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57170\n\nReviewed By: astaff\n\nDifferential Revision: D28118217\n\nPulled By: mruberry\n\nfbshipit-source-id: 1b830eed944a664c3b1b3e936b87096fef0c0ca2", "pr_number": "57170", "files_changed": ["test/test_linalg.py"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "a5288a0244": {"title": "Sparse support for division rounding_mode argument (#51989)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/51989\n\nTest Plan: Imported from OSS\n\nReviewed By: astaff\n\nDifferential Revision: D28118114\n\nPulled By: mruberry\n\nfbshipit-source-id: 2a76ee55c3845552e57e93d54628ce3c2fab3399", "pr_number": "51989", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "test/test_sparse.py"], "labels": ["Merged", "cla signed", "module: sparse", "open source"]}, "7c8d0069c4": {"title": "grad_fn getter for optional strings (#55225)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/55225\n\nTest Plan: Imported from OSS\n\nReviewed By: astaff\n\nDifferential Revision: D28118113\n\nPulled By: mruberry\n\nfbshipit-source-id: 711723922cff3afa220e03d926cee5884e167706", "pr_number": "55225", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_autograd_functions.py"], "labels": ["Merged", "cla signed", "open source"]}, "2be115336b": {"title": "Fix torch.ormqr for non Fortran-contiguous inputs (#57314)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57314\n\nTest Plan: Imported from OSS\n\nReviewed By: astaff\n\nDifferential Revision: D28118029\n\nPulled By: mruberry\n\nfbshipit-source-id: e2ef65093cc5f77769adc7066c76f0607b5559a9", "pr_number": "57314", "files_changed": ["aten/src/TH/generic/THTensorLapack.cpp", "test/test_linalg.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "41099ef71c": {"title": "OpInfo: mvlgamma (#56907)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56907\n\nReviewed By: astaff\n\nDifferential Revision: D28118669\n\nPulled By: mruberry\n\nfbshipit-source-id: f54ad6dc64ddb6bcfca5c5c7fd8f395cd9761128", "pr_number": "56907", "files_changed": ["test/test_torch.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "0ecdbfebff": {"title": "s/InplaceOrView/ADInplaceOrView/g (#57372)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57372\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57324\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D28121821\n\nPulled By: ailzhang\n\nfbshipit-source-id: f568dd2505f6279da9ffb93ce1d22e0f98c606bb", "pr_number": "57372", "files_changed": ["BUILD.bazel", "aten/src/ATen/TensorIndexing.cpp", "aten/src/ATen/TracerMode.h", "aten/src/ATen/core/LegacyTypeDispatch.h", "aten/src/ATen/core/VariableFallbackKernel.cpp", "aten/src/ATen/cpp_custom_type_hack.h", "aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/templates/Functions.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/InferenceMode.cpp", "c10/core/InferenceMode.h", "c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h", "c10/core/impl/LocalDispatchKeySet.cpp", "caffe2/CMakeLists.txt", "test/cpp/api/grad_mode.cpp", "test/cpp/api/inference_mode.cpp", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/gen_variable_factories.py", "tools/autograd/gen_variable_type.py", "tools/autograd/templates/ADInplaceOrViewType.cpp", "tools/autograd/templates/InplaceOrViewType.cpp", "tools/build_variables.bzl", "torch/csrc/autograd/VariableTypeManual.cpp", "torch/csrc/autograd/VariableTypeUtils.h", "torch/csrc/autograd/python_variable_indexing.cpp", "torch/csrc/jit/codegen/cuda/executor.cpp", "torch/csrc/jit/frontend/tracer.cpp", "torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "bb640efa40": {"title": "ns for fx: add missing add_relu and mul_relu patterns (#56927)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56927\n\nAdds the connection of `torch.add` to `toq.add_relu` and of `torch.mul`\nto `toq.mul_relu`.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D28003475\n\nfbshipit-source-id: a12871feacf84c5afb0e1cc47e708e285695ffeb", "pr_number": "56927", "files_changed": ["torch/quantization/ns/pattern_utils.py"], "labels": ["Merged", "cla signed"]}, "ce4449918a": {"title": "Port reverse binary ops to `OpInfo` (#56471)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54296\nTracking Issue https://github.com/pytorch/pytorch/issues/54261\n\n**Summary:**\n- `rsub` (aten function) was already ported\n- Ported tests for its dunder version: `__rsub__`\n- Ported tests for the other dunder functions: `__radd__`, `__rmul__`, `__rdiv__`, `__rpow__`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56471\n\nReviewed By: ngimel\n\nDifferential Revision: D28142843\n\nPulled By: mruberry\n\nfbshipit-source-id: 3d1bd88a4f124774f48d33a7ca7bfc7f796360df", "pr_number": "56471", "files_changed": ["test/test_fx.py", "test/test_fx_experimental.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "5c68072ee8": {"title": "add support for complex input to `torch.testing.assert_(equal|close)` (#57162)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57162\n\nReviewed By: ngimel\n\nDifferential Revision: D28141902\n\nPulled By: mruberry\n\nfbshipit-source-id: fd35e73e10167e3e44da4daf6582183bc4a0de7f", "pr_number": "57162", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "4a872f8539": {"title": "Add cross OpInfo (#55483)", "body": "Summary:\nOne of the tasks in https://github.com/pytorch/pytorch/issues/54261.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55483\n\nReviewed By: ngimel\n\nDifferential Revision: D28143091\n\nPulled By: mruberry\n\nfbshipit-source-id: 0b98226a1811f61cb90d2248dd4425135a096551", "pr_number": "55483", "files_changed": ["test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed", "hackathon"]}, "46a32e075c": {"title": "Improve BatchNorm1d training performance (CPU) (#57033)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57033\n\nCPU part of gh-38915\n\nBatchNorm1d is implemented by looping over the channels, selecting one channel\nat a time and performing cpu_serial_kernel loops per-channel. For (N, C)\ncontiguous layout this results in a sub-optimal strided memory access pattern;\nguarunteeing no elements will ever be in the same cache line.\n\nI fix this by passing the entire input into one `TensorIterator` and letting\nit decide which dimensions to iterate over and how to divide work among threads.\n\nFor statistic updates and the backward function, I use `at::mean` and `at::sum`\ninstead of the ad-hoc reductions there. Not only does this allow better memory\naccess patterns, it also enables vectorization and so performance improves for\nBatchNorm2d as well. Unfortunately, `at::var` and `at::var_mean` don't perform\nas well so I've left the other reductions as they were.\n\nOverall, on my machine this takes the 1d example from 24 ms down to 4 ms and\nthe 2d example from 2.5 ms down to 2 ms.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D28142333\n\nPulled By: ngimel\n\nfbshipit-source-id: 066fe4f37f29b6458005e513e85faa398eeb9e2d", "pr_number": "57033", "files_changed": ["aten/src/ATen/native/Normalization.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "e845158b1a": {"title": "Assert that GIL is not held in blocking destructors (#57030)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57030\n\nPR #57029 is not perfect; there are still obscure situations in which\nwe might allocate a shared_ptr to an RpcAgent that doesn't have a\nno GIL constructor, so this PR adds the other half of the equation:\nassert that we don't hold the GIL when running a blocking destructor.\nThis makes it possible to detect potential deadlocks even if the\ncode doesn't deadlock in practice (because you got lucky and none\nof the threads you blocked on tried to also take out the GIL).\n\nI considered whether or not to make this DEBUG_ONLY.  For now it's\nnot, so I can get better CI coverage, and because this test only\nhappens in destructors of objects that die rarely.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D28030582\n\nPulled By: ezyang\n\nfbshipit-source-id: a7d7f6545223c4823c7f6036dfe29bd2edaf60a5", "pr_number": "57030", "files_changed": ["c10/util/DeadlockDetection.cpp", "c10/util/DeadlockDetection.h", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "154eca0309": {"title": "OpInfo: ravel, view, view_as (#56910)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56910\n\nReviewed By: ngimel\n\nDifferential Revision: D28141867\n\nPulled By: mruberry\n\nfbshipit-source-id: bff49d40d7e3bb36bc83d1405bd77f5529eeffe9", "pr_number": "56910", "files_changed": ["test/test_fx.py", "test/test_fx_experimental.py", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "87242d2393": {"title": "Eliminate global usage of torch.set_default_dtype in test_autograd (#56446)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56446\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D28000589\n\nPulled By: mruberry\n\nfbshipit-source-id: c8fb2907d656138e72ecf8fb3e572591f8972900", "pr_number": "56446", "files_changed": ["test/test_autograd.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "0a0e024648": {"title": "use importlib instead of imp as it support python 3.5+ (#57160)", "body": "Summary:\nPrevent some annoying deprecation warning when importing cpp_extensions\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57160\n\nReviewed By: astaff\n\nDifferential Revision: D28096751\n\nPulled By: albanD\n\nfbshipit-source-id: f169ad4c4945b0fff54c0339052a29f95b9f1831", "pr_number": "57160", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed"]}, "f332a8bdff": {"title": "Implement result() function in MPI Work classes (#57168)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57168\n\nImplement result() for MPI which wasn't previously supported.\n\nSome user rely on output args, however in future usecases (e.g. DDP comm hook) we need to return the result explicitly.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D28129125\n\nPulled By: agolynski\n\nfbshipit-source-id: d6abcd2114163471c045043534a0a3377f2579b4", "pr_number": "57168", "files_changed": ["torch/lib/c10d/ProcessGroup.hpp", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupMPI.hpp", "torch/lib/c10d/test/ProcessGroupMPITest.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c7d8d8f925": {"title": "[BE] Improve has_bf16_support (#57408)", "body": "Summary:\nUse `functools.lru_cache` to avoid calling this function multiple time\nCheck that we are running on Linux platform before trying to open\n\"/proc/cpuinfo\"\nDo not spawn new process, but simply open(\"/proc/cpuinfo\").read() and\nsearch the output for the keywords\n\nFixes https://github.com/pytorch/pytorch/issues/57360\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57408\n\nReviewed By: driazati\n\nDifferential Revision: D28136769\n\nPulled By: malfet\n\nfbshipit-source-id: ab476774c3be2913cb576d98d47a2f7ec03c19aa", "pr_number": "57408", "files_changed": ["test/test_mkldnn.py"], "labels": ["Merged", "cla signed"]}, "c0d39ba680": {"title": "Replace 11.2 linux CI with 11.3 (#57222)", "body": "Summary:\nLet's see how 11.3 holds up!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57222\n\nTest Plan: CUDA 11.3 has passed build and test below.\n\nReviewed By: malfet\n\nDifferential Revision: D28152554\n\nPulled By: janeyx99\n\nfbshipit-source-id: 84b687660b9a5b6337b65d6aaaaf003ea94b2864", "pr_number": "57222", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml"], "labels": ["Merged", "cla signed"]}, "264d87985a": {"title": "Use ld.gold by default to link in CI (#57061)", "body": "Summary:\nThis adds an option to CMake to use `ld.gold` to link rather than `ld` (which symlinks to `ld.bfd` on Ubuntu by default). This shouldn't change any functionality, only a mild improvement on link times during builds (shaves off 1 minute) on CI.\n\nVerify by searching for `ld.gold is available` in [the logs](https://circleci.com/api/v1.1/project/github/pytorch/pytorch/13046834/output/105/0?file=true&allocation-id=608c434338107e5b6cf938a1-0-build%2F7BDA2FF1)\n](https://our.intern.facebook.com/intern/diff/28123522/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57061\n\nPulled By: driazati\n\nReviewed By: janeyx99\n\nDifferential Revision: D28123522\n\nfbshipit-source-id: 5a60798ca4785427fd92bbf3b3aa5f63730e9b20", "pr_number": "57061", "files_changed": [".circleci/scripts/binary_populate_env.sh", "CMakeLists.txt", "cmake/Summary.cmake"], "labels": ["Merged", "ci/binaries", "cla signed"]}, "ac71432c54": {"title": "[PyTorch][Edge] Add api to get bytecode version from runtime (#56948)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56948\n\nAdd api to get runtime bytecode version\n\n## Test\nBoth `caffe2/test/cpp/jit/test_lite_interpreter.cpp` and `caffe2/test/mobile/test_bytecode.py` pass\nghstack-source-id: 127939889\n\nTest Plan: Both `caffe2/test/cpp/jit/test_lite_interpreter.cpp` and `caffe2/test/mobile/test_bytecode.py` pass\n\nReviewed By: raziel, iseeyuan\n\nDifferential Revision: D27987811\n\nfbshipit-source-id: 35ed9bd626aecffc226f6dacfa046e6cdabfed51", "pr_number": "56948", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "tools/build_variables.bzl", "torch/csrc/jit/mobile/runtime_compatibility.cpp", "torch/csrc/jit/mobile/runtime_compatibility.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a6f60cf4f0": {"title": "[12/n] Rename last_keep_alives to last_heartbeats in _RendezvousState (#57141)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57141\n\nPer feedback this PR renames `last_keep_alives` to `last_heartbeats` in `_RendezvousState`.\nghstack-source-id: 127629442\n\nTest Plan: Run the updated unit tests.\n\nReviewed By: tierex\n\nDifferential Revision: D28058948\n\nfbshipit-source-id: 0db12eac56a47a426a7a48fb5c93ac6a08b0d22e", "pr_number": "57141", "files_changed": ["test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py", "torch/distributed/elastic/rendezvous/dynamic_rendezvous.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "233004b4c8": {"title": "[13/n] Extend the return type of RendezvousBackend's set_state method (#57142)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57142\n\nThis PR extends the return type of `RendezvousBackend`'s `set_state` method with an additional boolean flag that specifies whether the write attempt has succeeded.\nghstack-source-id: 127629538\n\nTest Plan: Run the updated unit tests.\n\nReviewed By: tierex\n\nDifferential Revision: D28058980\n\nfbshipit-source-id: 26333790c39386891beb155b20ba1291d2cbdd03", "pr_number": "57142", "files_changed": ["test/distributed/elastic/rendezvous/rendezvous_backend_test.py", "torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", "torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", "torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "1b745efbe8": {"title": "[14/n] Introduce a name attribute to _PeriodicTimer (#57143)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57143\n\nThis PR introduces a `name` attribute in `_PeriodicTimer` for testing and debugging purposes.\nghstack-source-id: 127684751\n\nTest Plan: Run the new and updated unit tests.\n\nReviewed By: tierex\n\nDifferential Revision: D28059045\n\nfbshipit-source-id: 9eb067300aea21a99577e6cd8a354f7eb749f4a6", "pr_number": "57143", "files_changed": ["test/distributed/elastic/rendezvous/utils_test.py", "torch/distributed/elastic/rendezvous/utils.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "e841f335aa": {"title": "[RELAND] [CUDA graphs] Avoid sync errors when graph capturing cudnn rnn calls that use cudnn dropout (#57373)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/pull/56433 was reverted because the test perceived internal dropout state creation as a memory leak. This PR resubmits with the leak check skipped.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57373\n\nReviewed By: anjali411\n\nDifferential Revision: D28152186\n\nPulled By: ezyang\n\nfbshipit-source-id: 9a593fcdbbabbb09dc4e4221191663e94b697503", "pr_number": "57373", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp", "test/test_cuda.py"], "labels": ["Merged", "cla signed", "module: cuda graphs", "open source", "triaged"]}, "160304a81d": {"title": "fix comments in ATenNVRTC.h (#57318)", "body": "Summary:\nAdding a function in ATenNVRTC.h also requires changing Lazy NVRTC.cpp, but this was missing in the comments.\nAlso fix a typo.\n\nCC jjsjann123\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57318\n\nReviewed By: anjali411\n\nDifferential Revision: D28146223\n\nPulled By: ezyang\n\nfbshipit-source-id: be69241a4b41ac7361a8c9f978fa4c837f41fbd1", "pr_number": "57318", "files_changed": ["aten/src/ATen/cuda/nvrtc_stub/ATenNVRTC.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "33eea146ee": {"title": "torch.clamp with tensor min and max (#52695)", "body": "Summary:\nFixes gh-2793\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52695\n\nReviewed By: mruberry\n\nDifferential Revision: D27395977\n\nPulled By: ezyang\n\nfbshipit-source-id: f86aa240feb034d42e4c45447e72218f6a773c24", "pr_number": "52695", "files_changed": ["aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_torch.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_autograd_functions.py", "tools/pyi/gen_pyi.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/onnx/symbolic_helper.py", "torch/onnx/symbolic_opset11.py", "torch/onnx/symbolic_opset9.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d68ad3cb1e": {"title": "Add a shortcut to test all torchbench models. (#57311)", "body": "Summary:\nThis PR adds a shortcut of specifying all models in TorchBench CI\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57311\n\nTest Plan:\nCI\n\nRUN_TORCHBENCH: ALL\n\nReviewed By: bitfort\n\nDifferential Revision: D28160198\n\nPulled By: xuzhao9\n\nfbshipit-source-id: 67c292bc98868979d868d4cf1e599c38e0da94b5", "pr_number": "57311", "files_changed": [".github/scripts/run_torchbench.py", ".github/workflows/run_torchbench.yml"], "labels": ["Merged", "cla signed"]}, "589072afa1": {"title": "Fix return type of getDeviceMap (#57487)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/pull/57294 changed behaviour to\nreturn `c10::Device` rather that `c10::DeviceIndex`, but missed method bind:\nhttps://github.com/pytorch/pytorch/blob/1a6f827ae6c585be19973f071a6a05ad8f46b6a1/torch/csrc/distributed/rpc/init.cpp#L606-L611\nthat cast return type to map of c10::DeviceIndex rather than\nc10::Device\n\nDo not ignore cast error when compiling this code\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57487\n\nReviewed By: nikithamalgifb\n\nDifferential Revision: D28158750\n\nPulled By: malfet\n\nfbshipit-source-id: d57d869cceca8b7ed06d4d638e2b911da8236ed4", "pr_number": "57487", "files_changed": ["torch/CMakeLists.txt", "torch/csrc/distributed/rpc/init.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "6d3bb01b1a": {"title": "Sequence Blob NVM Reader to Selectively NVMify Ads Embeddings in A*", "body": "Summary:\nThis diff enabled mapping a selected set of Ads embeddings to the T17 host on hierarchical memory (nvmify). To achieve that the following is implemented:\n\n- Allow fo OTHER net to be both onnxified and nvmified\n  - For that an allowlist placement policy is added to the nvmify stack\n  - onnxifi_transform is lightly updated to accept a blacklist of operators based on name\n  - nvm transform is broken into two parts, op replacement, and blob update.\n  - A drived class `SeqBlobNVMReader` is defined which adds the functionality to load blobs to the card or nvm.\n\nTest Plan:\n* Unit test\n* Run predictor replayer: selectively load the following ads embedding to NVM as in `--caffe2_nvm_dram_placement_file=/home/hanli/nvm_allowlist`:\n```\nSPARSE_AD_ACCOUNT_ID\nSPARSE_NEW_AD_ID_COARSE\nSPARSE_NEW_AD_ID_REFINED\nSPARSE_NEW_CAMPAIGN_ID\nSPARSE_NEW_TARGET_ID\nSPARSE_NEW_AD_CLUSTER_ID\nSPARSE_NEW_PAGE_ID\nSPARSE_NEW_STORY_ID\nSPARSE_NEW_VIDEO_ID\nSPARSE_ENTITY_EQUIVALENCE_KEY\nSPARSE_ENTITY_EQUIVALENCE_KEY_NO_CREATIVE\n```\nmajor parameter change in sigrid_remote_predictor_glow_nnpi:\n```\n--caffe2_nets_to_nvmify=DISAGG_ACC_REMOTE_OTHER \\\n--caffe2_nvm_sls_ops=SparseLengthsSumFused8BitRowwise,SparseLengthsWeightedSumFused8BitRowwise,SparseLengthsSumFused4BitRowwise,SparseLengthsWeightedSumFused4BitRowwise,SparseLengthsSum4BitRowwiseSparse \\\n--caffe2_nvm_table_path=/home/hanli/tables/225412100_2870/ \\\n--caffe2_nvm_dram_placement_file=/home/hanli/nvm_allowlist \\\n--caffe2_nvm_dram_placement_policy=by_file_allowlist \\\n--caffe2_predictor_nets_to_load=DISAGG_ACC_REMOTE_OTHER\n```\nIn predictor log, observe that the blobs to be NVMified are transformed in op types, skipped in Onnxifi transform, and deferred loaded and do NVM net transform:\n```\nI0416 09:59:29.550690 662344 Nvmifier.cpp:142] ^[[92mReplacing SparseLengthsSumFused4BitRowwise with NVM variant.^[[0m\nI0416 09:59:29.550701 662344 Nvmifier.cpp:142] ^[[92mReplacing SparseLengthsSumFused4BitRowwise with NVM variant.^[[0m\nI0416 09:59:29.550705 662344 Nvmifier.cpp:142] ^[[92mReplacing SparseLengthsSumFused4BitRowwise with NVM variant.^[[0m\nI0416 09:59:29.550712 662344 Nvmifier.cpp:142] ^[[92mReplacing SparseLengthsSumFused4BitRowwise with NVM variant.^[[0m\nI0416 09:59:29.550715 662344 Nvmifier.cpp:142] ^[[92mReplacing SparseLengthsSumFused4BitRowwise with NVM variant.^[[0m\nI0416 09:59:29.550721 662344 Nvmifier.cpp:142] ^[[92mReplacing SparseLengthsSumFused4BitRowwise with NVM variant.^[[0m\n\n...\nI0416 09:59:31.665369 662344 onnxifi_transformer.cc:1097] Skipping blocklisted op SparseLengthsSumFused4BitRowwiseNVM at pos 770\nI0416 09:59:31.667042 662344 onnxifi_transformer.cc:1097] Skipping blocklisted op SparseLengthsSumFused4BitRowwiseNVM at pos 777\nI0416 09:59:31.667294 662344 onnxifi_transformer.cc:1097] Skipping blocklisted op SparseLengthsSumFused4BitRowwiseNVM at pos 779\nI0416 09:59:31.668828 662344 onnxifi_transformer.cc:1097] Skipping blocklisted op SparseLengthsSumFused4BitRowwiseNVM at pos 786\nI0416 09:59:31.668843 662344 onnxifi_transformer.cc:1097] Skipping blocklisted op SparseLengthsSumFused4BitRowwiseNVM at pos 787\nI0416 09:59:31.669909 662344 onnxifi_transformer.cc:1097] Skipping blocklisted op SparseLengthsSumFused4BitRowwiseNVM at pos 792\n\n...\n\nI0416 10:01:09.087282 662344 Nvmifier.cpp:346]  found the name: table0\nI0416 10:01:09.373975 662344 Nvmifier.cpp:374] ^[[96mSaved /home/hanli/tables/225412100_2870/table0^[[0m\nI0416 10:01:09.376008 662344 Nvmifier.cpp:343]  filename: sparse_nn_sparse_arch_SPARSE_NEW_AD_ID_COARSE_dedicated_13_w_EmbeddingFusedUint4Quantization\n..\n\nI0416 10:11:05.310854 662344 Nvmifier.cpp:161] ^[[95mNVMifying the model.^[[0m\nI0416 10:11:05.310887 662344 Nvmifier.cpp:185]  found the name: table0 for sparse_nn_sparse_arch_SPARSE_NEW_AD_ID_COARSE_dedicated_13_w_EmbeddingFusedUint4Quantization\nI0416 10:11:07.580587 662344 Nvmifier.cpp:185]  found the name: table4 for sparse_nn_sparse_arch_SPARSE_AD_ACCOUNT_ID_dedicated_20_w_EmbeddingFusedUint4Quantization\nI0416 10:11:07.580648 662344 Nvmifier.cpp:185]  found the name: table3 for sparse_nn_sparse_arch_SPARSE_ENTITY_EQUIVALENCE_KEY_dedicated_22_w_EmbeddingFusedUint4Quantization\nI0416 10:11:07.580667 662344 Nvmifier.cpp:185]  found the name: table5 for sparse_nn_sparse_arch_SPARSE_NEW_TARGET_ID_dedicated_29_w_EmbeddingFusedUint4Quantization\nI0416 10:11:07.580682 662344 Nvmifier.cpp:185]  found the name: table2 for sparse_nn_sparse_arch_SPARSE_NEW_AD_ID_REFINED_dedicated_30_w_EmbeddingFusedUint4Quantization\nI0416 10:11:07.580695 662344 Nvmifier.cpp:185]  found the name: table1 for sparse_nn_sparse_arch_SPARSE_NEW_STORY_ID_dedicated_35_w_EmbeddingFusedUint4Quantization\n\n```\nMake sure model is properly loaded:\n```\nI0415 21:42:48.400249 873685 ModelManagerBase.cpp:806] Loaded 225412100_2870 in 730944 ms (63800 ms of IO)  memory used 8744167456 byte(s)\n```\n* Only load user embedding to NVM to make sure baseline use case is not broken by this diff:\n```\n--caffe2_nets_to_nvmify=DISAGG_ACC_REMOTE_REQUEST_ONLY \\\n--caffe2_nvm_sls_ops=SparseLengthsSumFused8BitRowwise,SparseLengthsWeightedSumFused8BitRowwise,SparseLengthsSumFused4BitRowwise,SparseLengthsWeightedSumFused4BitRowwise,SparseLengthsSum4BitRowwiseSparse \\\n--caffe2_nvm_table_path=/home/hanli/tables/225412100_2870/\n```\nMake sure model is loaded:\n```\nLoaded 225412100_2870 in 381139 ms (56313 ms of IO)  memory used 7043933560 byte(s)\n```\n* Run feed replayer: `buck-out/gen/sigrid/feed/prediction_replayer/fully_remote_replayer_main --use_new_encoding_for_ads_services --use_new_encoding_from_model_id_to_shard_id --request_file_path /data/users/hanli/f266405843.requests --model_id=265540157_0 --replayer_thread_count=30 --sigrid_predictor_single_host=2401:db00:272c:602e:face:0:10:0 --sigrid_predictor_single_port=7444 --num_iterations=5 --qps=100 --client_name=predictor_v1` (load predictor as in P411172400)\nOutput:\n```\nI0428 21:20:25.106635 1396182 FullyRemoteReplayer.cpp:107] Loading requests from /data/users/hanli/f266405843.requests\nI0428 21:20:25.547982 1396182 FullyRemoteReplayer.cpp:109] Requests size : 6699\nI0428 21:20:25.548146 1396182 Client.cpp:274] V1 tier name:  V2 tier name: sigrid.predictor.fully_remote_test V2 fully remote tier name:\nI0428 21:20:25.548153 1396182 Client.cpp:282] [MF] Migration Framework (traffic routing) enabled: false\nI0428 21:20:25.548172 1396182 ModelRemoteStatus.cpp:206] Selection probabilities znode path: /configerator-gz/.prn\nI0428 21:20:25.674162 1396265 ModelRemoteStatus.cpp:612] Found 0 host, 0 shards in predictor tier\nI0428 21:20:25.674181 1396182 ModelRemoteStatus.cpp:557] Refresh sigrid model succeeded: 1\nI0428 21:21:26.252820 1396265 ModelRemoteStatus.cpp:612] Found 0 host, 0 shards in predictor tier\nI0428 21:21:26.252851 1396265 ModelRemoteStatus.cpp:557] Refresh sigrid model succeeded: 1\nI0428 21:22:22.225976 1396182 PredictionReplayer.cpp:67] Previous request took too long, not reaching target QPS\nI0428 21:22:26.252643 1396265 ModelRemoteStatus.cpp:612] Found 0 host, 0 shards in predictor tier\nI0428 21:22:26.252678 1396265 ModelRemoteStatus.cpp:557] Refresh sigrid model succeeded: 1\nI0428 21:23:26.252959 1396265 ModelRemoteStatus.cpp:612] Found 0 host, 0 shards in predictor tier\nI0428 21:23:26.252987 1396265 ModelRemoteStatus.cpp:557] Refresh sigrid model succeeded: 1\nI0428 21:24:26.253135 1396265 ModelRemoteStatus.cpp:612] Found 0 host, 0 shards in predictor tier\nI0428 21:24:26.253166 1396265 ModelRemoteStatus.cpp:557] Refresh sigrid model succeeded: 1\nI0428 21:25:27.252734 1396265 ModelRemoteStatus.cpp:612] Found 0 host, 0 shards in predictor tier\nI0428 21:25:27.252763 1396265 ModelRemoteStatus.cpp:557] Refresh sigrid model succeeded: 1\nI0428 21:26:03.172894 1396182 FullyRemoteReplayer.cpp:59] cpu time p25, p50, p75, p95, p99 9570 13011 16218 20788 24840\nI0428 21:26:03.172927 1396182 FullyRemoteReplayer.cpp:61] wait time p25, p50, p75, p95, p99 11845 15958 19946 26579 31842\nI0428 21:26:03.172940 1396182 FullyRemoteReplayer.cpp:63] wall time p25, p50, p75, p95, p99 16194 20888 25303 31692 37387\n```\n\nReviewed By: ehsanardestani\n\nDifferential Revision: D27701121\n\nfbshipit-source-id: e898abc6957c839e402a9763172cf85d9bb84cbd", "pr_number": null, "files_changed": ["caffe2/opt/glow_net_transform.cc", "caffe2/opt/glow_net_transform.h"], "labels": []}, "c0309af1f3": {"title": "Actually report mac stats (#57511)", "body": "Summary:\nGive credentials to pytorch mac tests in CI so that test reports can be uploaded to S3.\n\nMaster runs have not been uploaded to S3 as the credentials were missing. https://app.circleci.com/pipelines/github/pytorch/pytorch/311990/workflows/2b2fbb72-b613-4986-8842-eccd93e7cdae/jobs/12945609/steps\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57511\n\nReviewed By: samestep\n\nDifferential Revision: D28165041\n\nPulled By: janeyx99\n\nfbshipit-source-id: a4a9c793029838bdab41af19dbce1c8c49f7122d", "pr_number": "57511", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", "tools/print_test_stats.py"], "labels": ["Merged", "cla signed"]}, "15975cf6a6": {"title": "To add priority of int/int? over int[] on signature matching and adding {h,v,d}split methods (#57346)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54555\n\nIt has been discussed in the issue https://github.com/pytorch/pytorch/issues/54555 that {h,v,d}split methods unexpectedly matches argument of single int[] when it is expected to match single argument of int. The same unexpected behavior can happen in other functions/methods which can take both int[] and int? as single argument signatures.\n\nIn this PR we solve this problem by giving higher priority to int/int? arguments over int[] while sorting signatures.\n\nWe also add methods of {h,v,d}split methods here, which helped us to discover this unexpected behavior.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57346\n\nReviewed By: ezyang\n\nDifferential Revision: D28121234\n\nPulled By: iramazanli\n\nfbshipit-source-id: 851cf40b370707be89298177b51ceb4527f4b2d6", "pr_number": "57346", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "tools/autograd/gen_python_functions.py", "torch/_tensor_docs.py"], "labels": ["Merged", "cla signed"]}, "4143483d95": {"title": "[RPC Framework] Create a separate remote module template when moving CPU tensors to a cuda device is not enabled (#57413)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57413\n\nAn internal test fails because somehow `Tuple[()]` is not considered compatible with `Tuple[Any]` in TorchScript, even if the code that involves this type of variables is not executed at all.\n\nTherefore, create separate templates for instantiation to avoid typing check failure. This can address the FIXME left in https://github.com/pytorch/pytorch/pull/57288\n\n#Closes: https://github.com/pytorch/pytorch/issues/51670\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- RemoteModule -j 1\n\nbuck test mode/dev-nosan caffe2/torch/fb/training_toolkit/applications/sparse_nn/batch_distributed_inference/tests:batch_distributed_inference_test -- test_load_di_parts\n\nReviewed By: wanchaol\n\nDifferential Revision: D28138864\n\nfbshipit-source-id: 39e3e67b0c3979b607ff104d84b4fb1070ffefd6", "pr_number": "57413", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/distributed/nn/jit/instantiator.py", "torch/distributed/nn/jit/templates/remote_module_template.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "1d3a9bff3c": {"title": "Swap CUDA 10.1 and CPU CI for windows (#57493)", "body": "Summary:\nThis change temporarily disables CUDA testing on PRs, but keeps it on master.\nThis is likely to increase the number of reverts, but this is necessary as a stop-gap measure to cap the CI costs growth.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57493\n\nReviewed By: seemethere\n\nDifferential Revision: D28162697\n\nPulled By: janeyx99\n\nfbshipit-source-id: 1bc529a405f7d63c07f4bd9f8ceca8da450743fc", "pr_number": "57493", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", ".jenkins/pytorch/win-test.sh"], "labels": ["Merged", "cla signed", "module: ci", "module: windows"]}, "75f6dcf8b5": {"title": "protect destructors of python bindings that can be kept alive by c++ objects (#57488)", "body": "Summary:\nSuch a deadlock was found for PyFunctionPreHook after adding https://github.com/pytorch/pytorch/pull/57057\nThis is fixing all occurrences in torch/csrc/autograd\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57488\n\nReviewed By: malfet\n\nDifferential Revision: D28163321\n\nPulled By: albanD\n\nfbshipit-source-id: 4daf1db69674e73967fc7c5ca2a240c61340e7ca", "pr_number": "57488", "files_changed": ["torch/csrc/autograd/python_anomaly_mode.h", "torch/csrc/autograd/python_engine.cpp", "torch/csrc/autograd/python_function.h", "torch/csrc/autograd/python_hook.cpp"], "labels": ["Merged", "cla signed"]}, "5c7e35c689": {"title": "[RPC Framework] Clang-format remote_module.py and instantiator.py (#57414)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57414\n\nghstack-source-id: 127927609\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28138870\n\nfbshipit-source-id: 04894abaf2e713dc559cd9795197f85539b25e17", "pr_number": "57414", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/distributed/nn/jit/instantiator.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "67f874de8f": {"title": "[resubmit] Remove sync for randperm on small tensors. (#54113) (#57364)", "body": "Summary:\n- [x] check MaskRCNN\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57364\n\nReviewed By: anjali411\n\nDifferential Revision: D28166385\n\nPulled By: ngimel\n\nfbshipit-source-id: 42804b52cc837a95fc1d7ea49b430b55598be7bb", "pr_number": "57364", "files_changed": ["aten/src/ATen/native/cuda/Randperm.cu", "aten/src/ATen/native/cuda/Randperm.cuh", "aten/src/ATen/test/cuda_distributions_test.cu", "test/test_tensor_creation_ops.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "3cc733e451": {"title": "fix for nvtxstring not printing name for aten kernels (#57407)", "body": "Summary:\naten kernels have a sequence number of -1\n\nIn order to ensure the names are properly printed in every case, we must change the >= 0 to => -1\n\nExample of bug:\n![Capture](https://user-images.githubusercontent.com/20074092/116767312-45959280-a9e4-11eb-92a3-c2236a00d481.PNG)\nExample of fix:\n![image](https://user-images.githubusercontent.com/20074092/116919709-82d96a80-ac06-11eb-8b74-e34cf1214ea5.png)\nAdditionally, while fixing and investigating this issue another issue was detected and has now been filed:\nhttps://github.com/pytorch/pytorch/issues/57476\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57407\n\nReviewed By: anjali411\n\nDifferential Revision: D28165818\n\nPulled By: ngimel\n\nfbshipit-source-id: dd3d245f1ea23c4b2edfcedbed3b47705ec1e966", "pr_number": "57407", "files_changed": ["torch/csrc/autograd/profiler_legacy.cpp"], "labels": ["Merged", "cla signed", "open source"]}, "3db45bcb91": {"title": "Compilation error fix for torch/csrc/distributed/rpc/init.cpp (#57500)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57500\n\nTest Plan: Imported from OSS\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28162887\n\nPulled By: agolynski\n\nfbshipit-source-id: b6fafd64778fc09a5e832b0a557ae70f06951454", "pr_number": "57500", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/tensorpipe_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5439977352": {"title": "[Static Runtime] Revamp op schema check (#57521)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57521\n\nWhen an op is added to static runtime, we manually check the schema (not with the jit schema check, more with IValue.IsTensor()/IsInt() etc) and make sure it's the one we do support. If the schema doesn't match, SR would throw an exception with TORCH_CHECK, which makes the entire graph invalid for SR.\n\nThis diff tries to make the op with unsupported schema to use the fallback path and make it go through the dispatcher instead:\n\n```\n  if (node->kind() != prim::ListConstruct &&\n      node->kind() != prim::TupleConstruct &&\n      node->kind() != prim::DictConstruct && node->kind() != prim::ListUnpack) {\n    const Operator& op = node->getOperator();\n    TORCH_CHECK(op.hasOperation());\n    op_ = op.getOperation(node);\n    VLOG(1) << \"Fallback interpreter for node: \" << PrintNode(node);\n  }\n```\n\nThe 2-arg `torch.norm`, which the SR `torch.norm impl doesn't support (only 3, 4, 5 args are supported), now can run in static runtime with fallback mode.\n\n(Note: this ignores all push blocking failures!)\n\nReviewed By: ajyu\n\nDifferential Revision: D27531447\n\nfbshipit-source-id: 0a9c2662ac73ed0393a23cc3a2c7df45fdb00fdd", "pr_number": "57521", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/fusion.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "dc49299078": {"title": "Allow passing cpu to CUDA RPC device maps (#57019)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57019\n\nBased on https://github.com/pytorch/pytorch/pull/56043\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D28169796\n\nPulled By: beauby\n\nfbshipit-source-id: 7fcf623de07c74c4f1ab415b7e20b518876a567a", "pr_number": "57019", "files_changed": ["torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/distributed/rpc/api.py", "torch/distributed/rpc/options.py", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "03b5d87980": {"title": "fix(docs): `torch.add` and `torch.mul` (#54672)", "body": "Summary:\nfixes https://github.com/pytorch/pytorch/issues/39425\nhttps://11813267-65600975-gh.circle-artifacts.com/0/docs/generated/torch.add.html\nhttps://11813267-65600975-gh.circle-artifacts.com/0/docs/generated/torch.mul.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54672\n\nReviewed By: ailzhang\n\nDifferential Revision: D27328523\n\nPulled By: zou3519\n\nfbshipit-source-id: c804e3312b63ee209fef8bdfd8a92d46a345aa21", "pr_number": "54672", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "open source"]}, "01e4444211": {"title": "Tiny typo fix (#57113)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57113\n\nReviewed By: astaff\n\nDifferential Revision: D28122605\n\nPulled By: zou3519\n\nfbshipit-source-id: dcf30ce38366d62befd784d7b3878c2ad1e3b86b", "pr_number": "57113", "files_changed": ["torch/linalg/__init__.py"], "labels": ["Merged", "cla signed", "open source"]}, "aa5ff7cc91": {"title": "irange for Indexing.cu (#57479)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57479\n\nTest Plan: Sandcastle\n\nReviewed By: walterddr, ngimel\n\nDifferential Revision: D28135714\n\nfbshipit-source-id: 4fe4559b25165c59bd69180bfd439b74cedc0942", "pr_number": "57479", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu"], "labels": ["Merged", "cla signed", "fb-exported"]}, "f4a921600a": {"title": "[PyTorch, Mobile] Serialization format change for source range (#54284)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54284\n\nIn order to bring mobile deployment, via lite interpreter, on feature\nparity with JIT, with respect model level debug information we must make\nmodel level debug information available to mobile runtime.\nAt the moment, model level debug information is stored in SourceRange\nwhich associates node's of graph to where the come from in original\npython source code.\nThis information is serialized as part of debug_pkl and deserialized\nwhen JIT loads the model and reads the model code.\nOn lite interpreter, we do not have access to all the functionality of\nJIT and hence we cannot load model in the same way as JIT, by reading\ncode, constructing module hierarchy and graph corresponding module\nmethods etc. Instead in, lite interpreter, only bytecode corresonding to\nthe compiled graph, Code, is saved.\nThus in order to annotate OPs in the bytecode with equivalent\nSourceRange information we do the following:\n1. During model serialization, we create a unique tag for each source\nrange of the model.\n2. Create a map of <SourceRange, tag>\n3. During debug_pkl serialization we save tag along with SourceRange, on\ntop of byte offset.\n4. During bytecode generation, the methods of the top module are\nlowered. During this process methods are inlined. In the inlined graph,\nwhen the node of a graph is lowered to bytecode, we query node's source\nrange and look it up against the map.\n5. Resulting source range tag is serialized in module_debug_info.\n6. During model deserialization, we read all the debug_pkl records in\nthe archieve and create a map of <tag, SourceRange>\n7. This map can be used to find source code information.\n\nDuring mobile runtime:\n1. We read all the debug_pkl records and create <tag=debug_handle,\nSourceRange> map.\n   1.1 This map, MobileDebugInfo, is a member of mobile Module.\n2. Interpreter catches appropriate exceptions and sets the thread local\ndebug handle and rethrows the exception.\n3. In Function's run method we catch exception and query current debug\nhandle where the exception happened.\n4. Query MobileDebugInfo with debug handle to retrieve source range and\naugment error with source range info.\n\nThis information is still incomplete as it does not contain entire\ncallstack.\n\nIn the following diffs we will serialize InlinedCallStack directly.\n\nNote that compilation is gated by SYMBOLICATE_MOBILE_DEBUG_HANDLE macro,\nso that mobile builds can avoid building MobileDebugInfo, source range\nand source range pickler/unpickler. Later we will add path where, if\nbuilding without debug support stack trace will contain only debug\nhandles. They can be symbolicated later.\n\nTest Plan:\nPorted bunch of source range tests from test_jit.py. Added on more test\nin test_lite_interpreter.py\n\nImported from OSS\n\nReviewed By: raziel\n\nDifferential Revision: D27174722\n\nfbshipit-source-id: a7b7c6088ce16dec37e823c7fefa4f0b61047e12", "pr_number": "54284", "files_changed": ["CMakeLists.txt", "caffe2/CMakeLists.txt", "test/mobile/test_lite_script_module.py", "test/test_jit.py", "tools/build_variables.bzl", "torch/csrc/jit/frontend/source_range.cpp", "torch/csrc/jit/frontend/source_range.h", "torch/csrc/jit/mobile/debug_info.cpp", "torch/csrc/jit/mobile/debug_info.h", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/interpreter.cpp", "torch/csrc/jit/mobile/interpreter.h", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/mobile/module.h", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/source_range_serialization.cpp", "torch/csrc/jit/serialization/source_range_serialization.h", "torch/utils/model_dump/__init__.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e0fc473e47": {"title": "[Pytorch, Mobile] Serialize inlined callstack pointer with debug handle. (#55062)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55062\n\nThis diff introduces the following changes:\n1. InlinedCallStack pickler/serializer is introduced. It is serialized\nas a tuple of {module_instance_info, source range tag, callee:InlinedCallStack}\nModule instance info is serialized as tuple of {class_type_name,\ninstance_name}.\nNote that callee of the serialized inlined callstack points to the tuple\nof already serialized callstack. This means the first callstack ptr to\nserialize, will serialize entire path of the tree, where some callee\nnodes might be shared with callstack pointers that will be serialized\nsubsequently. Pickler supports memoization of pickled objects, where if\na tuple has been serialized then object id is obtained instead of\nserialized object again. Thus we stll serialize the tree and not every\npath from the root separately. Furthermore, InlinedCallStackSerializer\nalso uses cache to lookup the pointer and return the serialized IValue.\nFurthermore, note that we must also serialize the source range of\nInlinedCallStack. In order to this serializer requires map of\nsource-range-tags-to-source-range map. This was done in the previous\ndiff, where as part of source range serialization we also generate\nunique tags. These are the tags that are serialized in InlinedCallStack.\nThus during deserialization we would have to deserialize source range\nbefore deserializing InlinedCallStacks.\n2. Furthermore, each serialized InlinedCallStack is serialized with a\nunique debug_handle and source range tag.\nBackendDebugHandleManager manages generation of\nunique debug handles and saves the map of\ndebug-handles-to-{source_range_tag, inlined-callstack-ptr}.\nThis map is then serialized as callstack_debug_map.pkl. Note that\ninlined callstack is not sufficient to get all the source information\nsince it contains source information about the nodes which are inlined.\nThe top-of-the-stack (or bottom) node, which is the actual op node, is\nnot part of the inlined callstack pointer and thus the source range of\nthis node is serialized separately using source_range_tag. This is\nsimilar to how JIT creates callstack in\ntorch/csrc/jit/runtime/interpreter.cpp\n\nUnique debug handles facilitates exception throwing or profiling using\njust the debug handle without any further qualifications, such as which\nfunction or module the inlined-callstack belongs to.\n\nFurthermore, this diff refactors the old mobile code for tracking\nmodule hierarchy information per op. Mainly now bytecode serialization\nwill serialize debug handles corresponding to ops/nodes in graph and\nhave callstack_debug_map.pkl help generate:\n1. Entire callstack and\n2. Module hierarchy information.\n\nTest Plan:\npython test/mobile/test_lite_script_module.py TestLiteScriptModule\n./build/bin/test_jit --gtest_filter=*ModuleInfo\n\nImported from OSS\n\nReviewed By: raziel\n\nDifferential Revision: D27468709\n\nfbshipit-source-id: 53e2413e7703ead01c77718b7c333c7c6ff50a23", "pr_number": "55062", "files_changed": ["test/cpp/jit/CMakeLists.txt", "test/cpp/jit/test_cs_debug_info_serialization.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/mobile/test_lite_script_module.py", "tools/build_variables.bzl", "torch/csrc/jit/backends/backend_debug_handler.cpp", "torch/csrc/jit/backends/backend_debug_handler.h", "torch/csrc/jit/frontend/source_range.h", "torch/csrc/jit/ir/scope.cpp", "torch/csrc/jit/ir/scope.h", "torch/csrc/jit/mobile/debug_info.cpp", "torch/csrc/jit/mobile/debug_info.h", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/method.h", "torch/csrc/jit/mobile/module.cpp", "torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp", "torch/csrc/jit/serialization/callstack_debug_info_serialization.h", "torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import_export_constants.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "bb3c6699a5": {"title": "[Pytorch Mobile DebugInfo Serialization] Save debug handles for all instructions. (#55252)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55252\n\nEarlier for bytecode serialization we were saving debug handles only for OPs and not all\ninstructions. This PR makes changes to add that for all instructions.\n\nTest Plan:\npython test/mobile/test_lite_script_module.py TestLiteScriptModule\n\nImported from OSS\n\nReviewed By: dreiss\n\nDifferential Revision: D27542502\n\nfbshipit-source-id: cff75118c721ce9f0c2f60d2c9471481f05264ca", "pr_number": "55252", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "test/mobile/test_lite_script_module.py", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "5326ec60e6": {"title": "[Inlined Callstack Fix] Fix inlined callstack for blocks of the node. (#56562)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56562\n\nEarlier inlined callstack was annotated only nodes. This left out nodes\nsuch as If which have block of nodes. These nodes should also be updated\nsimilarly.\n\nTest Plan:\nAdded test in test_misc\n\nImported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D27902516\n\nfbshipit-source-id: 4e65c686fa6b4977e8719db45f71f7d2599d4d8e", "pr_number": "56562", "files_changed": ["test/cpp/jit/test_misc.cpp", "torch/csrc/jit/ir/ir.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "34d853a524": {"title": "[fx2trt] example for lowering model to trt with FX based tooling (#57298)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57298\n\nSome of the code is borrowed from NVIDIA-AI-IOT/torch2trt https://github.com/NVIDIA-AI-IOT/torch2trt/tree/master/torch2trt.\n\nMove fx2trt stuff to fx/experimental/fx2trt.\n\nAdd an example in fx/experimental/fx2trt/example/fx2trt_example.py that shows how we lower resnet18 to TensorRT using FX.\n\nTODO: Include license from NVIDIA-AI-IOT/torch2trt\n\nTest Plan: CI\n\nReviewed By: jackm321\n\nDifferential Revision: D28102144\n\nfbshipit-source-id: 1a7b03e45b8ab3fcc355d097d73afeec2efc3328", "pr_number": "57298", "files_changed": ["torch/fx/experimental/fx2trt/__init__.py", "torch/fx/experimental/fx2trt/converter/__init__.py", "torch/fx/experimental/fx2trt/converter/vanilla_converter.py", "torch/fx/experimental/fx2trt/example/fx2trt_example.py", "torch/fx/experimental/fx2trt/fx2trt.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "42d073a7e9": {"title": "Look for unqualified ignore in .pyi, not just .py (#57468)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57468\n\nTest Plan:\nOn the commit that expanded the lints but didn't remove the `# type: ignore` comment, the quick-checks job failed:\n\n- https://github.com/pytorch/pytorch/runs/2493713340\n\nIn contrast, on the tip of this PR, both the quick-checks job and the mypy job succeed:\n\n- https://github.com/pytorch/pytorch/runs/2493744907\n- https://github.com/pytorch/pytorch/runs/2493746144\n\nReviewed By: driazati\n\nDifferential Revision: D28153020\n\nPulled By: samestep\n\nfbshipit-source-id: 5e21bde38ab741e87b3e5f3d45e7e50456fd7ec9", "pr_number": "57468", "files_changed": [".github/workflows/lint.yml", "torch/nn/parallel/scatter_gather.pyi"], "labels": ["Merged", "cla signed"]}, "b3c0ef4a40": {"title": "Revert back to old assert behavior in as_view (#57499)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57499\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D28162814\n\nPulled By: albanD\n\nfbshipit-source-id: e3a970107ab59bb15794f0f82ee12c771caa93d5", "pr_number": "57499", "files_changed": ["torch/csrc/autograd/VariableTypeUtils.h"], "labels": ["Merged", "cla signed"]}, "28c24ec3e8": {"title": "[numpy] polygamma: int -> float promotion (#57462)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/42515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57462\n\nReviewed By: mrshenli\n\nDifferential Revision: D28187104\n\nPulled By: ezyang\n\nfbshipit-source-id: 4072589ad1cb9766e7721d006d43701820922d56", "pr_number": "57462", "files_changed": ["aten/src/ATen/native/Math.h", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "aten/src/ATen/native/native_functions.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "bca1949dc9": {"title": "[typing] suppress errors in `fbcode/caffe2` - batch 2", "body": "Test Plan: Sandcastle\n\nDifferential Revision: D28191118\n\nfbshipit-source-id: 59421c7346903597308b0fdf8a0984f56664fb4f", "pr_number": null, "files_changed": ["caffe2/python/workspace_test.py"], "labels": []}, "383e451036": {"title": "Implement torch.sort with cub::DeviceSegmentedRadixSort (#56821)", "body": "Summary:\nBenchmark:\n```python\nimport torch\nimport itertools\n\nfor i in range(1000):\n    torch.arange(100000, device='cuda')\n\ndef run50_sync(f):\n    for _ in range(50):\n        f()\n    torch.cuda.synchronize()\n\nfor i, j in itertools.product([512, 4096, 8192], repeat=2):\n    print(i,j)\n    t = torch.randn(i, j, device='cuda')\n    torch.cuda.synchronize()\n    %timeit run50_sync(lambda: torch.sort(t))\n    torch.cuda.synchronize()\n    %timeit run50_sync(lambda: torch.sort(t, dim=0))\n    print()\n```\n\nBefore\n```\n512 512\n4.02 ms \u00b1 28.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5 ms \u00b1 15.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n512 4096\n40.7 ms \u00b1 74.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n33.9 ms \u00b1 186 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n512 8192\n71.7 ms \u00b1 636 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n66.4 ms \u00b1 163 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n4096 512\n27.6 ms \u00b1 27.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n46.6 ms \u00b1 101 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n4096 4096\n262 ms \u00b1 1.14 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n321 ms \u00b1 1.32 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n4096 8192\n520 ms \u00b1 5.47 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n661 ms \u00b1 853 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n8192 512\n54.6 ms \u00b1 133 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n83.2 ms \u00b1 320 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n8192 4096\n521 ms \u00b1 1.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n645 ms \u00b1 1.47 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n8192 8192\n1.04 s \u00b1 2.4 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.34 s \u00b1 541 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\nAfter\n```\n512 512\n4.65 ms \u00b1 62.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n5.75 ms \u00b1 62.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n512 4096\n30.3 ms \u00b1 261 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n39.4 ms \u00b1 421 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n512 8192\n59.7 ms \u00b1 344 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n77 ms \u00b1 601 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n4096 512\n32.2 ms \u00b1 376 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n37.1 ms \u00b1 211 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n4096 4096\n204 ms \u00b1 471 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n272 ms \u00b1 1.87 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n4096 8192\n422 ms \u00b1 3.25 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n562 ms \u00b1 4.66 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n8192 512\n63.1 ms \u00b1 595 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n72.7 ms \u00b1 532 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n8192 4096\n401 ms \u00b1 3.08 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n573 ms \u00b1 2.59 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n8192 8192\n831 ms \u00b1 7.86 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n1.2 s \u00b1 9.17 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56821\n\nReviewed By: mrshenli\n\nDifferential Revision: D28172609\n\nPulled By: ngimel\n\nfbshipit-source-id: 87314a6985a84d326304ff5220df5661ef00d710", "pr_number": "56821", "files_changed": ["aten/src/ATen/cuda/cub.cuh", "aten/src/ATen/native/cuda/Sort.cu", "test/test_sort_and_select.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "1fc89d9ffc": {"title": "Use proper Google Analytics id (#56578)", "body": "Summary:\nThis PR fixes the GA id and relies on `pytorch-sphinx-theme`  to set the GA script instead of hard-coding it (this is supported since https://github.com/pytorch/pytorch_sphinx_theme/pull/110 was merged).\n\nSimilar PRs were opened and merged in torchchvision/audio/text, e.g.: https://github.com/pytorch/vision/pull/3700\n\nCC brianjo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56578\n\nReviewed By: mrshenli\n\nDifferential Revision: D28199244\n\nPulled By: ranman\n\nfbshipit-source-id: a20b7fd1b1da3ebff491286c3eeb1410f3c80670", "pr_number": "56578", "files_changed": ["docs/source/_templates/layout.html", "docs/source/conf.py"], "labels": ["Merged", "cla signed"]}, "76d9070d10": {"title": "Replace windows CUDA 11.2 CI with 11.3 (#57223)", "body": "Summary:\nTesting 11.3 with current CI.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57223\n\nTest Plan:\nRelevant CI (11.3) pass!\n\nDisclaimer: Skipped test_inverse_errors_large for CUDA 11.3 as it failed. Issue documented at https://github.com/pytorch/pytorch/issues/57482.\n\nReviewed By: malfet\n\nDifferential Revision: D28169393\n\nPulled By: janeyx99\n\nfbshipit-source-id: 9f5cf7b6737ee6196de92bd80918a5bfbe5510ea", "pr_number": "57223", "files_changed": [".circleci/config.yml", ".circleci/scripts/windows_cuda_install.sh", ".circleci/scripts/windows_cudnn_install.sh", ".circleci/verbatim-sources/workflows/workflows-scheduled-ci.yml", "test/test_linalg.py"], "labels": ["Merged", "cla signed"]}, "00d6472b4d": {"title": "tools: Add render_junit script (#57327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57327\n\nRenders junit results to the console\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D28154514\n\nPulled By: seemethere\n\nfbshipit-source-id: 02e34930b4f0bd257b4e359623b06a4b8f8e996d", "pr_number": "57327", "files_changed": ["tools/render_junit.py"], "labels": ["Merged", "cla signed", "module: ci"]}, "8c9e42baaf": {"title": ".github: Add render_test_results job (#57472)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57472\n\nThis render should put us with some feature parity to the CircleCI web UI renders for j(x)unit test reports, should make it so you don't have to look through a long list of logs to see what tests failed for which job\n\nRender should look somewhat similar to\n![image](https://user-images.githubusercontent.com/1700823/116908744-1bb4b980-abf8-11eb-904c-e93ea4d2f805.png)\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D28154513\n\nPulled By: seemethere\n\nfbshipit-source-id: 02d918b5c4cb6e236b806db48c3debe44de69660", "pr_number": "57472", "files_changed": [".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "2b6c09c11e": {"title": "Add futures to ProcessGroupMPI work (but not including Send/Recv) and python DDP comm hook testing (#57214)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57214\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D28200791\n\nPulled By: agolynski\n\nfbshipit-source-id: 83f814abd4f2eea70e383ed373b04aae8291be55", "pr_number": "57214", "files_changed": ["test/distributed/test_c10d_nccl.py", "torch/lib/c10d/ProcessGroupMPI.cpp", "torch/lib/c10d/ProcessGroupMPI.hpp", "torch/lib/c10d/test/ProcessGroupMPITest.cpp", "torch/testing/_internal/common_distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7175d49122": {"title": "[Dist profiling] Add is_async field (#57253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57253\n\nThis PR:\n\n1. Adds is_async getter/setter to RecordFunction\n2. Adds is_async field to LegacyEvent and KinetoEvent, read from RecordFunction\n3. Modifies python profiler code to check is_async via this flag (and keeps the old thread check as well)\n4. Sets profiling of c10d collectives as async in ProcessGroup.cpp\n5. Modifies tests to ensure is_async is set\n\nThis also fixes flaky tests such as #50840 and #56690 which have been flaky due to the profiling part (https://github.com/pytorch/pytorch/pull/56963 tried to do so as well but this is a better approach).\nghstack-source-id: 128021158\n\nTest Plan: CI\n\nReviewed By: walterddr, ilia-cher\n\nDifferential Revision: D28086719\n\nfbshipit-source-id: 4473db4aed939a71fbe9db5d6655f3008347cb29", "pr_number": "57253", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h", "torch/_C/_autograd.pyi", "torch/autograd/profiler.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/csrc/autograd/profiler_legacy.cpp", "torch/csrc/autograd/profiler_legacy.h", "torch/lib/c10d/ProcessGroup.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "eb39da6b52": {"title": "Always run as many quick-checks steps as possible (#57572)", "body": "Summary:\nThis is essentially a continuation of https://github.com/pytorch/pytorch/issues/56700. Currently, some of the steps in **Lint / quick-checks** (such as the trailing newlines check) still don't always run if an earlier steps fail; example: https://github.com/pytorch/pytorch/runs/2504623867\n\nThis PR adds some more `if`s to remaining steps, so that they, too, can still run even when earlier steps fail.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57572\n\nTest Plan:\n- https://github.com/pytorch/pytorch/runs/2504706736 before this PR, many steps get skipped if an early step fails\n- https://github.com/pytorch/pytorch/runs/2504778437 using this PR's technique, those steps still run\n- https://github.com/pytorch/pytorch/runs/2504787234 if the requirements step doesn't run, steps still get skipped\n- https://github.com/pytorch/pytorch/runs/2504796695 after this PR, `quick-checks` still succeeds\n\nReviewed By: driazati\n\nDifferential Revision: D28205900\n\nPulled By: samestep\n\nfbshipit-source-id: bea856e15bdd17ee66e9ebba019ce91133b17bcd", "pr_number": "57572", "files_changed": [".github/workflows/lint.yml"], "labels": ["Merged", "cla signed"]}, "d728491fc1": {"title": "[RFC] [PyTorch Edge] Simplify error logging in mobile/import.cpp (#55711)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55711\n\nCurrently, there is some complex logic that tries to handle all exceptions but re-throws them as a `c10::Error` so that it can log the error message. I'm looking for context on why this was added. The current logic (after talking with swolchok) seems equivalent, simpler, and also preserves the original stack trace from where the exception was originally thrown. This is useful when viewing the backtrace in logview. Re-throwing an exception using `TORCH_CHECK(false, message)` results in the original exception stack trace getting lost, so we want to avoid that.\nghstack-source-id: 128043281\n\nTest Plan: Build.\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27688352\n\nfbshipit-source-id: b7b1a29b652b31da80d72f16d284e48b8623377b", "pr_number": "55711", "files_changed": ["c10/util/ScopeExit.h", "torch/csrc/jit/mobile/import.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "45a3231bb8": {"title": "[codemod] Enforce proper use of emplacy functions", "body": "Summary: The goal of this diff is enforce proper use of \"emplacy\" functions. In each case, this saves at worst a move constructor call, and at best a full copy of the object (in the case of a constructor call where the object does not have a move constructor).\n\nTest Plan: CI.\n\nReviewed By: marksantaniello\n\nDifferential Revision: D27888714\n\nfbshipit-source-id: 235d0b31066463588c7e4ab86e132c430a352500", "pr_number": null, "files_changed": ["caffe2/core/operator_schema.h"], "labels": []}, "cd9995ae14": {"title": "Update Gloo submodule (#57586)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57586\n\nReviewed By: mrshenli\n\nDifferential Revision: D28210701\n\nPulled By: pbelevich\n\nfbshipit-source-id: 5edce939ee50bc0488e190a61b6fd10c635dff67", "pr_number": "57586", "files_changed": ["third_party/gloo"], "labels": ["Merged", "cla signed"]}, "133d8abbfc": {"title": "Compute nvrtc during libtorch build (#57579)", "body": "Summary:\nThe warning is completely harmless, but it still its nice not to emit it\nwhen it could be computed.\n\nFixes https://github.com/pytorch/pytorch/issues/53350\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57579\n\nReviewed By: walterddr\n\nDifferential Revision: D28208938\n\nPulled By: malfet\n\nfbshipit-source-id: 8dcc3f1bff7c5ed2c0157268c3063228d3c445b6", "pr_number": "57579", "files_changed": ["cmake/public/cuda.cmake"], "labels": ["Merged", "cla signed"]}, "aeaa91bff6": {"title": "mkldnn gelu (#53615)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53615\n\nReviewed By: anjali411\n\nDifferential Revision: D28154396\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 7a9d4d37dc06e54e3249c531a034667b5a2afc46", "pr_number": "53615", "files_changed": ["aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/mkldnn/Relu.cpp", "aten/src/ATen/native/native_functions.yaml", "test/jit/test_freezing.py", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "9ec6883442": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D28216577\n\nfbshipit-source-id: ce31fb98320a31eb947bdd31c68aaafed034df79", "pr_number": null, "files_changed": ["torch/csrc/distributed/rpc/rpc_agent.cpp"], "labels": []}, "a9dc9535f6": {"title": "ns for fx: move relatedness mapping to mappings file (#57171)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57171\n\nNo logic change, just moving the mapping to a file where\nthe other mappings are.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28077978\n\nfbshipit-source-id: 4049d6a498156a5dffe3a03d2f4abc79da7bf907", "pr_number": "57171", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/mappings.py", "torch/quantization/ns/pattern_utils.py"], "labels": ["Merged", "cla signed"]}, "44bb15cfd3": {"title": "ns for fx: add more type to relationship mapping (#57184)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57184\n\nAdd remaining types to the relationship mapping to have full coverage\nof ops quantization knows about, except binary ops and RNNs.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher.test_op_relationship_mapping\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28077979\n\nfbshipit-source-id: 0f6070c8a995032978702d088803f89ff25f2a7f", "pr_number": "57184", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/mappings.py"], "labels": ["Merged", "cla signed"]}, "76f29d53bf": {"title": "ns for fx: change matching to only match known types (#57186)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57186\n\nBefore this PR, we matched any pair of nodes with equal or related\ntypes.\n\nThis PR changes the behavior to only match nodes whose type is in\nthe allowlist (the relatedness mappings). This will prevent matching\nuser defined modules, unless users add them to the mappings.\n\nThis is motivated by a couple of things:\n1. if user defined types are matched, it can break scriptability of the\n   model with loggers attached. This happens whenever the user module\n   has a return type of anything other than a Tensor or a tuple of\n   Tensors.\n2. we tried the past behavior on a couple of models, and it hasn't been\n   useful.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\npython test/test_quantization.py TestFXGraphMatcherModels\npython test/test_quantization.py TestFXNumericSuiteCoreAPIs\npython test/test_quantization.py TestFXNumericSuiteCoreAPIsModels\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28077981\n\nfbshipit-source-id: 0a698e52b807cda47e6923310448a985b26eb362", "pr_number": "57186", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/graph_matcher.py", "torch/quantization/ns/pattern_utils.py"], "labels": ["Merged", "cla signed"]}, "49adac65c4": {"title": "ns for fx: clean up manual string names of related ops (#57210)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57210\n\nRemoves the manually specified string name for sets of\nrelated ops, and replaces it with an automatically generated\nindex. The manual name was arbitrary and ok for an MVP, but\nis not safe for wide usage.\n\nAlso, adds APIs for users to add custom functions to the\nrelatedness map by either pairing it to a known function\nor creating a new relatedness set.\n\nTest Plan:\n```\npython test/test_quantization.py TestFXGraphMatcher\n```\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D28077977\n\nfbshipit-source-id: e64a1ad6cd063014d74cdad189b0a612b1143435", "pr_number": "57210", "files_changed": ["test/quantization/test_numeric_suite_fx.py", "torch/quantization/ns/mappings.py"], "labels": ["Merged", "cla signed"]}, "0787d781c5": {"title": "Fix compatibility problem with LSTMs and torch.save (#57558)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57558\n\nFixes #53359\n\nIf someone directly saves an nn.LSTM in PyTorch 1.7 and then loads it in PyTorch\n1.8, it errors out with the following:\n```\n(In PyTorch 1.7)\nimport torch\nmodel = torch.nn.LSTM(2, 3)\ntorch.save(model, 'lstm17.pt')\n\n(In PyTorch 1.8)\nmodel = torch.load('lstm17.pt')\nAttributeError: 'LSTM' object has no attribute 'proj_size'\n```\n\nAlthough we do not officially support this (directly saving modules via\ntorch.save), it used to work and the fix is very simple. This PR adds an\nextra line to `__setstate__`: if the state we are passed does not have\na `proj_size` attribute, we assume it was saved from PyTorch 1.7 and\nolder and set `proj_size` equal to 0.\n\nTest Plan:\nI wrote a test that tests `__setstate__`. But also,\n\nRun the following:\n```\n(In PyTorch 1.7)\nimport torch\nx = torch.ones(32, 5, 2)\nmodel = torch.nn.LSTM(2, 3)\ntorch.save(model, 'lstm17.pt')\ny17 = model(x)\n\n(Using this PR)\nmodel = torch.load('lstm17.pt')\nx = torch.ones(32, 5, 2)\ny18 = model(x)\n```\nand finally compare y17 and y18.\n\nReviewed By: mrshenli\n\nDifferential Revision: D28198477\n\nPulled By: zou3519\n\nfbshipit-source-id: e107d1ebdda23a195a1c3574de32a444eeb16191", "pr_number": "57558", "files_changed": ["test/test_nn.py", "torch/nn/modules/rnn.py"], "labels": ["Merged", "cla signed"]}, "da8cc355a3": {"title": "Relax tp_new so that it is OK to call (#57544)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57544\n\nInstead of removing tp_new from the superclass (which causes\nsuper().__new__ to not work), I now still install tp_new on the\nsuperclass, but verify that you are not trying to directly\nconstruct _TensorBase.\n\nFixes https://github.com/pytorch/pytorch/issues/57421\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28189475\n\nPulled By: ezyang\n\nfbshipit-source-id: 9397a3842a77f5428d182dd62244b42425bca827", "pr_number": "57544", "files_changed": ["test/test_torch.py", "torch/csrc/autograd/python_variable.cpp"], "labels": ["Merged", "cla signed"]}, "9e7814d539": {"title": "Reland: [StaticRuntime] Use NNC's call_raw API to reduce call overheads. (#57553)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57553\n\nRelanding #57329 (the entire stack) which was reverted because I forgot\nto guard a new test with `ifdef LLVM`.\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D28195048\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 50052a2f20f84940b83d1dd1241c8659ff06e014", "pr_number": "57553", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "fb9a32b7b4": {"title": "[PyTorch][Edge] Add api to get bytecode model version (#56801)", "body": "Summary:\nAdd an api `_get_bytecode_version` to get version number given a bytecode model in both cxx and python, and the input can be both from file path and buffer.\n## Test\nCI (new added unit test will run as part of `pytorch_core-buck`)\n\n1. run test_lite_interpreter.cpp\n2. `python test/mobile/test_bytecode.py`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56801\n\nghstack-source-id: 128169647\n\nTest Plan:\nCI (new added unit test will run as part of `pytorch_core-buck`)\n\n1. run test_lite_interpreter.cpp\n2. `python test/mobile/test_bytecode.py`\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27961417\n\nfbshipit-source-id: f786cc9573d855feecff0b4fe8e5363e25f5728c", "pr_number": "56801", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_interpreter.cpp", "test/mobile/test_bytecode.py", "tools/build_variables.bzl", "torch/_C/__init__.pyi.in", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import.h", "torch/csrc/jit/mobile/model_compatibility.cpp", "torch/csrc/jit/mobile/model_compatibility.h", "torch/csrc/jit/python/script_init.cpp", "torch/csrc/jit/serialization/import.cpp", "torch/csrc/jit/serialization/import_read.cpp", "torch/csrc/jit/serialization/import_read.h", "torch/jit/mobile/__init__.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "17035f6aab": {"title": "Speedup render_junit (#57641)", "body": "Summary:\nJUnitXml.__iadd__() is very slow\nBut since testsuites are flattened anyway in\n`convert_junit_to_testcases` concatenate flattened tests right away\n\nAs result, parsing test-reports folder with 393 files and 25+ test cases\ntakes .5 sec instead of 193 sec\n\nFix typing errors and add script to mypy-strict\n\nPrint warning, rather than abort if xml can not be parsed\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57641\n\nReviewed By: samestep\n\nDifferential Revision: D28224401\n\nPulled By: malfet\n\nfbshipit-source-id: 3efc079c1c0deef8fff5ddf083268885b28418f9", "pr_number": "57641", "files_changed": [".github/workflows/lint.yml", "mypy.ini", "tools/render_junit.py"], "labels": ["Merged", "cla signed"]}, "7115a4b870": {"title": "Clang format ProcessGroupNCCL.cpp (#56840)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56840\n\nPer comments in https://github.com/pytorch/pytorch/pull/56427/files\nghstack-source-id: 128142665\n\nTest Plan: Ci\n\nReviewed By: SciPioneer\n\nDifferential Revision: D27980768\n\nfbshipit-source-id: 0158ae1cfd892ff3385ffa0084dd7ef9de014f8c", "pr_number": "56840", "files_changed": ["torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "15c092b888": {"title": "Revert \"Make grad mode error just a warning (#56401)\" (#57640)", "body": "Summary:\nThis reverts commit 63dac82444cc522f177b801d9f0cd2e22417c2f4.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57640\n\nReviewed By: soulitzer, yuguo68\n\nDifferential Revision: D28223946\n\nPulled By: albanD\n\nfbshipit-source-id: 641b87cff1e2f08162ca8cacae333105e89438f1", "pr_number": "57640", "files_changed": ["test/cpp/api/grad_mode.cpp", "test/test_autograd.py", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "534c457d3d": {"title": "add a standalone extra file loader for pytorch model (#57591)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57591\n\nAccording to dhruvbird we should be able to read a file from pytorch model (which is a zip file) using miniz. This diff added a standalone loader so user can load a JSON (or other type) file in the extra folder of the model. The whole point is to avoid loading pytorch library first, which can be complex (voltron, dynamic loading etc).\n\nWith this the hand tracking inference config (D27937516) can no longer depends on pytorch or use dynamic_pytorch. Previous it uses torch::jit::_load_extra_only_for_mobile which requires pytorch to be loaded first. We want to avoid doing that.\n\nTest Plan: buck test caffe2/fb/dynamic_pytorch:extract_file_test\n\nReviewed By: dhruvbird\n\nDifferential Revision: D28140492\n\nfbshipit-source-id: 2fd1570523841f4c35dc2ad8dfde5f1d396a74fa", "pr_number": "57591", "files_changed": ["third_party/miniz-2.0.8/miniz.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "69e64b2632": {"title": "[Flaky tests] Fix flaky rpc profiling tests (#57517)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57517\n\nFixes the flaky tests https://github.com/pytorch/pytorch/issues/45145\nand https://github.com/pytorch/pytorch/issues/45067.\n\nThe root cause is that it is not the case that all remote events will be\nchildren of the record function remote event, as other events can sometimes be\nprofiled under the hood such as the issue described in\nhttps://github.com/pytorch/pytorch/issues/43868.\n\nWe fix this issue by verifying that the set of events that are children on the\nremote end and children on the local end are the same, without necessarily\nenforcing specific events to be logged.\n\nTested by running the test 1000+ times and verifying it passed. Will also test on CI box before landing\nghstack-source-id: 128200041\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D28166602\n\nfbshipit-source-id: 8145857da4642aef31f360b20db00f4328abe2ca", "pr_number": "57517", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "65fad0ebd2": {"title": "Expand Kineto platform support (ci-all) (#56323)", "body": "Summary:\nExpanding support to all builds\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56323\n\nTest Plan: CI\n\nReviewed By: malfet\n\nDifferential Revision: D28171478\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 16bc752d1be3cbaeda5316f5d8a687ae05a83d22", "pr_number": "56323", "files_changed": ["cmake/Dependencies.cmake", "cmake/TorchConfig.cmake.in", "scripts/xcode_build.rb", "test/test_profiler.py", "third_party/kineto", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h"], "labels": ["Merged", "cla signed"]}, "e5179e960e": {"title": "Share VS Code settings/extensions nicely (#57671)", "body": "Summary:\nThis is a second attempt at https://github.com/pytorch/pytorch/issues/51214. It should achieve the same goals with (as far as I can tell) no disadvantages, but the advantages are a bit less pronounced than in the more dictatorial approach that https://github.com/pytorch/pytorch/issues/51214 took:\n\n- Unfortunately, I was unable to figure out how to include [the `mypy` configuration given in the docstring of `tools.mypy_wrapper.main`](https://github.com/pytorch/pytorch/blob/7115a4b8707115b81417481fb6240617ae934806/tools/mypy_wrapper.py#L81-L89), because as walterddr pointed out, `\"${env:HOME}/miniconda3/envs/pytorch/bin/python\"` is not guaranteed to be correct on everyone's machine:\n  ```json\n  {\n    \"python.linting.enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"python.linting.mypyPath\": \"${env:HOME}/miniconda3/envs/pytorch/bin/python\",\n    \"python.linting.mypyArgs\": [\n      \"${workspaceFolder}/tools/mypy_wrapper.py\"\n    ]\n  }\n  ```\n\n  Importantly, this does not work:\n  ```json\n  \"python.linting.mypyPath\": \"${workspaceFolder}/tools/mypy_wrapper.py\"\n  ```\n  This is because VS Code does not run the given `mypy` command inside of the user's specified virtual environment, so for instance, on my system, setting the `mypy` command to directly call `tools/mypy_wrapper.py` results in using `mypy 0.782` instead of the correct `mypy 0.812`.\n\n  Sadly, [this](https://code.visualstudio.com/docs/editor/variables-reference#_configuration-variables) does not work either, although I'm not sure why:\n  ```json\n  {\n    \"python.linting.mypyPath\": \"${config:python.pythonPath}\",\n    \"python.linting.mypyArgs\": [\n      \"${workspaceFolder}/tools/mypy_wrapper.py\"\n    ]\n  }\n  ```\n\n- As a result, `git clean -fdx; tools/vscode_settings.py` still results in some loss of useful configuration.\n\nOne other thing to note: as `.vscode/settings_recommended.json` shows, there are some configuration sections that only take effect within the context of a `\"[language]\"`, so currently, if a dev already has one of those settings, it would be entirely overwritten by `tools/vscode_settings.py` rather than a graceful merge. This could probably be fixed by using a deep merge instead of the current shallow merge strategy.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57671\n\nTest Plan:\nIf you want, you can typecheck the small script added by this PR (no output is expected):\n```sh\ntools/mypy_wrapper.py $PWD/tools/vscode_settings.py\n```\nYou can also try running it to update your own VS Code workspace settings:\n```sh\ntools/vscode_settings.py\n```\nThis should have minimal impact on your existing `tools/settings.json` file other than enabling the few explicitly recommended settings (e.g. it should not reorder or remove any of your existing settings).\n\nReviewed By: malfet\n\nDifferential Revision: D28230390\n\nPulled By: samestep\n\nfbshipit-source-id: 53a7907229e5807c77531cae4f9ab9d469fd7684", "pr_number": "57671", "files_changed": [".gitignore", ".vscode/extensions.json", ".vscode/settings_recommended.json", "mypy-strict.ini", "tools/README.md", "tools/vscode_settings.py"], "labels": ["Merged", "cla signed"]}, "cd22bdf236": {"title": "[PyTorch] Autoformat c10, round 2 (#57645)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57645\n\nSecond round of autoformatting changes since the first pass became too large.\nghstack-source-id: 128199695\n\nTest Plan: CI\n\nReviewed By: zertosh\n\nDifferential Revision: D28131430\n\nfbshipit-source-id: 24b03e38b087f31e8cac2404bebcd401c55b6cab", "pr_number": "57645", "files_changed": ["c10/util/DeadlockDetection.cpp", "c10/util/DeadlockDetection.h", "c10/util/Exception.cpp", "c10/util/Exception.h"], "labels": ["Merged", "cla signed"]}, "0b51ee311d": {"title": "Add missing return statement from 57057 (#57669)", "body": "Summary:\nFixes a bug introduced by https://github.com/pytorch/pytorch/issues/57057\n\ncc ailzhang while writing the tests, I realized that for these functions, we don't properly set the CreationMeta in no grad mode and Inference mode. Added a todo there.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57669\n\nReviewed By: soulitzer\n\nDifferential Revision: D28231005\n\nPulled By: albanD\n\nfbshipit-source-id: 08a68d23ded87027476914bc87f3a0537f01fc33", "pr_number": "57669", "files_changed": ["test/cpp/api/inference_mode.cpp", "torch/csrc/autograd/VariableTypeUtils.h"], "labels": ["Merged", "cla signed"]}, "7627dd568a": {"title": "hardswish reland (#57652)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57652\n\nTest Plan: Imported from OSS\n\nReviewed By: Krovatkin\n\nDifferential Revision: D28226724\n\nPulled By: eellison\n\nfbshipit-source-id: 585a91ffab7a855b5600e79130a37be25ef9b354", "pr_number": "57652", "files_changed": ["test/test_fx.py", "test/test_jit_fuser_te.py", "test/test_ops.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ba500c5c90": {"title": "Add call_kwargs(args, kwargs) method to torch::deploy api (#57484)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57484\n\nTo be used by PyTorchPredictor integration for deploy.\n\nTest Plan: tested via new unit tests\n\nReviewed By: suo\n\nDifferential Revision: D28154522\n\nfbshipit-source-id: 5ba57a8d7f01686180e6fd47663635ec3ab2120d", "pr_number": "57484", "files_changed": ["torch/csrc/deploy/deploy.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.h", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["Merged", "Reverted", "cla signed", "fb-exported"]}, "27af9b0462": {"title": "Fix flaky test_rref_context_debug_info (#57526)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57526\n\nThis test would create an RRef, delete that rref and then create two\nmore RRefs and validate total rrefs were 2 in the end.\n\nDue to the async nature of delete, sometimes the RRef would not be deleted\nuntil the assertion was made. As a result, I've fixed this by waiting for the\nRRef to be deleted at the appropriate time.\n\n#Closes: https://github.com/pytorch/pytorch/issues/55382\nghstack-source-id: 128037566\n\nTest Plan: waitforbuildbot\n\nReviewed By: H-Huang\n\nDifferential Revision: D28173151\n\nfbshipit-source-id: e4f34ff4e49b72cfc9e67a72c482f5e05159eda5", "pr_number": "57526", "files_changed": ["torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "866b19e95d": {"title": "[paramcomms] support for in and out split sizes", "body": "Summary: Adding way to accept in and out split sizes.\n\nTest Plan:\n{F613245151}\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1620153506%2F127.0.0.1%2Flibkineto_activities_1112677.json.gz&bucket=gpu_traces\nNOTE: ignore the GPU user showing up in CPU - the issue is fixed in the diff above the stack D28196723\n\nUPDATED: now the sizes are encoded as arrays in .json\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1620259313%2F127.0.0.1%2Flibkineto_activities_3944235.json.gz&bucket=gpu_traces\n\nReviewed By: kingchc\n\nDifferential Revision: D28052211\n\nfbshipit-source-id: 4ab7d425fc722907d9bbcfad7e364d031ff69b29", "pr_number": null, "files_changed": ["torch/lib/c10d/ParamCommsUtils.cpp", "torch/lib/c10d/ParamCommsUtils.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": []}, "ba78bf1363": {"title": "[standaloneRunner] fix another GIL mutithreading issue exposed by torch::jit::toIValue() (#57688)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57688\n\nP412982836 says that `torch::jit::toIValue()` will also touch GIL through `torch::jit::createGenericDict()` (P412848640)\nSo we have to move `torch::jit::toIValue()` out of multithreading execution\n\nReviewed By: hyuen\n\nDifferential Revision: D28236527\n\nfbshipit-source-id: 43a33dbcfc828cc42c5e1230c8f5cb415bf7bde4", "pr_number": "57688", "files_changed": ["torch/csrc/jit/python/pybind_utils.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "126ea1ccad": {"title": "relax type equality constraint for scalars (#57532)", "body": "Summary:\nCurrently we require type equality for `torch.testing.assert_(equal|close)`:\n\nhttps://github.com/pytorch/pytorch/blob/3db45bcb915a32ffa60378205fd9d96c45d7113f/torch/testing/_asserts.py#L509-L513\n\nThat means `assert_equal(1, 1.0)` will correctly fail. Although the type of a scalar is similiar to a dtype of a tensor, `assert_equal(1, 1.0, check_dtype=False)` will also fail while `assert_equal(torch.as_tensor(1), torch.as_tensor(1.0), check_dtype=False)` will pass.\n\nTo make the interface more consistent, this PR relaxes the type equality constraint, by disabling it in case both inputs are scalars.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57532\n\nReviewed By: ngimel\n\nDifferential Revision: D28242428\n\nPulled By: mruberry\n\nfbshipit-source-id: b643c77f48b64fc2c8a43925120d2b634ec336b5", "pr_number": "57532", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "module: testing", "open source", "triaged"]}, "8bbe383877": {"title": "[Static Runtime] Fix bugs in logit (#57578)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57578\n\nThe original impl in SR assumes that eps is a constant, which is true most of the times. However it could be a graph input as well. This diff fixes this issue. Unit tests are added as well.\n\nReviewed By: edvgha\n\nDifferential Revision: D28207975\n\nfbshipit-source-id: 9a10dec159f3804e43ef74aaa20c3ec6c79548c9", "pr_number": "57578", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "fc657b547a": {"title": "[kineto] set the correct device id for GenericTraceActivity", "body": "Summary: while merging ClientTraceActivity and GenericTraceActivity, we accidentally adopted CTA's behavior of returning process id over its `device`. This causes GTA to show up in CPU timeline rather than associated GPU's\n\nTest Plan:\nbefore\n\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1620113910%2F127.0.0.1%2Flibkineto_activities_1270242.json.gz&bucket=gpu_traces\n\n{F613233496}\n\nafter\n\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1620115859%2F127.0.0.1%2Flibkineto_activities_1511899.json.gz&bucket=gpu_traces\n\n{F613231643}\n\nReviewed By: gdankel\n\nDifferential Revision: D28196723\n\nfbshipit-source-id: eb8330c14e7c43a470bb4df4811b80754d96535b", "pr_number": null, "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": []}, "7870450706": {"title": "[PyTorch] Use c10::ThreadLocal instead thread_local in record_function.cpp for specific __GLIBCXX__ on Android (#57689)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57689\n* Older versions of libgnustd have issues with thread_local C++ qualifier on Android devices prior to r17+. Use c10::tls<> wrapper with smart pointer semantics in such cases.\n* Convenient macro `C10_DEFINE_TLS_static` was added as well:\n\n```\n  // Define static TLS variable str_tls_ of type std::string\n  C10_DEFINE_TLS_static(std::string, str_tls_);\n\n  //////// Excercise it ////////\n  {\n     *str_tls_ = \"abc\";\n     assert(str_tls_->length(), 3);\n  }\n```\nghstack-source-id: 128233742\n\nTest Plan: CI +\n\nReviewed By: ilia-cher\n\nDifferential Revision: D27875779\n\nfbshipit-source-id: 7764f96ac1e121051c6ea66eabcedb9ef54d290e", "pr_number": "57689", "files_changed": ["aten/src/ATen/record_function.cpp", "c10/test/util/ThreadLocal_test.cpp", "c10/util/ThreadLocal.h"], "labels": ["Merged", "cla signed", "fb-exported"]}, "1292602375": {"title": "Avoid re-extracting DataPtrs when forwarding values between Futures (#57433)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57433\n\nIn a bunch of cases we need to \"forward\" between one future and another, typically because we need to convert the type of the data (e.g., from Message to PyObject). In most of these cases the DataPtrs of the value don't change, and yet the new future must re-extract them from scratch. By allowing the user to obtain the vector of extracted DataPtrs from the old future, we can allow them to \"shortcut\" this step.\n\nAlso, this change is a requirement for the next PR to work, since the next PR would otherwise cause us to attempt extracting DataPtrs from Message instances, which doesn't work (because Message is a custom class), but thanks to this PR we actually skip that.\n\nghstack-source-id: 128184663\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28118298\n\nfbshipit-source-id: 70e333ea6a4f8d4d9a86514c350028d412469ee1", "pr_number": "57433", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/torchscript_functions.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "69de4940f3": {"title": "Ensure devices are preserved when forwarding between futures (#57432)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57432\n\nIn a bunch of places we were creating a future and then \"forwarding\" the value of another future to it once that other future completed. (This was in order to convert the type of the value, or to \"merge\" multiple futures into one). However when doing so we often created a child future with an empty set of devices, which meant it didn't support CUDA, and thus would cause a silent synchronization/correctness bug if the parent future did actually contain CUDA tensors.\n\nOne way this could have been caught earlier would have been to have Future always extract the DataPtrs, even in CPU-only mode, in order to ensure they always reside on the expected set of devices. Unfortunately this might have some averse perf effects thus should be done carefully.\nghstack-source-id: 128184667\n\nTest Plan: eyes\n\nReviewed By: mrshenli\n\nDifferential Revision: D28143045\n\nfbshipit-source-id: 9af1abf270366dc1df0d4857d6a8cc73668af9d1", "pr_number": "57432", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8e9bbd3113": {"title": "Make DataPtr extraction in CUDAFuture faster for Python values (#56918)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56918\n\nRe-importing a Python module each time is a bit expensive, and it's unnecessary because this is a private module which won't change and thus we can cache the value once we first extract it.\n\nghstack-source-id: 128184666\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D27985910\n\nfbshipit-source-id: be40ae9b67ab8ea6c07bc2cb9a78d2c2c30b35d3", "pr_number": "56918", "files_changed": ["torch/csrc/jit/python/python_ivalue.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "7ffadf6e46": {"title": "Replace DeviceIndexes with Devices in RRefs (#57442)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57442\n\nWe did this for the RPC agents and for ivalue::Future, the last one (I think) is RRef.\nghstack-source-id: 128184664\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28144368\n\nfbshipit-source-id: eeacab6006f72118cbec542a02322f2e391c67a3", "pr_number": "57442", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7d4121d1d2": {"title": "Make RRefContext get devices from RPC agent when creating OwnerRRef (#57443)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57443\n\nBased on the comments in https://github.com/pytorch/pytorch/pull/57355, I started looking at the callsites of `getOrCreateOwnerRRef` and `createOwnerRRef`, and noticed that many of them didn't specify the `devices` argument, which was optional and thus defaulted to `{}`, which created a CPU-only Future inside the OwnerRRef. (Such callsites were, for example, in `processPythonRemoteCall` and `processBaseScriptRemoteCall`, or `PyRRef::unpickle`, ...).\n\nSome (or all?) of these callsites might still have worked thanks to the RRef's own handling of CUDA streams and events, however we intend to remove that in https://github.com/pytorch/pytorch/pull/57355. I think it would be a safer and more generic solution to always create OwnerRRefs with the full set of devices supported by the RPC agent, and this is in fact easy to do since the RRefContext has access to the RPC agent. This means that all OwnerRRefs, no matter how they're created, will support CUDA if the agent does. This also allows us to stop requiring to specify devices when creating a OwnerRRef by hand in Python.\nghstack-source-id: 128184665\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28144365\n\nfbshipit-source-id: 1f2d446873f31ee297415c46b94126b6502b12d3", "pr_number": "57443", "files_changed": ["torch/csrc/distributed/rpc/init.cpp", "torch/csrc/distributed/rpc/py_rref.cpp", "torch/csrc/distributed/rpc/py_rref.h", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/rref_context.h", "torch/csrc/distributed/rpc/rref_impl.h", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "db7b31358f": {"title": "Fix internal assert in CUDA caching allocator when trying to allocate ~2^64 memory (#57571)", "body": "Summary:\nWhen the memory requested is huge, some internal logic in CUDA caching allocator could overflow. The result of the overflow is the caching allocator gives a confusing error message.\n\nFor example:\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.utils import cpp_extension\ncuda_source = \"\"\"\n#include <c10/cuda/CUDACachingAllocator.h>\nvoid my_fun(void)\n{\n    size_t temp_storage_bytes = 18446744073708433663UL;\n    auto& caching_allocator = *::c10::cuda::CUDACachingAllocator::get();\n    auto temp_storage = caching_allocator.allocate(temp_storage_bytes);\n    return;\n}\n\"\"\"\ncpp_source = \"\"\"\n    void my_fun(void);\n\"\"\"\nmodule = torch.utils.cpp_extension.load_inline(\n    name=\"cuda_test_extension\",\n    cpp_sources=cpp_source,\n    cuda_sources=cuda_source,\n    functions=\"my_fun\",\n    extra_cuda_cflags=[\"--extended-lambda\"],\n    verbose=True,\n)\nmodule.my_fun()\nprint('done')\n```\n\ngives\n\n```\nTraceback (most recent call last):\n  File \"/home/gaoxiang/misc/caching-allocator.py\", line 26, in <module>\n    module.my_fun()\nRuntimeError: p.block != nullptr && p.block->ptr != nullptrINTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":991, please report a bug to PyTorch.\nException raised from alloc_block at ../c10/cuda/CUDACachingAllocator.cpp:991 (most recent call first):\nframe #0: <unknown function> + 0x83e93 (0x7f424f05ee93 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe https://github.com/pytorch/pytorch/issues/1: <unknown function> + 0x83bf9 (0x7f424f05ebf9 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe https://github.com/pytorch/pytorch/issues/2: <unknown function> + 0x839bd (0x7f424f05e9bd in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe https://github.com/pytorch/pytorch/issues/3: std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > ()>::operator()() const + 0x4c (0x7f428a3350a2 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so)\nframe https://github.com/pytorch/pytorch/issues/4: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x40 (0x7f424f05dc34 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe https://github.com/pytorch/pytorch/issues/5: c10::detail::torchCheckFail(char const*, char const*, unsigned int, char const*) + 0x97 (0x7f424f05c42f in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10.so)\nframe https://github.com/pytorch/pytorch/issues/6: <unknown function> + 0x6948b4 (0x7f42978fd8b4 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)\nframe https://github.com/pytorch/pytorch/issues/7: <unknown function> + 0x22373 (0x7f424f0e2373 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe https://github.com/pytorch/pytorch/issues/8: <unknown function> + 0x1fa6c (0x7f424f0dfa6c in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe https://github.com/pytorch/pytorch/issues/9: <unknown function> + 0x2337a (0x7f424f0e337a in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe https://github.com/pytorch/pytorch/issues/10: <unknown function> + 0x23f18 (0x7f424f0e3f18 in /home/gaoxiang/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)\nframe https://github.com/pytorch/pytorch/issues/11: my_fun() + 0x4b (0x7f4200338f74 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)\nframe https://github.com/pytorch/pytorch/issues/12: torch::detail::wrap_pybind_function_impl_<void (&)()>(void (&)(), std::integer_sequence<unsigned long>)::{lambda()https://github.com/pytorch/pytorch/issues/1}::operator()() const + 0x3f (0x7f420031e575 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)\nframe https://github.com/pytorch/pytorch/issues/13: <unknown function> + 0x570f2 (0x7f42003350f2 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)\nframe https://github.com/pytorch/pytorch/issues/14: <unknown function> + 0x536e2 (0x7f42003316e2 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)\nframe https://github.com/pytorch/pytorch/issues/15: <unknown function> + 0x4ef2f (0x7f420032cf2f in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)\nframe https://github.com/pytorch/pytorch/issues/16: <unknown function> + 0x4ef93 (0x7f420032cf93 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)\nframe https://github.com/pytorch/pytorch/issues/17: <unknown function> + 0x3e7f2 (0x7f420031c7f2 in /home/gaoxiang/.cache/torch_extensions/cuda_test_extension/cuda_test_extension.so)\n<omitting python frames>\nframe https://github.com/pytorch/pytorch/issues/30: __libc_start_main + 0xd5 (0x7f42c60bab25 in /usr/lib/libc.so.6)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57571\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D28224574\n\nPulled By: ezyang\n\nfbshipit-source-id: df440961f6eaf58048af36ae2a06c59f3c18baec", "pr_number": "57571", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp", "test/test_cuda.py"], "labels": ["Merged", "cla signed", "module: cuda", "open source", "triaged"]}, "59d794b2c3": {"title": "Port CPU torch.ormqr to ATen (#57315)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57315\n\nThis PR ports `torch.ormqr` from TH to ATen.\nCUDA path will be implemented in a follow-up PR.\nWith ATen port, support for complex and batched inputs is added.\nThe tests are rewritten and OpInfo entry is added.\n\nWe can implement the least squares solver with geqrf + ormqr +\ntriangular_solve. So it's useful to have this function renewed at least for the\ninternal code.\n\nResolves https://github.com/pytorch/pytorch/issues/24748\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D28242070\n\nPulled By: mruberry\n\nfbshipit-source-id: f070bb6ac2f5a3269b163b22f7354e9089ed3061", "pr_number": "57315", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THLapack.cpp", "aten/src/TH/generic/THLapack.h", "aten/src/TH/generic/THTensorLapack.cpp", "aten/src/TH/generic/THTensorLapack.h", "test/test_linalg.py", "torch/_torch_docs.py", "torch/linalg/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "module: porting", "open source"]}, "35fab44eaf": {"title": "Add CUDA support for torch.ormqr (#57316)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57316\n\nCUDA support is implemented using cuSOLVER.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D28242071\n\nPulled By: mruberry\n\nfbshipit-source-id: 6f0a1c50c21c376d2ee2907bddb618c6a600db1f", "pr_number": "57316", "files_changed": ["aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/CUDASolver.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h", "aten/src/ATen/native/native_functions.yaml", "test/test_linalg.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "f1a62264f3": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D28250914\n\nfbshipit-source-id: 8bec4e0806891a045becf59c2d2f44f12bc41926", "pr_number": null, "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": []}, "ad31aa652c": {"title": "Fixed the error in conv1d example (#57356)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/51225\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57356\n\nReviewed By: albanD\n\nDifferential Revision: D28173174\n\nPulled By: malfet\n\nfbshipit-source-id: 5e813306f2e2f7e0412ffaa5d147441134739e00", "pr_number": "57356", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "a7ba0f08f3": {"title": "Update internal code for torch.lu_solve (#56611)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56611\n\nThe goal of this refactoring is to make the `torch.linalg.solve`\nto be a composition of calls to `lu_stub` and `lu_solve_stub`.\nOnce `lu_stub` and `lu_solve_stub` have cuSOLVER-based codepath,\n`torch.linalg.solve` will have it as well.\n\nReplaced lu_solve_helper with DECLARE_DISPATCH for lu_solve_stub.\nRemoved unnecessary copy improving the performance (see https://github.com/pytorch/pytorch/pull/56611#issuecomment-824303673).\nSplit MAGMA-based `apply_lu_solve` into `apply_lu_solve_looped_magma`\nand `apply_lu_solve_batched_magma`. This simplifies future dispatch to\ncuSOLVER and cuBLAS.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D28142279\n\nPulled By: mruberry\n\nfbshipit-source-id: 9d4baf650ca7a40b800616794408b34342d8d68f", "pr_number": "56611", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "dc06f52480": {"title": "Add result() to ProcessGroupGloo::AsyncWork's (#57565)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57565\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28255120\n\nPulled By: agolynski\n\nfbshipit-source-id: 1e904d4fe024d5b99cb642f8689ca32be0581e82", "pr_number": "57565", "files_changed": ["test/distributed/test_c10d_gloo.py", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp"], "labels": ["Merged", "cla signed", "module: c10d", "oncall: distributed"]}, "d83d1d3741": {"title": "TensorIterator: documentation on the order of creation (#57550)", "body": "Summary:\nAdds documentation to TensorIterator and TensorIteratorConfig that outputs need to be added first before inputs.\n\nFixes https://github.com/pytorch/pytorch/issues/57343\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57550\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D28198135\n\nPulled By: mrshenli\n\nfbshipit-source-id: 363603cac968bf786a4a6a64e353307c54d541b1", "pr_number": "57550", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h"], "labels": ["Merged", "cla signed", "module: TensorIterator", "module: docs", "open source"]}, "1101a5f6e9": {"title": "[paramcomms] support for in and out split sizes (#57709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57709\n\nNOTE: initial commit got reverted D28247764\n\nAdding way to accept in and out split sizes.\n\nTest Plan:\n{F613245151}\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1620153506%2F127.0.0.1%2Flibkineto_activities_1112677.json.gz&bucket=gpu_traces\nNOTE: ignore the GPU user showing up in CPU - the issue is fixed in the diff above the stack D28196723 (https://github.com/pytorch/pytorch/commit/fc657b547aefb4ab5e83f0bf2da946e7c3dd98c2)\n\nUPDATED: now the sizes are encoded as arrays in .json\nhttps://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1620259313%2F127.0.0.1%2Flibkineto_activities_3944235.json.gz&bucket=gpu_traces\n\nReviewed By: kingchc\n\nDifferential Revision: D28248333\n\nfbshipit-source-id: cee523612667cb37170c94e3c40dab5fba432225", "pr_number": "57709", "files_changed": ["torch/lib/c10d/ParamCommsUtils.cpp", "torch/lib/c10d/ParamCommsUtils.hpp", "torch/lib/c10d/ProcessGroupNCCL.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "b2936ad8fa": {"title": "Improve BatchNorm1d performance (CUDA) (#57034)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57034\n\nResolves gh-38915\n\nFor the example given in the issue, BatchNorm1d on cuDNN is around 12x slower\nthan BatchNorm2d. Internally, cuDNN expects at least a 4d tensor (N, C, H, W)\nso these two modules actually call the same cuDNN code. My assumption is that\ncuDNN just isn't optimized for H=W=1.\n\nInstead, this disables cudnn for 2d batch_norm inputs and improves the CUDA\nimplementation of `native_batch_norm` to be competative with cuDNN. For the\nexample in the issue, `BatchNorm1d` now takes 335 us compared to 6.3 ms before,\nor a 18x speedup.\n\nBefore this change, nvprof shows:\n```\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   99.64%  630.95ms       100  6.3095ms  5.6427ms  8.8800ms  void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=0, int=2>(cudnnTensorStruct, float const *, cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int=512, bool=0, int=2>, cudnnTensorStruct*, float const *, float const , cudnnTensorStruct*, cudnnTensorStruct*, cudnnTensorStruct**, float const *, float const *, float const *, cudnnTensorStruct*, cudnnTensorStruct*)\n```\n\nBut after, it shows:\n```\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   54.76%  14.352ms       100  143.52us  123.52us  756.28us  _ZN2at6native27unrolled_elementwise_kernelIZZZNS0_72_GLOBAL__N__48_tmpxft_001e82d0_00000000_7_Normalization_cpp1_ii_db66e07022batch_norm_elementwiseERKNS_6TensorES5_RKN3c108optionalIS3_EESA_S5_S5_ENKUlvE_clEvENKUlvE2_clEvEUlfffffE_NS_6detail5ArrayIPcLi6EEE16OffsetCalculatorILi5EjESI_ILi1EjENS0_6memory15LoadWithoutCastENSL_16StoreWithoutCastEEEviT_T0_T1_T2_T3_T4_\n                   35.09%  9.1951ms       100  91.950us  84.415us  362.17us  void at::native::reduce_kernel<int=256, int=2, at::native::ReduceOp<float, at::native::WelfordOps<float, float, int, float, thrust::pair<float, float>>, unsigned int, float, int=2>>(float)\n                    0.71%  186.14us       100  1.8610us  1.8240us  1.9840us  _ZN2at6native72_GLOBAL__N__48_tmpxft_001e82d0_00000000_7_Normalization_cpp1_ii_db66e07045unrolled_elementwise_kernel_for_multi_outputsILi3EZZZNS1_34batch_norm_update_stats_and_invertERKNS_6TensorES5_S5_S5_ddlENKUlvE_clEvENKUlvE2_clEvEUlffffE_NS_6detail5ArrayIPcLi7EEE23TrivialOffsetCalculatorILi4EjESD_ILi3EjEEEviT0_T1_T2_T3_\n                    0.59%  153.37us       100  1.5330us  1.4720us  2.6240us\n\t\t\t\t\t\t\t\t\t\tvoid at::native::vectorized_elementwise_kernel<int=4,\n\t\t\t\t\t\t\t\t\t\tat::native::BUnaryFunctor<at::native::AddFunctor<long>>,\n\t\t\t\t\t\t\t\t\t\tat::detail::Array<char*, int=2>>(int, long,\n\t\t\t\t\t\t\t\t\t\tat::native::AddFunctor<long>)\n```\n\nI think there is similar scope to improve the backward implementation.\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D28142447\n\nPulled By: ngimel\n\nfbshipit-source-id: c70109780e206fa85e50a31e90a1cb4c533199da", "pr_number": "57034", "files_changed": ["aten/src/ATen/AccumulateType.cpp", "aten/src/ATen/AccumulateType.h", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/cuda/ReduceMomentKernel.cu", "benchmarks/operator_benchmark/pt/batchnorm_test.py", "c10/macros/Macros.h"], "labels": ["Merged", "Reverted", "cla signed", "open source"]}, "da06ae73a3": {"title": "[c2] Fix flaky test_spatial_bn_multi_batch_grad", "body": "Summary: Removed the deadline restriction since the first run can take more than the deadline, wile subsequent runs are shorter.\n\nReviewed By: ngimel\n\nDifferential Revision: D28260077\n\nfbshipit-source-id: 8ed2f5c16bc184bf4fae0a59b662fa1da2d4dd0a", "pr_number": null, "files_changed": ["caffe2/python/operator_test/spatial_bn_op_test.py"], "labels": []}, "52d1b91d38": {"title": "Give Python sub-version in GHA CUDA workflow name (#57770)", "body": "Summary:\nAddresses part of https://github.com/pytorch/pytorch/issues/57686#issuecomment-833672132. Evidence that the Python version is indeed 3.6: https://github.com/pytorch/pytorch/runs/2520276328\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57770\n\nTest Plan: CI would be nice, but this workflow does not currently run on PRs.\n\nReviewed By: malfet\n\nDifferential Revision: D28265048\n\nPulled By: samestep\n\nfbshipit-source-id: 513caf52a8f18d6e529e0934bf024f49e1571926", "pr_number": "57770", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7.yml", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3.6-gcc7.yml"], "labels": ["Merged", "cla signed"]}, "aedcff7275": {"title": "fix codegen for lite_interpreter (#57761)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57761\n\nReviewed By: cccclai\n\nDifferential Revision: D28262513\n\nPulled By: walterddr\n\nfbshipit-source-id: 40fe82de540791f19fdf349e71b05a12b9a57ad0", "pr_number": "57761", "files_changed": [".circleci/cimodel/data/simple/android_definitions.py", ".circleci/config.yml", ".circleci/verbatim-sources/build-parameters/pytorch-build-params.yml", ".circleci/verbatim-sources/job-specs/job-specs-custom.yml", "caffe2/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "241c2f4496": {"title": "Add Gelu To NNC (#57753)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57753\n\nI'm not adding symbolic gradient because that is being added in https://github.com/pytorch/pytorch/pull/46785.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D28262765\n\nPulled By: eellison\n\nfbshipit-source-id: be365a2d392d7ac4bcc099a184762249ec2e18a6", "pr_number": "57753", "files_changed": ["test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "78fb9c2f5b": {"title": "Reorder gc.py imports (#57779)", "body": "Summary:\nA tiny PR that reorder imports and run autopep8\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57779\n\nReviewed By: malfet\n\nDifferential Revision: D28269455\n\nPulled By: samestep\n\nfbshipit-source-id: 7d3176efad96e3a8ac1cdc76a5018c7ffa00c449", "pr_number": "57779", "files_changed": [".circleci/ecr_gc_docker/gc.py"], "labels": ["Merged", "cla signed"]}, "cb1272a846": {"title": "update doc in build section (#56686)", "body": "Summary:\nWhy:\nTo keep VS version always updated in README\n1. update VS version link in CI. It's more convenient for my PR robot to update the version in README once the VS in CI is updated. and permlink isn't stable.\n2. Move `building on legacy code` to development tips. The table is big and it looks the REAMD not updated at the first sight.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56686\n\nReviewed By: janeyx99\n\nDifferential Revision: D28272060\n\nPulled By: samestep\n\nfbshipit-source-id: 4bb879ea2914cc8bcd68343a9ed230418e1f9268", "pr_number": "56686", "files_changed": ["CONTRIBUTING.md", "README.md"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "1f1e2dab6b": {"title": "Remove optional type for ord parameter in vector_norm (#57662)", "body": "Summary:\nAs per discussion here https://github.com/pytorch/pytorch/pull/57127#discussion_r624948215\n\nNote that we cannot remove the optional type from the `dim` parameter because the default is to flatten the input tensor which cannot be easily captured by a value other than `None`\n\n### BC Breaking Note\nThis PR changes the `ord` parameter of `torch.linalg.vector_norm` so that it no longer accepts `None` arguments. The default behavior of `2` is equivalent to the previous default of `None`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57662\n\nReviewed By: albanD, mruberry\n\nDifferential Revision: D28228870\n\nPulled By: heitorschueroff\n\nfbshipit-source-id: 040fd8055bbe013f64d3c8409bbb4b2c87c99d13", "pr_number": "57662", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "test/test_linalg.py", "tools/autograd/derivatives.yaml", "torch/csrc/api/include/torch/linalg.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/linalg/__init__.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "ca8090f81b": {"title": "[Pytorch Edge] Enable eager symbolication in benchmarking binary (#57705)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57705\n\nThis will enable module level debug info for benchmarking binary.\n\nTest Plan: Run on AIBench\n\nReviewed By: larryliu0820\n\nDifferential Revision: D28230948\n\nfbshipit-source-id: 5d06c6853d049ff678995a2ed4a86f4e6c85bdc7", "pr_number": "57705", "files_changed": ["torch/csrc/jit/serialization/callstack_debug_info_serialization.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "319b08be59": {"title": "Add call_kwargs(args, kwargs) method to torch::deploy api (#57748)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57748\n\nTo be used by PyTorchPredictor integration for deploy.\n\nOriginal commit changeset: 4d41efc733b2\n\nTest Plan: tested via new unit tests\n\nReviewed By: suo\n\nDifferential Revision: D28258525\n\nfbshipit-source-id: 8b9436e47501d7c1c16e79909e668100f825711e", "pr_number": "57748", "files_changed": ["torch/csrc/deploy/deploy.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.h", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "ee79413b6a": {"title": "[testing] change unaryufunc default dtypes (#57616)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/pull/56646#pullrequestreview-644839124\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57616\n\nReviewed By: albanD\n\nDifferential Revision: D28249129\n\nPulled By: mruberry\n\nfbshipit-source-id: 2cfc837fd49100d2b1b2a09d9ca6db93e089e099", "pr_number": "57616", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "159a2404bd": {"title": "fft: Increase tolerance for nd-fft tests (#57576)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/56820\n\nThe test only fails for inverse n-dim functions with `norm=\"forward\"`. The relative error for isn't actually any bigger than other norm modes though. It's just that the magnitude of the result is bigger, so the absolute tolerance is less relative each element. So, I just increase the relative tolerance  to compensate.\n\nThis `precisionOverride` is already applied to `fftn` and `rfftn` for exactly the same reason.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57576\n\nReviewed By: albanD\n\nDifferential Revision: D28249222\n\nPulled By: mruberry\n\nfbshipit-source-id: 734c7c1ae8236b253d6e3cd2218c05d21901c567", "pr_number": "57576", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: fft", "module: testing", "open source", "triaged"]}, "6eec730a73": {"title": "[testing] atan2: Enable cases where self broadcasts (#57608)", "body": "Summary:\nJust a follow-up\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57608\n\nReviewed By: albanD\n\nDifferential Revision: D28249409\n\nPulled By: mruberry\n\nfbshipit-source-id: a1ce2cd736ac5547cecb3e21aaa50637917284bc", "pr_number": "57608", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "4cb3c60c20": {"title": "OpInfo: float_power (#57648)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/54295 (`float_power`)\n\ncc: mruberry kshitij12345 krshrimali\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57648\n\nReviewed By: albanD\n\nDifferential Revision: D28249489\n\nPulled By: mruberry\n\nfbshipit-source-id: 0ae5ce0d8b154724ae59f5f5b4412e34b0128d0a", "pr_number": "57648", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "1f7309dfe3": {"title": "[testing] clean-up test_unary_ufuncs.py (#57615)", "body": "Summary:\nSome clean-ups\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57615\n\nReviewed By: albanD\n\nDifferential Revision: D28249173\n\nPulled By: mruberry\n\nfbshipit-source-id: 19a300f6aa267932a7a92c2f5f377488f69bd822", "pr_number": "57615", "files_changed": ["test/test_unary_ufuncs.py"], "labels": ["Merged", "cla signed", "open source"]}, "9e6b7e6e6e": {"title": "OpInfo: expand and expand_as (#57606)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57606\n\nReviewed By: albanD\n\nDifferential Revision: D28249191\n\nPulled By: mruberry\n\nfbshipit-source-id: d985ab4e8a99b116c45953e621092929a9a8028e", "pr_number": "57606", "files_changed": ["test/test_fx.py", "test/test_fx_experimental.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "adaf80bcbe": {"title": "Update internal code for at::_lu_with_info (#56612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56612\n\nThe goal of this refactoring is to make the `torch.linalg.solve`\nto be a composition of calls to `lu_stub` and `lu_solve_stub`.\nOnce `lu_stub` and `lu_solve_stub` have cuSOLVER-based codepath,\n`torch.linalg.solve` will have it as well.\n\nReplaced `lu_with_info_{cpu, cuda}` with one function that calls\nto `lu_stub`.\nSplit MAGMA-based `apply_lu` into `apply_lu_looped_magma`\nand `apply_lu_batched_magma`. This simplifies the future switch to\ncuSOLVER and cuBLAS libraries.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28248756\n\nPulled By: mruberry\n\nfbshipit-source-id: 40e02b5be4ff5f78885bcc95685aba581043e096", "pr_number": "56612", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebra.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["Merged", "cla signed", "module: linear algebra", "open source"]}, "9aa1461a68": {"title": "Make wrapPropagateTLSState more generic (#57634)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57634\n\n`wrapPropagateTLSState` was restricting its argument to be an argument-less function, and I need to relax this for later work.\n\nAlso, it was requiring its argument to be converted to `std::function`, and also returned a `std::function`. Each creation of a `std::function` could cause a heap allocation. It's not particularly expensive, but here we can easily avoid it by having `wrapPropagateTLSState` directly operate on generic callables (thus, possibly, raw lambdas).\nghstack-source-id: 128295264\n\nTest Plan: CI\n\nReviewed By: ilia-cher\n\nDifferential Revision: D28178782\n\nfbshipit-source-id: d657f5751514974518606dd4fc4175e805dcb90a", "pr_number": "57634", "files_changed": ["aten/src/ATen/ThreadLocalState.h", "test/cpp/jit/test_misc.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/torchscript_functions.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "36e47af58b": {"title": "Pass reference to parent future in callbacks (#57635)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57635\n\nNote: this PR looks massive, but it's just one simple change, codemodded many times.\n\nIn many cases, a callback needs to access the value/error produced by the parent future. In Python this was easy because the callback was invoked with the parent future as argument, and could thus inspect it. In C++ the callbacks didn't take any arguments, thus in many cases we worked around this by capturing the future in its own callback. This is risky (leads to reference cycle and thus memory leak) and must be done carefully (spoiler: sometimes we weren't).\nghstack-source-id: 128296580\n\nTest Plan: CI\n\nReviewed By: wanchaol\n\nDifferential Revision: D28178783\n\nfbshipit-source-id: 6de02c4568be42123372edc008f630d5ddae0081", "pr_number": "57635", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/test/ivalue_test.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/rpc/e2e_test_base.h", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref_context.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/torchscript_functions.cpp", "torch/csrc/jit/python/pybind_utils.h", "torch/csrc/jit/runtime/graph_executor.cpp", "torch/csrc/jit/runtime/interpreter.cpp", "torch/lib/c10d/ProcessGroupNCCL.cpp", "torch/lib/c10d/default_comm_hooks.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "45012da298": {"title": "Migrate from shared_ptr to intrusive_ptr for Future (#57636)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57636\n\nThe \"preferred\" pointer holder for Future is `intrusive_ptr` (e.g., `then` returns an `intrusive_ptr`, `toFuture` returns `intrusive_ptr`, ...). However in RPC we often wrap it with `shared_ptr`. This probably dates back to when we had a separate Future type, before the merge.\n\nAt the boundary between RPC and JIT this difference becomes a bit annoying, as conversions between the pointer types are needed. I think it would be simpler and more consistent to always use `intrusive_ptr`, also in RPC.\n\nThis PR was produced mainly by find-and-replace, plus a couple of manual fixes.\nghstack-source-id: 128296581\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D28187972\n\nfbshipit-source-id: d4609273a1550b4921910e85d2198e02f31c905b", "pr_number": "57636", "files_changed": ["torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/csrc/distributed/autograd/engine/dist_engine.cpp", "torch/csrc/distributed/autograd/engine/dist_engine.h", "torch/csrc/distributed/autograd/utils.cpp", "torch/csrc/distributed/autograd/utils.h", "torch/csrc/distributed/rpc/process_group_agent.cpp", "torch/csrc/distributed/rpc/process_group_agent.h", "torch/csrc/distributed/rpc/python_functions.cpp", "torch/csrc/distributed/rpc/python_functions.h", "torch/csrc/distributed/rpc/request_callback.cpp", "torch/csrc/distributed/rpc/request_callback.h", "torch/csrc/distributed/rpc/request_callback_impl.cpp", "torch/csrc/distributed/rpc/request_callback_impl.h", "torch/csrc/distributed/rpc/request_callback_no_python.cpp", "torch/csrc/distributed/rpc/request_callback_no_python.h", "torch/csrc/distributed/rpc/rpc_agent.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/distributed/rpc/rref_impl.cpp", "torch/csrc/distributed/rpc/rref_impl.h", "torch/csrc/distributed/rpc/tensorpipe_agent.cpp", "torch/csrc/distributed/rpc/tensorpipe_agent.h", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.cpp", "torch/csrc/distributed/rpc/testing/faulty_process_group_agent.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a911c4fc1c": {"title": "New: Initial support for sparse complex tensors constructors for CPU/CUDA (#57125)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57125\n\nI'm opening this PR, solving the last issued reported before merging PR #54153\n\nhttps://github.com/pytorch/pytorch/pull/54153#issuecomment-827997616,\n\nSolves gh-50690\n\nTest Plan: Imported from OSS\n\nReviewed By: astaff\n\nDifferential Revision: D28112702\n\nPulled By: ezyang\n\nfbshipit-source-id: 915681954edb14b7c19c3ffe641af2d2e6649576", "pr_number": "57125", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/native/cuda/Nonzero.cu", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/TH/THTensor.h", "aten/src/TH/THTensorEvenMoreMath.cpp", "aten/src/TH/THVector.h", "aten/src/TH/generic/THTensorEvenMoreMath.cpp", "test/test_autograd.py", "test/test_sparse.py", "test/test_type_promotion.py", "tools/autograd/gen_variable_type.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "open source"]}, "6f2c0cccdd": {"title": "New: sparse complex: add linear algebra, addmm (#57129)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57129\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99, astaff\n\nDifferential Revision: D28112701\n\nPulled By: ezyang\n\nfbshipit-source-id: 1b253453dc19e908fb18d0b1a83738243e0a8d59", "pr_number": "57129", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensorMath.cpp", "aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "test/test_sparse.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["Merged", "cla signed", "open source"]}, "27f672a0fc": {"title": "Fix test reporting regression (#57795)", "body": "Summary:\nHere is why another move of this single line is needed:\n - Regardless of whether test-run failed or succeeded it's good to\n   report number of tests executed\n - `docker cp || echo` always succeeds so could safely be executed\n   before any other step in \"Report test results\"\n - This command should not be part of \"Run tests\" step, otherwise it would not get executed if any of the test failed (if it must be part of \"Run tests\" step, it should be prefixed with [trap](https://tldp.org/LDP/Bash-Beginners-Guide/html/sect_12_02.html) command and defined before `docker exec` step\n\nThis fixes \"regression\" introduced by https://github.com/pytorch/pytorch/pull/56725 although real culprit here is lack of documentation\n\nHere is an example of PR where test results are not reported back due to\nthe failure: https://app.circleci.com/pipelines/github/pytorch/pytorch/317199/workflows/584a658b-c742-4cbb-8f81-6bb4718a0c04/jobs/13209736/steps\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57795\n\nReviewed By: samestep\n\nDifferential Revision: D28275510\n\nPulled By: malfet\n\nfbshipit-source-id: 622f3bfca96a1ee9b8959590b28a26046eb37ea3", "pr_number": "57795", "files_changed": [".circleci/config.yml", ".circleci/verbatim-sources/job-specs/pytorch-job-specs.yml"], "labels": ["Merged", "cla signed"]}, "0dd0151c64": {"title": "add `torch.testing` to docs (#57247)", "body": "Summary:\nRedo of https://github.com/pytorch/pytorch/issues/56373 out of stack.\n\n ---\n\nTo reviewers: **please be nitpicky**. I've read this so often that I probably missed some typos and inconsistencies.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57247\n\nReviewed By: albanD\n\nDifferential Revision: D28247402\n\nPulled By: mruberry\n\nfbshipit-source-id: 71142678ee5c82cc8c0ecc1dad6a0b2b9236d3e6", "pr_number": "57247", "files_changed": ["docs/source/index.rst", "docs/source/testing.rst", "test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "161ea537f0": {"title": "[reland] Remove unused code in windows_build_definition.py (#57107)", "body": "Summary:\nI accidentally reverted https://github.com/pytorch/pytorch/issues/56230 in https://github.com/pytorch/pytorch/issues/56128 when resolving conflicts.. This PR relands https://github.com/pytorch/pytorch/issues/56230\n\nCC mszhanyi\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57107\n\nReviewed By: astaff\n\nDifferential Revision: D28096003\n\nPulled By: seemethere\n\nfbshipit-source-id: ea616d6b5cb0b04841d2f4cc30bd130ade4a364c", "pr_number": "57107", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "b5b158a6c6": {"title": "Be more lenient with network exceptions in trigger_azure_pipeline.py (#57714)", "body": "Summary:\nFixes possible failure caused by network instability.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57714\n\nReviewed By: ngimel\n\nDifferential Revision: D28288555\n\nPulled By: malfet\n\nfbshipit-source-id: 2deedf3fe1a95dae5a68d599d9603f3da4702e8e", "pr_number": "57714", "files_changed": [".circleci/scripts/trigger_azure_pipeline.py"], "labels": ["Merged", "cla signed", "open source"]}, "626ae7f036": {"title": "Copy edit of TorchScript Language Reference (#57694)", "body": "Summary:\nInitial copy edit of the file.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57694\n\nReviewed By: malfet, ngimel\n\nDifferential Revision: D28289209\n\nPulled By: holly1238\n\nfbshipit-source-id: 7035d6790767a2f758e6019ae63df16537ef2725", "pr_number": "57694", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "c07babbcf1": {"title": "[Gradient Compression] Divide by world size before all_reduce to avoid overflow (#57410)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57410\n\nFP16 gradient compression may run into 'inf' issue. switching to division before allreduce can avoid this problem.\nghstack-source-id: 127877083\n\nTest Plan:\nbefore chage\n\nf268909897\n\nafter change:\nf270950609\n\nIf you still sees 'grad_norm = inf' after enabling fp16 hook, you can resume the training and turning off the hook.\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28128628\n\nfbshipit-source-id: 0b6648637713e4f321e39c9ccb645a6b6f1750a0", "pr_number": "57410", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "96e1a83fb2": {"title": "Add Gloo TCP_TLS transport (#56442)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/56442\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D27896285\n\nPulled By: pbelevich\n\nfbshipit-source-id: 589af59ca4c7c9bab2329f079382c09b71cfcf9e", "pr_number": "56442", "files_changed": [".jenkins/pytorch/build.sh", ".jenkins/pytorch/create_test_cert.py", ".jenkins/pytorch/run_glootls_test.sh", ".jenkins/pytorch/test.sh", "CMakeLists.txt", "tools/print_test_stats.py", "torch/lib/c10d/GlooDeviceFactory.cpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "9234d7fc27": {"title": "[PyTorch] Use MaybeOwned and avoid resize in bmm_cuda (#56115)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56115\n\nNo reason to size it wrong and then resize it. Also, no reason to unconditionally go through the dispatcher.\nghstack-source-id: 128354110\n\nTest Plan: Existing CI\n\nReviewed By: ngimel\n\nDifferential Revision: D27768757\n\nfbshipit-source-id: 5dcb1fed5c5fa6707ee15359a26fde2a9a888b7f", "pr_number": "56115", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu"], "labels": ["Merged", "cla signed"]}, "3d2ce60539": {"title": "[PyTorch] Remove dead get/setTLSCallbacks APIs (#56492)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56492\n\nThese are documented as internal-only and aren't called.\nghstack-source-id: 128354112\n\nTest Plan: CI\n\nReviewed By: ilia-cher\n\nDifferential Revision: D27834789\n\nfbshipit-source-id: 4a1aa320f952249db51945ff77563558fa884266", "pr_number": "56492", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h"], "labels": ["Merged", "cla signed"]}, "2043093217": {"title": "Add correction parameter to std/var (#50903)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50903\n\nFirst part of #50010. Also fixes #51127.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D27911345\n\nPulled By: mruberry\n\nfbshipit-source-id: 7138fddc935802918ab9ff19f4bc1b9f4d745d41", "pr_number": "50903", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/LegacyTHFunctionsCPU.h", "aten/src/ATen/core/NamedRegistrations.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/cuda/ReduceMomentKernel.cu", "aten/src/ATen/native/native_functions.yaml", "aten/src/TH/generic/THTensorMath.h", "aten/src/TH/generic/THTensorMoreMath.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/onnx/test_pytorch_onnx_onnxruntime.py", "test/test_reductions.py", "tools/autograd/derivatives.yaml", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/onnx/symbolic_opset9.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "db412a6885": {"title": "Avoid 2 extra copies when reducing sparse tensors and fix result() vs inplace output discrepancy (#57822)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57822\n\n* `AsyncSparseAllreduceWork` can avoid copying output tensors, since we keep all the results alive by means of modifying input vector directly\n* `AsyncSparseAllreduceWork` now returns inputs back to user instead of former behavior where it returned copies of inputs. This is consistent with other operations and process group implementations\n* `AsyncSparseAllreduceCUDAWork` is now copying tensors directly from CPU to input tensors avoiding extra copy `output` -> `outputs` -> `inputs`. inputs are being returned to back to user. This is consistent with other operations and process group implementations.\n\noverall AsyncSparseAllreduceCUDAWork is now avoiding 2 extra copies (as AsyncSparseAllreduceCUDAWork is using AsyncSparseAllreduceWork's impl)\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D28298325\n\nPulled By: agolynski\n\nfbshipit-source-id: 18e2104413cdf5e73a01aad464e2613807779297", "pr_number": "57822", "files_changed": ["torch/lib/c10d/ProcessGroupGloo.cpp"], "labels": ["Merged", "cla signed", "module: c10d", "oncall: distributed"]}, "c88167d2ed": {"title": "Respect .ini for flake8 and mypy (#57752)", "body": "Summary:\nPreviously `make quicklint` would lint all changed files for both mypy `ini`s, regardless of whether that file was actually supposed to be run under that configuration. This PR fixes that so we are using `tools/mypy_wrapper.py` to check if files should be included.\n\nThere's a similar change for `flake8` so that it now only outputs errors once and correctly excludes the paths in `.flake8`.\n\nThis also adds a bunch of tests to ensure that `make lint` and `make quicklint` both work and that `make quicklint` is excluding and including what it should.\n\nFixes #57644\n](https://our.intern.facebook.com/intern/diff/28259692/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57752\n\nPulled By: driazati\n\nReviewed By: samestep\n\nDifferential Revision: D28259692\n\nfbshipit-source-id: 233d355781230f11f98a6f61e2c07e9f5e737e24", "pr_number": "57752", "files_changed": [".github/workflows/lint.yml", ".github/workflows/test_tools.yml", "Makefile", "tools/actions_local_runner.py", "tools/test/test_actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "b0c27b44cf": {"title": "Enable backward/forward compatibility for TS runtime (#57498)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57498\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28162448\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 5c21ced42a22aca7cee089e876e9d98d32f68955", "pr_number": "57498", "files_changed": ["test/expect/TestJit.test_import_method.expect", "test/expect/TestJit.test_pretty_printer-loop_use_test.expect", "test/expect/TestJit.test_pretty_printer-while_if_test.expect", "test/expect/TestJit.test_pretty_printer-while_test.expect", "test/jit/test_ignorable_args.py", "test/test_jit.py", "torch/csrc/jit/runtime/calculate_necessary_args.h", "torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.h", "torch/csrc/jit/runtime/interpreter/code_impl.h", "torch/csrc/jit/serialization/python_print.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "4fad8d1a2c": {"title": "Update the default detach semantic for forward mode AD (#57820)", "body": "Summary:\nThis makes detach both forward and backward non-differentiable by default.\nYou can pass the `only_backward_mode=True` argument to make it forward differentiable but backward non-differentiable.\n\nThe important side effect of this change is that, by default, detach is not tracking any view information.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57820\n\nReviewed By: ezyang\n\nDifferential Revision: D28287633\n\nPulled By: albanD\n\nfbshipit-source-id: bdc4726fcd05889f6ac84e5a3a3ef71b2ec41015", "pr_number": "57820", "files_changed": ["test/test_autograd.py", "torch/_tensor.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["Merged", "cla signed"]}, "73f22bcbf9": {"title": "[fx ir] Handle cases in GraphDrawer when shape, type or stride are not present (#57845)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57845\n\nAs title says\n\nTest Plan: N/A\n\nReviewed By: 842974287\n\nDifferential Revision: D28295999\n\nfbshipit-source-id: f2cbf80c468f13685b17bb396c1f48972744ced0", "pr_number": "57845", "files_changed": ["torch/fx/passes/graph_drawer.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "e9c3ce30d4": {"title": "Fix flaky test_barrier_timeout_global. (#57523)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57523\n\n`_test_barrier_timeout` would run a barrier on rank 1 and sleep for\n`timeout` on other ranks. In some cases if the other ranks would be faster,\nthey would enter the sleep call much earlier than rank 0 would enter barrier.\nAs a result, they would exit before the timeout is up and rank 0 would receive\na connection closed error instead of a timeout error. This would result in the\nbarrier call exiting before the timeout and the subsequent assertion failing.\n\n#Closes: https://github.com/pytorch/pytorch/issues/57176\nghstack-source-id: 128278775\n\nTest Plan:\n1) waitforbuildbot\n2) Tested synthetically by forcing a rank to exit earlier.\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28170821\n\nfbshipit-source-id: a67456a1784dd0657f264c4f5498638e0aa00de2", "pr_number": "57523", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8c04593c0a": {"title": "[PyTorch Edge] Add backport to export old bytecode models (#56802)", "body": "Summary:\nAdd an api to backport a model vn to model vi. It accept an input model (file or buffer) and output a model (file or buffer) with an expected bytecode version.\n\nIn this change, the input is a model and it can come from a file or buffer. The output is a model and can be either file path or buffer.\n\nWhen backport fails, function return false with a warning message :\n```\n/Users/chenlai/pytorch/cmake-build-debug/bin/test_jit --gtest_filter=LiteInterpreterTest.BackPortByteCodeModelV4:LiteInterpreterTest/*.BackPortByteCodeModelV4:*/LiteInterpreterTest.BackPortByteCodeModelV4/*:*/LiteInterpreterTest/*.BackPortByteCodeModelV4 --gtest_color=no\nTesting started at 2:32 PM ...\nCUDA not available. Disabling CUDA and MultiCUDA tests\n\n[W backport.cpp:419] Warning: Backport doesn't support backport to version3 (function _backport_for_mobile_impl)\nProcess finished with exit code 0\n```\n\n## Test\n1. Run both `caffe2/test/cpp/jit/test_lite_interpreter.cpp` and `caffe2/test/mobile/test_bytecode.py`.\n2. Run all prod models with backport api.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56802\n\nghstack-source-id: 128425510\n\nTest Plan: CI\n\nReviewed By: raziel, iseeyuan\n\nDifferential Revision: D27844651\n\nfbshipit-source-id: 8a803cf6c76433ee0a3049b1a5570585d569f8d6", "pr_number": "56802", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_interpreter.cpp", "test/mobile/test_bytecode.py", "tools/build_variables.bzl", "torch/_C/__init__.pyi.in", "torch/csrc/jit/mobile/backport.cpp", "torch/csrc/jit/mobile/backport.h", "torch/csrc/jit/mobile/backport_manager.cpp", "torch/csrc/jit/mobile/backport_manager.h", "torch/csrc/jit/mobile/model_compatibility.cpp", "torch/csrc/jit/mobile/model_compatibility.h", "torch/csrc/jit/python/script_init.cpp", "torch/jit/mobile/__init__.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "74d493cc07": {"title": "[RPC Framework] Support passing RemoteModule as an arg (#57695)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57695\n\nAdd pickling/unpickling support for `RemoteModule`.\n\n#Closes: https://github.com/pytorch/pytorch/issues/57516\nghstack-source-id: 128472946\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_send_remote_module_over_the_wire\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_send_remote_module_with_a_new_attribute_over_the_wire\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- RemoteModule\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28233108\n\nfbshipit-source-id: 94eea2251fa53fb71912457c80d0a1e44504fc85", "pr_number": "57695", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py", "torch/testing/_internal/distributed/rpc_utils.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "4db88307d9": {"title": "[RPC Framework] Add a link to the tutorial in RemoteModule docstring (#57875)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57875\n\nThis tutorial combines DDP and RemoteModule.\nghstack-source-id: 128482681\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28305382\n\nfbshipit-source-id: 572e1ec4b4aa00735fff16a6ce6ae4c7cad0b27f", "pr_number": "57875", "files_changed": ["torch/distributed/nn/api/remote_module.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "94080f45ab": {"title": "[RPC Framework] Update rpc.rst (#57876)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57876\n\nghstack-source-id: 128484049\n\nTest Plan: N/A\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D28305719\n\nfbshipit-source-id: cc0d79fb46077a0d1cf6026c373893e7d3b7761e", "pr_number": "57876", "files_changed": ["docs/source/rpc.rst"], "labels": ["Merged", "cla signed"]}, "bc2540f0be": {"title": "benchmark rpc ps (#57454)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57454\n\nDDP with NCCL AllReduce for the entire model experiment from Quip https://fb.quip.com/iQUtAeKIxWpF\n\nI have been testing this on the AI cluster. There seem to be some connection problems with RPC when using multiple trainers or parameter servers.\n\n```\nNamespace(bconfig_id='3', dconfig_id='DummyData', mconfig_id='DummyModel', pconfig_id='None', tconfig_id='DdpNcclTrainer')\n\nbenchmark warmup done\n\nmetrics for trainer=0\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| name                              |      min |     max |     mean |   variance |     stdev |\n+===================================+==========+=========+==========+============+===========+\n| backward_metric,backward          | 2.45248  | 4.18304 | 3.972    | 0.097122   | 0.311644  |\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| batch_level_metric,batch_all      | 4.11955  | 4.58138 | 4.31439  | 0.00229848 | 0.0479424 |\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| foward_metric,forward_pass        | 0.141312 | 1.4807  | 0.222566 | 0.0555432  | 0.235676  |\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| hook_future_metric,nccl_allreduce | 0.191488 | 3.54099 | 3.11694  | 0.557106   | 0.746395  |\n+-----------------------------------+----------+---------+----------+------------+-----------+\nmetrics for trainer=1\n+-----------------------------------+----------+---------+----------+-------------+------------+\n| name                              |      min |     max |     mean |    variance |      stdev |\n+===================================+==========+=========+==========+=============+============+\n| backward_metric,backward          | 2.4617   | 2.59174 | 2.51196  | 0.000938276 | 0.0306313  |\n+-----------------------------------+----------+---------+----------+-------------+------------+\n| batch_level_metric,batch_all      | 4.22605  | 4.71757 | 4.27921  | 0.00468424  | 0.0684415  |\n+-----------------------------------+----------+---------+----------+-------------+------------+\n| foward_metric,forward_pass        | 0.807936 | 1.50118 | 0.846008 | 0.00601693  | 0.0775688  |\n+-----------------------------------+----------+---------+----------+-------------+------------+\n| hook_future_metric,nccl_allreduce | 0.108544 | 0.1536  | 0.11222  | 2.16726e-05 | 0.00465538 |\n+-----------------------------------+----------+---------+----------+-------------+------------+\nmetrics for all trainer\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| name                              |      min |     max |     mean |   variance |     stdev |\n+===================================+==========+=========+==========+============+===========+\n| backward_metric,backward          | 2.45248  | 4.18304 | 3.24198  | 0.584391   | 0.764455  |\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| batch_level_metric,batch_all      | 4.11955  | 4.71757 | 4.2968   | 0.00378467 | 0.0615197 |\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| foward_metric,forward_pass        | 0.141312 | 1.50118 | 0.534287 | 0.128284   | 0.358167  |\n+-----------------------------------+----------+---------+----------+------------+-----------+\n| hook_future_metric,nccl_allreduce | 0.108544 | 3.54099 | 1.61458  | 2.5456     | 1.59549   |\n+-----------------------------------+----------+---------+----------+------------+-----------+\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang, ngimel\n\nDifferential Revision: D28296175\n\nPulled By: gcramer23\n\nfbshipit-source-id: 5dd208fc86f8b5558d7c8860d685bb25c2e09fe7", "pr_number": "57454", "files_changed": ["benchmarks/distributed/rpc/parameter_server/BenchmarkConfigurations.py", "benchmarks/distributed/rpc/parameter_server/README.md", "benchmarks/distributed/rpc/parameter_server/bash_experiment_scripts/ddp_nccl_allreduce.sh", "benchmarks/distributed/rpc/parameter_server/bash_experiment_scripts/helper_functions.sh", "benchmarks/distributed/rpc/parameter_server/benchmark_class_helper.py", "benchmarks/distributed/rpc/parameter_server/configurations/benchmark_configurations.json", "benchmarks/distributed/rpc/parameter_server/configurations/data_configurations.json", "benchmarks/distributed/rpc/parameter_server/configurations/model_configurations.json", "benchmarks/distributed/rpc/parameter_server/configurations/parameter_server_configurations.json", "benchmarks/distributed/rpc/parameter_server/configurations/trainer_configurations.json", "benchmarks/distributed/rpc/parameter_server/data/DummyData.py", "benchmarks/distributed/rpc/parameter_server/launcher.py", "benchmarks/distributed/rpc/parameter_server/metrics/CPUMetric.py", "benchmarks/distributed/rpc/parameter_server/metrics/CUDAMetric.py", "benchmarks/distributed/rpc/parameter_server/metrics/MetricBase.py", "benchmarks/distributed/rpc/parameter_server/metrics/MetricsLogger.py", "benchmarks/distributed/rpc/parameter_server/metrics/ProcessedMetricsPrinter.py", "benchmarks/distributed/rpc/parameter_server/models/DummyModel.py", "benchmarks/distributed/rpc/parameter_server/trainers/DdpNcclTrainer.py", "benchmarks/distributed/rpc/parameter_server/trainers/DdpTrainerBase.py", "benchmarks/distributed/rpc/parameter_server/trainers/TrainerBase.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3a66a1cb99": {"title": "[clang-tidy] Exclude cppcoreguidelines-avoid-magic-numbers (#57841)", "body": "Summary:\nAdd cppcoreguidelines-avoid-magic-numbers exclusion to clang-tidy\nRemove existing nolint warnings using following script:\n```\nfor file in `git ls-files | grep -v \\.py`; do gsed '/^ *\\/\\/ NOLINTNEXTLINE(cppcoreguidelines-avoid-magic-numbers)/d' -i  $file; done\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57841\n\nReviewed By: samestep\n\nDifferential Revision: D28295045\n\nPulled By: malfet\n\nfbshipit-source-id: 7c6e8d1213c9593f169ed3df6a916498f1a97163", "pr_number": "57841", "files_changed": [".clang-tidy", "aten/src/ATen/CPUGeneratorImpl.cpp", "aten/src/ATen/Context.cpp", "aten/src/ATen/DLConvertor.cpp", "aten/src/ATen/SparseTensorUtils.cpp", "aten/src/ATen/benchmarks/quantize_per_channel.cpp", "aten/src/ATen/benchmarks/stateful_conv1d.cpp", "aten/src/ATen/benchmarks/tensor_add.cpp", "aten/src/ATen/core/Formatting.cpp", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/core/boxing/KernelFunction_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_function_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_function_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_legacy_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_lambda_test.cpp", "aten/src/ATen/core/boxing/impl/kernel_stackbased_test.cpp", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor_test.cpp", "aten/src/ATen/core/dispatch/backend_fallback_test.cpp", "aten/src/ATen/core/ivalue.cpp", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "aten/src/ATen/native/Activation.cpp", "aten/src/ATen/native/AdaptiveAveragePooling3d.cpp", "aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/Distance.cpp", "aten/src/ATen/native/Distributions.cpp", "aten/src/ATen/native/Embedding.cpp", "aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/FractionalMaxPool3d.cpp", "aten/src/ATen/native/GridSampler.cpp", "aten/src/ATen/native/Integration.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/LossMultiLabelMargin.cpp", "aten/src/ATen/native/MaxUnpooling.cpp", "aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp", "aten/src/ATen/native/NaiveDilatedConvolution.cpp", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/PixelShuffle.cpp", "aten/src/ATen/native/RNN.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReplicationPadding.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorIteratorReduce.cpp", "aten/src/ATen/native/TensorTransformations.cpp", "aten/src/ATen/native/UpSampleNearest3d.cpp", "aten/src/ATen/native/UpSampleTrilinear3d.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cpu/DistanceOpsKernel.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cpu/LerpKernel.cpp", "aten/src/ATen/native/cpu/MultinomialKernel.cpp", "aten/src/ATen/native/cpu/PowKernel.cpp", "aten/src/ATen/native/cpu/SoftMaxKernel.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cpu/UpSampleKernel.cpp", "aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp", "aten/src/ATen/native/mkldnn/MKLDNNConversions.cpp", "aten/src/ATen/native/mkldnn/MkldnnTensorMath.cpp", "aten/src/ATen/native/quantized/QTensor.cpp", "aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp", "aten/src/ATen/native/quantized/cpu/int_repr_quant.cpp", "aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp", "aten/src/ATen/native/quantized/cpu/qbatch_norm.cpp", "aten/src/ATen/native/quantized/cpu/qconv.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp", "aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp", "aten/src/ATen/native/quantized/cpu/qlinear.cpp", "aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp", "aten/src/ATen/native/quantized/cpu/qrelu.cpp", "aten/src/ATen/native/quantized/cpu/qsigmoid.cpp", "aten/src/ATen/native/quantized/fake_quant_per_tensor_affine.cpp", "aten/src/ATen/native/sparse/SparseTensor.cpp", "aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp", "aten/src/ATen/nnapi/nnapi_model_loader.cpp", "aten/src/ATen/quantized/Quantizer.cpp", "aten/src/ATen/test/Dict_test.cpp", "aten/src/ATen/test/NamedTensor_test.cpp", "aten/src/ATen/test/apply_utils_test.cpp", "aten/src/ATen/test/atest.cpp", "aten/src/ATen/test/basic.cpp", "aten/src/ATen/test/broadcast_test.cpp", "aten/src/ATen/test/cpu_caching_allocator_test.cpp", "aten/src/ATen/test/cpu_generator_test.cpp", "aten/src/ATen/test/cpu_profiling_allocator_test.cpp", "aten/src/ATen/test/cpu_rng_test.cpp", "aten/src/ATen/test/dlconvertor_test.cpp", "aten/src/ATen/test/extension_backend_test.cpp", "aten/src/ATen/test/half_test.cpp", "aten/src/ATen/test/ivalue_test.cpp", "aten/src/ATen/test/math_kernel_test.cpp", "aten/src/ATen/test/memory_format_test.cpp", "aten/src/ATen/test/mobile_memory_cleanup.cpp", "aten/src/ATen/test/native_test.cpp", "aten/src/ATen/test/quantized_test.cpp", "aten/src/ATen/test/scalar_tensor_test.cpp", "aten/src/ATen/test/scalar_test.cpp", "aten/src/ATen/test/tensor_interop_test.cpp", "aten/src/ATen/test/tensor_iterator_test.cpp", "aten/src/ATen/test/test_parallel.cpp", "aten/src/ATen/test/thread_init_test.cpp", "aten/src/ATen/test/undefined_tensor_test.cpp", "aten/src/ATen/test/vec256_test_all_types.cpp", "aten/src/ATen/test/vmap_test.cpp", "aten/src/ATen/test/vulkan_test.cpp", "aten/src/ATen/test/wrapdim_test.cpp", "aten/src/TH/THAllocator.cpp", "aten/src/TH/THGeneral.cpp", "benchmarks/cpp/convolution.cpp", "c10/core/GeneratorImpl.cpp", "c10/core/TensorImpl.cpp", "c10/test/core/DispatchKeySet_test.cpp", "c10/test/core/impl/SizesAndStrides_test.cpp", "c10/test/util/Array_test.cpp", "c10/test/util/Bitset_test.cpp", "c10/test/util/C++17_test.cpp", "c10/test/util/ConstexprCrc_test.cpp", "c10/test/util/Half_test.cpp", "c10/test/util/LeftRight_test.cpp", "c10/test/util/Metaprogramming_test.cpp", "c10/test/util/TypeIndex_test.cpp", "c10/test/util/TypeList_test.cpp", "c10/test/util/accumulate_test.cpp", "c10/test/util/bfloat16_test.cpp", "c10/test/util/either_test.cpp", "c10/test/util/intrusive_ptr_test.cpp", "c10/test/util/irange_test.cpp", "c10/test/util/logging_test.cpp", "c10/test/util/optional_test.cpp", "c10/test/util/ordered_preserving_dict_test.cpp", "c10/test/util/string_view_test.cpp", "c10/test/util/typeid_test.cpp", "caffe2/core/blob_test.cc", "caffe2/core/context.cc", "caffe2/core/context_test.cc", "caffe2/core/init.cc", "caffe2/core/int8_serialization.cc", "caffe2/core/memonger.cc", "caffe2/core/net.cc", "caffe2/core/net_async_tracing_test.cc", "caffe2/core/net_dag_utils_test.cc", "caffe2/core/net_simple.cc", "caffe2/core/net_test.cc", "caffe2/core/nomnigraph/tests/SubgraphMatcherTest.cc", "caffe2/core/nomnigraph/tests/TarjansImplTest.cc", "caffe2/core/observer_test.cc", "caffe2/core/operator.cc", "caffe2/core/operator_schema_test.cc", "caffe2/core/operator_test.cc", "caffe2/core/parallel_net_test.cc", "caffe2/core/plan_executor.cc", "caffe2/core/plan_executor_test.cc", "caffe2/core/prof_dag_counters.cc", "caffe2/core/stats_test.cc", "caffe2/core/timer_test.cc", "caffe2/core/transform_test.cc", "caffe2/distributed/file_store_handler.cc", "caffe2/ideep/operators/adam_op.cc", "caffe2/ideep/operators/dropout_op.cc", "caffe2/ideep/operators/pool_op.cc", "caffe2/ideep/operators/quantization/int8_given_tensor_fill_op.cc", "caffe2/ideep/operators/quantization/int8_pool_op.cc", "caffe2/ideep/operators/quantization/int8_relu_op.cc", "caffe2/ideep/operators/relu_op.cc", "caffe2/ideep/operators/spatial_batch_norm_op.cc", "caffe2/observers/time_observer_test.cc", "caffe2/onnx/backend.cc", "caffe2/operators/batch_matmul_op_test.cc", "caffe2/operators/batch_moments_op.cc", "caffe2/operators/box_with_nms_limit_op.cc", "caffe2/operators/collect_and_distribute_fpn_rpn_proposals_op.cc", "caffe2/operators/conv_transpose_op_mobile_test.cc", "caffe2/operators/dropout_op.cc", "caffe2/operators/feature_maps_ops.cc", "caffe2/operators/fused_rowwise_nbitfake_conversion_ops.cc", "caffe2/operators/fused_rowwise_random_quantization_ops.cc", "caffe2/operators/gelu_op.cc", "caffe2/operators/generate_proposals_op_test.cc", "caffe2/operators/generate_proposals_op_util_boxes_test.cc", "caffe2/operators/generate_proposals_op_util_nms_test.cc", "caffe2/operators/gru_unit_op.cc", "caffe2/operators/h_softmax_op.cc", "caffe2/operators/half_float_ops_test.cc", "caffe2/operators/heatmap_max_keypoint_op.cc", "caffe2/operators/jsd_op.cc", "caffe2/operators/last_n_window_collector.cc", "caffe2/operators/lengths_reducer_fused_nbit_rowwise_ops.cc", "caffe2/operators/lengths_reducer_ops.cc", "caffe2/operators/lengths_reducer_rowwise_8bit_ops.cc", "caffe2/operators/listwise_l2r_op.cc", "caffe2/operators/lpnorm_op.cc", "caffe2/operators/lstm_unit_op.cc", "caffe2/operators/mish_op.cc", "caffe2/operators/pow_op.cc", "caffe2/operators/quantized/int8_roi_align_op_test.cc", "caffe2/operators/quantized/int8_test.cc", "caffe2/operators/reservoir_sampling.cc", "caffe2/operators/resize_3d_op.cc", "caffe2/operators/resize_op.cc", "caffe2/operators/rmac_regions_op.cc", "caffe2/operators/rnn/recurrent_network_executor.cc", "caffe2/operators/roi_align_gradient_op.cc", "caffe2/operators/roi_align_op.cc", "caffe2/operators/roi_align_rotated_op.cc", "caffe2/operators/rsqrt_op.cc", "caffe2/operators/segment_reduction_op.cc", "caffe2/operators/softmax_with_loss_op.cc", "caffe2/operators/sparse_dropout_with_replacement_op.cc", "caffe2/operators/sparse_lp_regularizer_op.cc", "caffe2/operators/spatial_batch_norm_gradient_op.cc", "caffe2/operators/spatial_batch_norm_op.cc", "caffe2/operators/spatial_softmax_with_loss_op.cc", "caffe2/operators/sqr_op.cc", "caffe2/operators/sqrt_op.cc", "caffe2/operators/string_ops_test.cc", "caffe2/operators/stylizer_ops.cc", "caffe2/operators/text_file_reader_utils.cc", "caffe2/operators/text_file_reader_utils_test.cc", "caffe2/operators/utility_ops_test.cc", "caffe2/operators/weighted_sample_op.cc", "caffe2/opt/backend_cutting.cc", "caffe2/opt/backend_cutting_test.cc", "caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/converter_nomigraph_test.cc", "caffe2/opt/device_test.cc", "caffe2/opt/distributed_test.cc", "caffe2/opt/mobile_test.cc", "caffe2/opt/onnxifi_op.cc", "caffe2/opt/onnxifi_transformer.cc", "caffe2/opt/optimize_ideep.cc", "caffe2/opt/split_slss_test.cc", "caffe2/perfkernels/embedding_lookup_avx2.cc", "caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_avx2.cc", "caffe2/perfkernels/embedding_lookup_fused_8bit_rowwise_idx_avx2.cc", "caffe2/perfkernels/embedding_lookup_idx_avx2.cc", "caffe2/perfkernels/fused_nbit_rowwise_conversion.cc", "caffe2/perfkernels/math_cpu_avx2.cc", "caffe2/perfkernels/math_cpu_base.cc", "caffe2/predictor/emulator/data_filler_test.cc", "caffe2/predictor/predictor_test.cc", "caffe2/quantization/server/activation_distribution_observer.cc", "caffe2/quantization/server/caffe2_dnnlowp_utils.cc", "caffe2/quantization/server/conv_dnnlowp_acc16_op.cc", "caffe2/quantization/server/conv_dnnlowp_op.cc", "caffe2/quantization/server/dnnlowp.cc", "caffe2/quantization/server/elementwise_linear_dnnlowp_op.cc", "caffe2/quantization/server/elementwise_sum_dnnlowp_op_avx2.cc", "caffe2/quantization/server/fb_fc_packed_op.cc", "caffe2/quantization/server/fbgemm_pack_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_acc16_op.cc", "caffe2/quantization/server/fully_connected_dnnlowp_op.cc", "caffe2/quantization/server/fully_connected_fake_lowp_op_avx2.cc", "caffe2/quantization/server/group_norm_dnnlowp_op.cc", "caffe2/quantization/server/group_norm_dnnlowp_op_avx2.cc", "caffe2/quantization/server/kl_minimization.cc", "caffe2/quantization/server/lstm_unit_dnnlowp_op.cc", "caffe2/quantization/server/norm_minimization.cc", "caffe2/quantization/server/norm_minimization_avx2.cc", "caffe2/quantization/server/p99.cc", "caffe2/quantization/server/sigmoid.cc", "caffe2/quantization/server/spatial_batch_norm_dnnlowp_op.cc", "caffe2/quantization/server/spatial_batch_norm_dnnlowp_op_avx2.cc", "caffe2/quantization/server/tanh.cc", "caffe2/quantization/server/transpose.cc", "caffe2/queue/blobs_queue.cc", "caffe2/serialize/inline_container.cc", "caffe2/serialize/inline_container_test.cc", "caffe2/sgd/adadelta_op.cc", "caffe2/sgd/adagrad_fused.cc", "caffe2/sgd/adagrad_op.cc", "caffe2/sgd/adam_op.cc", "caffe2/sgd/decay_adagrad_op.cc", "caffe2/sgd/ftrl_op.cc", "caffe2/sgd/lars_op.cc", "caffe2/sgd/momentum_sgd_op.cc", "caffe2/sgd/rowwise_adagrad_fused.cc", "caffe2/sgd/storm_op.cc", "caffe2/sgd/wngrad_op.cc", "caffe2/sgd/yellowfin_op.cc", "caffe2/share/contrib/depthwise/depthwise3x3_conv_op.cc", "caffe2/share/contrib/depthwise/depthwise3x3_conv_op_test.cc", "caffe2/share/contrib/nnpack/conv_op.cc", "caffe2/share/contrib/nnpack/nnpack_test.cc", "caffe2/transforms/pattern_net_transform_test.cc", "caffe2/utils/bench_utils.cc", "caffe2/utils/cpuid.cc", "caffe2/utils/fixed_divisor_test.cc", "caffe2/utils/math_cpu.cc", "caffe2/utils/math_test.cc", "caffe2/utils/murmur_hash3.cc", "caffe2/utils/proto_utils.cc", "caffe2/utils/simple_queue_test.cc", "caffe2/utils/threadpool/ThreadPool.cc", "modules/detectron/select_smooth_l1_loss_op.cc", "modules/detectron/smooth_l1_loss_op.cc", "modules/detectron/softmax_focal_loss_op.cc", "modules/observers/net_observer_reporter_print.cc", "modules/observers/perf_observer.cc", "test/cpp/api/any.cpp", "test/cpp/api/autograd.cpp", "test/cpp/api/dataloader.cpp", "test/cpp/api/expanding-array.cpp", "test/cpp/api/fft.cpp", "test/cpp/api/functional.cpp", "test/cpp/api/init.cpp", "test/cpp/api/integration.cpp", "test/cpp/api/jit.cpp", "test/cpp/api/memory.cpp", "test/cpp/api/misc.cpp", "test/cpp/api/module.cpp", "test/cpp/api/moduledict.cpp", "test/cpp/api/modulelist.cpp", "test/cpp/api/modules.cpp", "test/cpp/api/nn_utils.cpp", "test/cpp/api/optim.cpp", "test/cpp/api/parallel_benchmark.cpp", "test/cpp/api/parameterdict.cpp", "test/cpp/api/rnn.cpp", "test/cpp/api/sequential.cpp", "test/cpp/api/serialize.cpp", "test/cpp/api/special.cpp", "test/cpp/api/static.cpp", "test/cpp/api/tensor.cpp", "test/cpp/api/tensor_indexing.cpp", "test/cpp/api/tensor_options.cpp", "test/cpp/api/tensor_options_cuda.cpp", "test/cpp/api/transformer.cpp", "test/cpp/jit/test_argument_spec.cpp", "test/cpp/jit/test_autodiff.cpp", "test/cpp/jit/test_backend.cpp", "test/cpp/jit/test_backend_compiler_lib.cpp", "test/cpp/jit/test_custom_class_registrations.cpp", "test/cpp/jit/test_custom_operators.cpp", "test/cpp/jit/test_fuser.cpp", "test/cpp/jit/test_ir.cpp", "test/cpp/jit/test_lite_interpreter.cpp", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/jit/test_misc.cpp", "test/cpp/jit/test_module_api.cpp", "test/cpp/jit/test_save_load.cpp", "test/cpp/jit/test_schema_matching.cpp", "test/cpp/jit/test_subgraph_utils.cpp", "test/cpp/jit/test_utils.cpp", "test/cpp/tensorexpr/test_aten.cpp", "test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_conv.cpp", "test/cpp/tensorexpr/test_cpp_codegen.cpp", "test/cpp/tensorexpr/test_expr.cpp", "test/cpp/tensorexpr/test_external_calls.cpp", "test/cpp/tensorexpr/test_ir_printer.cpp", "test/cpp/tensorexpr/test_ir_verifier.cpp", "test/cpp/tensorexpr/test_kernel.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_memdependency.cpp", "test/cpp/tensorexpr/test_reductions.cpp", "test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/test_train.cpp", "test/cpp/tensorexpr/test_train_impl.cpp", "test/cpp/tensorexpr/test_type.cpp", "test/cpp/tensorexpr/tutorial.cpp", "test/mobile/nnc/test_context.cpp", "third_party/miniz-2.0.8/miniz.c", "torch/csrc/api/include/torch/data/datasets/chunk.h", "torch/csrc/api/include/torch/linalg.h", "torch/csrc/api/include/torch/nn/functional/distance.h", "torch/csrc/api/include/torch/nn/functional/loss.h", "torch/csrc/api/include/torch/nn/functional/padding.h", "torch/csrc/api/include/torch/nn/functional/upsampling.h", "torch/csrc/api/include/torch/nn/functional/vision.h", "torch/csrc/api/include/torch/nn/init.h", "torch/csrc/api/include/torch/nn/options/activation.h", "torch/csrc/api/include/torch/nn/options/adaptive.h", "torch/csrc/api/include/torch/nn/options/batchnorm.h", "torch/csrc/api/include/torch/nn/options/distance.h", "torch/csrc/api/include/torch/nn/options/dropout.h", "torch/csrc/api/include/torch/nn/options/embedding.h", "torch/csrc/api/include/torch/nn/options/instancenorm.h", "torch/csrc/api/include/torch/nn/options/loss.h", "torch/csrc/api/include/torch/nn/options/normalization.h", "torch/csrc/api/include/torch/nn/options/transformer.h", "torch/csrc/api/include/torch/nn/options/transformerlayer.h", "torch/csrc/api/include/torch/nn/utils/clip_grad.h", "torch/csrc/api/include/torch/optim/adagrad.h", "torch/csrc/api/include/torch/optim/adam.h", "torch/csrc/api/include/torch/optim/adamw.h", "torch/csrc/api/include/torch/optim/lbfgs.h", "torch/csrc/api/include/torch/optim/rmsprop.h", "torch/csrc/api/src/data/datasets/mnist.cpp", "torch/csrc/api/src/nn/init.cpp", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/profiler_legacy.h", "torch/csrc/autograd/utils/wrap_outputs.h", "torch/csrc/cuda/Module.cpp", "torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/example/benchmark.cpp", "torch/csrc/deploy/test_deploy.cpp", "torch/csrc/distributed/rpc/rpc_agent.h", "torch/csrc/generic/Storage.cpp", "torch/csrc/generic/serialization.cpp", "torch/csrc/jit/codegen/cuda/executor_kernel_arg.h", "torch/csrc/jit/codegen/cuda/kernel_cache.h", "torch/csrc/jit/codegen/fuser/codegen.cpp", "torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp", "torch/csrc/jit/codegen/fuser/executor.cpp", "torch/csrc/jit/frontend/lexer.cpp", "torch/csrc/jit/frontend/parse_string_literal.h", "torch/csrc/jit/frontend/script_type_parser.cpp", "torch/csrc/jit/frontend/tree.h", "torch/csrc/jit/passes/batch_mm.cpp", "torch/csrc/jit/passes/create_functional_graphs.cpp", "torch/csrc/jit/passes/decompose_ops.cpp", "torch/csrc/jit/passes/frozen_ops_to_mkldnn.cpp", "torch/csrc/jit/passes/graph_fuser.cpp", "torch/csrc/jit/passes/guard_elimination.cpp", "torch/csrc/jit/passes/inline_autodiff_subgraphs.h", "torch/csrc/jit/passes/liveness.h", "torch/csrc/jit/passes/onnx/peephole.cpp", "torch/csrc/jit/passes/onnx/shape_type_inference.cpp", "torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp", "torch/csrc/jit/passes/peephole_alias_sensitive.cpp", "torch/csrc/jit/passes/quantization/helper.cpp", "torch/csrc/jit/passes/quantization/insert_quant_dequant.cpp", "torch/csrc/jit/passes/utils/memory_dag.h", "torch/csrc/jit/passes/utils/subgraph_utils.h", "torch/csrc/jit/runtime/autodiff.cpp", "torch/csrc/jit/runtime/logging.cpp", "torch/csrc/jit/runtime/register_ops_utils.h", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp", "torch/csrc/jit/runtime/static/impl.cpp", "torch/csrc/jit/serialization/import_source.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/unpickler.h", "torch/csrc/jit/tensorexpr/block_codegen.h", "torch/csrc/jit/tensorexpr/cuda_codegen.cpp", "torch/csrc/jit/tensorexpr/expr.cpp", "torch/csrc/jit/tensorexpr/external_functions.cpp", "torch/csrc/jit/tensorexpr/hash_provider.h", "torch/csrc/jit/tensorexpr/ir.h", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/operators/conv2d.cpp", "torch/csrc/serialization.cpp", "torch/csrc/utils/invalid_arguments.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/throughput_benchmark-inl.h", "torch/csrc/utils/throughput_benchmark.h"], "labels": ["Merged", "cla signed", "oncall: distributed", "oncall: jit"]}, "a46e927b1a": {"title": "[torch] handle embedding bag with empty bag (#57446)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57446\n\nGPU EmbeddingBag is handling L == 0 . Matching CPU version.\n\nTest Plan: buck test //deeplearning/fbgemm/fbgemm_gpu:split_table_batched_embeddings_test -- test_forward\n\nReviewed By: jiyuanzFB\n\nDifferential Revision: D28145090\n\nfbshipit-source-id: d91d0050ddd5636293a8965d3eece02633918f4c", "pr_number": "57446", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp"], "labels": ["Merged", "cla signed", "fb-exported"]}, "88a1e8eb01": {"title": "Add EMA to DecayAdagrad (#57866)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57866\n\nAs titled\n\nTest Plan: f271267365\n\nReviewed By: lanlanfb\n\nDifferential Revision: D28292875\n\nfbshipit-source-id: f6532048eb558afce87fdada3b7dfa8457a1f538", "pr_number": "57866", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "737f48dfc5": {"title": "Remove _save_data() and _load_data() from mobile (#57879)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57879\n\n_save_data() and _load_data() were designed as a protocol of data serialization of trainer client. As confirmed with kwanmacher and dreiss , they are not used. In addition, there's no plan to use them in Federated Learning flow. Remove them for now.\n\nTest Plan: Imported from OSS\n\nReviewed By: kwanmacher\n\nDifferential Revision: D28306682\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 1b993ce4d78e372ae9b83bcbe496a196f9269d47", "pr_number": "57879", "files_changed": ["test/cpp/jit/test_lite_trainer.cpp", "torch/csrc/jit/mobile/export_data.cpp", "torch/csrc/jit/mobile/export_data.h", "torch/csrc/jit/mobile/import_data.cpp", "torch/csrc/jit/mobile/import_data.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "e8fb167b17": {"title": "[PyTorch Edge] Reuse constant table from ts in bytecode (#56002)", "body": "Summary:\n## Note:\n**This change will include the feature, but the feature is not on. It will be enabled and bytecode version will be bumped in D27844651 (https://github.com/pytorch/pytorch/commit/8c04593c0a486bea7e2cbec348298d348742e096).**\n\nJit will generate constant tensor, and it locates in the constant folder (can find them after unzip model.ptl). Bytecode generated by lite interpreter also includes constant tensor, which are almost the same with the constant tensor value from jit. This pr will let lite interpreter reuses the constant tensor from jit, instead of reproducing the similar tensor values. The reading and writing session will be as following.\n\nMore details and background can found in [Lite Interpreter Model Size Issue](https://fb.quip.com/OSidAcjhL9LS).\nData size comparison can be found in [Model size analysis](https://fb.quip.com/oEm6A4bhbo06)\n\n### Write\n1. In `export_module.cpp`, store all constant tensor value from jit in an `unordered_map constants_from_jit`, where the tensor value use tensor string as a hash. constants_from_jit is a map: (tensor) => (archive_name, index). When writing bytecode archive `writeByteCode()`, the map `constants_from_jit` will also be passed all the way to it's pickler.\n\n2. In `pickler.cpp`, a new map tensors_archive_table_ is added. It is also a map: (tensor) => (archive_name, index). The corresponding function to update the map is `updateTensorsArchiveTable`. When pushing the storage of a tensor, if the tensor exists in `tensors_archive_table_`, the root key will be `{archive_name}/{index}`, instead of `{index}`. For example, the tensor\n```\n     torch._utils._rebuild_tensor_v2(pers.obj(('storage', torch.FloatStorage, '0', 'cpu', 90944),),\n       0,\n       (1, 116, 28, 28),\n       (90944, 784, 28, 1),\n       False,\n       collections.OrderedDict()),\n```\nwill be like following instead\n```\n     torch._utils._rebuild_tensor_v2(pers.obj(('storage', torch.FloatStorage, 'constants/0', 'cpu', 90944),),\n       0,\n       (1, 116, 28, 28),\n       (90944, 784, 28, 1),\n       False,\n       collections.OrderedDict()),\n```\n\n**Note**:  Only tensors in bytecode archive will be different. The tensors in other archive remains the same, because `updateTensorsArchiveTable()` is only called when `use_tensors_archive_table_` is `true`, and `tensors_archive_table_` is only set as `true` when `bytecode_version` is a valid number.\n\n### Read\n1. In `import.cpp`, the function `read_record` passed to Unpickler is updated. The argument of `read_record` is the root key. In version 4, the root key will just be index, and `archive_name_plus_slash` + `name` will be used to get the tensor. With this change (version 5+), `read_record` will check if slash exists in the argument `name`. If it does, it means the argument is `archive_name/index`, and it can be used to get tensor directly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56002\n\nghstack-source-id: 128498244\n\nTest Plan:\n### Verify the new model generated from this pr can reuse constant table and the numerical result is the same.\n1. Build pytorch locally. `MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ USE_CUDA=0 DEBUG=1 MAX_JOBS=16 python setup.py develop`\n2. Run `python save_lite.py`\n```\nimport torch\n\n# ~/Documents/pytorch/data/dog.jpg\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'shufflenet_v2_x1_0', pretrained=True)\nmodel.eval()\n\n# sample execution (requires torchvision)\nfrom PIL import Image\nfrom torchvision import transforms\nimport pathlib\nimport tempfile\nimport torch.utils.mobile_optimizer\n\ninput_image = Image.open('~/Documents/pytorch/data/dog.jpg')\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\n\nwith torch.no_grad():\n    output = model(input_batch)\n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\nprint(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nprint(torch.nn.functional.softmax(output[0], dim=0))\n\ntraced = torch.jit.trace(model, input_batch)\nsum(p.numel() * p.element_size() for p in traced.parameters())\ntf = pathlib.Path('~/Documents/pytorch/data/data/example_debug_map_with_tensorkey.ptl')\n\ntorch.jit.save(traced, tf.name)\nprint(pathlib.Path(tf.name).stat().st_size)\ntraced._save_for_lite_interpreter(tf.name)\nprint(pathlib.Path(tf.name).stat().st_size)\nprint(tf.name)\n\n```\n\n3. Run `python test_lite.py`\n```\nimport torch\nfrom torch.jit.mobile import _load_for_lite_interpreter\n# sample execution (requires torchvision)\nfrom PIL import Image\nfrom torchvision import transforms\n\ninput_image = Image.open('~/Documents/pytorch/data/dog.jpg')\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\nreload_lite_model = _load_for_lite_interpreter('~/Documents/pytorch/experiment/example_debug_map_with_tensorkey.ptl')\n\nwith torch.no_grad():\n    output_lite = reload_lite_model(input_batch)\n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\nprint(output_lite[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nprint(torch.nn.functional.softmax(output_lite[0], dim=0))\n\n```\n4. Compare the result with pytorch in master and pytorch built locally with this change, and see the same output.\n5. The model size was 16.1 MB and becomes 12.9 with this change.\n\nSize comparison in production models:\n\n{F603127047}\n\nReviewed By: iseeyuan\n\nDifferential Revision: D27759891\n\nfbshipit-source-id: 34e0cb8149011c46c1910165b545c137d7a0b855", "pr_number": "56002", "files_changed": ["torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import_read.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/pickler.h"], "labels": ["Merged", "Reverted", "cla signed", "oncall: jit"]}, "0c2d38264a": {"title": "Improve BatchNorm1d performance (CUDA) (#57786)", "body": "Summary:\nPart of gh-38915, resubmit of gh-57034\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57786\n\nReviewed By: mruberry\n\nDifferential Revision: D28290284\n\nPulled By: ngimel\n\nfbshipit-source-id: 8768578ba9ace6a948cb8145c0091e0ea49b12da", "pr_number": "57786", "files_changed": ["aten/src/ATen/AccumulateType.cpp", "aten/src/ATen/AccumulateType.h", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/SharedReduceOps.h", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "aten/src/ATen/native/cuda/ReduceMomentKernel.cu", "benchmarks/operator_benchmark/pt/batchnorm_test.py", "c10/macros/Macros.h"], "labels": ["Merged", "cla signed", "open source"]}, "14282232d9": {"title": "Fix `generate_not_implemented_tests` not testing unknown types correctly (#56997)", "body": "Summary:\nCurrently, the test code is not testing unknown types correctly because `op` is overwritten in the for-loop (i.e., currently only `__ior__` is tested).\nThis PR fixes the test `generate_not_implemented_tests` to bind operator name to each method, and remove operators currently unsupported (`__rand__`, \u2026).\n\ncc/ mruberry This fix is be needed to add tests for the operator we are going to introduce (e.g., `__rand__`)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56997\n\nReviewed By: astaff\n\nDifferential Revision: D28118465\n\nPulled By: mruberry\n\nfbshipit-source-id: c5a466a7604262ed5490862300d47043aff63d0b", "pr_number": "56997", "files_changed": ["test/test_binary_ufuncs.py"], "labels": ["Merged", "cla signed", "open source"]}, "fc55290e5b": {"title": "Fix distributed autograd gradients synchronization (#57792)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57792\n\nThere are two problems when using CUDA RPC with distributed autograd\nand distributed optimizer:\n\n1) In local autograd engine, all autograd functions/nodes, including\nAccumualteGrad will use the forward stream for backward computation.\nBut distributed autograd skips AccumulateGrad autograd function/node\nand directly calls into `AccumulateGrad::accumulateGrad`. As the\nresult, it will use the default stream to accumulate gradients\ninstead of the forward stream. This commit changes that and uses the\nforward stream to accumulate gradients, matching forward behavior.\n2) Distributed optimizer and distributed autograd backward are\nseparate RPC calls, and CUDA streams are not synchronized across\ndifferent RPC calls. As a result, distributed optimizer might\nconsume gradients before they are ready. This commit uses CUDA\nevents to record the completion of gradient computation, and use\nthose events to block current streams when getGradients() are called.\n\nTest Plan: Imported from OSS\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D28274876\n\nPulled By: mrshenli\n\nfbshipit-source-id: 22e607152324ae918084066cde8c5dbb418bba7c", "pr_number": "57792", "files_changed": ["test/cpp/dist_autograd/test_dist_autograd.cpp", "torch/csrc/distributed/autograd/context/container.cpp", "torch/csrc/distributed/autograd/context/context.cpp", "torch/csrc/distributed/autograd/context/context.h", "torch/testing/_internal/distributed/rpc/dist_autograd_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "5c67d8dfd3": {"title": "ATen lu_unpack. Required for making `torch.lu_solve` differentiable. (#46913)", "body": "Summary:\nBackward methods for `torch.lu` and `torch.lu_solve` require the `torch.lu_unpack` method.\nHowever, while `torch.lu` is a Python wrapper over a native function, so its gradient is implemented via `autograd.Function`,\n`torch.lu_solve` is a native function, so it cannot access `torch.lu_unpack` as it is implemented in Python.\n\nHence this PR presents a native (ATen) `lu_unpack` version. It is also possible to update the gradients for `torch.lu` so that backward+JIT is supported (no JIT for `autograd.Function`) with this function.\n\n~~The interface for this method is different from the original `torch.lu_unpack`, so it is decided to keep it hidden.~~\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46913\n\nReviewed By: astaff\n\nDifferential Revision: D28117714\n\nPulled By: mruberry\n\nfbshipit-source-id: befd33db12ecc147afacac792418b6f4948fa4a4", "pr_number": "46913", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebra.h", "aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_linalg.py", "test/test_namedtuple_return_api.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "torch/_torch_docs.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/functional.py", "torch/jit/_builtins.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: autograd", "module: linear algebra", "open source", "triaged"]}, "50e22e1e08": {"title": "Remove tmp folder when run unit test (#57800)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57799\n\nRemove tmp folder when run cpp_api_parity test cases one by one.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57800\n\nReviewed By: mruberry\n\nDifferential Revision: D28303956\n\nPulled By: ngimel\n\nfbshipit-source-id: ec313d0c14ae432bc3862988eb00742810ef53e2", "pr_number": "57800", "files_changed": ["test/cpp_api_parity/functional_impl_check.py", "test/cpp_api_parity/module_impl_check.py"], "labels": ["Merged", "cla signed", "open source"]}, "300363b54f": {"title": "CLN Removes unused RReLU code (#57672)", "body": "Summary:\nFollow up from https://github.com/pytorch/pytorch/issues/49788\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57672\n\nReviewed By: mruberry\n\nDifferential Revision: D28291489\n\nPulled By: ngimel\n\nfbshipit-source-id: 4691051165756d38ef37b48a78f456fa44d27022", "pr_number": "57672", "files_changed": ["aten/src/THCUNN/RReLU.cu", "aten/src/THCUNN/generic/THCUNN.h"], "labels": ["Merged", "cla signed", "open source"]}, "cbfce376a8": {"title": "[AutoAccept][Codemod][FBSourceClangFormatLinter] Daily `arc lint --take CLANGFORMAT`", "body": "Reviewed By: zertosh\n\nDifferential Revision: D28319469\n\nfbshipit-source-id: 8295597a8ee16b2fef3f7aacdd6c892cb22db988", "pr_number": null, "files_changed": ["c10/test/util/string_view_test.cpp", "test/cpp/jit/test_lite_trainer.cpp", "test/cpp/tensorexpr/test_boundsinference.cpp", "test/cpp/tensorexpr/test_cpp_codegen.cpp", "test/cpp/tensorexpr/test_external_calls.cpp", "test/cpp/tensorexpr/test_loopnest.cpp", "test/cpp/tensorexpr/test_memdependency.cpp", "test/cpp/tensorexpr/test_registerizer.cpp", "test/cpp/tensorexpr/test_simplify.cpp", "test/cpp/tensorexpr/tutorial.cpp", "torch/csrc/jit/runtime/static/impl.cpp"], "labels": []}, "bc798cdc1d": {"title": "Add run_master_build workflow (#57899)", "body": "Summary:\nAutomatically generate this workflow by filtering all jobs that has *filters:branches:only:master* restriction\n\nAdd probot config to schedule this workflow if `ci/master` label is set on PR\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57899\n\nReviewed By: walterddr\n\nDifferential Revision: D28311838\n\nPulled By: malfet\n\nfbshipit-source-id: 63df81212279f5edd8463d1f6b22f37253c53a98", "pr_number": "57899", "files_changed": [".circleci/config.yml", ".circleci/generate_config_yml.py", ".circleci/verbatim-sources/header-section.yml", ".github/pytorch-circleci-labels.yml"], "labels": ["Merged", "ci/master", "cla signed"]}, "727c1d69d7": {"title": "Remove unnecessary indirection through torch::autograd::impl::pyobj/set_pyobj (#57733)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57733\n\nI'm going to be modifying the APIs here, so the less API surface\ncovering these functions the better.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28289082\n\nPulled By: ezyang\n\nfbshipit-source-id: 4b71270bb82e0d6baa4dfed2f2e4ee8831f590b5", "pr_number": "57733", "files_changed": ["torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/variable.cpp", "torch/csrc/autograd/variable.h"], "labels": ["Merged", "cla signed"]}, "d115e81a32": {"title": "Fix document around DDP uneven inputs (#57448)", "body": "Summary:\nTypo fix and additional clarifications on the API.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57448\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28153264\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 9bd35d918299ad7e080785d755f97b966f826615", "pr_number": "57448", "files_changed": ["torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "710a83d09f": {"title": "Remove code and logic for old style custom autograd Function (#57357)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/30696\n\n### Release Notes\nInstantiating a custom autograd function is now deprecated. Users should call `.apply()` on the class itself because it is a static method.\n\n--end release notes--\n - There are a couple error messages that we can't entirely remove because accessing these attributes of the autograd function instance may segfault (due to cdata being nullptr). Also added a TORCH_CHECK for the name attribute which previously segfaulted.\n - Error message updated to convey 1) old-style functions have been deprecated 2) this access pattern was once valid\n - Updates variable -> Tensor for some error messages\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57357\n\nReviewed By: mrshenli\n\nDifferential Revision: D28193095\n\nPulled By: soulitzer\n\nfbshipit-source-id: f021b105e9a3fd4a20d6ee3dfb6a06a8c34b10ca", "pr_number": "57357", "files_changed": ["test/test_autograd.py", "torch/autograd/function.py", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_function.h"], "labels": ["Merged", "cla signed", "module: deprecation"]}, "29753339b7": {"title": "Do not download slow test when on sandcastle (#57953)", "body": "Summary:\nDownloading slow_test list on SC causes timeout, this is even a bigger issue since `common_utils.py` is reused in many internal projects/modules.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57953\n\nTest Plan: CI\n\nReviewed By: janeyx99\n\nDifferential Revision: D28325527\n\nfbshipit-source-id: ae47c9e43ad6f416008005bb26ceb2f3d6966f2e", "pr_number": "57953", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "b587354e4c": {"title": "Add Python-3.9 CI testing (#50992)", "body": "Summary:\nSkip number of tests adjust typing handling\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50992\n\nReviewed By: walterddr\n\nDifferential Revision: D26170388\n\nPulled By: malfet\n\nfbshipit-source-id: 47852512aa3d5c25faf6687bcd0b1cbb332b0b20", "pr_number": "50992", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh", ".circleci/docker/common/install_conda.sh", "test/distributed/test_c10d_spawn_gloo.py", "test/distributed/test_c10d_spawn_nccl.py", "test/run_test.py", "test/test_fx.py", "torch/utils/data/_typing.py"], "labels": ["Merged", "cla signed"]}, "fea3824214": {"title": "Ensure torch.save() deterministic output (#57536)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/42163.\n\n## {emoji:1f525} Pitch\n\nCurrently, the binary outputs produced by `torch.save()` are non-deterministic (as pointed out in https://github.com/pytorch/pytorch/issues/42163). This means that running a simple snippet that creates a tensor (or a model) twice will produce output files with a different `md5` sum.\n\n**Why does this occur?**\nThe cause of this behavior lies in the fact that the `obj._cdata` is used to identify a tensor and is written to a file, but the `_cdata` attribute is of course non-deterministic:\nhttps://github.com/pytorch/pytorch/blob/a80b215a9ac089cdd3586060467615fa3c4bffe2/torch/serialization.py#L416\n\n**Why does this matter?**\nReproducibility is essential for many Machine Learning projects.\nFor instance, when using [`dvc`](https://dvc.org/) you would expect that if none of the dependencies of a stage  of a ML pipeline has changed, then running the same stage another time will produce the same binary output. For the reasons explained above, with `torch` this was not the case, so this PR tries to fix this issue.\n\n## {emoji:1f4cc} Content of this PR\n### What changes?\n- The `persistent_id()` function now returns a deterministic value, rather than `obj._cdata` (which depends on runtime).\n- As a consequence, `torch.save(obj, \"output.pt\")` produces a deterministic output, i.e. the `md5` hash of `output.pt` is determinstic. See **Test 1** and **Test 2** below.\n\n### What does not change?\n- If an `obj` contains several tensors that share the same underlying data (e.g. they are views of the same tensor),the `obj_key` returned by `persistent_id()` is still going to be the same for all of them\n- As a consequence, serialization optimizes disk storage by storing only necessary tensors, rather than writing one tensor per view. See **Test 3** below.\n\n## \ufffd How to test\n\n### Test 1: snipped from https://github.com/pytorch/pytorch/issues/42163\nConsider the following `snippet_1.py` (from https://github.com/pytorch/pytorch/issues/42163).\n```python\nimport hashlib\nimport torch\n\ndef get_sha256_hash(file: str, chunk_size: int = 4096) -> str:\n    hasher = hashlib.sha256()\n    with open(file, \"rb\") as fh:\n        for chunk in iter(lambda: fh.read(chunk_size), b\"\"):\n            hasher.update(chunk)\n    return hasher.hexdigest()\n\nfile = \"tensor.pt\"\nhashes = []\nfor _ in range(5):\n    obj = torch.ones(1)\n    torch.save(obj, file)\n    hashes.append(get_sha256_hash(file)[:8])\n    del obj\n\nhash = hashes[0]\nassert all(other == hash for other in hashes[1:])\nprint(hash)\n```\n\nOn `master` you obtain an error\n```bash\n$ python snippet_1.py\nTraceback (most recent call last):\n  File \"save_tensor.py\", line 84, in <module>\n    assert all(other == hash for other in hashes[1:])\nAssertionError\n```\nwhile on this PR branch you should get the following consistent behaviour:\n```bash\n$ for run in {1..2}; do python snippet_1.py; done\n600a83cb\n600a83cb\n```\n\n### Test 2: Deterministic save of `Tensor` and `nn.Module` instances\nConsider the following `snippet_2.py`\n```python\nimport torch\ntorch.manual_seed(0)\nx = torch.tensor([8., 8., 5., 0.])\ntorch.save(x, \"out_tensor.pt\")\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\ntorch.save(model, \"out_model.pt\")\n```\nOn `master` branch, the `md5` hash of `out_tensor.pt` and `out_model.pt` are non-determinstic, for instance you may get\n```bash\n$ for run in {1..2}; do python snippet_2.py; md5 out_*pt; done\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_model.pt) = 92dca4a310b691e893f3cb41d64d5af1\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_tensor.pt) = a4ef290583f50a9c203a42d0cfc078af\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_model.pt) = de3cb9791a66af8aed77ed7224bd1d5c\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_tensor.pt) = 3b8a6009d3a0be5b9dd94152dcc0c7cb\n```\nwhile on this PR branch you should get the following consistent behaviour:\n```bash\n$ for run in {1..2}; do python snippet_2.py; md5 out_*pt; done\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_model.pt) = dba75fd50a190e4e7fa89b7a2477bab7\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_tensor.pt) = 029f52f0706d6c813cc796d3cdcd3eb0\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_model.pt) = dba75fd50a190e4e7fa89b7a2477bab7\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_tensor.pt) = 029f52f0706d6c813cc796d3cdcd3eb0\n```\n\n### Test 3: Views of the same tensor are not re-written to file\nConsider the following `snippet_3.py`.\n```python\nimport torch\ntorch.manual_seed(0)\nx = torch.rand(1_000, 1_000)\ny = x.T\nz = x.view(1_000_000, 1)\n\ntorch.save({\"x\": x}, \"out_tensor_x.pt\")\ntorch.save({\"x\": x, \"y\": y, \"z\": z}, \"out_tensor_xyz.pt\")\n```\nBoth on `master` branch and on this  PR branch you should get two output files with same size:\n```bash\n$ python snippet_3.py && du -sh out_tensor*pt && md5 out_*pt\n3.8M    out_tensor_x.pt\n3.8M    out_tensor_xyz.pt\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_tensor_x.pt) = eda516d9156177b27bdc2a75c9064d9b\nMD5 (https://github.com/pytorch/pytorch/commit/bc9e8af21875dafafe9bbd25c8f542b20b2e660f) (out_tensor_xyz.pt) = 333b869f5b93ced7b8649ab1571eb8e3\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57536\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28304728\n\nPulled By: ailzhang\n\nfbshipit-source-id: 49788e566a3cd2c6c36dc801e6bdd8f42c9459cb", "pr_number": "57536", "files_changed": ["torch/serialization.py"], "labels": ["Merged", "cla signed", "open source"]}, "bf053a1296": {"title": "Fix hasattr support type (#57950)", "body": "Summary:\n`hasattr` is partially supported. This PR fixes that in the builtin table.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57950\n\nReviewed By: pbelevich\n\nDifferential Revision: D28329005\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: c4cfba9badcc8f7cbc8250a5c21dfb62b35a83fc", "pr_number": "57950", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["Merged", "cla signed"]}, "ba07aaf211": {"title": "Fix typo in warning for spawn method (#57927)", "body": "Summary:\nFix typo in warning for spawn method\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57927\n\nReviewed By: suo\n\nDifferential Revision: D28326390\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b0c12b1020d713865687f94f28ab2873ae260c23", "pr_number": "57927", "files_changed": ["torch/multiprocessing/spawn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "98fcdb8005": {"title": "add cuda memory and distributed metadata (#57252)", "body": "Summary:\nImplementation for https://github.com/pytorch/kineto/issues/155\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57252\n\nReviewed By: gdankel\n\nDifferential Revision: D28294662\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 3c71ffa333e341ff8113e891681a4905f54802dc", "pr_number": "57252", "files_changed": ["torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/profiler/profiler.py"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "ebb1b74f65": {"title": "Fix json parse error for profiler call stack (#57099)", "body": "Summary:\nThe call stack in profiler result json file lacks surrounding double quotes, and result in json parse error.\nThis PR just add it.\n\nilia-cher\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57099\n\nReviewed By: gdankel\n\nDifferential Revision: D28324182\n\nPulled By: ilia-cher\n\nfbshipit-source-id: dc479a023bb25de27c414629a27d624d64457c3e", "pr_number": "57099", "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "f0f69c5dc1": {"title": "torch.where is now mentioning Bool rather than Byte when given wrong dtype mask (#57942)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/49769\n\ntorch.where is now mentioning Bool instead of Byte when given wrong dtype mask\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57942\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28330921\n\nPulled By: ngimel\n\nfbshipit-source-id: 44a01e1daf1790308804ca7bb606f745c3eb71e1", "pr_number": "57942", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "f1d01b9488": {"title": "Disable test for quicklint (#57968)", "body": "Summary:\nDisabling until we fix https://github.com/pytorch/pytorch/issues/57967\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57968\n\nPulled By: driazati\n\nReviewed By: samestep\n\nDifferential Revision: D28330226\n\nfbshipit-source-id: 7ea130e0cf7b94959a7db18838d21e4711716625", "pr_number": "57968", "files_changed": ["tools/test/test_actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "747312bf61": {"title": "Support for accumulate nodes traversal and to access op names in the compare function (#57685)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57685\n\n- Accumulate traversal : `minimizer.settings.traverse_method = \"accumulate\" `\n   - Feature\n   - net_min_tests\n- Return op name to the compare function so that we can map the cosine similarity to the individual ops\n- Fix the settings combinations in net_min_tests\n\nTest Plan:\nbuck test glow/fb/nnpi/lowering:net_min_tests\n\nNNPI_LOG_LEVEL=5 USE_INF_API=1 buck run mode/opt -j 12 --config fbcode//cxx.link_weight=3 --config misc.strip_binaries=debug-non-line -c glow.nnpi_project_name='fb-nnpi-nextgen' ai_codesign/video/inference:xrayvideo_2019a_eval -- --job create --model_a model_prod --device_a PTCPU --trace_a none --model_b model_v3 --device_b NNPI --trace_b fusion --replace_b true --log_level INFO --use_scrambled false --save_repro false --num_ab_runs 0 --symbolic_trace_b true --save_modified_model_b false\n\nUSE_INF_API=1 buck test glow/fb/nnpi/lowering:net_min_tests\n\nReviewed By: 842974287\n\nDifferential Revision: D27867010\n\nfbshipit-source-id: 6a756468b1f1fe24ef0400669d911825a7562484", "pr_number": "57685", "files_changed": ["torch/fx/experimental/fx2trt/example/fx2trt_example.py", "torch/fx/passes/net_min_base.py", "torch/fx/passes/tools_common.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "38500d5d7b": {"title": "[RPC Framework] Move the annotation w/ bold effect out of the quotes (#57965)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57965\n\nThe bold effect does not work under quotes, so move it out.\nghstack-source-id: 128570357\n\nTest Plan:\nlocally view\n\n{F614715259}\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28329694\n\nfbshipit-source-id: 299b427f4c0701ba70c84148f65203a6e2d6ac61", "pr_number": "57965", "files_changed": ["docs/source/rpc.rst"], "labels": ["Merged", "cla signed"]}, "fc9c486044": {"title": "Add enabling default instructions flag for mobile (#57778)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57778\n\nTest Plan: Imported from OSS\n\nReviewed By: iseeyuan\n\nDifferential Revision: D28268997\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 5571b233d03d3aa80c820ee4245b4d0d3b70f924", "pr_number": "57778", "files_changed": ["torch/csrc/jit/runtime/interpreter.cpp", "torch/csrc/jit/runtime/interpreter.h", "torch/csrc/jit/runtime/interpreter/code_impl.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "b8ca1219de": {"title": "Add tests for custom state_dict save/load methods in TorchScript (#57886)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57886\n\nReviewed By: jamesr66a\n\nDifferential Revision: D28309228\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: 6ac60b1d4a8017aefb6f6dff49cde598de000265", "pr_number": "57886", "files_changed": ["test/jit/test_module_apis.py", "test/test_jit.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "c714596027": {"title": "[kineto] Update Kineto submodule, cupti library paths (#57789)", "body": "Summary:\nUpdate kineto submodule, improve cupti detection\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57789\n\nTest Plan: CI\n\nReviewed By: ngimel\n\nDifferential Revision: D28297175\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 5895270fae160097ae8872a592984d0e4a1b187b", "pr_number": "57789", "files_changed": ["cmake/Dependencies.cmake", "test/test_profiler.py", "third_party/kineto", "torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/profiler/__init__.py", "torch/profiler/profiler.py"], "labels": ["Merged", "cla signed"]}, "71ca3e99af": {"title": "Only use actually mismatched elements for reporting in `torch.testing` (#57923)", "body": "Summary:\nRedo of https://github.com/pytorch/pytorch/issues/57135 out of stack\n\n ---\n\nCurrently all values are used for the reported absolute and relative differences. This usually works fine, but breaks down for the extremals:\n\n```python\ntorch.testing.assert_close(torch.tensor([1.0, 0.0]), torch.tensor([2.0, 0.0]))\n```\n\n```\n[...]\nGreatest absolute difference: 1.0 at 0 (up to 1e-05 allowed)\nGreatest relative difference: nan at 1 (up to 1.3e-06 allowed)\n```\n\nAlthough the second element is matching it is listed as offender for the greatest relative difference. The `NaN` stems from the `0 / 0` division.\n\nTo overcome this, we should only use the values that were considered a mismatch for the reported stats.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57923\n\nReviewed By: ngimel\n\nDifferential Revision: D28317316\n\nPulled By: mruberry\n\nfbshipit-source-id: 4c604493bbe13b37f41225ea9af9e839a7304161", "pr_number": "57923", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "a0d686c9cd": {"title": "OpInfo: select (#57731)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57731\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28318229\n\nPulled By: mruberry\n\nfbshipit-source-id: ec9058fd188b82de80d3a2f1a1ba07f36d8d0741", "pr_number": "57731", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "5840c8cfd8": {"title": "[nccl] log rank when communicator is aborted (#57974)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57974\n\nWe see this error quite a bit in internal workflows, would be useful\nto have this additional logging information here.\nghstack-source-id: 128602199\n\nTest Plan: CI\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28331693\n\nfbshipit-source-id: 25398c6a3420a2b594d79aa8f46936cd0addd426", "pr_number": "57974", "files_changed": ["torch/lib/c10d/NCCLUtils.hpp"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "ba84c91197": {"title": "Deprecate torch.lstsq (#57743)", "body": "Summary:\n**BC-breaking note:**\n\nThis PR deprecates torch.lstsq; it adds an upgrade guide for how to use torch.linalg.lstsq instead.\n\nIt DOES NOT remove torch.lstsq, but warns once when it's called\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57743\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28318196\n\nPulled By: mruberry\n\nfbshipit-source-id: 0d6df29648a91a44c7d0ac58062c1099fcb61fb8", "pr_number": "57743", "files_changed": ["aten/src/ATen/LegacyTHFunctionsCPU.cpp", "aten/src/ATen/cuda/LegacyTHFunctionsCUDA.cpp", "torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: bc-breaking", "open source"]}, "a93314dec3": {"title": "Alias det, slogdet, matrix_power, inverse, pinverse (#57821)", "body": "Summary:\nWhen doing this, I realised that `torch.linalg.pinv` did not have a note on the problems of its derivative (`torch.pinverse` did have it), so I added that.\n\nAs I was at it, I made a bit more explicit the recommendation for some functions in `torch.linalg`  to prefer other functions. I also changed the mentions of \"stable\" to \"numerically stable\" as discussed with IvanYashchuk and mruberry\n\nIf it seems like too much, I'm happy to move the recommendations part of `torch.linalg` to a different PR, but it was such a small thing that I figured it wouldn't be that big a deal if it was here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57821\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28317959\n\nPulled By: mruberry\n\nfbshipit-source-id: 6b116561bf3cba46fadc5ac14448e5d28ea88039", "pr_number": "57821", "files_changed": ["torch/_torch_docs.py", "torch/linalg/__init__.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "36a22967b7": {"title": "[fx ir] Handle the case when output consumes get_attr directly (#57844)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57844\n\nReviewed By: 842974287\n\nDifferential Revision: D28294298\n\nfbshipit-source-id: db337fadca9f10f208324c9da6d95620178a189b", "pr_number": "57844", "files_changed": ["torch/fx/passes/split_utils.py"], "labels": ["cla signed", "fb-exported", "fx"]}, "4fef1c1d74": {"title": "Deprecate torch.cholesky (#57725)", "body": "Summary:\n**BC-breaking note:**\n\nThis PR deprecates torch.cholesky in favor of torch.linalg.cholesky. A upgrade guide is added to the documentation for torch.cholesky.\n\nNote this PR DOES NOT remove torch.cholesky.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57725\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28318260\n\nPulled By: mruberry\n\nfbshipit-source-id: e7ba049321810e70f4de08e6ac37ff800e576152", "pr_number": "57725", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "test/distributions/test_distributions.py", "torch/_lobpcg.py", "torch/_torch_docs.py", "torch/distributions/lowrank_multivariate_normal.py", "torch/distributions/multivariate_normal.py"], "labels": ["Merged", "cla signed", "module: bc-breaking", "open source"]}, "24087d07ca": {"title": "Deprecate QR (#57745)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57745\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28318164\n\nPulled By: mruberry\n\nfbshipit-source-id: b8e3cb9d7ab33f30c8653ec39f932a8af8bd2a50", "pr_number": "57745", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "test/jit/test_export_modes.py", "test/typing/reveal/namedtuple.py", "torch/_linalg_utils.py", "torch/_lowrank.py", "torch/_torch_docs.py", "torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py", "torch/nn/init.py"], "labels": ["Merged", "cla signed", "module: deprecation", "module: linear algebra", "open source"]}, "3ec16035f2": {"title": "TST Migrates some of test_nn.py from assertEqualIgnoreTypes to assertEqual (#57642)", "body": "Summary:\nRelated to https://github.com/pytorch/pytorch/issues/38095, https://github.com/pytorch/pytorch/issues/50006\n\nMigrates some of `test_nn.py` from `assertEqualIgnoreTypes` to `assertEqual`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57642\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28317761\n\nPulled By: mruberry\n\nfbshipit-source-id: 6bea6f669569922b2a391d1523917edde976f014", "pr_number": "57642", "files_changed": ["test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ee48bd089c": {"title": "Support mix of int32 and int64 offsets/indices for EmbeddingBag and its variants (#55189)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55189\n\nCurrently EmbeddingBag and it variants support either int32 or int64 indices/offsets. We have use cases where there are mix of int32 and int64 indices which are not supported yet. To avoid introducing too many branches we could simply cast offsets type to indices type when they are not the same.\n\nTest Plan: unit tests\n\nReviewed By: allwu\n\nDifferential Revision: D27482738\n\nfbshipit-source-id: deeadd391d49ff65d17d016092df1839b82806cc", "pr_number": "55189", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "test/test_nn.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "415ae54c31": {"title": "Deprecate torch.eig (#57727)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57727\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28317984\n\nPulled By: mruberry\n\nfbshipit-source-id: fa1aa1b78fd3611ac208bca93e2b745a1bac41f1", "pr_number": "57727", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "test/test_autograd.py", "torch/_torch_docs.py", "torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["Merged", "cla signed", "module: bc-breaking", "open source"]}, "7707efed8f": {"title": "Deprecate matrix_rank (#57734)", "body": "Summary:\nThis one's straightforward\n\n**BC-breaking Note**\n\nThis PR deprecates matrix_rank in favor of linalg.matrix_rank. An upgrade guide from matrix_rank to linalg.matrix_rank is provided in the documentation of matrix_rank.\n\nIt DOES NOT remove matrix_rank.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57734\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28318301\n\nPulled By: mruberry\n\nfbshipit-source-id: b9a27f58fdad72f408ca8b83a70c9b1fc2ef28e9", "pr_number": "57734", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: bc-breaking", "open source"]}, "43f6deb6e4": {"title": "Deprecate chain_matmul (#57735)", "body": "Summary:\nThis one's easy. I also included a bugfix.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57735\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28318277\n\nPulled By: mruberry\n\nfbshipit-source-id: c3c4546a11ba5b555b99ee79b1ce6c0649fa7323", "pr_number": "57735", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "torch/functional.py"], "labels": ["Merged", "cla signed", "module: bc-breaking", "open source"]}, "cf7a0e5af4": {"title": "Use RPC context streams to cover serde ops (#57926)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57926\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D28316526\n\nPulled By: mrshenli\n\nfbshipit-source-id: 1907ec8f46e40fa5049d810c6ad959263361b6aa", "pr_number": "57926", "files_changed": ["test/cpp/rpc/test_tensorpipe_serialization.cpp", "torch/csrc/distributed/rpc/tensorpipe_utils.cpp", "torch/testing/_internal/distributed/rpc/rpc_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "9e94921a55": {"title": "combine consecutive layes on the same device (#55973)", "body": "Summary:\nImplements proposal https://github.com/pytorch/pytorch/issues/53438\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55973\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D28340034\n\nPulled By: mrzzd\n\nfbshipit-source-id: d7fe476c0364603f36d41f348769245dac0acd88", "pr_number": "55973", "files_changed": ["test/distributed/pipeline/sync/test_pipe.py", "torch/distributed/pipeline/sync/pipe.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "3e46d6c9e4": {"title": "Update docs to mention CUDA support for Future (#50048)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50048\n\nTo reflect the many changes introduced recently.\n\nIn my mind, CUDAFuture should be considered a \"private\" subclass, which in practice should always be returned as a downcast pointer to an ivalue::Future. Hence, we should document the CUDA behavior in the superclass, even if it's CUDA-agnostic, since that's the interface the users will see also for CUDA-enabled futures.\nghstack-source-id: 128640983\n\nTest Plan: Built locally and looked at them.\n\nReviewed By: mrshenli\n\nDifferential Revision: D25757474\n\nfbshipit-source-id: c6f66ba88fa6c4fc33601f31136422d6cf147203", "pr_number": "50048", "files_changed": ["docs/source/futures.rst", "torch/_C/__init__.pyi.in", "torch/futures/__init__.py"], "labels": ["Merged", "Stale", "cla signed"]}, "dbedb1fa1c": {"title": "[CUDA graphs] Sync after replay (#57556)", "body": "Summary:\nRight now** there's a bug in libcuda.so that triggers sometimes when graphs with certain topologies are replayed back to back without a sync in between. Replays that hit this bug turn into spaghetti: kernels reordered ignoring dependencies, kernels elided, corrupted results. Currently, the only workaround I know that fixes all our repros is a manual sync between replays.\n\nI'll remove the sync (or special case it based on cuda version) in a later PR, as soon as a fixed libcuda.so is available.\n\nThe only substantive change is the cudaDeviceSynchronize, other lines changed are de-indenting an unneeded scope.\n\n** The bug is in current and semi-recent public versions of libcuda.so. We discovered the bug recently and we're not sure yet which public release was first affected. The version that ships with 11.3 is definitely affected, versions that shipped with 11.1 and earlier are likely not affected.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57556\n\nReviewed By: mruberry\n\nDifferential Revision: D28343043\n\nPulled By: ngimel\n\nfbshipit-source-id: 3b907241aebdb8ad47ae96a6314a8b02de7bfa77", "pr_number": "57556", "files_changed": ["aten/src/ATen/cuda/CUDAGraph.cpp", "test/test_cuda.py"], "labels": ["Merged", "cla signed", "module: cuda", "module: cuda graphs", "open source", "triaged"]}, "fa318911be": {"title": "Enable geometric ops, exp2, expm1, rsqrt & erfc for BFloat16 on CUDA (#57913)", "body": "Summary:\nOps enabled for BFloat16 on CUDA (12 in total):\n\n`acos`\n`asin`\n`atan`\n`cosh`\n`sin`\n`sinh`\n`tan`\n`sinc`\n`exp2`\n`erfc`\n`expm1`\n`rsqrt`\n\nEnabled backward for `cos` on CUDA.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57913\n\nReviewed By: mruberry\n\nDifferential Revision: D28342969\n\nPulled By: ngimel\n\nfbshipit-source-id: 3c140fe408cbf93b21296a52d95ef0a0ccd96503", "pr_number": "57913", "files_changed": ["aten/src/ATen/native/cuda/ForeachUnaryOp.cu", "aten/src/ATen/native/cuda/UnaryGeometricKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "067147ac7d": {"title": "Enable BFloat16 for `logaddexp` & `logaddexp2` on CUDA (#57908)", "body": "Summary:\nEnabled BFloat16 for `logaddexp` & `logaddexp2` on CUDA, with a [workaround](https://github.com/pytorch/pytorch/pull/57908#issuecomment-837320532) suggested by zasdfgbnm.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57908\n\nReviewed By: mruberry\n\nDifferential Revision: D28344976\n\nPulled By: ngimel\n\nfbshipit-source-id: edef654b5819b236fbd9996f962115beb6e147e1", "pr_number": "57908", "files_changed": ["aten/src/ATen/native/cuda/LogAddExpKernel.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "4fb8676cea": {"title": "Add dot implementation for BFloat16 on CUDA (#57903)", "body": "Summary:\nEnabled `dot` for BFloat16 on CUDA (version 11+).\nIt also enabled `matmul` & `vdot` for BFloat16.\nBackward for `matmul` isn't supported for `BFloat16`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57903\n\nReviewed By: mruberry\n\nDifferential Revision: D28346031\n\nPulled By: ngimel\n\nfbshipit-source-id: 0917e9e0d6cf3694f45fe1c7e76370581502036a", "pr_number": "57903", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDABlas.h", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "90f05c005d": {"title": "refactor multi_head_attention_forward (#56674)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56674\n\n`torch.nn.functional.multi_head_attention_forward` supports a long tail of options and variations of the multihead attention computation. Its complexity is mostly due to arbitrating among options, preparing values in multiple ways, and so on - the attention computation itself is a small fraction of the implementation logic, which is relatively simple but can be hard to pick out.\n\nThe goal of this PR is to\n- make the internal logic of `multi_head_attention_forward` less entangled and more readable, with the attention computation steps easily discernible from their surroundings.\n- factor out simple helpers to perform the actual attention steps, with the aim of making them available to other attention-computing contexts.\n\nNote that these changes should leave the signature and output of `multi_head_attention_forward` completely unchanged, so not BC-breaking. Later PRs should present new multihead attention entry points, but deprecating this one is out of scope for now.\n\nChanges are in two parts:\n- the implementation of `multi_head_attention_forward` has been extensively resequenced, which makes the rewrite look more total than it actually is. Changes to argument-processing logic are largely confined to a) minor perf tweaks/control flow tightening, b) error message improvements, and c) argument prep changes due to helper function factoring (e.g. merging `key_padding_mask` with `attn_mask` rather than applying them separately)\n- factored helper functions are defined just above `multi_head_attention_forward`, with names prefixed with `_`. (A future PR may pair them with corresponding modules, but for now they're private.)\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D28344707\n\nPulled By: bhosmer\n\nfbshipit-source-id: 3bd8beec515182c3c4c339efc3bec79c0865cb9a", "pr_number": "56674", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed"]}, "502eb664ae": {"title": "OpInfo: chunk (#57935)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57935\n\nReviewed By: ngimel\n\nDifferential Revision: D28346217\n\nPulled By: mruberry\n\nfbshipit-source-id: 331995aa18fd2983fc2122a9af31fba43ab9839c", "pr_number": "57935", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ea421fb249": {"title": "enable static graph training in DDP (#55248)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55248\n\nThis PR provides enable static graph training when users call _set_static_graph(). This can help support more use cases in DDP without performance regression, also can potentially improve performance when there are unused parameters in the graph.\n1. first iteration records graph states like how many times a grad is calculated, whether the grad is used or not. then first iteration queues a delay_all_reduce call back to all reduce grads.\n2. Since autograd call back is associated with current target graph task, the delay_all_all call back should be associated with out-most backward graph task. A DDP sink layer is added in DDP forward loop so that we can queue the delay_all_reduce call back in the sink layer.\n3. after first iterations, DDP will use the saved graph states to determine whether a grad is used or not. whether a grad is ready for communication.\n4. rebuilt bucket is called in second iteration, after graph states are recorded in first iteration.\n5. if the graph states change, DDP will throw errors\nghstack-source-id: 128599464\n\nTest Plan: unit tests. adding more tests\n\nReviewed By: rohan-varma\n\nDifferential Revision: D27539964\n\nfbshipit-source-id: 74de1ad2719465be67bab8688d6e293cd6e3a246", "pr_number": "55248", "files_changed": ["test/distributed/test_c10d_gloo.py", "test/distributed/test_c10d_nccl.py", "torch/csrc/distributed/c10d/init.cpp", "torch/lib/c10d/reducer.cpp", "torch/lib/c10d/reducer.hpp", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py", "torch/testing/_internal/distributed/pipe_with_ddp_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "7faac089ca": {"title": "Enable cusolver potrf batched for Cholesky decomposition when cuda >= 11.3 (#57788)", "body": "Summary:\nThis PR enables the usage of cusolver potrf batched as the backend of Cholesky decomposition (`torch.linalg.cholesky` and `torch.linalg.cholesky_ex`) when cuda version is greater than or equal to 11.3.\n\nBenchmark available at https://github.com/xwang233/code-snippet/tree/master/linalg/cholesky-new. It is seen that cusolver potrf batched performs better than magma potrf batched in most cases.\n\n## cholesky dispatch heuristics:\n\n### before:\n\n- batch size == 1: cusolver potrf\n- batch size > 1: magma xpotrf batched\n\n### after:\n\ncuda >= 11.3:\n- batch size == 1: cusolver potrf\n- batch size > 1: cusolver potrf batched\n\ncuda < 11.3 (not changed):\n- batch size == 1: cusolver potrf\n- batch size > 1: magma xpotrf batched\n\n ---\n\nSee also https://github.com/pytorch/pytorch/issues/42666 #47953 https://github.com/pytorch/pytorch/issues/53104 #53879\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57788\n\nReviewed By: ngimel\n\nDifferential Revision: D28345530\n\nPulled By: mruberry\n\nfbshipit-source-id: 3022cf73b2750e1953c0e00a9e8b093dfc551f61", "pr_number": "57788", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cu", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ac44569b0d": {"title": "make ddp logging api to be private (#57999)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57999\n\nmake ddp logging api to be private\nghstack-source-id: 128607185\n\nTest Plan: unit test\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28338485\n\nfbshipit-source-id: bd2ae7c78904e93eed88be91876f5a832b5b7886", "pr_number": "57999", "files_changed": ["test/distributed/test_c10d_common.py", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "Reverted", "cla signed", "oncall: distributed"]}, "18edb77a28": {"title": "Add `pad_sequence` as a native function (#57868)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/56229\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57868\n\nReviewed By: mruberry\n\nDifferential Revision: D28334174\n\nPulled By: ngimel\n\nfbshipit-source-id: f1647718ada596686117703b682c0af7e92e16f5", "pr_number": "57868", "files_changed": ["aten/src/ATen/native/PackedSequence.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/_C/_nn.pyi.in", "torch/csrc/api/include/torch/nn/utils/rnn.h", "torch/nn/utils/rnn.py", "torch/nn/utils/rnn.pyi"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "a02305925c": {"title": "[local lint] Force color output on mypy (#58071)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58071\n\nThere's an environment variable that mypy will use to force color output, so turn that on if the runner detects a terminal.\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D28360742\n\nPulled By: driazati\n\nfbshipit-source-id: c0dc372a44ab3a16e67115ce54784f4d5a4833ee", "pr_number": "58071", "files_changed": ["tools/actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "698be31262": {"title": "Adding support for normalization of __is__ op (#57862)", "body": "Summary:\nnormalizing `__is__` to `eq`, and `__isnot__` to `ne` in the case of bools.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57862\n\nTest Plan:\n```\npython test/test_jit.py TestPeephole\n```\n11 Tests, 1 skipped, no failures\nFixes https://github.com/pytorch/pytorch/issues/57387\n\nReviewed By: eellison\n\nDifferential Revision: D28335646\n\nPulled By: Gamrix\n\nfbshipit-source-id: c9f885044b32897ba35483091bcf7037759b7517", "pr_number": "57862", "files_changed": ["test/jit/test_peephole.py", "torch/csrc/jit/passes/normalize_ops.cpp", "torch/testing/_internal/jit_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "eb1ffa91d8": {"title": "[pyper] allow static runtime on and glow on simultaneously (#57972)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57972\n\nAllow static runtime to be on when glow is on. This should be fine as long as glow AOT has already been run.\n\nTest Plan: Test on replayer with remote_other net. D28291326 fixes remaining issue removing loops from the remote_other model. Need to test on regenerated model.\n\nReviewed By: hlu1\n\nDifferential Revision: D28275514\n\nfbshipit-source-id: ee78972660dfdc3fcfb9af2cf7ebb19ee745a4f1", "pr_number": "57972", "files_changed": ["torch/csrc/jit/runtime/static/impl.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "dd876120f9": {"title": "Out version for aten::repeat (#57683)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57683\n\nSupport aten::repeat for static runtime\n\nTest Plan: buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest\n\nReviewed By: hlu1\n\nDifferential Revision: D27639482\n\nfbshipit-source-id: e6e706cb1d52750eea74f19536245f0484e945e6", "pr_number": "57683", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "4ef94265e9": {"title": "Add Futures to ProcessGroupGloo (#57818)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57818\n\nTest Plan: Imported from OSS\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28304171\n\nPulled By: agolynski\n\nfbshipit-source-id: dbf7f5538890d138582831aa0279ede89619ea1e", "pr_number": "57818", "files_changed": ["test/distributed/test_c10d_gloo.py", "torch/lib/c10d/ProcessGroupGloo.cpp", "torch/lib/c10d/ProcessGroupGloo.hpp", "torch/lib/c10d/test/ProcessGroupGlooTest.cpp", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "module: c10d", "oncall: distributed"]}, "8b12c8e8b3": {"title": "Fixes: register_full_backward_hook crash if first argument don't require a gradient (#57944) (#57945)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57944\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57945\n\nReviewed By: mruberry\n\nDifferential Revision: D28351929\n\nPulled By: albanD\n\nfbshipit-source-id: d0db898e6bf13d1877cd81892a5a65c7854c8102", "pr_number": "57945", "files_changed": ["test/test_nn.py", "torch/utils/hooks.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "c36055bb42": {"title": "Make mypy_wrapper.py accept multiple filenames (#57998)", "body": "Summary:\nA followup to https://github.com/pytorch/pytorch/issues/57752.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57998\n\nTest Plan:\n```\nmypy --config=mypy-strict.ini\npython tools/test/test_mypy_wrapper.py\npython tools/test/test_actions_local_runner.py -k mypy\n```\n\nReviewed By: driazati\n\nDifferential Revision: D28338531\n\nPulled By: samestep\n\nfbshipit-source-id: ae31e3fa4a2b8060c200f9a13f768beaf2f55694", "pr_number": "57998", "files_changed": ["mypy.ini", "tools/actions_local_runner.py", "tools/mypy_wrapper.py", "tools/test/test_actions_local_runner.py", "tools/test/test_mypy_wrapper.py"], "labels": ["Merged", "cla signed"]}, "a1f9a3c643": {"title": "Fix UB in library.h (#57962)", "body": "Summary:\nThe function name and return type both are called `class_`, therefore they are ambiguous and this is UB and does not work on NVCC. See the tests for the failure case.\n\nThanks for the help of Thibaut Lutz from NVIDIA's compiler team.\n\ncc: yueyericardo ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57962\n\nReviewed By: mruberry\n\nDifferential Revision: D28359400\n\nPulled By: ezyang\n\nfbshipit-source-id: c64ec89203f99f656611aba34f7424eed7bc9e7c", "pr_number": "57962", "files_changed": ["test/cpp_extensions/torch_library.cu", "torch/library.h"], "labels": ["Merged", "cla signed", "open source"]}, "1f83d8eec2": {"title": "[Static Runtime] Return nullptr if the number of input args doesn't match (#58018)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58018\n\n- Add checks for the number of input args and return nullptr if it doesn't match. This is intended to make Static Runtime more robust so that op schema change is less likely to break things. Imagine that a new arg is added to an op or a new overload is added that has this added arg, SR would simply ignore this added arg. If this arg has a default value, SR would run the model with the default value and give you wrong results, which can be hard to track down.\n\nReviewed By: ajyu\n\nDifferential Revision: D28047955\n\nfbshipit-source-id: 01067059edd5cfea80c4ee121829f7733b11f601", "pr_number": "58018", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "d9ea93181b": {"title": "Some types for remote_module (#58012)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58012\n\nTest Plan: Sandcastle\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28334611\n\nfbshipit-source-id: 5e4645a7de65e064cb6a919cdc2372151ec48d44", "pr_number": "58012", "files_changed": ["torch/distributed/nn/api/remote_module.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "7a23a5e8e9": {"title": "Shut up sccache couldn't connect error (#58047)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58047\n\nThis error ALWAYS gets picked up by Dr. CI and IT DRIVES ME NUTS.\nConsign it to the /dev/null bin.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D28352658\n\nPulled By: ezyang\n\nfbshipit-source-id: a55f99ed76728d46f02d6a61a45c7691e8be7a47", "pr_number": "58047", "files_changed": [".jenkins/pytorch/common.sh"], "labels": ["Merged", "cla signed"]}, "e573987bea": {"title": "remove syncs in one_hot (#57902)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55579\nNow on cuda one-hot relies on device-side asserts thrown by scatter\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57902\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28328698\n\nPulled By: ngimel\n\nfbshipit-source-id: 1cd13e2c123c733cde7dbe4cbe6ff5335063bb70", "pr_number": "57902", "files_changed": ["aten/src/ATen/native/Onehot.cpp", "test/test_nn.py"], "labels": ["Merged", "cla signed"]}, "0d4dc6cb39": {"title": "Let submodules be collected as args/kwargs (#57840)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57840\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D28294984\n\nPulled By: ansley\n\nfbshipit-source-id: d64fe109a349516da69d2d17f58e42f98af564fd", "pr_number": "57840", "files_changed": ["test/test_fx.py", "torch/fx/symbolic_trace.py"], "labels": ["Merged", "cla signed", "fx"]}, "8b816e9010": {"title": "To implement gradient for Pytorch (#54617)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/56129\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/54617\n\nReviewed By: anjali411\n\nDifferential Revision: D28057452\n\nPulled By: iramazanli\n\nfbshipit-source-id: 9bd86679282d34f5e5393e6447121586517eb4f0", "pr_number": "54617", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/torch.rst", "test/test_fx_experimental.py", "test/test_torch.py", "tools/autograd/gen_python_functions.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "8824f49e68": {"title": "Split `test_testing.py::TestAsserts` for multiple devices (#56365)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56365\n\nFollow-up to https://github.com/pytorch/pytorch/pull/54784#discussion_r614156172. Instead of having one large testcase where most methods are decorated with `onlyCPU`, this factors out all tests that actually need another device into a separate test case.\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr, albanD\n\nDifferential Revision: D28247529\n\nPulled By: mruberry\n\nfbshipit-source-id: 946e7694b70e736941565f29b5dd459ed7fbca4e", "pr_number": "56365", "files_changed": ["test/test_testing.py"], "labels": ["Merged", "cla signed", "open source"]}, "e9e125475e": {"title": "[Static Runtime] Add schema check to aten::repeat and fb::fast_gather (#58106)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58106\n\nFollowup for D28047955 (https://github.com/pytorch/pytorch/commit/1f83d8eec25c7339bd3e2862baf9b389e6a738a4).\n\nReviewed By: ajyu\n\nDifferential Revision: D28369472\n\nfbshipit-source-id: 36aa10082589f4b6f0cc2d79f032fe72a19cda57", "pr_number": "58106", "files_changed": ["torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "32acc96f78": {"title": "[Static Runtime] Fix bug in aten::clone (#58100)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58100\n\naten::clone has a second arg, memory_format, which was not previously supported.\n\nReviewed By: ajyu\n\nDifferential Revision: D28347171\n\nfbshipit-source-id: e083cc24c3228048429bba3497326415bc3d1f5a", "pr_number": "58100", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "c790fd2bf8": {"title": "ATen lu_unpack. Required for making `torch.lu_solve` differentiable. (#46913)", "body": "Summary:\nBackward methods for `torch.lu` and `torch.lu_solve` require the `torch.lu_unpack` method.\nHowever, while `torch.lu` is a Python wrapper over a native function, so its gradient is implemented via `autograd.Function`,\n`torch.lu_solve` is a native function, so it cannot access `torch.lu_unpack` as it is implemented in Python.\n\nHence this PR presents a native (ATen) `lu_unpack` version. It is also possible to update the gradients for `torch.lu` so that backward+JIT is supported (no JIT for `autograd.Function`) with this function.\n\n~~The interface for this method is different from the original `torch.lu_unpack`, so it is decided to keep it hidden.~~\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/46913\n\nReviewed By: albanD\n\nDifferential Revision: D28355725\n\nPulled By: mruberry\n\nfbshipit-source-id: 281260f3b6e93c15b08b2ba66d5a221314b00e78", "pr_number": "46913", "files_changed": ["aten/src/ATen/native/LinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebra.h", "aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_autograd.py", "test/test_linalg.py", "test/test_namedtuple_return_api.py", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "torch/_torch_docs.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/functional.py", "torch/jit/_builtins.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "module: autograd", "module: linear algebra", "open source", "triaged"]}, "c3d40fdf56": {"title": "[ATen] Use expect_contiguous in layer_norm (#58067)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58067\n\n- Use expect_contiguous in layer_norm to avoid unnecessary refcount bumps when the tensors are contiguous\n- Clean up some leftovers from the hacky wrappers removal cleanup: use c10::MaybeOwned<Tensor> for bias tensors\n- Skip dispatcher for at::empty in the layer_norm impl in Static Runtime\n\nTest Plan: CI\n\nReviewed By: swolchok\n\nDifferential Revision: D28214298\n\nfbshipit-source-id: 73150fa62d5c18f41a2264f8e56bbe5e377ad045", "pr_number": "58067", "files_changed": ["aten/src/ATen/native/cuda/layer_norm_kernel.cu", "aten/src/ATen/native/layer_norm.cpp", "aten/src/ATen/native/layer_norm.h", "aten/src/ATen/native/quantized/cpu/qnormalization.cpp", "torch/csrc/jit/runtime/static/ops.cpp", "torch/csrc/jit/runtime/static/ops.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "d623fb7e04": {"title": "Add a disclaimer about limited CUDA support in RPC (#58023)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58023\n\nClearly state that some features of RPC aren't yet compatible with CUDA.\nghstack-source-id: 128688856\n\nTest Plan: None\n\nReviewed By: agolynski\n\nDifferential Revision: D28347605\n\nfbshipit-source-id: e8df9a4696c61a1a05f7d2147be84d41aeeb3b48", "pr_number": "58023", "files_changed": ["docs/source/rpc.rst"], "labels": ["Merged", "cla signed"]}, "a07a0190f9": {"title": "enable deterministic path for index_put with accumulate=False on CPU and CUDA (#57839)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57839\n\nwe reuse the `index_put_accum_kernel`, rename it to  `index_put_deterministic_kernel` and add a bool `accumulate` in `index_backward_kernel`\n\nTest Plan:\nbuck test mode/opt //caffe2/test:torch -- test_index_put_non_accumulate_deterministic\n\n    \u2713 Pass: caffe2/test:torch - test_index_put_non_accumulate_deterministic_cpu (test_torch.TestTorchDeviceTypeCPU) (5.120)\nSummary\n  Pass: 1\n  Skip: 1\n    \u21bb caffe2/test:torch - test_index_put_non_accumulate_deterministic_meta (test_torch.TestTorchDeviceTypeMETA)\n  ListingSuccess: 1\n\nbuck test mode/opt //caffe2/test:torch_cuda -- test_index_put_non_accumulate_deterministic\n\n    \u2713 ListingSuccess: caffe2/test:torch_cuda - main (6.397)\n    \u2713 Pass: caffe2/test:torch_cuda - test_index_put_non_accumulate_deterministic_cuda (test_torch.TestTorchDeviceTypeCUDA) (26.030)\n    \u2713 Pass: caffe2/test:torch_cuda - main (26.030)\nSummary\n  Pass: 2\n  ListingSuccess: 1\n\nReviewed By: ngimel\n\nDifferential Revision: D28290699\n\nfbshipit-source-id: df8bbe7af2e72017566161b05b85737fda4ceb3f", "pr_number": "57839", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LegacyThrustHelpers.cu", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "14badd9929": {"title": "enable deterministic path for index_copy_cuda with index_put (#57870)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57870\n\nthis is similar to index_add_cuda with index_put accumulate = True\n\nTest Plan:\nbuck test mode/opt //caffe2/test:torch_cuda -- test_index_copy_deterministic\n\n    \u2713 ListingSuccess: caffe2/test:torch_cuda - main (9.229)\n    \u2713 Pass: caffe2/test:torch_cuda - test_index_copy_deterministic_cuda (test_torch.TestTorchDeviceTypeCUDA) (25.750)\n    \u2713 Pass: caffe2/test:torch_cuda - main (25.750)\n\nReviewed By: ngimel\n\nDifferential Revision: D28291041\n\nfbshipit-source-id: 7f0cf3ec72805f3617fd1de9ff03e1d49114fed8", "pr_number": "57870", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "db13119fc4": {"title": "Deprecate symeig (#57732)", "body": "Summary:\nThis one had a tricky usage of `torch.symeig` that had to be replaced. I tested the replacement locally though.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57732\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28328189\n\nPulled By: mruberry\n\nfbshipit-source-id: 7f000fcbf2b029beabc76e5a89ff158b47977474", "pr_number": "57732", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "test/test_autograd.py", "test/test_linalg.py", "torch/_linalg_utils.py", "torch/_lobpcg.py", "torch/_torch_docs.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/distributions/constraints.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "e1078d42f0": {"title": "std/var: Return real results for complex input (#58066)", "body": "Summary:\nFixes gh-56627\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58066\n\nReviewed By: ngimel\n\nDifferential Revision: D28372987\n\nPulled By: mruberry\n\nfbshipit-source-id: c34d55f1a48047ceefa298ef2f4f33ad7dd1e577", "pr_number": "58066", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_reductions.py"], "labels": ["Merged", "cla signed", "open source"]}, "c7fb0a0e82": {"title": "Remove beta warning for use_deterministic_algorithms (#58074)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/58073\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58074\n\nReviewed By: ngimel\n\nDifferential Revision: D28373676\n\nPulled By: mruberry\n\nfbshipit-source-id: cae9a92ebbf6ac5f8d3008aa6a6a9cd5c1041c9f", "pr_number": "58074", "files_changed": ["aten/src/ATen/Context.cpp", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "module: determinism", "open source", "triaged"]}, "ff982ef73d": {"title": "OpInfo: reshape, reshape_as and minor clean-up (#57460)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57460\n\nReviewed By: nairbv\n\nDifferential Revision: D28151675\n\nPulled By: anjali411\n\nfbshipit-source-id: 2b3bcadab3ff5d1761b2922b63afd70a354e785c", "pr_number": "57460", "files_changed": ["test/test_fx.py", "test/test_fx_experimental.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "5ea87f9c24": {"title": "Grammatically updated the tech docs (complex_numbers.rst) (#57540)", "body": "Summary:\nSmall grammatical change in complex_numbers.rst .\n-You can see the changes in the screenshot below -\n![Capture](https://user-images.githubusercontent.com/38073192/117013956-01aed000-acf9-11eb-9d17-1e369de68585.PNG)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57540\n\nReviewed By: albanD\n\nDifferential Revision: D28233650\n\nPulled By: mrshenli\n\nfbshipit-source-id: 0cec7bb1f4bd61e929e2a8fc5292bc20b77aee35", "pr_number": "57540", "files_changed": ["docs/source/complex_numbers.rst"], "labels": ["Merged", "cla signed", "module: docs", "open source", "triaged"]}, "30f26c5893": {"title": "Reimplement torch::flip based on advanced indexing (#56713)", "body": "Summary:\n## Rationale\nThis PR improves the performance of `torch::flip` by using `TensorIterator` as the same fashion as using `AdvancedIndexing`. Which means that this implementation is semantically equivalent to indexing a tensor using reverse indices `A[dim0_size - 1:0 ..., dimN_size-1:0, ...]`.\n\n## Benchmark results\nThe following benchmark compares the runtime of this implementation of `flip` against the current implementation, AdvancedIndexing with reversed indices, as well as OpenCV one. The comparison scenarios consider a 4D tensor `[B, C, H, W]`, where the dimensions flipped correspond to `H` (vertical flip) and `W` (horizontal flip) under float32 and uint8 datatypes.\n\nThe benchmark implementation details can be found in https://github.com/andfoy/flip-benchmarks/blob/main/5_Stable_implementation/benchmarks.py. Additionally, there are correctness tests against the current flip implementation in https://github.com/andfoy/flip-benchmarks/blob/main/5_Stable_implementation/main.cpp, which tests against different layouts, datatypes and contiguous/non-contiguous tensors.\n\nThe following plots correspond to the means of the runtime of each operator after 100 samples. As it is possible to observe, the latest implementation of flip has a runtime similar to the indexing one. Also, the performance gains are up to 6X under some scenarios.\n\n### Horizontal flip (float)\n![bokeh_plot](https://user-images.githubusercontent.com/1878982/115766715-e72a3d80-a36d-11eb-8552-9005028900b1.png)\n\n### Horizontal flip (uint8)\n![bokeh_plot(1)](https://user-images.githubusercontent.com/1878982/115766720-e7c2d400-a36d-11eb-822d-44046882c976.png)\n\n### Vertical flip (float)\n![bokeh_plot(2)](https://user-images.githubusercontent.com/1878982/115766721-e7c2d400-a36d-11eb-8f4b-d44c8c33d104.png)\n\n### Vertical flip (uint8)\n![bokeh_plot(3)](https://user-images.githubusercontent.com/1878982/115766725-e85b6a80-a36d-11eb-907a-cfcddba555ad.png)\n\ncc fmassa vfdev-5\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56713\n\nReviewed By: datumbox\n\nDifferential Revision: D28255088\n\nPulled By: fmassa\n\nfbshipit-source-id: 5b8684812357c331e83a677b99cf0d78f0821678", "pr_number": "56713", "files_changed": ["aten/src/ATen/native/TensorTransformations.cpp"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "4faa427383": {"title": "Remove printout from distributed tests (#58095)", "body": "Summary:\nThese were added to help debug a flaky test, the flaky test has since been resolved.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58095\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28368077\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 9618f64de2b7015401bb8cb7816b09e1a44e0fef", "pr_number": "58095", "files_changed": ["torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "c52700dbcd": {"title": "[wip] enhance DDPSink to work for general outputs (#57073)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57073\n\nEnhances use of DDPSink to work for all output types DDP supports as per https://github.com/pytorch/pytorch/issues/55876.\n\nTODO: Add additional testing for tuple, list, dict return types\nghstack-source-id: 128726768\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D27756985\n\nfbshipit-source-id: 2e0408649fb2d6a46d6c33155a24c4c1723dd799", "pr_number": "57073", "files_changed": ["torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "d212bf1863": {"title": "Enable `BFloat16` for `nan_to_num` on CUDA (#58063)", "body": "Summary:\nEnabled BFloat16 for `nan_to_num` on CUDA. For comparison with numpy, a [workaround suggested](https://github.com/pytorch/pytorch/issues/57982#issuecomment-839150556) by ngimel is being used - the OpInfo's `sample.kwargs` is used to set two `numpy.kwargs`, viz. `posinf` & `neginf` for `BFloat16`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58063\n\nReviewed By: mruberry\n\nDifferential Revision: D28373478\n\nPulled By: ngimel\n\nfbshipit-source-id: 6493b560d83632a8519c1d3bfc5c54be9b935fb9", "pr_number": "58063", "files_changed": ["aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source"]}, "16d617c3e5": {"title": "test experiment script (#57925)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57925\n\n1. adds test_scripts.py that will run added scripts and verify that there are no errors\n2. adds local ddp_nccl_allreduce experiment script\n\ntest with command `pytest test_scripts.py`\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski\n\nDifferential Revision: D28382452\n\nPulled By: gcramer23\n\nfbshipit-source-id: 21028a990ebfedf1aad6b007a723c02403e8bea8", "pr_number": "57925", "files_changed": ["benchmarks/distributed/rpc/parameter_server/README.md", "benchmarks/distributed/rpc/parameter_server/bash_experiment_scripts/ddp_nccl_allreduce.sh", "benchmarks/distributed/rpc/parameter_server/bash_experiment_scripts/helper_functions.sh", "benchmarks/distributed/rpc/parameter_server/experiment_scripts/ddp_nccl_allreduce.sh", "benchmarks/distributed/rpc/parameter_server/test_scripts.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "94bb1150a7": {"title": "[ROCm] ubuntu version check in install_rocm.sh (#57751)", "body": "Summary:\nIn preparation for ROCm 4.2 release changing the apt repo name from xenial to ubuntu.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57751\n\nReviewed By: agolynski\n\nDifferential Revision: D28385479\n\nPulled By: malfet\n\nfbshipit-source-id: 10ad225b71857226d8e36eaa62eba4511d9362e7", "pr_number": "57751", "files_changed": [".circleci/docker/common/install_rocm.sh"], "labels": ["Merged", "Reverted", "cla signed", "module: rocm", "open source"]}, "8f83bfeb98": {"title": "Update CI images for rocm4.2 (#58017)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58017\n\nReviewed By: agolynski\n\nDifferential Revision: D28385181\n\nPulled By: malfet\n\nfbshipit-source-id: b4bb02d4dfaaa741ee6a804bbd7d7e9e394f7321", "pr_number": "58017", "files_changed": [".circleci/cimodel/data/simple/docker_definitions.py", ".circleci/config.yml", ".circleci/docker/build.sh"], "labels": ["Merged", "cla signed", "module: rocm", "open source"]}, "3603ba24d5": {"title": "Trigger Windows multi gpu tests on master (#57817)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57817\n\nReviewed By: mruberry\n\nDifferential Revision: D28368299\n\nPulled By: malfet\n\nfbshipit-source-id: 765ef740c25477ba8a5d41489ffad4e5d8456236", "pr_number": "57817", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml", ".jenkins/pytorch/win-test-helpers/choose_runtime_cuda_version.bat"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "9d56176034": {"title": "Fix splitter and add a unittest (#58075)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58075\n\nPull Request resolved: https://github.com/facebookresearch/pytext/pull/1687\n\nReviewed By: mikekgfb\n\nDifferential Revision: D28357724\n\nfbshipit-source-id: 36c2d211576a90107bc75468a39408ffecaeed43", "pr_number": "58075", "files_changed": ["torch/fx/passes/net_min_base.py", "torch/fx/passes/splitter_base.py", "torch/fx/passes/tools_common.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "35521a2629": {"title": "Fix some tensor operators to return `NotImplemented` for invalid inputs (#57934)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57719.\n\nThis PR fixes `torch.Tensor{__rsub__, __rdiv__, __rtruediv__, __pow__, __rmatmul__}` to return `NotImplemented` instead of raising a `TypeError`.\n\ncc/ mruberry: The first commit of this PR is the same as https://github.com/pytorch/pytorch/pull/56997/commits/1d209db1cc624fbbe99eee5c82b7b68273c1666e excepts the commit message.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57934\n\nReviewed By: mruberry\n\nDifferential Revision: D28351931\n\nPulled By: albanD\n\nfbshipit-source-id: 985457a44dba24d2496794dfb8c1661cbcd4ff8f", "pr_number": "57934", "files_changed": ["test/onnx/expect/TestOperators.test_rsub.expect", "test/test_binary_ufuncs.py", "test/test_fx.py", "test/test_fx_experimental.py", "torch/_tensor.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed", "open source", "triaged"]}, "87afcea0cc": {"title": "T90561249: Enforce kernel launch checks (#58116)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58116\n\nT90561249: Enforce kernel launch checks\n\nTest Plan: how to test?\n\nReviewed By: r-barnes\n\nDifferential Revision: D28367890\n\nfbshipit-source-id: 159dd3e14a4532c1325a0a332c02ef58d720a91b", "pr_number": "58116", "files_changed": ["aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/Randperm.cuh"], "labels": ["Merged", "cla signed", "fb-exported"]}, "53bc6f79f3": {"title": "Added DevOps PR and Nightly Build logic (#58007)", "body": "Summary:\nThis PR adds Azure DevOps support for running custom PyTorch unit tests on PyTorch PR and Nightly builds.\n\nPR Builds on Azure DevOps:\n- Ensures that the wheel artifacts for a given PR build is ready\n- Once the wheels are ready, PyTorch custom tests are run on torch installation from build wheels\n\nNightly Builds on Azure DevOps:\n- Cues 4 builds {Win,Linux}*{cpu, CUDA} to run PyTorch custom unit tests on nightly PyTorch builds.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58007\n\nReviewed By: seemethere, mruberry\n\nDifferential Revision: D28342428\n\nPulled By: malfet\n\nfbshipit-source-id: a454accf69163f9ba77845eeb54831ef91437981", "pr_number": "58007", "files_changed": [".azure_pipelines/job_templates/pytorch-template-unix.yml", ".azure_pipelines/job_templates/pytorch-template-win.yml", ".azure_pipelines/job_templates/wheel-wait-job-template.yml", ".azure_pipelines/job_templates/wheel-wait-template.yml", ".azure_pipelines/nightly-pytorch-tests-pipeline.yml", ".azure_pipelines/pytorch-tests-pipeline.yml", "docker/pytorch/ubuntu_cpu_gpu/Dockerfile"], "labels": ["Merged", "cla signed", "open source"]}, "af36d084fd": {"title": "reland [ROCm] ubuntu version check in install_rocm.sh (#58164)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58164\n\nReland of https://github.com/pytorch/builder/pull/764\n\nThis reverts commit 6404184700159213f7d64df62537e238822f8b15.\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D28387195\n\nPulled By: seemethere\n\nfbshipit-source-id: 905a14d9ed5e85a7dff7da3d1c4a628320ff7451", "pr_number": "58164", "files_changed": [".circleci/docker/common/install_rocm.sh"], "labels": ["Merged", "cla signed", "module: docker", "module: rocm"]}, "ab6b5fa036": {"title": "Add HIP (ROCm) semantics doc (#57871)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57871\n\nReviewed By: agolynski\n\nDifferential Revision: D28385510\n\nPulled By: malfet\n\nfbshipit-source-id: 9cf69e52d026a1cf74cc12d8727ca17ae026235e", "pr_number": "57871", "files_changed": ["docs/source/notes/hip.rst"], "labels": ["Merged", "cla signed", "module: rocm", "open source", "triaged"]}, "a88673e93e": {"title": "Enable cat wo conditionals iff cpu (#58026)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58026\n\nCat-without-conditionals is a valuable optimization on CPU but on GPU\nit can generate invalid code since it may introduce allocations (i.e. extra\nkernel launches)\nghstack-source-id: 128748630\n\nTest Plan: predictor\n\nReviewed By: navahgar\n\nDifferential Revision: D28347703\n\nfbshipit-source-id: f9e68cd7bcf5d316082ce8378ddf99f2d33fcc07", "pr_number": "58026", "files_changed": ["torch/csrc/jit/tensorexpr/kernel.cpp", "torch/csrc/jit/tensorexpr/kernel.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3c973de543": {"title": "HABANA Device registration key and Autograd key addition (#57094)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57094\n\nReviewed By: mruberry\n\nDifferential Revision: D28355895\n\nPulled By: wconstab\n\nfbshipit-source-id: 5d8b5762a69f444f4fe7f476891150fa5483d893", "pr_number": "57094", "files_changed": ["c10/core/Backend.h", "c10/core/Device.cpp", "c10/core/DeviceType.cpp", "c10/core/DeviceType.h", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h", "c10/core/TensorOptions.h", "torch/csrc/autograd/init.cpp", "torch/library.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "cbd1227809": {"title": "Add a note in the parametrize doc about the naming choice (#58142)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58142\n\nReviewed By: agolynski\n\nDifferential Revision: D28386655\n\nPulled By: albanD\n\nfbshipit-source-id: c2793ac377ef7082c1840e1a50604da3ff9c61ac", "pr_number": "58142", "files_changed": ["docs/source/nn.rst"], "labels": ["Merged", "cla signed"]}, "0bfcc3e3f4": {"title": "fix topk with k=0 on cuda (#58086)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58086\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D28364964\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 4d02bf5b27ca5e8b6f7b6cc6aa99d9e31233578b", "pr_number": "58086", "files_changed": ["aten/src/ATen/native/cuda/TensorTopK.cu", "test/test_sort_and_select.py"], "labels": ["Merged", "cla signed"]}, "614437751f": {"title": "make remote model instantiation async when possible (#58052)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/58052 for the cases where `module_interface_cls` is not provided\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58052\n\nReviewed By: mruberry\n\nDifferential Revision: D28369064\n\nPulled By: mrzzd\n\nfbshipit-source-id: 3ded7ea943a5ff0425bedc05448a59e6eefbeaaf", "pr_number": "58052", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "46e4b2dbda": {"title": "Convert assert -> cast. (#57458)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/55868.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57458\n\nReviewed By: mruberry\n\nDifferential Revision: D28365745\n\nPulled By: walterddr\n\nfbshipit-source-id: 35cc3fa85f87b0ef98cf970f620ab909d240c7be", "pr_number": "57458", "files_changed": ["test/test_overrides.py", "tools/render_junit.py", "torch/distributed/pipeline/sync/batchnorm.py", "torch/jit/mobile/__init__.py", "torch/nn/intrinsic/qat/modules/conv_fused.py", "torch/nn/modules/batchnorm.py", "torch/nn/modules/instancenorm.py", "torch/nn/modules/loss.py", "torch/nn/modules/rnn.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "581bf01074": {"title": "[Gradient Compression] Remove unnecessary warning on the rst file and the check on C++ version (#58170)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58170\n\nNow comm hook can be supported on MPI and GLOO backends besides NCCL. No longer need these warnings and check.\nghstack-source-id: 128799123\n\nTest Plan: N/A\n\nReviewed By: agolynski\n\nDifferential Revision: D28388861\n\nfbshipit-source-id: f56a7b9f42bfae1e904f58cdeccf7ceefcbb0850", "pr_number": "58170", "files_changed": ["docs/source/ddp_comm_hooks.rst", "torch/lib/c10d/reducer.cpp"], "labels": ["Merged", "cla signed", "module: ddp", "oncall: distributed"]}, "2073e866ad": {"title": "Switch GHA test stats S3 upload token (#58156)", "body": "Summary:\nTODOs:\n\n- [x] generate a temporary new token on this repo for testing purposes\n- [x] change the name of the S3 secret used in the workflow YAML definitions\n- [x] check the test plan\n- [x] replace the temporary token with a more permanent one\n- [x] check the test plan again\n- [x] uncomment the `if` statement that guards against uploading PR test stats\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58156\n\nTest Plan: Check the [ossci-metrics bucket](https://s3.console.aws.amazon.com/s3/buckets/ossci-metrics) after CI runs on this PR. Specifically, [this prefix](https://s3.console.aws.amazon.com/s3/buckets/ossci-metrics?region=us-east-1&prefix=test_time/a3445bfbd7efc602c28f5146fb562aa35ec50077/pytorch-linux-xenial-py3.6-gcc5.4/&showversions=false) has two objects under it.\n\nReviewed By: janeyx99\n\nDifferential Revision: D28393138\n\nPulled By: samestep\n\nfbshipit-source-id: 2c39c102652d471afa016cfc4942bb1e5bbb4163", "pr_number": "58156", "files_changed": [".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3.6-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed"]}, "9bfc1c4e0e": {"title": "[Gradient Compression] Update the docstring of fp16_compress_hook (#58168)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58168\n\nUpdate the documentation to be consistent to https://github.com/pytorch/pytorch/pull/57410.\nghstack-source-id: 128797174\n\nTest Plan: N/A\n\nReviewed By: agolynski, zhengwy888\n\nDifferential Revision: D28388160\n\nfbshipit-source-id: 6ba13ad9f9d7b4d003cdc112545573e452df8b65", "pr_number": "58168", "files_changed": ["torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "82b2013eac": {"title": "Delete move constructor on TensorImpl (#58048)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58048\n\nIt's never used, and it is also a bit dangerous, because a move\ntypically destroys the source location, but there may be other owning\nreferences to the original location!\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28390241\n\nPulled By: ezyang\n\nfbshipit-source-id: 68f22756ac066a7a0fc8baedd2b7834c01c2c534", "pr_number": "58048", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["Merged", "cla signed"]}, "3d5bb71020": {"title": "Back out \"[PyTorch Edge] Reuse constant table from ts in bytecode\" (#58099)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58099\n\nOriginal commit changeset: 34e0cb814901\nghstack-source-id: 128749184\n\nTest Plan: CI\n\nReviewed By: raziel, iseeyuan\n\nDifferential Revision: D28369142\n\nfbshipit-source-id: 631034126cebbd1c94ead6316b66e83a4812a890", "pr_number": "58099", "files_changed": ["torch/csrc/jit/serialization/export_module.cpp", "torch/csrc/jit/serialization/import_read.cpp", "torch/csrc/jit/serialization/pickler.cpp", "torch/csrc/jit/serialization/pickler.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "9063cb0a3c": {"title": "Infer types for arguments of methods not invoked directly by monkeytype (#57202)", "body": "Summary:\nSupport adding type annotations for class methods and nn.Module methods which are not invoked under the hood of MonkeyType\n\n** Changes **\n* This PR involves a slight change in how the example inputs are passed while scripting `class` and `nn.Module` objects.\n* The example inputs passed to `_script_pdt` is of the following format:\n     - example_inputs= [(obj.method1, (arg_list)), (obj.method2, (arg_list)),]\n* For nn.Modules, to infer types for `forward` methods, example_inputs can be passed in two ways:\n    - example_inputs= [(obj.forward, (arg_list, ))]\n    - example_inputs = [(obj, (arg_list, ) )]\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57202\n\nReviewed By: desertfire\n\nDifferential Revision: D28382827\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 5481467f3e909493bf3f439ee312056943508534", "pr_number": "57202", "files_changed": ["test/jit/test_pdt.py", "torch/jit/_script.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "9148f19e85": {"title": "enable support for nested containers in `torch.testing.assert(equal|close)` (#57270)", "body": "Summary:\nIn contrast to the initial opinion in https://github.com/pytorch/pytorch/issues/55385, there are legitimate use cases for nested containers. One such example is the [output of `LSTM`'s](https://pytorch.org/docs/stable/generated/torch.nn.LSTM):\n\n```python\noutput: Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] = torch.nn.LSTM()(input)\nassert_close(output, expected)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57270\n\nReviewed By: albanD\n\nDifferential Revision: D28249303\n\nPulled By: mruberry\n\nfbshipit-source-id: 75caa4414cc184ff0ce4cfc0dd5aafddfad42bcf", "pr_number": "57270", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["Merged", "cla signed", "open source"]}, "d09abf004c": {"title": "OpInfo: narrow (#58082)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58082\n\nReviewed By: agolynski\n\nDifferential Revision: D28379371\n\nPulled By: mruberry\n\nfbshipit-source-id: 484e560b1e6ceba234e497585ed308a27cd8b7a0", "pr_number": "58082", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "d230045fde": {"title": "Combine backtrace print into one string to avoid interleaving. (#56961)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56961\n\nAs described in https://github.com/pytorch/pytorch/issues/56583, the\nbacktrace amongst several processes was garbled.\nhttps://github.com/pytorch/pytorch/pull/56198 would've alleviated this to some\nextent, but this PR combines all the logging into just one string to reduce\ninterleaving further.\nghstack-source-id: 128730047\n\nTest Plan: waitforbuildbot\n\nReviewed By: cbalioglu\n\nDifferential Revision: D28013191\n\nfbshipit-source-id: 8bd8978a92ee2fbcd18472e1293d5809455b411b", "pr_number": "56961", "files_changed": ["BUILD.bazel", "c10/CMakeLists.txt", "c10/util/signal_handler.cpp"], "labels": ["Merged", "cla signed"]}, "8a45006765": {"title": "enable deterministic path for index_copy_cuda with index_put (#58144)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58144\n\nreland D28291041 (https://github.com/pytorch/pytorch/commit/14badd9929e3fb6aad74983c6ba82658be6ab109), which was reverted due to a type error from Tuple[torch.Tensor], seems that mypy requires Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n\nTest Plan:\nbuck test mode/opt //caffe2/test:torch_cuda -- test_index_copy_deterministic\n\n    \u2713 ListingSuccess: caffe2/test:torch_cuda - main (9.229)\n    \u2713 Pass: caffe2/test:torch_cuda - test_index_copy_deterministic_cuda (test_torch.TestTorchDeviceTypeCUDA) (25.750)\n    \u2713 Pass: caffe2/test:torch_cuda - main (25.750)\n\nReviewed By: ngimel\n\nDifferential Revision: D28383178\n\nfbshipit-source-id: 38896fd6ddd670cfcce36e079aee7ad52adc2a28", "pr_number": "58144", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "test/test_torch.py", "torch/__init__.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "166a8df65f": {"title": "[reland] make ddp logging api to be private (#58089)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58089\n\nmake ddp logging api to be private\nghstack-source-id: 128796419\n\nTest Plan: unit test\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28365412\n\nfbshipit-source-id: 374c01d443ffb47a3706f59e296d6e47eb5f4c85", "pr_number": "58089", "files_changed": ["test/distributed/test_c10d_common.py", "test/distributed/test_c10d_nccl.py", "torch/nn/parallel/distributed.py", "torch/testing/_internal/distributed/distributed_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "f1ac9b6598": {"title": "fix lint (#58203)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58203\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D28401974\n\nPulled By: suo\n\nfbshipit-source-id: cc244e0fc81c5f699ff4bd30754a3f6467f232c4", "pr_number": "58203", "files_changed": ["torch/package/package_exporter.py"], "labels": ["Merged", "cla signed"]}, "645a5f706a": {"title": "move `flatten_dense_tensors` and `unflatten_dense_tensors` to Native (#58006)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/55240\n\nCC ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58006\n\nReviewed By: agolynski\n\nDifferential Revision: D28386749\n\nPulled By: ngimel\n\nfbshipit-source-id: 4860c35d5ff95bcc38a243d7001180e7bd536314", "pr_number": "58006", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/_C/_nn.pyi.in", "torch/_utils.py", "torch/csrc/utils/tensor_flatten.cpp", "torch/csrc/utils/tensor_flatten.h"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "bc30c3165c": {"title": "Update docs for get_future support (#58107)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58107\n\nTest Plan: Imported from OSS\n\nReviewed By: SciPioneer\n\nDifferential Revision: D28387374\n\nPulled By: agolynski\n\nfbshipit-source-id: 70052afbb0b07ba341ea55f7ec30f7d9759b7bd4", "pr_number": "58107", "files_changed": ["docs/source/distributed.rst", "torch/csrc/distributed/c10d/init.cpp", "torch/nn/parallel/distributed.py"], "labels": ["Merged", "cla signed", "module: docs", "oncall: distributed"]}, "647282cb0c": {"title": "Add forward AD gradcheck (#57633)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57633\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski\n\nDifferential Revision: D28387765\n\nPulled By: albanD\n\nfbshipit-source-id: ed15049b5bdacca54f775b50ef166d540ba0b847", "pr_number": "57633", "files_changed": ["test/test_autograd.py", "test/test_overrides.py", "tools/autograd/derivatives.yaml", "torch/autograd/gradcheck.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "26b6d044cd": {"title": "Add forward AD test for op info (#57701)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57701\n\nThe new OpInfo flag has the following semantic:\n- If it says that it supports forward AD, we run gradcheck with forward AD to ensure it is correct\n- If it says that it does not support it, we check that the corresponding error is raised\n\nAll the added tests take 3s to run for CPU builds and 1min for GPU builds which should be pretty negligible compared to the test_ops runtime for each of these arch.\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski\n\nDifferential Revision: D28387767\n\nPulled By: albanD\n\nfbshipit-source-id: 369d76921c8460aa4548f9b5159b7297994672f5", "pr_number": "57701", "files_changed": ["test/test_ops.py", "tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "2279962162": {"title": "Codegen inplace forward AD formula from out of place one if needed (#57767)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57767\n\nTest Plan: Imported from OSS\n\nReviewed By: agolynski\n\nDifferential Revision: D28387764\n\nPulled By: albanD\n\nfbshipit-source-id: 7bf3929dd21425be653da112385e902aa50455a1", "pr_number": "57767", "files_changed": ["tools/autograd/derivatives.yaml", "tools/codegen/api/autograd.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "f4a5730a6b": {"title": "Add LowerSimpleTuples for freeze tuples (#57915)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57698\n\nFollow the suggestion mentioned in https://github.com/pytorch/pytorch/issues/57698\nadd a call to LowerSimpleTuples after the call:\nhttps://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/passes/freeze_module.cpp#L89.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57915\n\nReviewed By: agolynski\n\nDifferential Revision: D28387310\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 5becb608c5352240b30dfdf03a821d0297e9609c", "pr_number": "57915", "files_changed": ["test/jit/test_freezing.py", "torch/csrc/jit/passes/freeze_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit", "open source"]}, "c4a486f4b1": {"title": "Enable atan2 & hypot for BFloat16 on CUDA (#57905)", "body": "Summary:\nEnable `atan2` & `hypot` for BFloat16 on CUDA.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57905\n\nReviewed By: agolynski\n\nDifferential Revision: D28393706\n\nPulled By: ngimel\n\nfbshipit-source-id: e505e5f098d35e4f7417508443cb0fedf6562dd1", "pr_number": "57905", "files_changed": ["aten/src/ATen/native/cuda/BinaryGeometricKernels.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "e6d8f45523": {"title": "Enable `ceil`, `floor`, `frac`, `round` & `trunc` for BFloat16 on CUDA (#57910)", "body": "Summary:\nEnable `ceil`, `floor`, `frac`, `round` & `trunc` for BFloat16 on CUDA\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57910\n\nReviewed By: agolynski\n\nDifferential Revision: D28393469\n\nPulled By: ngimel\n\nfbshipit-source-id: b0f02ade7c6e2ed122aa5d80f6d442823dc1f221", "pr_number": "57910", "files_changed": ["aten/src/ATen/native/cuda/ForeachUnaryOp.cu", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "ci/all", "ci/master", "cla signed", "open source", "triaged"]}, "f9aa6b2432": {"title": "Enable lerp for BFloat16 on CUDA (#57907)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57907\n\nReviewed By: agolynski\n\nDifferential Revision: D28393597\n\nPulled By: ngimel\n\nfbshipit-source-id: 27ebfaf175c9eeb8d411ce782fdbc468082c6af3", "pr_number": "57907", "files_changed": ["aten/src/ATen/native/cuda/Lerp.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ce1a8620d9": {"title": "Enabled `roll` & `diag` for BFloat16 dtype on CUDA (#57916)", "body": "Summary:\nEnabled `roll` & `diag` for BFloat16 dtype on CUDA\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57916\n\nReviewed By: agolynski\n\nDifferential Revision: D28393534\n\nPulled By: ngimel\n\nfbshipit-source-id: fc1d8555b23a75f8b24c2ad826f89cd4e14cf487", "pr_number": "57916", "files_changed": ["aten/src/ATen/native/cuda/TensorTransformations.cu", "aten/src/ATen/native/cuda/TriangularOps.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "87f7fdfd5c": {"title": "Allow instruction counting to use shared memory as a staging ground. (And a couple other tweaks.) (#56711)", "body": "Summary:\nThis is actually something I discovered a while ago with the wall of serotonin. It was really easy for large scale runs to get bottlenecked on disk access. I have a hack in the working files of that machine to use `/dev/shm`, but I figured I should formalize and actually make a respectable utility.\n\nI also added a param to tweak the run cadence and print when a CorePool is created; these are just to make the CI logs a bit nicer. (A printout each second on a 40 minute CI job is a bit much...)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56711\n\nReviewed By: agolynski\n\nDifferential Revision: D28392248\n\nPulled By: robieta\n\nfbshipit-source-id: b6aa7445c488d8e4ab9d4b31ab18df4e12783d8f", "pr_number": "56711", "files_changed": ["benchmarks/instruction_counts/applications/ci.py", "benchmarks/instruction_counts/core/utils.py", "benchmarks/instruction_counts/execution/runner.py", "torch/utils/benchmark/utils/common.py", "torch/utils/benchmark/utils/cpp_jit.py", "torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py"], "labels": ["Merged", "cla signed"]}, "a31daf381f": {"title": "Move libtorch builds to be master-only (#58183)", "body": "Summary:\nThere were almost no libtorch specific regressions recently\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58183\n\nReviewed By: janeyx99\n\nDifferential Revision: D28393091\n\nPulled By: malfet\n\nfbshipit-source-id: 6dadd915ba574294afa6a95eaa759564af3154d4", "pr_number": "58183", "files_changed": [".circleci/cimodel/data/pytorch_build_data.py", ".circleci/cimodel/data/simple/binary_smoketest.py", ".circleci/config.yml"], "labels": ["Merged", "cla signed"]}, "6b1eeef601": {"title": "OpInfo: squeeze (#58080)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/54261\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58080\n\nReviewed By: agolynski\n\nDifferential Revision: D28379485\n\nPulled By: mruberry\n\nfbshipit-source-id: 2b288036f595a5bd6b948a072494ee87f82322ce", "pr_number": "58080", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ad4b2571b6": {"title": "Fix multi gpu test break on Windows (#58213)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58213\n\nReviewed By: robieta, mruberry\n\nDifferential Revision: D28405126\n\nPulled By: malfet\n\nfbshipit-source-id: 48c0aa8a113c554e3a007c1900fae2ff453cf85b", "pr_number": "58213", "files_changed": ["test/distributed/test_c10d_nccl.py"], "labels": ["Merged", "cla signed", "oncall: distributed", "open source"]}, "8a1dab3d26": {"title": "[tsm] add support for jetter to Role (base_image) for mast launches", "body": "Summary:\n1. Adds `ml_image` buck macro\n2. Adds `--run_path` option to `torch.distributed.run`\n3. Adds `tsm/driver/fb/test/patched/foo` (for unittesting)\n4. Changes to `distributed_sum` to use `ml_image` (see Test plan for how this was tested in local and mast)\n\nNOTE: need to enable jetter for flow and local schedulers (will do this on a separate diff since this diff is already really big)\n\nTest Plan:\n## Local Testing\n```\n# build the two fbpkgs (base and main)\nbuck run //pytorch/elastic/examples/distributed_sum/fb:torchx.examples.dist_sum.base\nbuck run //pytorch/elastic/examples/distributed_sum/fb:torchx.examples.dist_sum\n\n# fetch the fbpkgs\ncd ~/tmp\n\nfbpkg fetch --symlink-tags  -o -d . jetter:prod\nfbpkg fetch --symlink-tags  -o -d . torchx.examples.dist_sum.base\nfbpkg fetch --symlink-tags  -o -d . torchx.examples.dist_sum\n\njetter/LAST/jetter apply-and-run \\\n  torchx.examples.dist_sum.base/LAST/torchrun \\\n  torchx.examples.dist_sum/LAST \\\n  -- \\\n  --as_function \\\n  --rdzv_id foobar \\\n  --nnodes 1 \\\n  --nproc_per_node 2 \\\n  --max_restarts 0 \\\n  --role worker \\\n  --no_python \\\n~/torchx.examples.dist_sum/LAST/pytorch/elastic/examples/distributed_sum/fb/main.py\n```\n\n## Mast Testing\n```\nbuck-out/gen/pytorch/elastic/torchelastic/tsm/fb/cli/tsm.par run_ddp \\\n  --scheduler mast\n  --base_fbpkg torchx.examples.dist_sum.base:78f01b5 \\\n  --fbpkg torchx.examples.dist_sum:f38ab46 \\\n  --run_cfg hpcClusterUuid=MastNaoTestCluster,hpcIdentity=pytorch_r2p,hpcJobOncall=pytorch_r2p \\\n  --nnodes 2 \\\n  --resource T1 \\\n  --nproc_per_node 4 \\\n  --name kiuk_jetter_test \\\n pytorch/elastic/examples/distributed_sum/fb/main.py\n```\nRuns successfully: https://www.internalfb.com/mast/job/tsm_kiuk-kiuk_jetter_test_34c9f0fa?\n\nReviewed By: tierex, yifuwang\n\nDifferential Revision: D28177553\n\nfbshipit-source-id: 29daada4bc26e5ef0949bf75215f35e557bd35b8", "pr_number": null, "files_changed": ["torch/distributed/run.py"], "labels": []}, "e554731b32": {"title": "Hide set_enabled since it's not public facing. (#58078)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58078\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D28362048\n\nPulled By: ailzhang\n\nfbshipit-source-id: 4c78a7c58860ec4963bc8d05d133ea26e47dcf00", "pr_number": "58078", "files_changed": ["aten/src/ATen/ThreadLocalState.cpp", "c10/core/InferenceMode.cpp", "c10/core/InferenceMode.h"], "labels": ["Merged", "cla signed"]}, "fee7e8b91d": {"title": "Striding for lists Part 2 (#49352)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49352\n\nIn this PR, we replace all definitions of slice to take None parameters for the start, end, and step. This will simplify the compiler logic\n\nTest Plan:\ntest_jit test cases\n\nImported from OSS\n\nReviewed By: jamesr66a, nikithamalgifb\n\nDifferential Revision: D25929903\n\nfbshipit-source-id: 5bfc6bad514a8aafbef2dacc706f95f867fe85f1", "pr_number": "49352", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/RegisterSchema.cpp", "test/backward_compatibility/check_backward_compatibility.py", "test/cpp/jit/test_interpreter.cpp", "test/cpp/jit/test_utils.cpp", "test/jit/test_ignorable_args.py", "test/test_jit.py", "tools/autograd/derivatives.yaml", "torch/csrc/jit/frontend/ir_emitter.cpp", "torch/csrc/jit/passes/shape_analysis.cpp", "torch/csrc/jit/runtime/register_ops_utils.cpp", "torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/onnx/symbolic_opset10.py", "torch/onnx/symbolic_opset9.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "6997e7bd39": {"title": "Update Kineto submodule (#58179)", "body": "Summary:\nUpdate Kineto submodule, minor api changes\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58179\n\nTest Plan: CI\n\nReviewed By: gdankel\n\nDifferential Revision: D28391369\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 61fbf63d9ec2db66fac203944679e4b99cb0d568", "pr_number": "58179", "files_changed": ["cmake/Dependencies.cmake", "test/test_profiler.py", "third_party/kineto", "torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/profiler/profiler.py"], "labels": ["Merged", "cla signed"]}, "cf7d56d8f2": {"title": "Gradgradcheck to runs successfully with unrelated inputs (#58049)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/57649\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58049\n\nReviewed By: agolynski\n\nDifferential Revision: D28390033\n\nPulled By: albanD\n\nfbshipit-source-id: a0809b918321f3ea6fc59bfbec1f37e566d3611d", "pr_number": "58049", "files_changed": ["test/test_autograd.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "ab5c273950": {"title": "Remove the matmul complex backward skip (#58138)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58138\n\nrelated https://github.com/pytorch/pytorch/issues/55754\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D28403156\n\nPulled By: anjali411\n\nfbshipit-source-id: dca4dd643f190b314a8a4c01c698c6a1e5229f6f", "pr_number": "58138", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "complex_autograd", "module: complex"]}, "e8574b84bf": {"title": "Fix legacy tensor constructor/new matching incorrect signature with d\u2026 (#58108)", "body": "Summary:\n\u2026evice.\n\nPreviously, it was possible for torch.Tensor(tensor, device) or Tensor.new(tensor, device) to map to IntArrayRef or PyObject*.\n\nPyObject* was not a problem because that would error out later.\nBut IntArrayRef would create an uninitialized tensor, which is confusing.\n\nFixes https://github.com/pytorch/pytorch/issues/47112\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58108\n\nReviewed By: agolynski, mruberry\n\nDifferential Revision: D28372426\n\nPulled By: gchanan\n\nfbshipit-source-id: 795ab4f0561939d002a661c5cc14c6cdb579f31a", "pr_number": "58108", "files_changed": ["test/test_tensor_creation_ops.py", "torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "module: bc-breaking"]}, "52e9a192b1": {"title": "[ROCm] add 4.2 to nightly builds (#58143)", "body": "Summary:\nDepends on https://github.com/pytorch/builder/pull/764.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58143\n\nReviewed By: agolynski\n\nDifferential Revision: D28385532\n\nPulled By: malfet\n\nfbshipit-source-id: 1a37b1d4636327f8e1d0d5cfaa03f652565f8e38", "pr_number": "58143", "files_changed": [".circleci/cimodel/data/dimensions.py", ".circleci/config.yml"], "labels": ["Merged", "ci/binaries", "cla signed", "module: rocm", "open source", "triaged"]}, "e507771294": {"title": "[RPC Framework] Replace Python Pickler with internal RPC pickler for RemoteModule (#58019)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58019\n\nIn order to support sending `RemoteModule` over PRC, previously the pickling/unpickling of `RemoteModule` was implemented based on `__setstate__` and `__getstate__`. However, this means that the user can call regular Python pickler/unpickler to invoke the same logic,which should not be allowed.\n\nThis PR ensures that the pickling can only happen over RPC and not via regular python pickle.\n\nAdditionally, when a new attribute is added to `RemoteModule`, if it's not added to either `_REMOTE_MODULE_PICKLED_ATTRIBUTES` or `_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING`, this attribute will be ignored and an error message will be printed to std.err. However, it will not raise an exception like before, because such exception raised at the RPC layer will somehow cause timeout.\n\n#Closes: https://github.com/pytorch/pytorch/issues/57516\nghstack-source-id: 128868501\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_send_remote_module_over_the_wire\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_remote_module_py_pickle_not_supported\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_send_remote_module_with_a_new_attribute_ignored_over_the_wire\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- RemoteModule\n\nbuck test mode/dev-nosan //caffe2/torch/fb/csrc/concurrency/test:atomic_int_interprocess_test -- --exact 'caffe2/torch/fb/csrc/concurrency/test:atomic_int_interprocess_test - test_multiple_processes (caffe2.torch.fb.csrc.concurrency.test.atomic_int_interprocess_test.ForkMultipleProcessTest)'\nbuck test mode/dev //caffe2/torch/distributed/fb/test:app_test -- --exact 'caffe2/torch/distributed/fb/test:app_test - test_custom_init_rpc (caffe2.torch.distributed.fb.test.app_test.TestRpc)'\n\nReviewed By: mrshenli\n\nDifferential Revision: D28318270\n\nfbshipit-source-id: 7e7df2a6690f0860c4531a244d38789db424496f", "pr_number": "58019", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/distributed/rpc/internal.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "2afcb7e8fd": {"title": "Move Azure MultiGPU tests back to nightly (#58242)", "body": "Summary:\nAs its currently broken on master\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58242\n\nReviewed By: samestep\n\nDifferential Revision: D28414152\n\nPulled By: malfet\n\nfbshipit-source-id: 2566be294d62e39f9f7d316a039ab9372d685577", "pr_number": "58242", "files_changed": [".circleci/cimodel/data/windows_build_definitions.py", ".circleci/config.yml"], "labels": ["Merged", "cla signed"]}, "020e2ff115": {"title": "Add tests for PDT (#58211)", "body": "Summary:\nThis is a duplicate of the PR https://github.com/pytorch/pytorch/issues/56029\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58211\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D28403903\n\nPulled By: nikithamalgifb\n\nfbshipit-source-id: 290c46709c77c1a6fd647a2348419d12bf0a5ed6", "pr_number": "58211", "files_changed": ["test/jit/test_pdt.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "d8c6b74b0b": {"title": "Deprecate torch.solve (#57741)", "body": "Summary:\nDeprecate deprecate deprecate.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57741\n\nReviewed By: agolynski\n\nDifferential Revision: D28379337\n\nPulled By: mruberry\n\nfbshipit-source-id: a7a35ce1d3f25d8593698d89761c6c2d940db31a", "pr_number": "57741", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebra.cpp", "test/test_nn.py", "torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "module: bc-breaking", "open source"]}, "2294fd61c6": {"title": ".github: Add windows.4xlarge to scale-config.yml (#58198)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58198\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: samestep\n\nDifferential Revision: D28401791\n\nPulled By: seemethere\n\nfbshipit-source-id: dabaf58417114cc6138feca26d0121036476e04b", "pr_number": "58198", "files_changed": [".github/scale-config.yml"], "labels": ["Merged", "cla signed", "module: ci"]}, "1de9f51782": {"title": "[Pytorch Edge] Runtime ops compatibility api (#57570)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57570\n\nMove runtime ops compatibility api to OSS and introduce schema information\nghstack-source-id: 128789159\n\nTest Plan: unit test and manually ran it for a runtime with all (non custom) ops, and the bixray models unittest {P412728176}\n\nReviewed By: raziel\n\nDifferential Revision: D28203104\n\nfbshipit-source-id: 432a7d0247bccfb2e1ce90e8d41f81596efa3d67", "pr_number": "57570", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/mobile/runtime_compatibility.cpp", "torch/csrc/jit/mobile/runtime_compatibility.h"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "8888565597": {"title": "T90561249: Enforce kernel launch checks (#58178)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58178\n\nhttps://www.internalfb.com/T90561249: change the test to enforce\n\nTest Plan:\nbuck test //caffe2/test:kernel_launch_checks\n\nbefore fixing LinearAlgebra.cu and file close: https://www.internalfb.com/intern/testinfra/testconsole/testrun/1970324893386017/\n\nafter: https://www.internalfb.com/intern/testinfra/testconsole/testrun/2814749824394650/\n\nReviewed By: r-barnes\n\nDifferential Revision: D28390166\n\nfbshipit-source-id: 8a217ce8c0b204922005c731aa38bc03f70fabcc", "pr_number": "58178", "files_changed": ["aten/src/ATen/native/cuda/LinearAlgebra.cu", "test/test_kernel_launch_checks.py", "torch/testing/_check_kernel_launches.py"], "labels": ["Merged", "cla signed", "fb-exported"]}, "770f8cea2d": {"title": "Add support for real and imag tensor attributes (#54692)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/54692\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D28412240\n\nPulled By: anjali411\n\nfbshipit-source-id: e6965c55539a44260a812fdaa4a982f02067bb05", "pr_number": "54692", "files_changed": ["test/jit/test_complex.py", "torch/csrc/jit/frontend/sugared_value.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3c4a90ce38": {"title": "Revert \"Revert D28387764: Codegen inplace forward AD formula from out of place one if needed\" (#58231)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58231\n\nThis reverts commit 066e7699eb8c375a441e6de168da3ba7a73c3f27.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D28412480\n\nPulled By: albanD\n\nfbshipit-source-id: 7a231aa81b9e89537e6dca19642c4f12cd4b5ea5", "pr_number": "58231", "files_changed": ["test/test_autograd.py", "test/test_overrides.py", "tools/autograd/derivatives.yaml", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed"]}, "9b95568dc3": {"title": "update abs forward ad formula (#58235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58235\n\nthis is to make the opinfo change python only\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D28412937\n\nPulled By: albanD\n\nfbshipit-source-id: 1d6eb1e4baaa837c300ee8aa00b57986ba3e3eb2", "pr_number": "58235", "files_changed": ["tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed"]}, "061c7a1e17": {"title": "Overwrite with `ln` if libc10.so already exists (#58243)", "body": "Summary:\nThis should fix the issue noted in https://github.com/pytorch/pytorch/pull/57622#issuecomment-840612300 and demonstrated in [this run](https://github.com/pytorch/pytorch/runs/2566809365). Please review this PR carefully, because I do not know enough context to know whether this is the right thing to do.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58243\n\nTest Plan: n/a\n\nReviewed By: walterddr\n\nDifferential Revision: D28414358\n\nPulled By: samestep\n\nfbshipit-source-id: 0eb1c598f353ebac7f0a85c290be6fce4e00b6d5", "pr_number": "58243", "files_changed": [".jenkins/pytorch/test.sh"], "labels": ["Merged", "cla signed"]}, "ac9e79e561": {"title": "Add a new operator for fill_() function. (#56859) (#57596)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57596\n\nAdd the corresponding symbolic function and test for fill_() function.\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D28393520\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 3e177f88d3776d0d4a9d5e7ec7df4e6629738799\n\nCo-authored-by: Jay Zhang <jiz@microsoft.com>", "pr_number": "57596", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["Merged", "cla signed", "open source"]}, "2b0f481d3f": {"title": "Add support to to(device) op. (#56857) (#57599)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57599\n\nCurrently, if we call tensor.to() method and pass a device as the parameter. It will fail, because in symbolic function of to() we didn't handle such case.\n\nSo add a check in the beginning of this symbolic function, if this is a device cast, we return self directly. A test has also been added.\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D28393523\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: c41e3c0293932fc90dedb544daadd9c5d4b48792\n\nCo-authored-by: Jay Zhang <jiz@microsoft.com>", "pr_number": "57599", "files_changed": ["test/onnx/test_pytorch_onnx_onnxruntime.py", "torch/onnx/symbolic_opset9.py"], "labels": ["Merged", "cla signed", "open source"]}, "e1bb9d2d99": {"title": "Reimplement spectral_norm using new parametrization functionality (#57784)", "body": "Summary:\nAdds a new file under `torch/nn/utils/parametrizations.py` which should contain all the parametrization implementations\n\nFor spectral_norm we add the `SpectralNorm` module which can be registered using `torch.nn.utils.parametrize.register_parametrization` or using a wrapper: `spectral_norm`, the same API the old implementation provided.\n\nMost of the logic is borrowed from the old implementation:\n - Just like the old implementation, there should be cases when retrieving the weight should perform another power iteration (thus updating the weight) and cases where it shouldn't. For example in eval mode `self.training=True`, we do not perform power iteration.\n\nThere are also some differences/difficulties with the new implementation:\n - Using new parametrization functionality as-is there doesn't seem to be a good way to tell whether a 'forward' call was the result of parametrizations are unregistered (and leave_parametrizations=True) or when the injected property's getter was invoked. The issue is that we want perform power iteration in the latter case but not the former, but we don't have this control as-is. So, in this PR I modified the parametrization functionality to change the module to eval mode before triggering their forward call\n - Updates the vectors based on weight on initialization to fix https://github.com/pytorch/pytorch/issues/51800 (this avoids silently update weights in eval mode). This also means that we perform twice any many power iterations by the first forward.\n - right_inverse is just the identity for now, but maybe it should assert that the passed value already satisfies the constraints\n - So far, all the old spectral_norm tests have been cloned, but maybe we don't need so much testing now that the core functionality is already well tested\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57784\n\nReviewed By: ejguan\n\nDifferential Revision: D28413201\n\nPulled By: soulitzer\n\nfbshipit-source-id: e8f1140f7924ca43ae4244c98b152c3c554668f2", "pr_number": "57784", "files_changed": ["docs/source/nn.rst", "test/test_nn.py", "torch/nn/utils/__init__.py", "torch/nn/utils/parametrizations.py", "torch/nn/utils/parametrize.py", "torch/nn/utils/spectral_norm.py"], "labels": ["Merged", "cla signed"]}, "d833caaf6b": {"title": "[PyTorch Mobile][Forward/backward compatibility] Number of arguments for operators (#56845)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56845\n\nHandle forward/backward compatibility caused by added default arguments in mobile. As an example,\n\nIn older version, operator aten::foo's schema is\n```\nfoo(Tensor a, Tensor b) -> Tensor\n```\nIn the new version, the schema is updated to\n```\nfoo(Tensor a, Tensor b, int groups=1) -> Tensor\n```\n\n## Model file\nSerialize the number of specified arguments to each operator into the bytecode operator table. Before the operator table contains operator name and overload name:\n```\n('operators', (('aten::foo', ''),))\n```\nNow the number of specified arguments is added:\n```\n# bytecode version 6\n('operators', (('aten::foo', '', 2),))\n```\nwhere \"2\" means the number of specified arguments.\n\nSince there's bytecode schema change, the bytecode version number is bumped. This PR is to be landed after #56002 , where the version number is bumped from 4 to 5. This PR bumps the version number from 5 to 6.\n\n## Runtime and backward compatibility\nWhen the operator is found (either jit or c10), we have the OperatorHandle, where the operator schema can be accessed by\n```\nop.value().schema().arguments()\n```\nAdaptation is implemented to handle backward compatibility. For the example above, the new runtime holds the updated schema:\n```\nfoo(Tensor a, Tensor b, int groups=1) -> Tensor\n```\nWhereas the model file carries\n```\n(('aten::foo', ''), 2)\n```\nWe can implement a wrapper around the original function pointer to push the default argument to the stack.\n\n## Deliver time and forward compatibility\nAt model delivery time, two checks can be done:\n### Operator check\nTwo APIs to be provided:\n* Runtime: An API to get a runtime\u2019s ops and their schemas (i.e. the # of args). D27920185(WIP)\n* Model: An API to get a model\u2019s ops and their schema requirements (i.e. the # of args required).\n\nThe APIs can be used to check\n* runtime.ops() is a superset of model.ops()\n* for each op in model.ops() validate their schemas are compatible with those in runtime.ops() -- i.e. the # args required in a model op are <= # args in the runtime op.\n\nNote that only root ops in the model needs to be checked here. For transient ops it's not necessary. For example, if a root op, \"aten::root\" calls \"aten::foo\", it's \"aten::root\"'s responsibility to adapt to \"aten::foo\"'s change, or \"aten::root\" itself needs to be updated too.\n### Bytecode version backport (PR coming)\nWhen delivering a model with bytecode v6, if the runtime only works with bytecode v5 and lower, backport is needed.\n* The number of arguments is removed from the operator table\n* The bytecode version is changed from 6 to 5\n\nNote that this backport is a pure format change, it does not guarantee the backported model always runs in old runtime. The operator check mentioned before should be done first, before it\u2019s back ported to v5.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27986544\n\nPulled By: iseeyuan\n\nfbshipit-source-id: 143e19d4798cfb96b65095538dd648eead4e3fda", "pr_number": "56845", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp", "torch/csrc/jit/mobile/function.cpp", "torch/csrc/jit/mobile/function.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/runtime/interpreter/code_impl.h", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "26aeec35a1": {"title": "Disable more of quicklint test (#58257)", "body": "Summary:\nEssentially a followup to https://github.com/pytorch/pytorch/issues/57968. For now, this test is just too flaky to run on every PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58257\n\nTest Plan: The repro steps in https://github.com/pytorch/pytorch/issues/58253.\n\nReviewed By: walterddr\n\nDifferential Revision: D28424862\n\nPulled By: samestep\n\nfbshipit-source-id: 00aed872fe505db67e48414b1234505a71099262", "pr_number": "58257", "files_changed": ["tools/test/test_actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "f6532468c8": {"title": "Make norm and vector_norm use the same kernels. (#58214)", "body": "Summary:\nFixes a few problems with `torch.norm` (incorrect behavior for empty inputs and negative p, https://github.com/pytorch/pytorch/issues/52783, and incorrect imaginary part for complex).\nMost importantly, makes linalg_norm and vector_norm use the same kernels, reducing compile time and binary size.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58214\n\nReviewed By: ejguan\n\nDifferential Revision: D28422439\n\nPulled By: ngimel\n\nfbshipit-source-id: afe088a866963068e8c85eb9c3b2218a21ff2d48", "pr_number": "58214", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebra.cu", "aten/src/ATen/native/cuda/Blas.cpp", "aten/src/ATen/native/cuda/Blas.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu", "aten/src/ATen/native/cuda/ReduceNormKernel.cu"], "labels": ["Merged", "cla signed"]}, "3f9126f399": {"title": "Only quicklint files that exist (#58261)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/58253.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58261\n\nTest Plan: The repro steps for https://github.com/pytorch/pytorch/issues/58253.\n\nReviewed By: janeyx99\n\nDifferential Revision: D28425900\n\nPulled By: samestep\n\nfbshipit-source-id: b4abe910bd9ba5a34ec5a413d4df21b85f96a89f", "pr_number": "58261", "files_changed": ["tools/actions_local_runner.py"], "labels": ["Merged", "cla signed"]}, "7756cb6a5b": {"title": "Migrate pytorch_python_doc_build to github action (#57371)", "body": "Summary:\n# Changes\n\nThis PR migrates `pytorch_python_doc_build` from circleci to github actions.\n\nNoticeable changes\n- Refactor `docker cp` into a single `docker run` with volume mount, because the in circleci volume is not accessible from its remote docker engine\n- `pytorch_python_doc_push` job will have a race condition with circleci, which will be migrated in separate PRs\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57371\n\nReviewed By: samestep\n\nDifferential Revision: D28416289\n\nPulled By: zhouzhuojie\n\nfbshipit-source-id: 04caccccf3d7eb7e2225846a406a53ccda356d44", "pr_number": "57371", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed"]}, "d304bb070a": {"title": "Gelu Backward, Contribution from Kevin Stephano (#58249)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58249\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D28425381\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 21b7ac972220b6c35b285e3b66f05eb392002408", "pr_number": "58249", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_jit.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/jit_metaprogramming_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "993a35a8cb": {"title": "[Static Runtime] Support clamp.Tensor (#58191)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58191\n\nThere are two clamp overloads: clamp.Scalar and clamp.Tensor. SR needs to support both or has checks in place to avoid runtime errors. Supporting both is not too hard so here we are.\n\nReviewed By: edvgha\n\nDifferential Revision: D28371949\n\nfbshipit-source-id: 0ec6b8a0b8c6277e50d8e51e4e7a45aa62211e22", "pr_number": "58191", "files_changed": ["benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc", "torch/csrc/jit/runtime/static/ops.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "fd3d3ef900": {"title": "[RPC Framework] Add _script_module_reducer unconditionally for RecursiveScriptModule in RPC pickler (#58020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58020\n\nPreviously there is no RPC pickler for `RecursiveScriptModule`. Although it is a subclass of `ScriptModule`, the reducer of `ScriptModule` is not triggered for `RecursiveScriptModule` when a script remote module is sent over RPC.\n\nThis PR checkpoints the investigation of #58274, which makes sure that a RPC pickler is invoked here. This still cannot fix `test_send_remote_module_over_the_wire_script`. Will revisit this bug once there is a feature request from users.\n\nghstack-source-id: 128949642\n\nTest Plan:\nTODO: re-enable these tests\n\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_send_remote_module_over_the_wire_script\nbuck test mode/dev-nosan caffe2/test/distributed/rpc:process_group_agent -- test_remote_module_py_pickle_not_supported_script\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28346758\n\nfbshipit-source-id: 3cff84ca665da03da6ed6acb094a1f594fcd945e", "pr_number": "58020", "files_changed": ["torch/distributed/rpc/internal.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "8ac0917cc7": {"title": "add channels last support for AvgPool2d on CPU (#48918)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48918\n\nenable test case on AvgPool2d channels last for CPU\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D25399466\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9477b0c281c0de5ed981a97e2dcbe6072d7f0aef", "pr_number": "48918", "files_changed": ["aten/src/ATen/native/AveragePool2d.cpp", "aten/src/ATen/native/Pool.h", "aten/src/ATen/native/cpu/AvgPoolKernel.cpp", "test/test_nn.py", "tools/build_variables.bzl"], "labels": ["Merged", "Reverted", "ci/all", "cla signed", "open source"]}, "047ae6b713": {"title": "[profiler][small] CUDA synchronize guard, minor fix (#58254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58254\n\nDon't use CUDA synchronize when profiling in CPU only mode.\nminor fixes (a clarification for a doc string, fix spammy logging)\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: manual + CI\n\nReviewed By: gdankel, chaekit\n\nDifferential Revision: D28423667\n\nPulled By: ilia-cher\n\nfbshipit-source-id: 04c71727f528ae8e2e0ff90e88271608d291bc69", "pr_number": "58254", "files_changed": ["c10/core/CPUAllocator.cpp", "c10/core/CPUAllocator.h", "torch/autograd/profiler.py", "torch/profiler/profiler.py"], "labels": ["Merged", "cla signed"]}, "c524448dd1": {"title": "init hardshrink (#57749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57749\n\nadd to a fx test\n\nTest Plan: Imported from OSS\n\nReviewed By: huiguoo\n\nDifferential Revision: D28425974\n\nfbshipit-source-id: 195c7a1944decb7a2a99c2831cab38485f32be17", "pr_number": "57749", "files_changed": ["test/test_fx.py", "test/test_jit_fuser_te.py", "torch/csrc/jit/passes/tensorexpr_fuser.cpp", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/csrc/jit/tensorexpr/kernel.cpp", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/jit_metaprogramming_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "f3ead05d77": {"title": "hardtanh (#57750)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/57750\n\nTest Plan: Imported from OSS\n\nReviewed By: huiguoo\n\nDifferential Revision: D28425975\n\nfbshipit-source-id: a5e3dfbd6c77c595528c052e0b4325ef452983eb", "pr_number": "57750", "files_changed": ["torch/csrc/jit/runtime/symbolic_script.cpp", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/jit_metaprogramming_utils.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3072c97017": {"title": "Gelu Backward, Contribution from Kevin Stephano (#58249)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58249\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D28425629\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 494ab165d548aa76f036344ab1c19c5fd64bae82", "pr_number": "58249", "files_changed": ["test/test_jit_cuda_fuser.py", "torch/csrc/jit/runtime/symbolic_script.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "04970057d8": {"title": "Code-dedup in PowKernel (#57873)", "body": "Summary:\nBoth CPU and CUDA versions of PowKernel reimplement functionality that\nalready exists in UnaryOps, such as sqrt, rsqrt and reciprocal\n\nFind this out while looking at sluggish compilation of PowerKernel.cu:\n - Before the change it took 11m5s and resulted in 7.6Mb .o file\n - After the change compilation finished in 10m20s, and 6.4Mb .o file\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57873\n\nReviewed By: ezyang\n\nDifferential Revision: D28304929\n\nPulled By: malfet\n\nfbshipit-source-id: ac499476280de55a92044b1b041b1246eea74c64", "pr_number": "57873", "files_changed": ["aten/src/ATen/native/cpu/PowKernel.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/PowKernel.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed"]}, "452569dffb": {"title": "cfloat and cdouble functions (#58137)", "body": "Summary:\nThis adds the methods `Tensor.cfloat()` and `Tensor.cdouble()`.\n\nI was not able to find the tests for `.float()` functions. I'd be happy to add similar tests for these functions  once someone points me to them.\n\nFixes https://github.com/pytorch/pytorch/issues/56014\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58137\n\nReviewed By: ejguan\n\nDifferential Revision: D28412288\n\nPulled By: anjali411\n\nfbshipit-source-id: ff3653cb3516bcb3d26a97b9ec3d314f1f42f83d", "pr_number": "58137", "files_changed": ["test/test_torch.py", "tools/autograd/templates/python_variable_methods.cpp", "torch/_tensor_docs.py", "torch/nn/modules/module.py", "torch/overrides.py"], "labels": ["Merged", "cla signed", "module: complex", "open source", "triaged"]}, "38e606d056": {"title": "[RFC] Add method torch.jit._clone_module_with_class (#56152)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56152\n\nCurrently, the Bundled Inputs API mutates the module in-place. It adds class methods and not instance methods. This results in a small problem that one can't re-run an already executed cell in Bento if the class has already been subject to bundled inputs.\n\nIn addition, there is no way to add bundled inputs to a module that has bundled inputs added already. This API provides a way to solve this problem as well by adding an `ignored_methods` to the call to `clone()` by allowing the implementation of bundled inputs to pass in the methods that it will add as `ignored_methods` so that when it does try to add those methods, it will be able to do so successfully.\n\nWe'll have to be careful when ignoring those methods during the call to `torch.jit._clone_module_with_class` since any bundled input that relies on a user-provided method will need to be preserved and not ignored during the clone.\n\nLooking for feedback on whether this is an acceptable direction.\nghstack-source-id: 128908360\n\nTest Plan:\nAdded unit test and ran it as `buck test //caffe2/test:mobile`\n\nAlso see this Bento Notebook: https://www.internalfb.com/intern/anp/view/?id=550829\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27788394\n\nfbshipit-source-id: 48109cd4583506d4efdb345e4ba31385db23a273", "pr_number": "56152", "files_changed": ["test/test_mobile_optimizer.py", "torch/_C/__init__.pyi.in", "torch/csrc/jit/api/module.cpp", "torch/csrc/jit/api/module.h", "torch/csrc/jit/python/init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "a8122062c0": {"title": "[PyTorch Mobile]Add light version of RandomSampler (#58201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58201\n\nAdd light version of RandomSampler which can be used torch mobile.\n\nTest Plan: run unit test\n\nReviewed By: iseeyuan\n\nDifferential Revision: D28364467\n\nfbshipit-source-id: 3148129fa56533f5f4b76b63b60e8778eeaf815f", "pr_number": "58201", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_trainer.cpp", "tools/build_variables.bzl", "torch/csrc/jit/mobile/random.cpp", "torch/csrc/jit/mobile/random.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "cb7c6a536b": {"title": "[doc] update distributed optimizer doc (#58084)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58084\n\nupdate the doc for distributed optimizer with TorchScript support.\n\nTest Plan: Imported from OSS\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28363971\n\nPulled By: wanchaol\n\nfbshipit-source-id: df9d2acc1bbb2292d683d2231e1349b8d3946c8f", "pr_number": "58084", "files_changed": ["torch/distributed/optim/optimizer.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "88ff651e90": {"title": "torch.jit.ignore as a context manager (#55172)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/55172\n\nDescription:\nThis is part 1 of series of PRs for supporting torch.jit.ignore as context manager. Following features are implemented in this PR:\n\n- Unique name for the registered function under torch.jit.frontend module. The unique name is generated based on the file name and line number of context manager\n- Forcing user to explicitly annotate the input and outputs.\n- No side effects are considered.\n\nTest Plan: Imported from OSS\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D27895283\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 5d36d9aa5d457055a6bb1676f264647a745ec36a", "pr_number": "55172", "files_changed": [".circleci/docker/common/install_conda.sh", "README.md", "requirements.txt", "test/jit/test_ignore_context_manager.py", "test/test_jit.py", "tools/coverage_plugins_package/src/coverage_plugins/jit_plugin.py", "torch/_jit_internal.py", "torch/jit/__init__.py", "torch/jit/frontend.py"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "ccd7141919": {"title": "Modify DispatchKeyExtractor to also work for optional Tensors (#58283)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58283\n\nReviewed By: bhosmer\n\nDifferential Revision: D28436443\n\nPulled By: Chillee\n\nfbshipit-source-id: ba6aae74e8ec3c5a6157cc4517c29b36bdd4a30d", "pr_number": "58283", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h"], "labels": ["Merged", "cla signed"]}, "4f28c0b590": {"title": "Revert \"Revert D28387767: Add forward AD test for op info\" (#58230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58230\n\nThis reverts commit f88297c66bd36d075e9d50eb09a81bea74a669c6.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D28412496\n\nPulled By: albanD\n\nfbshipit-source-id: 5b8e30b5e80771dedf999c3aaa9791fc9026f8c1", "pr_number": "58230", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "Reverted", "cla signed"]}, "72a90c3ea5": {"title": "[metal] Add reflection_pad2d for metal (#58263)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58263\n\nAdd the `reflection_pad2d` op in preparation for newer xirp models.\n\nTest Plan:\nTest on device:\n```\narc focus2 pp-ios\n```\nTest on mac\n```\nbuck test pp-macos\n```\n\nReviewed By: xta0\n\nDifferential Revision: D27047892\n\nfbshipit-source-id: 815856e19e4885c352f5d7174866480db7641cdf", "pr_number": "58263", "files_changed": ["aten/src/ATen/native/metal/MetalShaders.h", "aten/src/ATen/native/metal/mpscnn/MPSCNNContext.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.h", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm", "aten/src/ATen/native/metal/ops/MetalPadding.mm"], "labels": ["Merged", "cla signed", "fb-exported"]}, "1f5ed1ff69": {"title": "[metal] Fix binary elementwise ops to handle inputs with mismatched dim() (#58262)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58262\n\nWhen broadcasting, it can be fine for input tensors to have a different number of dims. Fix the checks in arithmetic ops to accept these cases.\n\nTest Plan:\nTest on device:\n```\narc focus2 pp-ios\n```\nTest on mac\n```\nbuck test pp-macos\n```\n\nReviewed By: xta0\n\nDifferential Revision: D27093367\n\nfbshipit-source-id: 797eeffa1864291cb0e40277372842dca145c9c0", "pr_number": "58262", "files_changed": ["aten/src/ATen/native/metal/MetalTensorUtils.h", "aten/src/ATen/native/metal/MetalTensorUtils.mm", "aten/src/ATen/native/metal/mpscnn/tests/MPSCNNTests.mm", "aten/src/ATen/native/metal/ops/MetalBinaryElementwise.mm"], "labels": ["Merged", "cla signed", "fb-exported"]}, "2e26976ad3": {"title": "Disallow versionless Python shebangs (#58275)", "body": "Summary:\nSome machines don't have a versionless `python` on their PATH, which breaks these existing shebangs.\n\nI'm assuming that all the existing versionless `python` shebangs are meant to be `python3` and not `python2`; please let me know if my assumption was incorrect for any of these.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58275\n\nTest Plan: CI.\n\nReviewed By: zhouzhuojie\n\nDifferential Revision: D28428143\n\nPulled By: samestep\n\nfbshipit-source-id: 6562be3d12924db72a92a0207b060ef740f61ebf", "pr_number": "58275", "files_changed": [".circleci/ecr_gc_docker/docker_hub.py", ".circleci/ecr_gc_docker/gc.py", ".github/scripts/generate_linux_ci_workflows.py", ".github/scripts/lint_native_functions.py", ".github/scripts/regenerate_cancel_redundant_workflow.py", ".github/workflows/lint.yml", ".jenkins/pytorch/win-test-helpers/run_python_nn_smoketests.py", "Makefile", "aten/src/ATen/native/quantized/cpu/qnnpack/configure.py", "aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/configure.py", "binaries/bench_gen/bench_gen.py", "caffe2/contrib/aten/gen_op.py", "caffe2/contrib/gloo/gloo_test.py", "caffe2/core/nomnigraph/op_gen.py", "caffe2/python/allcompare_test.py", "caffe2/python/benchmark_generator.py", "caffe2/python/lazy_dyndep_test.py", "scripts/model_zoo/update-caffe2-models.py", "scripts/model_zoo/update-models-from-caffe2.py", "test/run_test.py", "test/scripts/run_cuda_memcheck.py", "test/test_testing.py", "tools/amd_build/build_amd.py", "tools/clang_tidy.py", "tools/code_coverage/oss_coverage.py", "tools/export_slow_tests.py", "tools/flake8_hook.py", "tools/git-clang-format", "torch/_appdirs.py", "torch/utils/hipify/hipify_python.py", "torch/utils/model_dump/__init__.py", "torch/utils/model_dump/__main__.py"], "labels": ["Merged", "cla signed"]}, "4bcaa5ae20": {"title": "Revert D28412496: Revert \"Revert D28387767: Add forward AD test for op info\"", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD28412496 (https://github.com/pytorch/pytorch/commit/4f28c0b5909d5122b0beffa49a579fc0d5fe0f80)\n\nOriginal commit changeset: 5b8e30b5e807\n\nfbshipit-source-id: 5a47aad4d5428e97e2d2b4acb4192909360870cd", "pr_number": null, "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "6e1718277c": {"title": "Make GHA test-reports upload regex more permissive (#58250)", "body": "Summary:\nCurrently, our test stats [uploaded to S3](https://s3.console.aws.amazon.com/s3/buckets/ossci-metrics?prefix=test_time/fee7e8b91d4434b976a339330bfa89bd827ab9ec/&showversions=false) by GitHub Actions are missing the reports from `test/custom_backend/test_custom_backend.py` and `test/custom_operator/test_custom_ops.py`. From [this log](https://github.com/pytorch/pytorch/runs/2573747177), we know that those tests are indeed being run, but the artifact on that workflow run shows that the XML files are currently not being uploaded for use in the render-test-results job. This PR makes the regex for that artifact upload more permissive.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58250\n\nTest Plan:\nFor context, before this PR, the test-reports artifact of Linux CI (pytorch-linux-xenial-py3.6-gcc5.4) before this PR looks like this:\n\n- `test-reports`\n  - `cpp-rpc`\n    - ...\n  - `cpp-unittest`\n    - ...\n  - `dist-gloo`\n    - ...\n  - `python-unittest`\n    - ...\n\nWait for Linux CI (pytorch-linux-xenial-py3.6-gcc5.4) to run on this PR, then download and unzip the test-reports artifact and check that its directory structure looks like this:\n\n- `custom_backend`\n  - `test-reports`\n    - `python-unittest`\n      - ...\n- `custom_operator`\n  - `test-reports`\n    - `python-unittest`\n      - ...\n- `test-reports`\n  - `cpp-rpc`\n    - ...\n  - `cpp-unittest`\n    - ...\n  - `dist-gloo`\n    - ...\n  - `python-unittest`\n    - ...\n\nAlso, [this run](https://github.com/pytorch/pytorch/runs/2579875947) shows the following line of output, which is exactly what we would expect to see if this PR correctly adds the 9 tests across `custom_backend` and `custom_operator`:\n\n> ```\n> Added    (across    2 suites)     9 tests, totaling +   0.10s\n> ```\n\nReviewed By: walterddr\n\nDifferential Revision: D28442396\n\nPulled By: samestep\n\nfbshipit-source-id: 893a397a8e701e4180e1812d6f83352b5920ced6", "pr_number": "58250", "files_changed": [".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3.6-gcc7.yml", ".github/workflows/pytorch-linux-xenial-py3.6-gcc5.4.yml"], "labels": ["Merged", "cla signed"]}, "c711c30c74": {"title": "Revert \"Revert D28387764: Codegen inplace forward AD formula from out of place one if needed\" (#58231)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58231\n\nThis reverts commit 066e7699eb8c375a441e6de168da3ba7a73c3f27.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D28412495\n\nPulled By: albanD\n\nfbshipit-source-id: 97dd4580baac903805ab66ad55fe9570dec993ee", "pr_number": "58231", "files_changed": ["tools/autograd/derivatives.yaml", "tools/codegen/api/autograd.py"], "labels": ["Merged", "cla signed"]}, "064923e635": {"title": "Improve native_batch_norm_backward performance (CUDA) (#58240)", "body": "Summary:\nFixes  https://github.com/pytorch/pytorch/issues/38915\n\nThe original code uses a single kernel to do both the reduction and the elementwise backward calculations. Whereas the  `SyncBatchNorm` kernels are split, which makes them slightly slower in some cases. I try to use the fused kernel when it's beneficial, but otherwise choose the optimized channels last split kernels. There is also eval mode, where the reduction is sometimes unnecessary in which case split kernels are a win even without channels last.\n\nBenchmarks on my system show significant speedups for channels last reductions and eval mode, with only a few minor slowdowns in training mode. These slowdowns are for 2 x 2048 shape in training, which is a small channels last inputs. But for larger batches or channels, the channels last kernels are much faster.\n\n|N   |C   |L   |training|backward|old   |new   |cudnn |\n|----|----|----|--------|--------|------|------|------|\n|1   |256 |3136|TRUE    |all     |70.25 |64.93 |68.90 |\n|    |    |    |TRUE    |self    |69.77 |64.61 |69.42 |\n|    |    |    |FALSE   |all     |70.10 |51.12 |x     |\n|    |    |    |FALSE   |self    |70.17 |51.17 |x     |\n|3136|256 |    |TRUE    |all     |554.08|76.63 |549.88|\n|    |    |    |TRUE    |self    |553.34|78.19 |552.36|\n|    |    |    |FALSE   |all     |565.40|55.09 |x     |\n|    |    |    |FALSE   |self    |565.71|54.84 |x     |\n|2   |8192|1   |TRUE    |all     |155.47|47.26 |202.26|\n|    |    |    |TRUE    |self    |155.46|48.36 |203.72|\n|    |    |    |FALSE   |all     |178.28|40.90 |x     |\n|    |    |    |FALSE   |self    |178.21|40.69 |x     |\n|2   |2048|1   |TRUE    |all     |43.50 |48.21 |57.47 |\n|    |    |    |TRUE    |self    |43.63 |47.24 |55.22 |\n|    |    |    |FALSE   |all     |49.36 |39.27 |x     |\n|    |    |    |FALSE   |self    |49.25 |42.02 |x     |\n|128 |8192|1   |TRUE    |all     |762.70|106.45|336.52|\n|    |    |    |TRUE    |self    |765.79|107.04|337.32|\n|    |    |    |FALSE   |all     |792.68|74.94 |x     |\n|    |    |    |FALSE   |self    |793.86|74.83 |x     |\n|128 |2048|1   |TRUE    |all     |188.37|46.20 |85.02 |\n|    |    |    |TRUE    |self    |188.47|47.57 |85.04 |\n|    |    |    |FALSE   |all     |191.57|40.44 |x     |\n|    |    |    |FALSE   |self    |190.13|41.55 |x     |\n|2   |8192|    |TRUE    |all     |156.03|43.01 |155.19|\n|    |    |    |TRUE    |self    |156.24|46.59 |156.93|\n|    |    |    |FALSE   |all     |179.34|40.06 |x     |\n|    |    |    |FALSE   |self    |179.20|41.85 |x     |\n|2   |2048|    |TRUE    |all     |44.05 |50.15 |44.21 |\n|    |    |    |TRUE    |self    |44.10 |48.97 |44.11 |\n|    |    |    |FALSE   |all     |49.72 |40.95 |x     |\n|    |    |    |FALSE   |self    |49.87 |43.43 |x     |\n|128 |8192|    |TRUE    |all     |775.19|96.60 |777.64|\n|    |    |    |TRUE    |self    |776.20|96.85 |774.21|\n|    |    |    |FALSE   |all     |797.64|68.01 |x     |\n|    |    |    |FALSE   |self    |806.25|68.05 |x     |\n|128 |2048|    |TRUE    |all     |188.49|48.10 |188.97|\n|    |    |    |TRUE    |self    |188.07|46.97 |187.98|\n|    |    |    |FALSE   |all     |192.32|43.78 |x     |\n|    |    |    |FALSE   |self    |193.72|40.82 |x     |\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58240\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28435158\n\nPulled By: ngimel\n\nfbshipit-source-id: bf62a1ee1c5d95a2caf55bee6176ae5c965688ec", "pr_number": "58240", "files_changed": ["aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Normalization.cuh", "test/test_nn.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "52bb8120b8": {"title": "Mention distributed profiling in documentation (#58286)", "body": "Summary:\nAdded a simple section indicating distributed profiling is expected to work similar to other torch operators, and is supported for all communication backends out of the box.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58286\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28436489\n\nPulled By: rohan-varma\n\nfbshipit-source-id: ce1905a987c0ede8011e8086a2c30edc777b4a38", "pr_number": "58286", "files_changed": ["docs/source/distributed.rst"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "0be334a1ba": {"title": "optimize channels last for BatchNorm2d on CPU (#48919)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48919\n\nmove data indexing utils\n\nparallel inference contiguous path\n\nparallel inference channels last path\n\nadd dim apply\n\noptimize update stats\n\nadd channels last support for backward\n\nRevert \"add channels last support for backward\"\n\nThis reverts commit cc5e29dce44395250f8e2abf9772f0b99f4bcf3a.\n\nRevert \"optimize update stats\"\n\nThis reverts commit 7cc6540701448b9cfd5833e36c745b5015ae7643.\n\nRevert \"add dim apply\"\n\nThis reverts commit b043786d8ef72dee5cf85b5818fcb25028896ecd.\n\nbug fix\n\nadd batchnorm nhwc test for cpu, including C=1 and HW=1\n\nTest Plan: Imported from OSS\n\nReviewed By: glaringlee\n\nDifferential Revision: D25399468\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: a4cd7a09cd4e1a8f5cdd79c7c32c696d0db386bd", "pr_number": "48919", "files_changed": ["aten/src/ATen/cpu/vec256/functional.h", "aten/src/ATen/native/Normalization.cpp", "aten/src/ATen/native/batch_norm.h", "aten/src/ATen/native/cpu/batch_norm_kernel.cpp", "aten/src/ATen/test/vec256_test_all_types.cpp", "aten/src/ATen/test/vec256_test_all_types.h", "test/test_nn.py"], "labels": ["Merged", "Reverted", "ci/all", "cla signed", "open source"]}, "94ef2b9b48": {"title": "[Pytorch] Better doc strings for bundled inputs (#56591)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/56591\n\ntitle\nghstack-source-id: 128926699\n\nTest Plan: na\n\nReviewed By: dreiss\n\nDifferential Revision: D27912185\n\nfbshipit-source-id: 1a8f267af21afb7b4393b9ec0792dd17c48e57cb", "pr_number": "56591", "files_changed": ["torch/utils/bundled_inputs.py"], "labels": ["Merged", "cla signed"]}, "73d51406fa": {"title": "[PyTorch Mobile]Move train related files to their own folder (#58205)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58205\n\nIt's worthing moving train related files into their own folder since we are adding more code under the mobile directory.\n\nThis diff does that.\n\nTest Plan: run unit tests and ci\n\nReviewed By: iseeyuan\n\nDifferential Revision: D28402432\n\nfbshipit-source-id: cd76a1c4f8ff06508cdc3aad8a169fbf34bb4995", "pr_number": "58205", "files_changed": ["caffe2/CMakeLists.txt", "test/cpp/jit/test_lite_trainer.cpp", "tools/build_variables.bzl", "torch/csrc/jit/mobile/export_data.cpp", "torch/csrc/jit/mobile/export_data.h", "torch/csrc/jit/mobile/optim/sgd.cpp", "torch/csrc/jit/mobile/optim/sgd.h", "torch/csrc/jit/mobile/random.cpp", "torch/csrc/jit/mobile/random.h", "torch/csrc/jit/mobile/sequential.cpp", "torch/csrc/jit/mobile/sequential.h", "torch/csrc/jit/mobile/train/export_data.cpp", "torch/csrc/jit/mobile/train/export_data.h", "torch/csrc/jit/mobile/train/optim/sgd.cpp", "torch/csrc/jit/mobile/train/optim/sgd.h", "torch/csrc/jit/mobile/train/random.cpp", "torch/csrc/jit/mobile/train/random.h", "torch/csrc/jit/mobile/train/sequential.cpp", "torch/csrc/jit/mobile/train/sequential.h"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "0a561f83ca": {"title": "[PyTorch Mobile]Fix unit test (#58202)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58202\n\nThis unit test was testing the wrong target. It should test the sampler under jit::mobile. This diff fixes it.\n\nTest Plan: run unit tests\n\nReviewed By: shreyanb98\n\nDifferential Revision: D28384839\n\nfbshipit-source-id: 35cc63be2e73ca9b1a7d30d6f67fffcfe5021fa2", "pr_number": "58202", "files_changed": ["test/cpp/jit/test_lite_trainer.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "d3fbb41c61": {"title": "[PyTorch Edge] share tensors in mobile with new api (#58182)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58182\n\nAs title, the v5 model format will be\n```\n(base) chenlai@chenlai-mp reuse_constant % zipinfo /Users/chenlai/Documents/pytorch/reuse_constant/tmp/zip/script_module_v5_unify.ptl\nArchive:  /Users/chenlai/Documents/pytorch/reuse_constant/tmp/zip/script_module_v5_unify.ptl\nZip file size: 3120 bytes, number of entries: 7\n-rw----     0.0 fat       77 bl stor 80-000-00 00:00 script_module_v4_unify/data.pkl\n-rw----     0.0 fat      240 bl defN 80-000-00 00:00 script_module_v4_unify/code/__torch__/___torch_mangle_5.py\n-rw----     0.0 fat      422 bl defN 80-000-00 00:00 script_module_v4_unify/code/__torch__/___torch_mangle_5.py.debug_pkl\n-rw----     0.0 fat       64 bl stor 80-000-00 00:00 script_module_v4_unify/constants/140245072983168.storage\n-rw----     0.0 fat      172 bl stor 80-000-00 00:00 script_module_v4_unify/constants.pkl\n-rw----     0.0 fat      678 bl stor 80-000-00 00:00 script_module_v4_unify/bytecode.pkl\n-rw----     0.0 fat        2 bl stor 80-000-00 00:00 script_module_v4_unify/version\n7 files, 1655 bytes uncompressed, 1453 bytes compressed:  12.2%\n```\nbytecode.pkl is:\n```\n(5,\n ('__torch__.___torch_mangle_5.TestModule.forward',\n  (('instructions',\n    (('STOREN', 1, 2),\n     ('DROPR', 1, 0),\n     ('LOADC', 0, 0),\n     ('LOADC', 1, 0),\n     ('MOVE', 2, 0),\n     ('OP', 0, 0),\n     ('LOADC', 1, 0),\n     ('OP', 1, 0),\n     ('RET', 0, 0))),\n   ('operators', (('aten::add', 'int'), ('aten::add', 'Scalar'))),\n   ('constants',\n    (torch._utils._rebuild_tensor_v2(pers.obj(('storage',\n          torch.DoubleStorage,\n          '140245072983168.storage',\n          'cpu',\n          8),),\n       0,\n       (2, 4),\n       (4, 1),\n       False,\n       collections.OrderedDict()),\n     1)),\n   ('types', ()),\n   ('register_size', 2)),\n  (('arguments',\n    ((('name', 'self'),\n      ('type', '__torch__.___torch_mangle_5.TestModule'),\n      ('default_value', None)),\n     (('name', 'y'), ('type', 'int'), ('default_value', None)))),\n   ('returns',\n    ((('name', ''), ('type', 'Tensor'), ('default_value', None)),)))))\n```\n\nconstants.pkl is:\n```\n(torch._utils._rebuild_tensor_v2(pers.obj(('storage', torch.DoubleStorage, '140245072983168.storage', 'cpu', 8),),\n   0,\n   (2, 4),\n   (4, 1),\n   False,\n   collections.OrderedDict()),)\n```\n\nBoth tensors will refer to the tensor in at the path `script_module_v4_unify/constants/140245072983168.storage`.\n\n## Note\nAccording to unify format, all tensors will be written to the folder `.data`, however, torch.jit.load() can't handle the unified format at this moment, so this change will write tensors at the `constants` folders, and mobile will write/read tensors from `constants` folder. such that the model can be interpreted by both jit and mobile.\nghstack-source-id: 129010347\n\nTest Plan: buck test mode/dev //caffe2/test/cpp/jit:jit\n\nReviewed By: raziel, iseeyuan\n\nDifferential Revision: D28375257\n\nfbshipit-source-id: 6544472db4c957c5ea037e0bb5112b637dd15897", "pr_number": "58182", "files_changed": ["torch/csrc/jit/mobile/backport_manager.cpp", "torch/csrc/jit/mobile/backport_manager.h", "torch/csrc/jit/mobile/import.cpp", "torch/csrc/jit/mobile/import.h", "torch/csrc/jit/mobile/model_compatibility.cpp", "torch/csrc/jit/serialization/export_module.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "3dc70d8f78": {"title": "[iOS][Metal] Add target for testing metal ops (#57832)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57832\n\n- Added a shared BUCK target: `//fbobjc/Apps/Internal/PyTorchMetalOpTester:PyTorchMetalOpTester`\n- Invoke this target from 3 Apps: pp-ios, pp-macos, PyTorchBenckmark\nghstack-source-id: 129037985\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan:\n## PyTorch Playground - macOS\n`buck test pp-macos`\n```\n\u279c  fbsource buck test pp-macos\nBuilding: finished in 0.3 sec (100%) 204/6264 jobs, 0 updated\n  Total time: 0.5 sec\nTesting: finished in 8.4 sec (6 PASS/0 FAIL)\nRESULTS FOR //fbobjc/Apps/Internal/PyTorchPlaygroundMac:PyTorchPlaygroundMacTests\nPASS     999ms  2 Passed   0 Skipped   0 Failed   ClassificationTests\nPASS      6.4s  1 Passed   0 Skipped   0 Failed   MetalOpsTests\nPASS     181ms  3 Passed   0 Skipped   0 Failed   PersonSegmentationTests\nUpdated test logs: buck-out/log/test.log\nTESTS PASSED\n```\n## PyTorch Playground - iOS\n- `arc focus2 pp-ios`\n- Build and run via Xcode\n{F613716289}\n\n### AI Bench\n- `buck run mode/mac aibench:run_bench_macos -- -b aibench/specifications/models/pytorch/metal/metal_op_test.json --platform ios --framework pytorch --remote --devices D53 (https://github.com/pytorch/pytorch/commit/871b1419de23352dba4eda665aca1471c5446d01)pAP`\n- Result: https://fburl.com/aibench/d2gtlndd\n\nReviewed By: xta0\n\nDifferential Revision: D28235867\n\nfbshipit-source-id: dcee8aee140b5f665a97efe278ee621f436c7c68", "pr_number": "57832", "files_changed": ["aten/src/ATen/native/metal/op_test_runner/MetalOpTestRunner.h", "aten/src/ATen/native/metal/op_test_runner/MetalOpTestRunner.mm"], "labels": ["Merged", "cla signed"]}, "a4075fca9a": {"title": "extract dispatch keys from optional Tensors (unboxed) (#58296)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/58296\n\nTest Plan: Imported from OSS\n\nReviewed By: Chillee\n\nDifferential Revision: D28436822\n\nPulled By: bhosmer\n\nfbshipit-source-id: 8031c9a3c121483dd0e5ed7b8b165952477108e4", "pr_number": "58296", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h"], "labels": ["Merged", "cla signed"]}, "00a46a5eb4": {"title": "Fix incorrect inplace sort in `topk` (#58314) (#58318)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/58314\n\nhttps://github.com/pytorch/pytorch/issues/55392 introduced a bug by not allocating a separate value tensor for sorting\nCC ngimel zasdfgbnm\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58318\n\nReviewed By: mruberry\n\nDifferential Revision: D28450698\n\nPulled By: ngimel\n\nfbshipit-source-id: dea1201ebfcbaab8536580b80f8321bda2468fc4", "pr_number": "58318", "files_changed": ["aten/src/ATen/native/cuda/TensorTopK.cu", "test/test_sort_and_select.py"], "labels": ["Merged", "cla signed", "open source"]}, "67583122f0": {"title": "Use pip3 instead of pip when building ECR GC image (#58334)", "body": "Summary:\nA followup to https://github.com/pytorch/pytorch/issues/58309, to fix the broken docker_for_ecr_gc_build_job:\n\n- https://app.circleci.com/pipelines/github/pytorch/pytorch/322672/workflows/4877ddfe-eee1-4116-91ae-6ee9dd3a97ad/jobs/13486207\n- https://app.circleci.com/pipelines/github/pytorch/pytorch/322710/workflows/8d33afb6-7b85-48c7-94fd-ac9176f4a16e/jobs/13488388\n- https://app.circleci.com/pipelines/github/pytorch/pytorch/322759/workflows/b480989a-b39e-48f7-929d-66f1bdc50c89/jobs/13490919\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58334\n\nTest Plan:\nBefore this PR, this fails:\n```\ncd .circleci/ecr_gc_docker && docker build .\n```\nAfter this PR, it succeeds.\n\nReviewed By: zhouzhuojie\n\nDifferential Revision: D28457290\n\nPulled By: samestep\n\nfbshipit-source-id: d8d8c8759412d9d7876d5908768ee5cb7261132d", "pr_number": "58334", "files_changed": [".circleci/ecr_gc_docker/Dockerfile"], "labels": ["Merged", "cla signed"]}, "998374a702": {"title": "[tsm] add support for jetter to Role (base_image) for mast launches (#58252)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58252\n\nPull Request resolved: https://github.com/pytorch/elastic/pull/149\n\n1. Adds `ml_image` buck macro\n2. Adds `--run_path` option to `torch.distributed.run`\n3. Adds `tsm/driver/fb/test/patched/foo` (for unittesting)\n4. Changes to `distributed_sum` to use `ml_image` (see Test plan for how this was tested in local and mast)\n\nNOTE: need to enable jetter for flow and local schedulers (will do this on a separate diff since this diff is already really big)\n\nTest Plan:\n## Local Testing\n```\n# build the two fbpkgs (base and main)\nbuck run //pytorch/elastic/examples/distributed_sum/fb:torchx.examples.dist_sum.base\nbuck run //pytorch/elastic/examples/distributed_sum/fb:torchx.examples.dist_sum\n\n# fetch the fbpkgs\ncd ~/tmp\n\nfbpkg fetch --symlink-tags  -o -d . jetter:prod\nfbpkg fetch --symlink-tags  -o -d . torchx.examples.dist_sum.base\nfbpkg fetch --symlink-tags  -o -d . torchx.examples.dist_sum\n\njetter/LAST/jetter apply-and-run \\\n  torchx.examples.dist_sum.base/LAST/torchrun \\\n  torchx.examples.dist_sum/LAST \\\n  -- \\\n  --as_function \\\n  --rdzv_id foobar \\\n  --nnodes 1 \\\n  --nproc_per_node 2 \\\n  --max_restarts 0 \\\n  --role worker \\\n  --no_python \\\n~/torchx.examples.dist_sum/LAST/pytorch/elastic/examples/distributed_sum/fb/main.py\n```\n\n## Mast Testing\n```\nbuck-out/gen/pytorch/elastic/torchelastic/tsm/fb/cli/tsm.par run_ddp \\\n  --scheduler mast\n  --base_fbpkg torchx.examples.dist_sum.base:78f01b5 \\\n  --fbpkg torchx.examples.dist_sum:f38ab46 \\\n  --run_cfg hpcClusterUuid=MastNaoTestCluster,hpcIdentity=pytorch_r2p,hpcJobOncall=pytorch_r2p \\\n  --nnodes 2 \\\n  --resource T1 \\\n  --nproc_per_node 4 \\\n  --name kiuk_jetter_test \\\n pytorch/elastic/examples/distributed_sum/fb/main.py\n```\nRuns successfully: https://www.internalfb.com/mast/job/tsm_kiuk-kiuk_jetter_test_34c9f0fa?\n\nReviewed By: tierex\n\nDifferential Revision: D28421033\n\nfbshipit-source-id: 96edcecf639143e31ec6c86ec713a2e2d7790f3d", "pr_number": "58252", "files_changed": ["test/distributed/launcher/run_test.py", "torch/distributed/run.py"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: distributed"]}, "bcacf91a71": {"title": "[fx_glow]Add Support for importing quantized linear in FXIRImporter (#57483)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/57483\n\nPull Request resolved: https://github.com/pytorch/glow/pull/5622\n\nQuantized linear has packed parameters. We want to unpack it so that it would be easier for graph optimization and importer to deal with the weight and bias. A customized remapping function is used to unpack quantized linear and map it to acc_op.linear.\n\nTest Plan: `buck test glow/fb/fx/nnpi_importer:test_importer`\n\nReviewed By: gcatron, jfix71, khabinov\n\nDifferential Revision: D27451237\n\nfbshipit-source-id: e46e961734788fd5333e227ca6143fd37c33204e", "pr_number": "57483", "files_changed": ["test/test_fx_experimental.py", "torch/fx/experimental/graph_manipulation.py", "torch/fx/passes/shape_prop.py"], "labels": ["Merged", "cla signed", "fb-exported", "fx"]}, "2436377a7d": {"title": "Remote the list for the attributes that will be ignored for pickling (#58345)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58345\n\n1. Add a sanity check to make sure any new attribute added to the constructor should be added to either `_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING` pr `_REMOTE_MODULE_ATTRIBUTES_IGNORE_FOR_PICKLING`.\n2. Update some comments and warning -- now if a new attribute is added after the construction, it will not be pickled. Previously it will trigger a runtime error, which is hard for unit test (one worker hit the runtime error, but the other worker will cause timeout).\nContext: https://github.com/pytorch/pytorch/pull/58019#discussion_r632322083\nghstack-source-id: 129070358\n\nTest Plan: unit test\n\nReviewed By: rohan-varma\n\nDifferential Revision: D28460744\n\nfbshipit-source-id: 8028186fc447c88fbf2bf57f5c5d321f42ba54ed", "pr_number": "58345", "files_changed": ["torch/distributed/nn/api/remote_module.py", "torch/testing/_internal/distributed/nn/api/remote_module_test.py"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "1a91892f90": {"title": "Added fix for missing ops aten::sorted.str (#58339)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58339\n\nThe operator was present as part of full_jit ops but wasn't included for mobile. The diff copies it for mobile\n\nTest Plan: buck run xplat/langtech/mobile:giga5_bin -- --voice /data/users/prabhavag/experiments/embedded_new_stateful_conv_may6/nicole_batch.giga5 --frontend /data/users/prabhavag/experiments/tools_pkg/en_US.embedded.frontend.titan --icudata xplat/third-party/icu/stubdata/reduced/icudt55l.dat --text \"haha\"\n\nReviewed By: iseeyuan\n\nDifferential Revision: D28452179\n\nfbshipit-source-id: ef7a929f1a6d40573438785a4959c1c1e39762f0", "pr_number": "58339", "files_changed": ["torch/csrc/jit/runtime/register_prim_ops.cpp", "torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp"], "labels": ["Merged", "cla signed", "fb-exported", "oncall: jit"]}, "71f4c5c1f4": {"title": "Fix \"ci/master\" workflow (#58335)", "body": "Summary:\nInclude jobs master-only jobs depends on to the workflow\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58335\n\nReviewed By: walterddr\n\nDifferential Revision: D28458406\n\nPulled By: malfet\n\nfbshipit-source-id: 217a8996daacd494af1bbc54e725bbcacc0c7784", "pr_number": "58335", "files_changed": [".circleci/config.yml", ".circleci/generate_config_yml.py"], "labels": ["Merged", "ci/master", "cla signed"]}, "9c7d5ed9b0": {"title": "Clarifies cholesky_ex role and makes batched support a common string (#58217)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/34272\n\nAlso updates and creates a common string for when the linear algebra operations support batching\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58217\n\nReviewed By: ngimel\n\nDifferential Revision: D28405908\n\nPulled By: mruberry\n\nfbshipit-source-id: a9d81a5a4712cfdedc22d614986d3707f10742a2", "pr_number": "58217", "files_changed": ["torch/linalg/__init__.py"], "labels": ["Merged", "cla signed"]}, "ee93a348de": {"title": "ENH Raises nicer error when calling module.train with invalid modes (#58247)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/46763\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58247\n\nReviewed By: ejguan\n\nDifferential Revision: D28418080\n\nPulled By: albanD\n\nfbshipit-source-id: fef8f4f641ef75e801ed8b8d04c4016579aea8b0", "pr_number": "58247", "files_changed": ["test/test_nn.py", "torch/nn/modules/module.py"], "labels": ["Merged", "cla signed", "module: nn", "module: ux", "open source"]}, "b1b9fb0147": {"title": "Specify the exact commit when triggering multi-gpu pipeline (#58219)", "body": "Summary:\nPreviously only the **branch** is specified when triggering the multi-gpu pipeline, which could result in incorrect commit being targeted, because when the pipeline actually runs there could be newer commit on the specified branch.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58219\n\nReviewed By: malfet, bdhirsh\n\nDifferential Revision: D28446453\n\nPulled By: seemethere\n\nfbshipit-source-id: 680c0b3a9f3f20b61787cc90fda73b87d66e6af8", "pr_number": "58219", "files_changed": [".circleci/scripts/trigger_azure_pipeline.py"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "1ad06ba3f5": {"title": "Wrap torch::deploy API functions in safe rethrow macros (#58192)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58192\n\nExceptions thrown by deploy internals need to be sanitized\nfor application safety.\n\nSee commment in deploy.h for detailed explanation.\n\nTest Plan: Added unit test\n\nReviewed By: suo\n\nDifferential Revision: D28371127\n\nfbshipit-source-id: c0ced2f194424a394c5852bd4ab5cb41b0f4e87b", "pr_number": "58192", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["Merged", "Reverted", "cla signed", "fb-exported"]}, "432676599c": {"title": "Stop installing libuv on Windows (#51936)", "body": "Summary:\nFixes #{issue number}\ngunandrose4u\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51936\n\nReviewed By: malfet\n\nDifferential Revision: D28467662\n\nPulled By: seemethere\n\nfbshipit-source-id: 28d203ee3af13d6a3158f188c2e889e310ee6010", "pr_number": "51936", "files_changed": [".jenkins/pytorch/win-test-helpers/setup_pytorch_env.bat", "cmake/Dependencies.cmake"], "labels": ["Merged", "cla signed", "open source", "triaged"]}, "eab59bae15": {"title": "Fix cmake_minimum_require in libshm (#58306)", "body": "Summary:\nDeprecation warning reported by cmake:\n\n```\nCMake Deprecation Warning at CMakeLists.txt (cmake_minimum_required):\n  Compatibility with CMake < 2.8.12 will be removed from a future version of CMake.\n  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n  CMake that the project does not need compatibility with older versions.\n```\n\nThis is the only place that requires bumping min version. There're two others but only in `third_party` folder.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58306\n\nReviewed By: bdhirsh\n\nDifferential Revision: D28446097\n\nPulled By: zhouzhuojie\n\nfbshipit-source-id: af5ef50e61bd57dc36089ebe62db70ba0081864c", "pr_number": "58306", "files_changed": ["torch/lib/libshm/CMakeLists.txt"], "labels": ["Merged", "cla signed"]}, "5a238eb96e": {"title": "Fix deadlock in Future due to lock inversion with GIL (#58382)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58382\n\nCalling markCompleted on a Future now first acquires the Future's mutex (as usual) but then sometimes tries to acquire the GIL during the DataPtr extraction while still holding the Future's mutex. (This happens when the value passed to markCompleted is a Python object). This can cause a deadlock if someone else calls any of the other methods of Future while holding the GIL.\n\nThere are two solutions to this: avoid holding the Future's mutex when extracting DataPtrs, and avoid holding the GIL while invoking the Future's method. In this PR I'm going for the latter, because it's a very simple immediate fix, but I believe this is brittle and that we should probably also consider the former fix.\nghstack-source-id: 129105358\n\nTest Plan: The repro in https://github.com/pytorch/pytorch/issues/58239 now doesn't deadlock.\n\nReviewed By: mrshenli\n\nDifferential Revision: D28472816\n\nfbshipit-source-id: 1bc9bca426dd004f9eb2568db1ffd38f014450e2", "pr_number": "58382", "files_changed": ["torch/csrc/jit/python/init.cpp"], "labels": ["Merged", "cla signed", "oncall: jit"]}, "affed3b04d": {"title": "Prevent lock inversions with GIL in Future (#58391)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58391\n\nAn additional (and hopefully more robust) way of fixing the same problem https://github.com/pytorch/pytorch/pull/58382 fixed.\nghstack-source-id: 129110325\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D28474154\n\nfbshipit-source-id: 625ebe782e380c60b3ead4c4ed8a51d4bc917153", "pr_number": "58391", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed"]}, "ae9b66dd94": {"title": "Fix TP agent not recording outgoing tensors with caching allocator (#58384)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58384\n\nWhen the caller send tensors within a request, it does so on fresh streams it obtained from the caching allocator. However it wasn't recording those tensors with the caching allocator. This carried the risk that, if those tensors were deleted before the async CUDA ops were done, the caching allocator could reuse the storage and thus overwrite the previous data while it was still being used.\nghstack-source-id: 129107582\n\nTest Plan: eyes\n\nReviewed By: mrshenli\n\nDifferential Revision: D28473429\n\nfbshipit-source-id: 3f2617048d984cec7a270858d282cecf1140ecf0", "pr_number": "58384", "files_changed": ["torch/csrc/distributed/rpc/utils.h"], "labels": ["Merged", "cla signed", "oncall: distributed"]}, "a3b33139da": {"title": "[Pytorch] Add non mutator bundled inputs method (#58408)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58408\n\nItd be nice to have a version of bundle inputs that didnt mutate the original class/object. So now there is!\nghstack-source-id: 129127316\n\nTest Plan: The new unittests\n\nReviewed By: dhruvbird\n\nDifferential Revision: D28460231\n\nfbshipit-source-id: f6f7a19e264bddfaa177304cbde40336060a237a", "pr_number": "58408", "files_changed": ["test/test_bundled_inputs.py", "torch/utils/bundled_inputs.py"], "labels": ["Merged", "cla signed"]}, "95fd1e9045": {"title": "reduce number of randperm template instantiations (#58362)", "body": "Summary:\nPer title, benchmarks in https://github.com/pytorch/pytorch/issues/54113 don't regress, size of torch_cuda_cu_generated_Randperm.cu.o goes 8562152 -> 2585792 for a single architecture, compilation time decreases also.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58362\n\nReviewed By: heitorschueroff\n\nDifferential Revision: D28477697\n\nPulled By: ngimel\n\nfbshipit-source-id: 32dbe44ca6b3807668d548512d7484f8488834c4", "pr_number": "58362", "files_changed": ["aten/src/ATen/native/cuda/Randperm.cu"], "labels": ["Merged", "cla signed"]}, "cce156ac94": {"title": ".github: Make on_pull_request a conditional block (#58363)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58363\n\nPrevious implemntation relied on us directly writing the yaml instead of\njust having a conditional block, this allows us better readability for\npull request triggers\n\nSigned-off-by: Eli Uriegas <seemethere101@gmail.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: walterddr\n\nDifferential Revision: D28465271\n\nPulled By: seemethere\n\nfbshipit-source-id: fd556bb6bac4954fcddb4a2b0383e996f292a794", "pr_number": "58363", "files_changed": [".github/scripts/generate_linux_ci_workflows.py", ".github/templates/linux_ci_workflow.yml.in", ".github/workflows/pytorch-linux-xenial-cuda10.2-cudnn7-py3.6-gcc7.yml"], "labels": ["Merged", "cla signed", "module: ci"]}}